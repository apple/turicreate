/**
\defgroup cppipc C++ Interprocess Communication Library

\page using_cppipc Introduction to CPPIPC
The \ref cppipc "CPPIPC Library" is an inter-process communication
library built on top of ZeroMQ sockets. It shares many similarities with other
IPC systems such as COM, XPCOM, DBus, etc. CPPIPC in particular is
designed to share several similarities with DBus (but also exposes a higher
level C++ object interface). What differentiates CPPIPC from other interprocess
communication systems, is that:

- It is built for C++ usability in mind
- It is designed to support large message communication (100MB to multiple GB
communication is not an issue). (All other reviewed IPC systems have very low
message size limits)

What CPPIPC is not:
- Not designed for secure communication (minimal authentication support)
- Not designed to be tolerant to message errors. (Though, a checksum could be
added)
- Not designed to be cross-architecture compatible. (Wire-format relies on
system native data representations. No attempt is made to correct for
endianness)
- No cross language support. (Only C++)

What CPPIPC is:
- Simple distributed object access.
- Reliable communication (fault tolerant)
- Works across multiple wire formats (tcp/ipc/inproc)

This tutorial will begin with the basics of CPPIPC, and leading up to the
implementation of a simple remote "integer" service.

The tutorial is divided into the following sections:
- \subpage using_cppipc_basics
- \subpage using_cppipc_advanced_object_creation

For technical details, see \subpage technical_details_cppipc .

\page using_cppipc_basics CPPIPC Basic Concepts
CPPIPC is a client-server architecture, where

-# Server offers a list of types that can be instantiated
-# Client connects to the server
-# Client asks for an object of a particular type to be created.
-# Server creates the object and client creates a "proxy" for the object.
-# Client uses the proxy object as if it is a local object, except that all
calls are evaluated on the server
-# When client destroys the object, the object on the server is destroyed.

The basic idea of CPPIPC is the triple split of
- Base Class
- Implementation Class
- Proxy Class

\section using_cppipc_basics_separation Base, Implementation and Proxy Separation
To communicate reliably, it is crucial that both client and server have a
common view of an object. To achieve this, we go with the common base class
model: a base class is used to define an interface, and an implementation of the
interface is provided on the server side. On the client side, a proxy class
(which also implements the interface) is provided, except that this proxy class
simply forwards all calls to the server.

For instance (this code will not actually work):
\code
// known by both client and server
class print_something_base {
 public:
  void print(std::string message);
};

// known only by client
class print_something_proxy: public print_something_base {
 public:
  void print(std::string message) {
    // forward 'message' to server
  }
}

// known only by server
class print_something: public print_something_base {
 public:
  void print(std::string message) {
    std::cout << message;
  }
}
\endcode

\section using_cppipc_base_and_proxy Base and Proxy classes

However, implementing a proxy class is hugely annoying (you have to implement
the class all over again), easy to get wrong (since there will be a decent
amount of copy and pasting), and there are several rules and stuff
(registration must be called, etc). We therefore provide a collection of
magic macros that make the implementation effort easier, though at the cost
of some of the most annoying looking compiler messages at minor syntax errors.

Here, we are going to implement a basic "counter" service. i.e. the server is
going to maintain an integer counter which we can read, and add to.

Begin by #including the appropriate headers
\code
#include <core/system/cppipc/cppipc.hpp>
#include <core/system/cppipc/magic_macros.hpp>
\endcode

We can then generate the interface and proxy objects using the \ref
\ref GENERATE_INTERFACE_AND_PROXY macro.

\code
GENERATE_INTERFACE_AND_PROXY(basic_counter_base,  basic_counter_proxy,
                              (int, add, (int))
                              (int, add_multiple, (int)(int))
                              (int, get_val, )
                            )
\endcode

For instance, here, we created an interface class called "basic_counter_base",
and a proxy class called "basic_counter_proxy" which extends the base class.
The base class defines 3 functions,
- a function called "add" which takes one integer and returns an integer.
  This function will add the integer to the counter, and return the new
  counter value.
- a function called "add_multiple" which takes two integers and returns an integer.
  This function will add the product of the input integers to the counter, and
  return the new counter value.
- a function called "get_val" which takes no arguments and returns an integer.
  This function will return the current counter value.

A few observations.
- Each "function definition" is not separated by commas, but is just
consecutive parenthesis blocks.
- Each input argument type of the function is in its own parentheses block.
- Even if a function takes no arguments, a final "comma" is necessary in the
function definition
- There must be no variable names in the arguments.

The calls are serialized using Turi Create's serialization interface. i.e. all
arguments and return types must be serializable. See \ref serialization for
details.

Known Limitations:
- No function overloads permitted
- This is a limitation of the C++ preprocessor. type names cannot have
commas in them. If you have a comma in a type, for instance \c
"std::map<std::string, size_t>" you will need to typedef it to a type name with
no commas before using it in the macro. For instance:

\code
// BAD!
GENERATE_INTERFACE_AND_PROXY(basic_counter_base,  basic_counter_proxy,
                              (std::map<std::string, size_t>, thingee, )
                            )
// GOOD!
typedef std::map<std::string, size_t> str_int_map_type;
GENERATE_INTERFACE_AND_PROXY(basic_counter_base,  basic_counter_proxy,
                              (str_int_map_type, thingee, )
                            )
\endcode

To see why you should always use the magic macros and not write your own
proxy/base object, see the documentation for GENERATE_INTERFACE_AND_PROXY


\section using_cppipc_implementation Implementation Classes

Now, the macro defines the base and proxy objects, but you have to define your
own implementation object. There are no rules for the implementation object
besides that you must inherit from the base, and implement all the functions
defined in the base.


\code
class basic_counter: public basic_counter_base {
 private:
   int value;
 public:
   basic_counter():value(0) {}

   int add(int a) {
     value += a;
     return value;
   }

   int add_multiple(int val, int count) {
     value += val * count;
     return value;
   }

   int get_val() {
     return value;
   }
};
\endcode

And you are done!

\section using_cppipc_client_and_server Implementing a Server and a Client Program

Put it all in a header (for simplicity). In a real system the implementation
will be placed separately from the proxy so the client compilation does not
inherit all the server's dependencies.

Next create 2 cpp files, one server, and one client.
\code
// Server Example
#include <iostream>
#include <core/system/cppipc/cppipc.hpp>
#include "basic_counter.hpp"

basic_counter_base* basic_counter_factory() {
  return new basic_counter;
}
int main(int argc, char** argv) {
  cppipc::comm_server server({}, "ipc:///tmp/cppipc_server_test");
  server.register_type<basic_counter_base>(basic_counter_factory);
  server.start();
  getchar();
}
\endcode

The "ipc:///tmp/cppipc_server_test" is a ZeroMQ endpoint address and is the
address the server will listen and wait on for connections. \c ipc:// is an
"interprocess socket" endpoint and should point to a filename (preferably
non-existent, and unused). It can also be \c tcp://[ip address]:[portnumber]
to have the server wait on a TCP/IP connection. Note that it must be an IP
address in this case. ZeroMQ does not do hostname resolution.

The \ref cppipc::comm_server::register_type "register_type" call is used to
register the "basic_counter_base" type with the server, and also inform the
server how to construct an instance of the object when asked by a client.
(As an alternative to defining a factory function, a lambda can be used as well).


\code
// Client Example
#include <iostream>
#include <core/system/cppipc/cppipc.hpp>
#include "basic_counter.hpp"

int main(int argc, char** argv) {
  // Connects to the server
  cppipc::comm_client client({}, "ipc:///tmp/cppipc_server_test");

  // Creates a proxy object. This calls the factory on the server to create
  // a basic_counter on the server. The proxy object on the client only
  // maintains an object ID.
  basic_counter_proxy proxy(client);

  //adds 50 to the counter
  proxy.add(50);

  //adds 12 * 5 to the counter
  proxy.add_multiple(12, 5);

  // prints the counter value
  std::cout << "Counter Value: " << proxy.get_val() << "\n";

  // when proxy is destroyed, it destroys the object on the server
}

\endcode


\page using_cppipc_advanced_object_creation Advanced Object Creation

(If you have not seen the \ref using_cppipc_basics section, you should. we will
 continue extending the basic_counter object defined in the previous section.)

Now, while the CPPIPC system is kind of nice for basic stuff, what should be
done in situations when I need to define a function which takes another
remote object as input (For instance, say I want to add two basic counters
together)? Or how do I create functions which return new remote objects (
for instance, if I want to "clone" a counter) ?

To handle this case, we perform some magic which are explained in
in the \ref technical_details_cppipc "Technical Details page".
But in short, you simply pass and return pointers to objects.

For instance, to implement a "clone" function, we simply extend the interface
with a clone function that returns a new base object (it must be a base,
since to the server, it is an implementation object, and to the client, it is a
proxy object)

\code
GENERATE_INTERFACE_AND_PROXY(basic_counter_base,  basic_counter_proxy,
                              (int, add, (int))
                              (int, add_multiple, (int)(int))
                              (int, get_val, )
                              (basic_counter_base*, clone, )
                            )
\endcode

On the server-side, we implement the clone() function. This just creates a new
basic_counter object, and returns it. The returned object will automatically
be registered and managed by the comm_server.

\code
 basic_counter_base* basic_counter::clone() {
   basic_counter* new_counter = new basic_counter;
   new_counter->value = value;
   return new_counter;
 }
\endcode

Now, to use this function on the client side,

\code
...
  // prints the counter value
  std::cout << "Counter Value: " << proxy.get_val() << "\n";

  // upcast it to proxy. Not strictly necessary
  // we can also just leave it as a pointer to base
  //
  // i.e.
  // basic_counter_base* new_counter = proxy.clone();
  basic_counter_proxy* new_counter =
      dynamic_cast<basic_counter_proxy*>(proxy.clone());

  // prints the new counter value
  std::cout << "New Counter Value: " << new_counter->get_val() << "\n";

  // its a pointer, so we must explicitly delete it
  // On deletion, it will delete the matching counter object on the server side
  delete new_counter;
\endcode

To take remote objects as arguments is similar. Here, we add a new "add_counter"
function which adds the value of another counter to the current counter.
Once again, it must take a pointer to base as the argument.

\code
GENERATE_INTERFACE_AND_PROXY(basic_counter_base,  basic_counter_proxy,
                              (int, add, (int))
                              (int, add_multiple, (int)(int))
                              (int, get_val, )
                              (basic_counter_base*, clone, )
                              (void, add_counter, (basic_counter_base*))
                            )
\endcode

While on the client side, the function is called with a pointer to a proxy object,
it is resurrected as a pointer to an implementation object on the server side.

\code
 void basic_counter::add_counter(basic_counter_base* other) {
   // upcast to the basic_counter.
   basic_counter* other_derived = dynamic_cast<basic_counter*>(other);
   value += other_derived->value;

   // once again, in this case this is not strictly necessary,
   // we can get by with keeping it in the base class and just write
   //
   // value += other->get_val();
 }
\endcode



Now, to use this function on the client side,

\code
  basic_counter_proxy counter1(client);
  basic_counter_proxy counter2(client);
  ... do stuff ..

  counter1.add_counter(&counter2);
\endcode

\page technical_details_cppipc Technical Details: CPPIPC

\section technical_details_cppipc_communication Communication

The CPPIPC client and server communicates with each other via libfault's ZeroMQ
sanity wrappers. Specifically, the libfault::async_request_socket (client) and
the libfault::async_reply_socket (server) which implements a reliable
asynchronous request-reply pattern. (i.e. Each request sent must be paired with
a reply.  Multiple requests can be sent simultaneously).

Libfault's wrappers were originally designed to watch for state changes on
Zookeeper and react accordingly. For instance, a collection of servers could
have running processes which implement a service name called "echo" (which echos
messages).  The service is implemented using the async_reply_socket which
registers a key called "echo" on Zookeeper, which keep tracks of the
TCP/IP address of each server process.  A client (with an async_reply_socket)
could then connect to the "echo" service and request messages can be sent to
the "service" reliably. i.e. When servers go down, they will be unregistered
on Zookeeper automatically and the clients will pick up those changes and
adapt accordingly.  Similarly when new servers come up. The restriction to
simple request-reply patterns (and pub/sub patterns) allow reliability to be
provided easily (as opposed to arbitrarily interesting protocols).

However, for the purposes of the current Unity engine, Zookeeper is quite
unnecessary since service migration/faults are not a concern. Instead we would
only like the reliable communication patterns implemented. As such,
libfault::reply_socket, libfault::request_socket, libfault::async_reply_socket,
and libfault::async_request_socket has been modified to operate without the use
of Zookeeper.

The Libfault interface is moderately well documented, but the internals are
generally not hugely well documented since a large part of it is in working
around ZeroMQ's oddities. We will try to go into a few relevant details here,
but you should look at ZeroMQ for the specifics.

\subsection technical_details_cppipc_zmq_msg zmq_msg_vector
To understand the deeper parts of the Comm layer, it is uesful to
get a quick brief scan at ZeroMQ, and in particular ZeroMQ's message object
(zmq_msg*) in http://api.zeromq.org/4-0:_start (for which I have a wrapper in
libfault::zmq_msg_vector). The

ZeroMQ message object (zmq_msg_t) is basically a reference counted character
buffer. It has several optimizations such as for small messages, it will handle
it in-place without a malloc, and for larger messages, it will use a reference
counted buffer on the heap. If you use the zmq_msg_* functions from ZeroMQ,
there will be no issues with leaks and such. Only thing that you have to be
careful of, is that zmq_msg_t should not be copied:
i.e.

\code
// given zmq_msg_t object msg1 and msg2
msg1 = msg2 // This is not safe!
\endcode

You should always manage zmq_msg_t object as pointers.

The \ref libfault::zmq_msg_vector object manages safely, an array of zmq_msg_t
buffers with appropriate iteration and manipulation capabilities (most
unusually pop_front, and push_back). The zmq_msg_vector is the key message
object that is sent and received.

It is important to note that ZeroMQ provides a \b message protocol abstraction
and not a \b stream protocol abstraction.  i.e. if you send a zmq_msg_vector
with 4 parts, the receiver will also receive a zmq_msg_vector with the same 4
parts. This makes the pop_front/push_front functionality extremely useful this
this allows you to easily stack additional headers on the sender and strip them
one by one on the receiver. (This is also how the DEALER-ROUTER sockets work to
tag messages with their appropriate destinations. See ZeroMQ Message Envelopes).

\subsection technical_details_cppipc_request_reply REQUEST-REPLY Pattern

The REQUEST-REPLY pattern is one of the simplest ZeroMQ pattern. Basically, the Reply
socket is a socket pattern where you can only perform alternating receive and
send operations. And the REQUEST socket is exactly the opposite where you can
only perform alternating send and receive operations. Thus the simplest pattern
is where a REQUEST socket connects to a Reply socket:

- Request socket sends a request message
- Request socket then waits on reply
- Reply socket receives the request message
- Reply socket sends the reply
- Request socket receives the reply and this can then repeat.

The cool part, is that multiple REQUEST sockets can connect to one Reply socket.
The strict sequential recv/send pattern however means that the reply socket
can only process one message at a time.

The REQUEST and REPLY socket are wrapped in \ref libfault::request_socket
and \ref libfault::reply_socket respectively and appropriately handles
situations such as message failures, malformed messages, timeouts, etc.

\subsection technical_details_cppipc_router_dealer (DEALER-ROUTER Pattern) Async Request-Async Reply

The DEALER is a generalized REQUEST socket, and the ROUTER is a generalized
REPLY socket. The key difference is that the DEALER and ROUTER sockets can
send and receive arbitrarily without restrictions. DEALER can also connect and
load balance requests to multiple servers, though we are not using that
capability.

The DEALER (async request) and ROUTER (async reply) sockets are wrapped in \ref
libfault::async_request_socket and \ref libfault::async_reply_socket respectively and
appropriately handles situations such as message failures, malformed messages,
timeouts, etc.

The \ref libfault::async_request_socket "async_request_socket" has the
following internal architecture:
- When a "send" is called, the message is dropped into an inproc PULL/PUSH pair
which collect messages sent from arbitrary threads to be processed by a
single thread. The send then immediately returns a future.
- The single thread (\ref libfault::async_request_socket::pull_socket_callback)
waits on the PUSH side of the inproc socket, picks up the message, and forwards
it to the appropriate destination. (Reason for the pull/push to single thread
thing is that in the "Zookeeper Service" case, this thread may actually need
to create new DEALER sockets and it simplifies the logic substantially if
the logic is centralized in one place.)
- Received messages from the server wake up a second callback (\ref
lifault::async_request_socket::remote_message_callback) which finds the
matching promise and resolves the original future

The \ref libfault::async_reply_socket "async_reply_socket" has a very similar
internal architecture, but in the opposite direction.
- Messages received from the ROUTER wake up a callback
(\ref libfault::async_reply_socket::wrapped_callback) which drop the message
into a queue protected by a mutex/condition variable pair.)
- A collection of handler threads waiting on the queue will wake up to wake
handle the message and produce replies.
- The replies from the handler threads are dropped into an inproc PULL/PUSH pair
to collect messages into a single thread.
- A callback waiting on the PULL socket
(\ref libfault::async_reply_socket::pull_socket_callback) forwards the messages
back out through the ROUTER.

\section technical_details_cppipc_object_registry Object Registry

The CPPIPC Server (\ref cppipc::comm_server) internally manages a list of
object types and the knowledge of how to create instances of such objects. So
when the client (\ref cppipc::comm_client) asks to create a new object
of a particular type, it can be instantiated on the server.

Types are registered by \b name of the base class and on the server, \ref
cppipc::comm_server::register_type is used to associate it with a
construction function. The actual list is really managed by the object_factory
(in cppipc/common/) which is in itself, a CPPIPC shared object (you will see
there is a proxy, base, and impl).

\note
This introduces an interesting chicken
and egg problem: how do you register the type of the object factory in the
first place? This is basically done by some manual construction and registration
of the object_factory in both the server and the client.
This design allows for a very simple means of proxy construction on the client
side: you will observe the implementation of \ref
cppipc::comm_client::make_object is only one line of code: directing the call
to the object_factory_proxy which simply reuses the regular IPC communication
channel to actually construct the object. This design means that the Client and
Server do not require any special additional handling for internal
coordination, but can simply use the same protocol as everything else.


\section technical_details_cppipc_function_registry Function Registry
In addition to a registry of types, we also need a registry of the functions
that can be called. This function list is known to both the client and server
and at the moment, no interesting "mangling" is performed. It is simply the full
name if the function in the form <typename>::<functionname>.

\note In an earlier attempt of the cppipc, the function name also includes the
complete C++ name mangled type signature thus allowing different overloads the
same function to have completely different registrations. This will be
substantially more robust since any changes in the signature will result in a
completely different function call making backward compatibility much easier to
achieve. However, as it turns out due to various reasons, Mac-CLang has a
different name mangling scheme than Linux thus when this is used, Mac can
no longer communicate with Linux servers, and vice versa.

The server needs the function registry to know how to convert a function
name to a function pointer. The client needs the function registry to perform
the inverse: conversion from a function pointer to a function name
(see \ref technical_details_cppipc_real_proxy_object to see why).

To make sure that both client and server are registered equivalently, the
actual function registration function is implemented in the exported base
class, as a __register__ function. See the \ref REGISTRATION_BEGIN and
\ref REGISTER macros. They basically implement a templated function of
the form:

\code
template <typename Registry>
static inline void __register__(Registry& reg) {
  reg.register_function(&basic_counter_base::add, "basic_counter_base::add");
  reg.register_function(&basic_counter_base::add_multiple, "basic_counter_base::add_multiple");
}
\endcode

Then when the type is registered on the comm_server, it simply calls:

\code
  template <typename T>
  void register_type(std::function<T*()> constructor_call) {
   T::__register__(*this);
   ...
  }
\endcode
to get all the member functions.

On the client side, the function registration happens on construction
of the object_proxy in pretty much the same way.

\section technical_details_cppipc_real_proxy_object The True Proxy Object
The proxy object generated by GENERATE_INTERFACE_AND_PROXY is not the true proxy
object, but is really only a light-weight wrapper around \ref cppipc::object_proxy
which actually implements the call logic. \ref cppipc::object_proxy is a
general purpose proxy which can wrap any interface class. For instance: in the
basic_counter example in \ref using_cppipc_client_and_server, instead of using
the basic_counter_proxy in the client example, I could equivalently write:

\code
// Client Example
#include <iostream>
#include <core/system/cppipc/cppipc.hpp>
#include "basic_counter.hpp"

int main(int argc, char** argv) {
  // Connects to the server
  cppipc::comm_client client({}, "ipc:///tmp/cppipc_server_test");

  // Creates a proxy object. This calls the factory on the server to create
  // a basic_counter on the server. The proxy object on the client only
  // maintains an object ID.
  object_proxy<basic_counter_base> proxy(client);

  //adds 50 to the counter
  proxy.call(&basic_counter_base::add, 50);

  //adds 12 * 5 to the counter
  proxy.call(&basic_counter_base::add_multiple, 12, 5);

  // prints the counter value
  std::cout << "Counter Value: " << proxy.call(&basic_counter_base::get_val) << "\n";

  // when proxy is destroyed, it destroys the object on the server
}
\endcode

Now, one might ask why does proxy.call take a function pointer as the first
argument and not simply the function name? Why not:

\code
proxy.call("basic_counter_base::add", 50);
\endcode

in which case, the client side does not need a function registry
(\ref technical_details_cppipc_function_registry)
The reason is that by using the function pointer, I can fully type check the
argument types at compile time, and automatically cast to the types the server
is expecting. This simplifies the process of exporting objects substantially
since we do not need a run-time "schema". Essentially the "schema" is defined
entirely by the type of the function, and we can rely on C++ typing to enforce
the type signature.

\section technical_details_cppipc_serialiation Basic Call Serialization/Deserialization

The basic call serialization/deserialization code is in core/system/cppipc/client/issue.hpp
and core/system/cppipc/server/dispatch_impl.hpp respectively. The call/issue (client) side
is surprisingly simple.

The actual \ref cppipc::issue "issue" call is a variadic template function
which tries to serialize the arguments of the call into the archive. The trick
is that to ensure that types match up on the client side, it is important to
cast the argument type right now (thus we need to know the Member Function
    pointer as well).

\code
template <typename MemFn, typename... Args>
void issue(turi::oarchive& msg,
           MemFn fn,
           const Args&... args);
\endcode

The issue function basically takes the argument types of fn, converting it to a
tuple over the argument types (\ref cppipc::function_args_to_tuple), which then
calls into \ref cppipc::detail::issue_disect which basically extracts of the
left most argument, and the left most entry of the tuple, cast the argument to
the appropriate type, serializing it, and calling \ref
cppipc::detail::issue_disect tail recursively until we run out of arguments.

The dispatch side is slightly more complicated since it most both deserialize,
call the target function, and serialize the result. But it basically operates
on the same principle. \ref cppipc::execute_disect takes a tuple of the
argument types and a list of arguments deserialized so far. It then extracts the
left most entry from the tuple and deserializes it, and tail recurses until
it runs out of arguments.

\section technical_details_cppipc_object_serialization Serializing Object Pointers
Now, how about the "magic-trick" involving the ability to serialize pointers to
shared objects and have then resurrect appropriately as either pointer to proxy
objects (on the client) or pointer to implementation objects (on the server)?
See \ref using_cppipc_advanced_object_creation.

This is accomplished very simply by hacking the serialization library.
It is useful to understand the \ref technical_details_serialization
"serialization library technical details" first.

The source for the serialization "hack" is in
ipc_deserializer.hpp/ipc_deserializer.cpp. We will walk through this slowly.

Firstly, we want to catch every attempt to serialize/deserialize exported
objects.  To do this we must be able to identify these objects, and the easiest
way to do so is to have them inherit from a common base class, and that will be
cppipc::ipc_object_base which is simply an empty base class. The
GENERATE_INTERFACE_AND_PROXY will automatically have the interface class inherit
from cppipc::ipc_object_base thus allowing all descendents
(proxy and implementation) to all be descendents of cppipc::ipc_object_base.

Next, we perform a partial specialization of the serialization classes
serialize_impl and deserialize_impl (see \ref technical_details_serialization
for details). Note that we are going to intercept attempts at serializing \b
pointers to the proxy/implementation classes. The enable_if line basically
means that the code exists if and only if T* is convertible to
cppipc::ipc_object_base*, i.e. T inherits from ipc_object_base. As a result
attempts to serialize other regular pointers (which is an unsafe operation anyway),
will not hit the following code.

\code
template <typename OutArcType, typename T>
struct serialize_impl<OutArcType, T*, true> {
  inline static
  typename std::enable_if<std::is_convertible<T*, cppipc::ipc_object_base*>::value>::type
  exec(OutArcType& oarc, const T* value) {
    ...
  }
};

template <typename InArcType, typename T>
struct deserialize_impl<InArcType, T*, true> {
  inline static
  typename std::enable_if<std::is_convertible<T*, cppipc::ipc_object_base*>::value>::type
  exec(InArcType& iarc, T*& value) {
    ...
  }
};
\endcode

Now, the key annoyance is that this code is the same on both the server and the
client, so I have to know whether I am on the server side, or the client side
and act appropriately.  To do that, I rely on a collection of two functions,
which are called by the \ref cppipc::comm_client and \ref cppipc::comm_server
immediately before attempting to serialize/deserialize a call.
\code
extern void set_deserializer_to_server(comm_server* server);
extern void set_deserializer_to_client(comm_client* client);
\endcode
In the implementation of these (in ipc_deserializer.cpp), they basically set a
thread local variable. This allows the functions above to be fully thread safe,
and allows both server/client to reside on the same machine in different threads
if necessary.

The serialize_impl and deserialize_impl structs can then use
\code
extern void get_deserialization_type(comm_server** server, comm_client** client);
\endcode
to figure out whether it is currently working on the server-side or the
client-side.


\subsection technical_details_cppipc_object_serialization_serializing Serializing Object Pointers-Serializing

Serialization is simple,
\code
  if (server) {
    // server to client message is a pair of "is new object" and "object ID"
    oarc << get_server_object_id(server, value);
  } else {
    oarc << (*value);
  }
\endcode

The proxy object has a built in save/load function that simply serializes the
object ID. For an impl object however, I will need to ask the comm_server
to find the object ID.

\subsection technical_details_cppipc_object_serialization_deserializing Serializing Object Pointers-Deserializing
Deserializing is slightly more involving. On the server-side, I have to search
on the comm_server object for the object matching the object ID,
\code
if (server) {
  size_t object_id;
  iarc >> object_id;
  void* obj = cppipc::detail::get_server_object_ptr(server, object_id);
  if (obj == NULL) {
    throw std::to_string(object_id) + " Object not found";
  }
  value = reinterpret_cast<T*>(obj);
}
\endcode

On the client side, the proxy object is constructed with the object_id received.
(The proxy_object's class name is always typedef'd to 'proxy_object_type')
\code
size_t object_id;
iarc >> object_id;
value = new typename
  std::remove_pointer<T>::type::proxy_object_type(*client, false,
                                                  object_id);
\endcode


\section technical_details_cppipc_limitations Limitations
The current serializer/deserializer was designed for performance in mind and is
meant to be used between "trusted" and equivalent systems.  We do not have
"type-aware" or "robust" serialization/deserialization. We rely heavily on the
client and server agreeing on the function argument types, and having
serialization behave the same way. If there are any malformed messages, or if
for whatever reason the client/server function types disagree, the serializer
will crash and burn.

*/
