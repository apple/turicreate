// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "NeuralNetwork.pb.h"

#include <algorithm>

#include <protobuf/stubs/common.h>
#include <protobuf/stubs/port.h>
#include <protobuf/stubs/once.h>
#include <protobuf/io/coded_stream.h>
#include <protobuf/wire_format_lite_inl.h>
#include <protobuf/io/zero_copy_stream_impl_lite.h>
// @@protoc_insertion_point(includes)

namespace CoreML {
namespace Specification {

void protobuf_ShutdownFile_NeuralNetwork_2eproto() {
  NeuralNetwork_default_instance_.Shutdown();
  NeuralNetworkImageScaler_default_instance_.Shutdown();
  NeuralNetworkMeanImage_default_instance_.Shutdown();
  NeuralNetworkPreprocessing_default_instance_.Shutdown();
  ActivationReLU_default_instance_.Shutdown();
  ActivationLeakyReLU_default_instance_.Shutdown();
  ActivationTanh_default_instance_.Shutdown();
  ActivationScaledTanh_default_instance_.Shutdown();
  ActivationSigmoid_default_instance_.Shutdown();
  ActivationLinear_default_instance_.Shutdown();
  ActivationSigmoidHard_default_instance_.Shutdown();
  ActivationPReLU_default_instance_.Shutdown();
  ActivationELU_default_instance_.Shutdown();
  ActivationThresholdedReLU_default_instance_.Shutdown();
  ActivationSoftsign_default_instance_.Shutdown();
  ActivationSoftplus_default_instance_.Shutdown();
  ActivationParametricSoftplus_default_instance_.Shutdown();
  ActivationParams_default_instance_.Shutdown();
  NeuralNetworkLayer_default_instance_.Shutdown();
  BorderAmounts_default_instance_.Shutdown();
  BorderAmounts_EdgeSizes_default_instance_.Shutdown();
  ValidPadding_default_instance_.Shutdown();
  SamePadding_default_instance_.Shutdown();
  WeightParams_default_instance_.Shutdown();
  ConvolutionLayerParams_default_instance_.Shutdown();
  InnerProductLayerParams_default_instance_.Shutdown();
  EmbeddingLayerParams_default_instance_.Shutdown();
  BatchnormLayerParams_default_instance_.Shutdown();
  PoolingLayerParams_default_instance_.Shutdown();
  PoolingLayerParams_ValidCompletePadding_default_instance_.Shutdown();
  PaddingLayerParams_default_instance_.Shutdown();
  PaddingLayerParams_PaddingConstant_default_instance_.Shutdown();
  PaddingLayerParams_PaddingReflection_default_instance_.Shutdown();
  PaddingLayerParams_PaddingReplication_default_instance_.Shutdown();
  ConcatLayerParams_default_instance_.Shutdown();
  LRNLayerParams_default_instance_.Shutdown();
  SoftmaxLayerParams_default_instance_.Shutdown();
  SplitLayerParams_default_instance_.Shutdown();
  AddLayerParams_default_instance_.Shutdown();
  MultiplyLayerParams_default_instance_.Shutdown();
  UnaryFunctionLayerParams_default_instance_.Shutdown();
  UpsampleLayerParams_default_instance_.Shutdown();
  BiasLayerParams_default_instance_.Shutdown();
  ScaleLayerParams_default_instance_.Shutdown();
  LoadConstantLayerParams_default_instance_.Shutdown();
  L2NormalizeLayerParams_default_instance_.Shutdown();
  FlattenLayerParams_default_instance_.Shutdown();
  ReshapeLayerParams_default_instance_.Shutdown();
  PermuteLayerParams_default_instance_.Shutdown();
  ReduceLayerParams_default_instance_.Shutdown();
  CropLayerParams_default_instance_.Shutdown();
  AverageLayerParams_default_instance_.Shutdown();
  MaxLayerParams_default_instance_.Shutdown();
  MinLayerParams_default_instance_.Shutdown();
  DotProductLayerParams_default_instance_.Shutdown();
  MeanVarianceNormalizeLayerParams_default_instance_.Shutdown();
  SequenceRepeatLayerParams_default_instance_.Shutdown();
  SimpleRecurrentLayerParams_default_instance_.Shutdown();
  GRULayerParams_default_instance_.Shutdown();
  LSTMParams_default_instance_.Shutdown();
  LSTMWeightParams_default_instance_.Shutdown();
  UniDirectionalLSTMLayerParams_default_instance_.Shutdown();
  BiDirectionalLSTMLayerParams_default_instance_.Shutdown();
  NeuralNetworkClassifier_default_instance_.Shutdown();
  NeuralNetworkRegressor_default_instance_.Shutdown();
}

void protobuf_InitDefaults_NeuralNetwork_2eproto_impl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::CoreML::Specification::protobuf_InitDefaults_DataStructures_2eproto();
  NeuralNetwork_default_instance_.DefaultConstruct();
  NeuralNetworkImageScaler_default_instance_.DefaultConstruct();
  NeuralNetworkMeanImage_default_instance_.DefaultConstruct();
  ::google::protobuf::internal::GetEmptyString();
  NeuralNetworkPreprocessing_default_instance_.DefaultConstruct();
  ActivationReLU_default_instance_.DefaultConstruct();
  ActivationLeakyReLU_default_instance_.DefaultConstruct();
  ActivationTanh_default_instance_.DefaultConstruct();
  ActivationScaledTanh_default_instance_.DefaultConstruct();
  ActivationSigmoid_default_instance_.DefaultConstruct();
  ActivationLinear_default_instance_.DefaultConstruct();
  ActivationSigmoidHard_default_instance_.DefaultConstruct();
  ActivationPReLU_default_instance_.DefaultConstruct();
  ActivationELU_default_instance_.DefaultConstruct();
  ActivationThresholdedReLU_default_instance_.DefaultConstruct();
  ActivationSoftsign_default_instance_.DefaultConstruct();
  ActivationSoftplus_default_instance_.DefaultConstruct();
  ActivationParametricSoftplus_default_instance_.DefaultConstruct();
  ActivationParams_default_instance_.DefaultConstruct();
  ::google::protobuf::internal::GetEmptyString();
  NeuralNetworkLayer_default_instance_.DefaultConstruct();
  BorderAmounts_default_instance_.DefaultConstruct();
  BorderAmounts_EdgeSizes_default_instance_.DefaultConstruct();
  ValidPadding_default_instance_.DefaultConstruct();
  SamePadding_default_instance_.DefaultConstruct();
  WeightParams_default_instance_.DefaultConstruct();
  ConvolutionLayerParams_default_instance_.DefaultConstruct();
  InnerProductLayerParams_default_instance_.DefaultConstruct();
  EmbeddingLayerParams_default_instance_.DefaultConstruct();
  BatchnormLayerParams_default_instance_.DefaultConstruct();
  PoolingLayerParams_default_instance_.DefaultConstruct();
  PoolingLayerParams_ValidCompletePadding_default_instance_.DefaultConstruct();
  PaddingLayerParams_default_instance_.DefaultConstruct();
  PaddingLayerParams_PaddingConstant_default_instance_.DefaultConstruct();
  PaddingLayerParams_PaddingReflection_default_instance_.DefaultConstruct();
  PaddingLayerParams_PaddingReplication_default_instance_.DefaultConstruct();
  ConcatLayerParams_default_instance_.DefaultConstruct();
  LRNLayerParams_default_instance_.DefaultConstruct();
  SoftmaxLayerParams_default_instance_.DefaultConstruct();
  SplitLayerParams_default_instance_.DefaultConstruct();
  AddLayerParams_default_instance_.DefaultConstruct();
  MultiplyLayerParams_default_instance_.DefaultConstruct();
  UnaryFunctionLayerParams_default_instance_.DefaultConstruct();
  UpsampleLayerParams_default_instance_.DefaultConstruct();
  BiasLayerParams_default_instance_.DefaultConstruct();
  ScaleLayerParams_default_instance_.DefaultConstruct();
  LoadConstantLayerParams_default_instance_.DefaultConstruct();
  L2NormalizeLayerParams_default_instance_.DefaultConstruct();
  FlattenLayerParams_default_instance_.DefaultConstruct();
  ReshapeLayerParams_default_instance_.DefaultConstruct();
  PermuteLayerParams_default_instance_.DefaultConstruct();
  ReduceLayerParams_default_instance_.DefaultConstruct();
  CropLayerParams_default_instance_.DefaultConstruct();
  AverageLayerParams_default_instance_.DefaultConstruct();
  MaxLayerParams_default_instance_.DefaultConstruct();
  MinLayerParams_default_instance_.DefaultConstruct();
  DotProductLayerParams_default_instance_.DefaultConstruct();
  MeanVarianceNormalizeLayerParams_default_instance_.DefaultConstruct();
  SequenceRepeatLayerParams_default_instance_.DefaultConstruct();
  SimpleRecurrentLayerParams_default_instance_.DefaultConstruct();
  GRULayerParams_default_instance_.DefaultConstruct();
  LSTMParams_default_instance_.DefaultConstruct();
  LSTMWeightParams_default_instance_.DefaultConstruct();
  UniDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  BiDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  ::google::protobuf::internal::GetEmptyString();
  NeuralNetworkClassifier_default_instance_.DefaultConstruct();
  NeuralNetworkRegressor_default_instance_.DefaultConstruct();
  NeuralNetwork_default_instance_.get_mutable()->InitAsDefaultInstance();
  NeuralNetworkImageScaler_default_instance_.get_mutable()->InitAsDefaultInstance();
  NeuralNetworkMeanImage_default_instance_.get_mutable()->InitAsDefaultInstance();
  NeuralNetworkPreprocessing_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationReLU_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationLeakyReLU_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationTanh_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationScaledTanh_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationSigmoid_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationLinear_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationSigmoidHard_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationPReLU_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationELU_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationThresholdedReLU_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationSoftsign_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationSoftplus_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationParametricSoftplus_default_instance_.get_mutable()->InitAsDefaultInstance();
  ActivationParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  NeuralNetworkLayer_default_instance_.get_mutable()->InitAsDefaultInstance();
  BorderAmounts_default_instance_.get_mutable()->InitAsDefaultInstance();
  BorderAmounts_EdgeSizes_default_instance_.get_mutable()->InitAsDefaultInstance();
  ValidPadding_default_instance_.get_mutable()->InitAsDefaultInstance();
  SamePadding_default_instance_.get_mutable()->InitAsDefaultInstance();
  WeightParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  ConvolutionLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  InnerProductLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  EmbeddingLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  BatchnormLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  PoolingLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  PoolingLayerParams_ValidCompletePadding_default_instance_.get_mutable()->InitAsDefaultInstance();
  PaddingLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  PaddingLayerParams_PaddingConstant_default_instance_.get_mutable()->InitAsDefaultInstance();
  PaddingLayerParams_PaddingReflection_default_instance_.get_mutable()->InitAsDefaultInstance();
  PaddingLayerParams_PaddingReplication_default_instance_.get_mutable()->InitAsDefaultInstance();
  ConcatLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  LRNLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  SoftmaxLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  SplitLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  AddLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  MultiplyLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  UnaryFunctionLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  UpsampleLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  BiasLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  ScaleLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  LoadConstantLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  L2NormalizeLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  FlattenLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  ReshapeLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  PermuteLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  ReduceLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  CropLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  AverageLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  MaxLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  MinLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  DotProductLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  MeanVarianceNormalizeLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  SequenceRepeatLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  SimpleRecurrentLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  GRULayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  LSTMParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  LSTMWeightParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  BiDirectionalLSTMLayerParams_default_instance_.get_mutable()->InitAsDefaultInstance();
  NeuralNetworkClassifier_default_instance_.get_mutable()->InitAsDefaultInstance();
  NeuralNetworkRegressor_default_instance_.get_mutable()->InitAsDefaultInstance();
}

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_InitDefaults_NeuralNetwork_2eproto_once_);
void protobuf_InitDefaults_NeuralNetwork_2eproto() {
  ::google::protobuf::GoogleOnceInit(&protobuf_InitDefaults_NeuralNetwork_2eproto_once_,
                 &protobuf_InitDefaults_NeuralNetwork_2eproto_impl);
}
void protobuf_AddDesc_NeuralNetwork_2eproto_impl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  protobuf_InitDefaults_NeuralNetwork_2eproto();
  ::CoreML::Specification::protobuf_AddDesc_DataStructures_2eproto();
  ::google::protobuf::internal::OnShutdown(&protobuf_ShutdownFile_NeuralNetwork_2eproto);
}

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_AddDesc_NeuralNetwork_2eproto_once_);
void protobuf_AddDesc_NeuralNetwork_2eproto() {
  ::google::protobuf::GoogleOnceInit(&protobuf_AddDesc_NeuralNetwork_2eproto_once_,
                 &protobuf_AddDesc_NeuralNetwork_2eproto_impl);
}
#ifdef GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer_NeuralNetwork_2eproto {
  StaticDescriptorInitializer_NeuralNetwork_2eproto() {
    protobuf_AddDesc_NeuralNetwork_2eproto();
  }
} static_descriptor_initializer_NeuralNetwork_2eproto_;
#endif  // GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER

namespace {

static void MergeFromFail(int line) GOOGLE_ATTRIBUTE_COLD GOOGLE_ATTRIBUTE_NORETURN;
static void MergeFromFail(int line) {
  ::google::protobuf::internal::MergeFromFail(__FILE__, line);
}

}  // namespace


// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetwork::kLayersFieldNumber;
const int NeuralNetwork::kPreprocessingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetwork::NeuralNetwork()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetwork)
}

void NeuralNetwork::InitAsDefaultInstance() {
}

NeuralNetwork::NeuralNetwork(const NeuralNetwork& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetwork)
}

void NeuralNetwork::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetwork::~NeuralNetwork() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetwork)
  SharedDtor();
}

void NeuralNetwork::SharedDtor() {
}

void NeuralNetwork::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetwork& NeuralNetwork::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetwork> NeuralNetwork_default_instance_;

NeuralNetwork* NeuralNetwork::New(::google::protobuf::Arena* arena) const {
  NeuralNetwork* n = new NeuralNetwork;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetwork::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetwork)
  layers_.Clear();
  preprocessing_.Clear();
}

bool NeuralNetwork::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetwork)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_layers:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_layers;
        if (input->ExpectTag(18)) goto parse_loop_preprocessing;
        input->UnsafeDecrementRecursionDepth();
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (tag == 18) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_preprocessing:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_loop_preprocessing;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetwork)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetwork)
  return false;
#undef DO_
}

void NeuralNetwork::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetwork)
  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetwork)
}

size_t NeuralNetwork::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetwork)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetwork::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetwork*>(&from));
}

void NeuralNetwork::MergeFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetwork)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetwork::UnsafeMergeFrom(const NeuralNetwork& from) {
  GOOGLE_DCHECK(&from != this);
  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
}

void NeuralNetwork::CopyFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetwork)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetwork::IsInitialized() const {

  return true;
}

void NeuralNetwork::Swap(NeuralNetwork* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetwork::InternalSwap(NeuralNetwork* other) {
  layers_.UnsafeArenaSwap(&other->layers_);
  preprocessing_.UnsafeArenaSwap(&other->preprocessing_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetwork::GetTypeName() const {
  return "CoreML.Specification.NeuralNetwork";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetwork::layers_size() const {
  return layers_.size();
}
void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetwork::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

inline const NeuralNetwork* NeuralNetwork::internal_default_instance() {
  return &NeuralNetwork_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkImageScaler::kChannelScaleFieldNumber;
const int NeuralNetworkImageScaler::kBlueBiasFieldNumber;
const int NeuralNetworkImageScaler::kGreenBiasFieldNumber;
const int NeuralNetworkImageScaler::kRedBiasFieldNumber;
const int NeuralNetworkImageScaler::kGrayBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkImageScaler::NeuralNetworkImageScaler()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkImageScaler)
}

void NeuralNetworkImageScaler::InitAsDefaultInstance() {
}

NeuralNetworkImageScaler::NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkImageScaler)
}

void NeuralNetworkImageScaler::SharedCtor() {
  ::memset(&channelscale_, 0, reinterpret_cast<char*>(&graybias_) -
    reinterpret_cast<char*>(&channelscale_) + sizeof(graybias_));
  _cached_size_ = 0;
}

NeuralNetworkImageScaler::~NeuralNetworkImageScaler() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkImageScaler)
  SharedDtor();
}

void NeuralNetworkImageScaler::SharedDtor() {
}

void NeuralNetworkImageScaler::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkImageScaler& NeuralNetworkImageScaler::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkImageScaler> NeuralNetworkImageScaler_default_instance_;

NeuralNetworkImageScaler* NeuralNetworkImageScaler::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkImageScaler* n = new NeuralNetworkImageScaler;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkImageScaler::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkImageScaler)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(NeuralNetworkImageScaler, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<NeuralNetworkImageScaler*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(channelscale_, graybias_);

#undef ZR_HELPER_
#undef ZR_

}

bool NeuralNetworkImageScaler::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkImageScaler)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float channelScale = 10;
      case 10: {
        if (tag == 85) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &channelscale_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(165)) goto parse_blueBias;
        break;
      }

      // optional float blueBias = 20;
      case 20: {
        if (tag == 165) {
         parse_blueBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &bluebias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(173)) goto parse_greenBias;
        break;
      }

      // optional float greenBias = 21;
      case 21: {
        if (tag == 173) {
         parse_greenBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &greenbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(181)) goto parse_redBias;
        break;
      }

      // optional float redBias = 22;
      case 22: {
        if (tag == 181) {
         parse_redBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &redbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(245)) goto parse_grayBias;
        break;
      }

      // optional float grayBias = 30;
      case 30: {
        if (tag == 245) {
         parse_grayBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &graybias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkImageScaler)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkImageScaler)
  return false;
#undef DO_
}

void NeuralNetworkImageScaler::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkImageScaler)
  // optional float channelScale = 10;
  if (this->channelscale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->channelscale(), output);
  }

  // optional float blueBias = 20;
  if (this->bluebias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(20, this->bluebias(), output);
  }

  // optional float greenBias = 21;
  if (this->greenbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(21, this->greenbias(), output);
  }

  // optional float redBias = 22;
  if (this->redbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(22, this->redbias(), output);
  }

  // optional float grayBias = 30;
  if (this->graybias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(30, this->graybias(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkImageScaler)
}

size_t NeuralNetworkImageScaler::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkImageScaler)
  size_t total_size = 0;

  // optional float channelScale = 10;
  if (this->channelscale() != 0) {
    total_size += 1 + 4;
  }

  // optional float blueBias = 20;
  if (this->bluebias() != 0) {
    total_size += 2 + 4;
  }

  // optional float greenBias = 21;
  if (this->greenbias() != 0) {
    total_size += 2 + 4;
  }

  // optional float redBias = 22;
  if (this->redbias() != 0) {
    total_size += 2 + 4;
  }

  // optional float grayBias = 30;
  if (this->graybias() != 0) {
    total_size += 2 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkImageScaler::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkImageScaler*>(&from));
}

void NeuralNetworkImageScaler::MergeFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetworkImageScaler::UnsafeMergeFrom(const NeuralNetworkImageScaler& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.channelscale() != 0) {
    set_channelscale(from.channelscale());
  }
  if (from.bluebias() != 0) {
    set_bluebias(from.bluebias());
  }
  if (from.greenbias() != 0) {
    set_greenbias(from.greenbias());
  }
  if (from.redbias() != 0) {
    set_redbias(from.redbias());
  }
  if (from.graybias() != 0) {
    set_graybias(from.graybias());
  }
}

void NeuralNetworkImageScaler::CopyFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetworkImageScaler::IsInitialized() const {

  return true;
}

void NeuralNetworkImageScaler::Swap(NeuralNetworkImageScaler* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkImageScaler::InternalSwap(NeuralNetworkImageScaler* other) {
  std::swap(channelscale_, other->channelscale_);
  std::swap(bluebias_, other->bluebias_);
  std::swap(greenbias_, other->greenbias_);
  std::swap(redbias_, other->redbias_);
  std::swap(graybias_, other->graybias_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkImageScaler::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkImageScaler";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkImageScaler

// optional float channelScale = 10;
void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return channelscale_;
}
void NeuralNetworkImageScaler::set_channelscale(float value) {
  
  channelscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// optional float blueBias = 20;
void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return bluebias_;
}
void NeuralNetworkImageScaler::set_bluebias(float value) {
  
  bluebias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// optional float greenBias = 21;
void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return greenbias_;
}
void NeuralNetworkImageScaler::set_greenbias(float value) {
  
  greenbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// optional float redBias = 22;
void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return redbias_;
}
void NeuralNetworkImageScaler::set_redbias(float value) {
  
  redbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// optional float grayBias = 30;
void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return graybias_;
}
void NeuralNetworkImageScaler::set_graybias(float value) {
  
  graybias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

inline const NeuralNetworkImageScaler* NeuralNetworkImageScaler::internal_default_instance() {
  return &NeuralNetworkImageScaler_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkMeanImage::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkMeanImage::NeuralNetworkMeanImage()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkMeanImage)
}

void NeuralNetworkMeanImage::InitAsDefaultInstance() {
}

NeuralNetworkMeanImage::NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkMeanImage)
}

void NeuralNetworkMeanImage::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetworkMeanImage::~NeuralNetworkMeanImage() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkMeanImage)
  SharedDtor();
}

void NeuralNetworkMeanImage::SharedDtor() {
}

void NeuralNetworkMeanImage::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkMeanImage& NeuralNetworkMeanImage::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkMeanImage> NeuralNetworkMeanImage_default_instance_;

NeuralNetworkMeanImage* NeuralNetworkMeanImage::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkMeanImage* n = new NeuralNetworkMeanImage;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkMeanImage::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkMeanImage)
  meanimage_.Clear();
}

bool NeuralNetworkMeanImage::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkMeanImage)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float meanImage = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_meanimage())));
        } else if (tag == 13) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10, input, this->mutable_meanimage())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkMeanImage)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkMeanImage)
  return false;
#undef DO_
}

void NeuralNetworkMeanImage::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkMeanImage)
  // repeated float meanImage = 1;
  if (this->meanimage_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_meanimage_cached_byte_size_);
  }
  for (int i = 0; i < this->meanimage_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteFloatNoTag(
      this->meanimage(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkMeanImage)
}

size_t NeuralNetworkMeanImage::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkMeanImage)
  size_t total_size = 0;

  // repeated float meanImage = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->meanimage_size();
    data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _meanimage_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkMeanImage::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkMeanImage*>(&from));
}

void NeuralNetworkMeanImage::MergeFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetworkMeanImage::UnsafeMergeFrom(const NeuralNetworkMeanImage& from) {
  GOOGLE_DCHECK(&from != this);
  meanimage_.UnsafeMergeFrom(from.meanimage_);
}

void NeuralNetworkMeanImage::CopyFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetworkMeanImage::IsInitialized() const {

  return true;
}

void NeuralNetworkMeanImage::Swap(NeuralNetworkMeanImage* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkMeanImage::InternalSwap(NeuralNetworkMeanImage* other) {
  meanimage_.UnsafeArenaSwap(&other->meanimage_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkMeanImage::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkMeanImage";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkMeanImage

// repeated float meanImage = 1;
int NeuralNetworkMeanImage::meanimage_size() const {
  return meanimage_.size();
}
void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_.Get(index);
}
void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
void NeuralNetworkMeanImage::add_meanimage(float value) {
  meanimage_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
const ::google::protobuf::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_;
}
::google::protobuf::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return &meanimage_;
}

inline const NeuralNetworkMeanImage* NeuralNetworkMeanImage::internal_default_instance() {
  return &NeuralNetworkMeanImage_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkPreprocessing::kFeatureNameFieldNumber;
const int NeuralNetworkPreprocessing::kScalerFieldNumber;
const int NeuralNetworkPreprocessing::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkPreprocessing::NeuralNetworkPreprocessing()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}

void NeuralNetworkPreprocessing::InitAsDefaultInstance() {
}

NeuralNetworkPreprocessing::NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}

void NeuralNetworkPreprocessing::SharedCtor() {
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_preprocessor();
  _cached_size_ = 0;
}

NeuralNetworkPreprocessing::~NeuralNetworkPreprocessing() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkPreprocessing)
  SharedDtor();
}

void NeuralNetworkPreprocessing::SharedDtor() {
  featurename_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_preprocessor()) {
    clear_preprocessor();
  }
}

void NeuralNetworkPreprocessing::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkPreprocessing& NeuralNetworkPreprocessing::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkPreprocessing> NeuralNetworkPreprocessing_default_instance_;

NeuralNetworkPreprocessing* NeuralNetworkPreprocessing::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkPreprocessing* n = new NeuralNetworkPreprocessing;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkPreprocessing::clear_preprocessor() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  switch (preprocessor_case()) {
    case kScaler: {
      delete preprocessor_.scaler_;
      break;
    }
    case kMeanImage: {
      delete preprocessor_.meanimage_;
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}


void NeuralNetworkPreprocessing::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_preprocessor();
}

bool NeuralNetworkPreprocessing::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkPreprocessing)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional string featureName = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_featurename()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->featurename().data(), this->featurename().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkPreprocessing.featureName"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_scaler;
        break;
      }

      // optional .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
      case 10: {
        if (tag == 82) {
         parse_scaler:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaler()));
        } else {
          goto handle_unusual;
        }
        goto after_meanimage;
        break;
      }

      // optional .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
      case 11: {
        if (tag == 90) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_meanimage()));
        } else {
          goto handle_unusual;
        }
       after_meanimage:
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkPreprocessing)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkPreprocessing)
  return false;
#undef DO_
}

void NeuralNetworkPreprocessing::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkPreprocessing)
  // optional string featureName = 1;
  if (this->featurename().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->featurename().data(), this->featurename().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkPreprocessing.featureName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->featurename(), output);
  }

  // optional .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  if (has_scaler()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *preprocessor_.scaler_, output);
  }

  // optional .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  if (has_meanimage()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *preprocessor_.meanimage_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkPreprocessing)
}

size_t NeuralNetworkPreprocessing::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkPreprocessing)
  size_t total_size = 0;

  // optional string featureName = 1;
  if (this->featurename().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->featurename());
  }

  switch (preprocessor_case()) {
    // optional .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
    case kScaler: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.scaler_);
      break;
    }
    // optional .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
    case kMeanImage: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.meanimage_);
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkPreprocessing::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkPreprocessing*>(&from));
}

void NeuralNetworkPreprocessing::MergeFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetworkPreprocessing::UnsafeMergeFrom(const NeuralNetworkPreprocessing& from) {
  GOOGLE_DCHECK(&from != this);
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  if (from.featurename().size() > 0) {

    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
}

void NeuralNetworkPreprocessing::CopyFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetworkPreprocessing::IsInitialized() const {

  return true;
}

void NeuralNetworkPreprocessing::Swap(NeuralNetworkPreprocessing* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkPreprocessing::InternalSwap(NeuralNetworkPreprocessing* other) {
  featurename_.Swap(&other->featurename_);
  std::swap(preprocessor_, other->preprocessor_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkPreprocessing::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkPreprocessing";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkPreprocessing

// optional string featureName = 1;
void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.GetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkPreprocessing::set_featurename(const ::std::string& value) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
void NeuralNetworkPreprocessing::set_featurename(const char* value) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
void NeuralNetworkPreprocessing::set_featurename(const char* value, size_t size) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
::std::string* NeuralNetworkPreprocessing::mutable_featurename() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  
  return featurename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkPreprocessing::set_allocated_featurename(::std::string* featurename) {
  if (featurename != NULL) {
    
  } else {
    
  }
  featurename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), featurename);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// optional .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
bool NeuralNetworkPreprocessing::has_scaler() const {
  return preprocessor_case() == kScaler;
}
void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
void NeuralNetworkPreprocessing::clear_scaler() {
  if (has_scaler()) {
    delete preprocessor_.scaler_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return has_scaler()
      ? *preprocessor_.scaler_
      : ::CoreML::Specification::NeuralNetworkImageScaler::default_instance();
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  if (!has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = new ::CoreML::Specification::NeuralNetworkImageScaler;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return preprocessor_.scaler_;
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}

// optional .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
bool NeuralNetworkPreprocessing::has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
void NeuralNetworkPreprocessing::clear_meanimage() {
  if (has_meanimage()) {
    delete preprocessor_.meanimage_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return has_meanimage()
      ? *preprocessor_.meanimage_
      : ::CoreML::Specification::NeuralNetworkMeanImage::default_instance();
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  if (!has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = new ::CoreML::Specification::NeuralNetworkMeanImage;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return preprocessor_.meanimage_;
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}

bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
inline const NeuralNetworkPreprocessing* NeuralNetworkPreprocessing::internal_default_instance() {
  return &NeuralNetworkPreprocessing_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationReLU::ActivationReLU()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationReLU)
}

void ActivationReLU::InitAsDefaultInstance() {
}

ActivationReLU::ActivationReLU(const ActivationReLU& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationReLU)
}

void ActivationReLU::SharedCtor() {
  _cached_size_ = 0;
}

ActivationReLU::~ActivationReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationReLU)
  SharedDtor();
}

void ActivationReLU::SharedDtor() {
}

void ActivationReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationReLU& ActivationReLU::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationReLU> ActivationReLU_default_instance_;

ActivationReLU* ActivationReLU::New(::google::protobuf::Arena* arena) const {
  ActivationReLU* n = new ActivationReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationReLU)
}

bool ActivationReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationReLU)
  return false;
#undef DO_
}

void ActivationReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationReLU)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationReLU)
}

size_t ActivationReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationReLU)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationReLU*>(&from));
}

void ActivationReLU::MergeFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationReLU)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationReLU::UnsafeMergeFrom(const ActivationReLU& from) {
  GOOGLE_DCHECK(&from != this);
}

void ActivationReLU::CopyFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationReLU)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationReLU::IsInitialized() const {

  return true;
}

void ActivationReLU::Swap(ActivationReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationReLU::InternalSwap(ActivationReLU* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationReLU

inline const ActivationReLU* ActivationReLU::internal_default_instance() {
  return &ActivationReLU_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLeakyReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLeakyReLU::ActivationLeakyReLU()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLeakyReLU)
}

void ActivationLeakyReLU::InitAsDefaultInstance() {
}

ActivationLeakyReLU::ActivationLeakyReLU(const ActivationLeakyReLU& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLeakyReLU)
}

void ActivationLeakyReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationLeakyReLU::~ActivationLeakyReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLeakyReLU)
  SharedDtor();
}

void ActivationLeakyReLU::SharedDtor() {
}

void ActivationLeakyReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLeakyReLU& ActivationLeakyReLU::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationLeakyReLU> ActivationLeakyReLU_default_instance_;

ActivationLeakyReLU* ActivationLeakyReLU::New(::google::protobuf::Arena* arena) const {
  ActivationLeakyReLU* n = new ActivationLeakyReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLeakyReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLeakyReLU)
  alpha_ = 0;
}

bool ActivationLeakyReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLeakyReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLeakyReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLeakyReLU)
  return false;
#undef DO_
}

void ActivationLeakyReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLeakyReLU)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLeakyReLU)
}

size_t ActivationLeakyReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLeakyReLU)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLeakyReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLeakyReLU*>(&from));
}

void ActivationLeakyReLU::MergeFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLeakyReLU)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationLeakyReLU::UnsafeMergeFrom(const ActivationLeakyReLU& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationLeakyReLU::CopyFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLeakyReLU)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationLeakyReLU::IsInitialized() const {

  return true;
}

void ActivationLeakyReLU::Swap(ActivationLeakyReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLeakyReLU::InternalSwap(ActivationLeakyReLU* other) {
  std::swap(alpha_, other->alpha_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLeakyReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationLeakyReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLeakyReLU

// optional float alpha = 1;
void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return alpha_;
}
void ActivationLeakyReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

inline const ActivationLeakyReLU* ActivationLeakyReLU::internal_default_instance() {
  return &ActivationLeakyReLU_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationTanh::ActivationTanh()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationTanh)
}

void ActivationTanh::InitAsDefaultInstance() {
}

ActivationTanh::ActivationTanh(const ActivationTanh& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationTanh)
}

void ActivationTanh::SharedCtor() {
  _cached_size_ = 0;
}

ActivationTanh::~ActivationTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationTanh)
  SharedDtor();
}

void ActivationTanh::SharedDtor() {
}

void ActivationTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationTanh& ActivationTanh::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationTanh> ActivationTanh_default_instance_;

ActivationTanh* ActivationTanh::New(::google::protobuf::Arena* arena) const {
  ActivationTanh* n = new ActivationTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationTanh)
}

bool ActivationTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationTanh)
  return false;
#undef DO_
}

void ActivationTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationTanh)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationTanh)
}

size_t ActivationTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationTanh)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationTanh*>(&from));
}

void ActivationTanh::MergeFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationTanh)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationTanh::UnsafeMergeFrom(const ActivationTanh& from) {
  GOOGLE_DCHECK(&from != this);
}

void ActivationTanh::CopyFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationTanh)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationTanh::IsInitialized() const {

  return true;
}

void ActivationTanh::Swap(ActivationTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationTanh::InternalSwap(ActivationTanh* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationTanh

inline const ActivationTanh* ActivationTanh::internal_default_instance() {
  return &ActivationTanh_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationScaledTanh::kAlphaFieldNumber;
const int ActivationScaledTanh::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationScaledTanh::ActivationScaledTanh()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationScaledTanh)
}

void ActivationScaledTanh::InitAsDefaultInstance() {
}

ActivationScaledTanh::ActivationScaledTanh(const ActivationScaledTanh& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationScaledTanh)
}

void ActivationScaledTanh::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationScaledTanh::~ActivationScaledTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationScaledTanh)
  SharedDtor();
}

void ActivationScaledTanh::SharedDtor() {
}

void ActivationScaledTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationScaledTanh& ActivationScaledTanh::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationScaledTanh> ActivationScaledTanh_default_instance_;

ActivationScaledTanh* ActivationScaledTanh::New(::google::protobuf::Arena* arena) const {
  ActivationScaledTanh* n = new ActivationScaledTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationScaledTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationScaledTanh)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(ActivationScaledTanh, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<ActivationScaledTanh*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(alpha_, beta_);

#undef ZR_HELPER_
#undef ZR_

}

bool ActivationScaledTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationScaledTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(21)) goto parse_beta;
        break;
      }

      // optional float beta = 2;
      case 2: {
        if (tag == 21) {
         parse_beta:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationScaledTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationScaledTanh)
  return false;
#undef DO_
}

void ActivationScaledTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationScaledTanh)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationScaledTanh)
}

size_t ActivationScaledTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationScaledTanh)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationScaledTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationScaledTanh*>(&from));
}

void ActivationScaledTanh::MergeFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationScaledTanh)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationScaledTanh::UnsafeMergeFrom(const ActivationScaledTanh& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationScaledTanh::CopyFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationScaledTanh)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationScaledTanh::IsInitialized() const {

  return true;
}

void ActivationScaledTanh::Swap(ActivationScaledTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationScaledTanh::InternalSwap(ActivationScaledTanh* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationScaledTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationScaledTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationScaledTanh

// optional float alpha = 1;
void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return alpha_;
}
void ActivationScaledTanh::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// optional float beta = 2;
void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return beta_;
}
void ActivationScaledTanh::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

inline const ActivationScaledTanh* ActivationScaledTanh::internal_default_instance() {
  return &ActivationScaledTanh_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoid::ActivationSigmoid()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoid)
}

void ActivationSigmoid::InitAsDefaultInstance() {
}

ActivationSigmoid::ActivationSigmoid(const ActivationSigmoid& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoid)
}

void ActivationSigmoid::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSigmoid::~ActivationSigmoid() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoid)
  SharedDtor();
}

void ActivationSigmoid::SharedDtor() {
}

void ActivationSigmoid::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoid& ActivationSigmoid::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoid> ActivationSigmoid_default_instance_;

ActivationSigmoid* ActivationSigmoid::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoid* n = new ActivationSigmoid;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoid::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoid)
}

bool ActivationSigmoid::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoid)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoid)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoid)
  return false;
#undef DO_
}

void ActivationSigmoid::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoid)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoid)
}

size_t ActivationSigmoid::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoid)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoid::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoid*>(&from));
}

void ActivationSigmoid::MergeFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoid)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationSigmoid::UnsafeMergeFrom(const ActivationSigmoid& from) {
  GOOGLE_DCHECK(&from != this);
}

void ActivationSigmoid::CopyFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoid)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationSigmoid::IsInitialized() const {

  return true;
}

void ActivationSigmoid::Swap(ActivationSigmoid* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoid::InternalSwap(ActivationSigmoid* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoid::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoid";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoid

inline const ActivationSigmoid* ActivationSigmoid::internal_default_instance() {
  return &ActivationSigmoid_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLinear::kAlphaFieldNumber;
const int ActivationLinear::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLinear::ActivationLinear()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLinear)
}

void ActivationLinear::InitAsDefaultInstance() {
}

ActivationLinear::ActivationLinear(const ActivationLinear& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLinear)
}

void ActivationLinear::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationLinear::~ActivationLinear() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLinear)
  SharedDtor();
}

void ActivationLinear::SharedDtor() {
}

void ActivationLinear::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLinear& ActivationLinear::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationLinear> ActivationLinear_default_instance_;

ActivationLinear* ActivationLinear::New(::google::protobuf::Arena* arena) const {
  ActivationLinear* n = new ActivationLinear;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLinear::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLinear)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(ActivationLinear, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<ActivationLinear*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(alpha_, beta_);

#undef ZR_HELPER_
#undef ZR_

}

bool ActivationLinear::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLinear)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(21)) goto parse_beta;
        break;
      }

      // optional float beta = 2;
      case 2: {
        if (tag == 21) {
         parse_beta:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLinear)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLinear)
  return false;
#undef DO_
}

void ActivationLinear::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLinear)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLinear)
}

size_t ActivationLinear::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLinear)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLinear::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLinear*>(&from));
}

void ActivationLinear::MergeFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLinear)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationLinear::UnsafeMergeFrom(const ActivationLinear& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationLinear::CopyFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLinear)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationLinear::IsInitialized() const {

  return true;
}

void ActivationLinear::Swap(ActivationLinear* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLinear::InternalSwap(ActivationLinear* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLinear::GetTypeName() const {
  return "CoreML.Specification.ActivationLinear";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLinear

// optional float alpha = 1;
void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return alpha_;
}
void ActivationLinear::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// optional float beta = 2;
void ActivationLinear::clear_beta() {
  beta_ = 0;
}
float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return beta_;
}
void ActivationLinear::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

inline const ActivationLinear* ActivationLinear::internal_default_instance() {
  return &ActivationLinear_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationSigmoidHard::kAlphaFieldNumber;
const int ActivationSigmoidHard::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoidHard::ActivationSigmoidHard()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoidHard)
}

void ActivationSigmoidHard::InitAsDefaultInstance() {
}

ActivationSigmoidHard::ActivationSigmoidHard(const ActivationSigmoidHard& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoidHard)
}

void ActivationSigmoidHard::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationSigmoidHard::~ActivationSigmoidHard() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoidHard)
  SharedDtor();
}

void ActivationSigmoidHard::SharedDtor() {
}

void ActivationSigmoidHard::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoidHard& ActivationSigmoidHard::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoidHard> ActivationSigmoidHard_default_instance_;

ActivationSigmoidHard* ActivationSigmoidHard::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoidHard* n = new ActivationSigmoidHard;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoidHard::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoidHard)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(ActivationSigmoidHard, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<ActivationSigmoidHard*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(alpha_, beta_);

#undef ZR_HELPER_
#undef ZR_

}

bool ActivationSigmoidHard::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoidHard)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(21)) goto parse_beta;
        break;
      }

      // optional float beta = 2;
      case 2: {
        if (tag == 21) {
         parse_beta:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoidHard)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoidHard)
  return false;
#undef DO_
}

void ActivationSigmoidHard::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoidHard)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoidHard)
}

size_t ActivationSigmoidHard::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoidHard)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoidHard::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoidHard*>(&from));
}

void ActivationSigmoidHard::MergeFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoidHard)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationSigmoidHard::UnsafeMergeFrom(const ActivationSigmoidHard& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationSigmoidHard::CopyFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoidHard)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationSigmoidHard::IsInitialized() const {

  return true;
}

void ActivationSigmoidHard::Swap(ActivationSigmoidHard* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoidHard::InternalSwap(ActivationSigmoidHard* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoidHard::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoidHard";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoidHard

// optional float alpha = 1;
void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return alpha_;
}
void ActivationSigmoidHard::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// optional float beta = 2;
void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return beta_;
}
void ActivationSigmoidHard::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

inline const ActivationSigmoidHard* ActivationSigmoidHard::internal_default_instance() {
  return &ActivationSigmoidHard_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationPReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationPReLU::ActivationPReLU()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationPReLU)
}

void ActivationPReLU::InitAsDefaultInstance() {
  alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

ActivationPReLU::ActivationPReLU(const ActivationPReLU& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationPReLU)
}

void ActivationPReLU::SharedCtor() {
  alpha_ = NULL;
  _cached_size_ = 0;
}

ActivationPReLU::~ActivationPReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationPReLU)
  SharedDtor();
}

void ActivationPReLU::SharedDtor() {
  if (this != &ActivationPReLU_default_instance_.get()) {
    delete alpha_;
  }
}

void ActivationPReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationPReLU& ActivationPReLU::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationPReLU> ActivationPReLU_default_instance_;

ActivationPReLU* ActivationPReLU::New(::google::protobuf::Arena* arena) const {
  ActivationPReLU* n = new ActivationPReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationPReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationPReLU)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}

bool ActivationPReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationPReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationPReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationPReLU)
  return false;
#undef DO_
}

void ActivationPReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationPReLU)
  // optional .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationPReLU)
}

size_t ActivationPReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationPReLU)
  size_t total_size = 0;

  // optional .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationPReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationPReLU*>(&from));
}

void ActivationPReLU::MergeFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationPReLU)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationPReLU::UnsafeMergeFrom(const ActivationPReLU& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
}

void ActivationPReLU::CopyFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationPReLU)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationPReLU::IsInitialized() const {

  return true;
}

void ActivationPReLU::Swap(ActivationPReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationPReLU::InternalSwap(ActivationPReLU* other) {
  std::swap(alpha_, other->alpha_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationPReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationPReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationPReLU

// optional .CoreML.Specification.WeightParams alpha = 1;
bool ActivationPReLU::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationPReLU::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

inline const ActivationPReLU* ActivationPReLU::internal_default_instance() {
  return &ActivationPReLU_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationELU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationELU::ActivationELU()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationELU)
}

void ActivationELU::InitAsDefaultInstance() {
}

ActivationELU::ActivationELU(const ActivationELU& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationELU)
}

void ActivationELU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationELU::~ActivationELU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationELU)
  SharedDtor();
}

void ActivationELU::SharedDtor() {
}

void ActivationELU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationELU& ActivationELU::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationELU> ActivationELU_default_instance_;

ActivationELU* ActivationELU::New(::google::protobuf::Arena* arena) const {
  ActivationELU* n = new ActivationELU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationELU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationELU)
  alpha_ = 0;
}

bool ActivationELU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationELU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationELU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationELU)
  return false;
#undef DO_
}

void ActivationELU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationELU)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationELU)
}

size_t ActivationELU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationELU)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationELU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationELU*>(&from));
}

void ActivationELU::MergeFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationELU)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationELU::UnsafeMergeFrom(const ActivationELU& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationELU::CopyFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationELU)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationELU::IsInitialized() const {

  return true;
}

void ActivationELU::Swap(ActivationELU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationELU::InternalSwap(ActivationELU* other) {
  std::swap(alpha_, other->alpha_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationELU::GetTypeName() const {
  return "CoreML.Specification.ActivationELU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationELU

// optional float alpha = 1;
void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return alpha_;
}
void ActivationELU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

inline const ActivationELU* ActivationELU::internal_default_instance() {
  return &ActivationELU_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationThresholdedReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationThresholdedReLU::ActivationThresholdedReLU()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationThresholdedReLU)
}

void ActivationThresholdedReLU::InitAsDefaultInstance() {
}

ActivationThresholdedReLU::ActivationThresholdedReLU(const ActivationThresholdedReLU& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationThresholdedReLU)
}

void ActivationThresholdedReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationThresholdedReLU::~ActivationThresholdedReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationThresholdedReLU)
  SharedDtor();
}

void ActivationThresholdedReLU::SharedDtor() {
}

void ActivationThresholdedReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationThresholdedReLU& ActivationThresholdedReLU::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationThresholdedReLU> ActivationThresholdedReLU_default_instance_;

ActivationThresholdedReLU* ActivationThresholdedReLU::New(::google::protobuf::Arena* arena) const {
  ActivationThresholdedReLU* n = new ActivationThresholdedReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationThresholdedReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationThresholdedReLU)
  alpha_ = 0;
}

bool ActivationThresholdedReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationThresholdedReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationThresholdedReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationThresholdedReLU)
  return false;
#undef DO_
}

void ActivationThresholdedReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationThresholdedReLU)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationThresholdedReLU)
}

size_t ActivationThresholdedReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationThresholdedReLU)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationThresholdedReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationThresholdedReLU*>(&from));
}

void ActivationThresholdedReLU::MergeFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationThresholdedReLU)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationThresholdedReLU::UnsafeMergeFrom(const ActivationThresholdedReLU& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationThresholdedReLU::CopyFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationThresholdedReLU)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationThresholdedReLU::IsInitialized() const {

  return true;
}

void ActivationThresholdedReLU::Swap(ActivationThresholdedReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationThresholdedReLU::InternalSwap(ActivationThresholdedReLU* other) {
  std::swap(alpha_, other->alpha_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationThresholdedReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationThresholdedReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationThresholdedReLU

// optional float alpha = 1;
void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return alpha_;
}
void ActivationThresholdedReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

inline const ActivationThresholdedReLU* ActivationThresholdedReLU::internal_default_instance() {
  return &ActivationThresholdedReLU_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftsign::ActivationSoftsign()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftsign)
}

void ActivationSoftsign::InitAsDefaultInstance() {
}

ActivationSoftsign::ActivationSoftsign(const ActivationSoftsign& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftsign)
}

void ActivationSoftsign::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftsign::~ActivationSoftsign() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftsign)
  SharedDtor();
}

void ActivationSoftsign::SharedDtor() {
}

void ActivationSoftsign::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftsign& ActivationSoftsign::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftsign> ActivationSoftsign_default_instance_;

ActivationSoftsign* ActivationSoftsign::New(::google::protobuf::Arena* arena) const {
  ActivationSoftsign* n = new ActivationSoftsign;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftsign::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftsign)
}

bool ActivationSoftsign::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftsign)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftsign)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftsign)
  return false;
#undef DO_
}

void ActivationSoftsign::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftsign)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftsign)
}

size_t ActivationSoftsign::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftsign)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftsign::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftsign*>(&from));
}

void ActivationSoftsign::MergeFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftsign)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationSoftsign::UnsafeMergeFrom(const ActivationSoftsign& from) {
  GOOGLE_DCHECK(&from != this);
}

void ActivationSoftsign::CopyFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftsign)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationSoftsign::IsInitialized() const {

  return true;
}

void ActivationSoftsign::Swap(ActivationSoftsign* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftsign::InternalSwap(ActivationSoftsign* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftsign::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftsign";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftsign

inline const ActivationSoftsign* ActivationSoftsign::internal_default_instance() {
  return &ActivationSoftsign_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftplus::ActivationSoftplus()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftplus)
}

void ActivationSoftplus::InitAsDefaultInstance() {
}

ActivationSoftplus::ActivationSoftplus(const ActivationSoftplus& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftplus)
}

void ActivationSoftplus::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftplus::~ActivationSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftplus)
  SharedDtor();
}

void ActivationSoftplus::SharedDtor() {
}

void ActivationSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftplus& ActivationSoftplus::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftplus> ActivationSoftplus_default_instance_;

ActivationSoftplus* ActivationSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationSoftplus* n = new ActivationSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftplus)
}

bool ActivationSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftplus)
  return false;
#undef DO_
}

void ActivationSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftplus)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftplus)
}

size_t ActivationSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftplus)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftplus*>(&from));
}

void ActivationSoftplus::MergeFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftplus)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationSoftplus::UnsafeMergeFrom(const ActivationSoftplus& from) {
  GOOGLE_DCHECK(&from != this);
}

void ActivationSoftplus::CopyFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftplus)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationSoftplus::IsInitialized() const {

  return true;
}

void ActivationSoftplus::Swap(ActivationSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftplus::InternalSwap(ActivationSoftplus* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftplus

inline const ActivationSoftplus* ActivationSoftplus::internal_default_instance() {
  return &ActivationSoftplus_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParametricSoftplus::kAlphaFieldNumber;
const int ActivationParametricSoftplus::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParametricSoftplus::ActivationParametricSoftplus()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParametricSoftplus)
}

void ActivationParametricSoftplus::InitAsDefaultInstance() {
  alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

ActivationParametricSoftplus::ActivationParametricSoftplus(const ActivationParametricSoftplus& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParametricSoftplus)
}

void ActivationParametricSoftplus::SharedCtor() {
  alpha_ = NULL;
  beta_ = NULL;
  _cached_size_ = 0;
}

ActivationParametricSoftplus::~ActivationParametricSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParametricSoftplus)
  SharedDtor();
}

void ActivationParametricSoftplus::SharedDtor() {
  if (this != &ActivationParametricSoftplus_default_instance_.get()) {
    delete alpha_;
    delete beta_;
  }
}

void ActivationParametricSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParametricSoftplus& ActivationParametricSoftplus::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationParametricSoftplus> ActivationParametricSoftplus_default_instance_;

ActivationParametricSoftplus* ActivationParametricSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationParametricSoftplus* n = new ActivationParametricSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParametricSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParametricSoftplus)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}

bool ActivationParametricSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParametricSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_beta;
        break;
      }

      // optional .CoreML.Specification.WeightParams beta = 2;
      case 2: {
        if (tag == 18) {
         parse_beta:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParametricSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParametricSoftplus)
  return false;
#undef DO_
}

void ActivationParametricSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParametricSoftplus)
  // optional .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // optional .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->beta_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParametricSoftplus)
}

size_t ActivationParametricSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParametricSoftplus)
  size_t total_size = 0;

  // optional .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  // optional .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParametricSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParametricSoftplus*>(&from));
}

void ActivationParametricSoftplus::MergeFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParametricSoftplus)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationParametricSoftplus::UnsafeMergeFrom(const ActivationParametricSoftplus& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
}

void ActivationParametricSoftplus::CopyFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParametricSoftplus)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationParametricSoftplus::IsInitialized() const {

  return true;
}

void ActivationParametricSoftplus::Swap(ActivationParametricSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParametricSoftplus::InternalSwap(ActivationParametricSoftplus* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParametricSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationParametricSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParametricSoftplus

// optional .CoreML.Specification.WeightParams alpha = 1;
bool ActivationParametricSoftplus::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// optional .CoreML.Specification.WeightParams beta = 2;
bool ActivationParametricSoftplus::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

inline const ActivationParametricSoftplus* ActivationParametricSoftplus::internal_default_instance() {
  return &ActivationParametricSoftplus_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParams::kLinearFieldNumber;
const int ActivationParams::kReLUFieldNumber;
const int ActivationParams::kLeakyReLUFieldNumber;
const int ActivationParams::kThresholdedReLUFieldNumber;
const int ActivationParams::kPReLUFieldNumber;
const int ActivationParams::kTanhFieldNumber;
const int ActivationParams::kScaledTanhFieldNumber;
const int ActivationParams::kSigmoidFieldNumber;
const int ActivationParams::kSigmoidHardFieldNumber;
const int ActivationParams::kELUFieldNumber;
const int ActivationParams::kSoftsignFieldNumber;
const int ActivationParams::kSoftplusFieldNumber;
const int ActivationParams::kParametricSoftplusFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParams::ActivationParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParams)
}

void ActivationParams::InitAsDefaultInstance() {
}

ActivationParams::ActivationParams(const ActivationParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParams)
}

void ActivationParams::SharedCtor() {
  clear_has_NonlinearityType();
  _cached_size_ = 0;
}

ActivationParams::~ActivationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParams)
  SharedDtor();
}

void ActivationParams::SharedDtor() {
  if (has_NonlinearityType()) {
    clear_NonlinearityType();
  }
}

void ActivationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParams& ActivationParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ActivationParams> ActivationParams_default_instance_;

ActivationParams* ActivationParams::New(::google::protobuf::Arena* arena) const {
  ActivationParams* n = new ActivationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParams::clear_NonlinearityType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ActivationParams)
  switch (NonlinearityType_case()) {
    case kLinear: {
      delete NonlinearityType_.linear_;
      break;
    }
    case kReLU: {
      delete NonlinearityType_.relu_;
      break;
    }
    case kLeakyReLU: {
      delete NonlinearityType_.leakyrelu_;
      break;
    }
    case kThresholdedReLU: {
      delete NonlinearityType_.thresholdedrelu_;
      break;
    }
    case kPReLU: {
      delete NonlinearityType_.prelu_;
      break;
    }
    case kTanh: {
      delete NonlinearityType_.tanh_;
      break;
    }
    case kScaledTanh: {
      delete NonlinearityType_.scaledtanh_;
      break;
    }
    case kSigmoid: {
      delete NonlinearityType_.sigmoid_;
      break;
    }
    case kSigmoidHard: {
      delete NonlinearityType_.sigmoidhard_;
      break;
    }
    case kELU: {
      delete NonlinearityType_.elu_;
      break;
    }
    case kSoftsign: {
      delete NonlinearityType_.softsign_;
      break;
    }
    case kSoftplus: {
      delete NonlinearityType_.softplus_;
      break;
    }
    case kParametricSoftplus: {
      delete NonlinearityType_.parametricsoftplus_;
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}


void ActivationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParams)
  clear_NonlinearityType();
}

bool ActivationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.ActivationLinear linear = 5;
      case 5: {
        if (tag == 42) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linear()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationReLU ReLU = 10;
      case 10: {
        if (tag == 82) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_relu()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
      case 15: {
        if (tag == 122) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_leakyrelu()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
      case 20: {
        if (tag == 162) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_thresholdedrelu()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationPReLU PReLU = 25;
      case 25: {
        if (tag == 202) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_prelu()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationTanh tanh = 30;
      case 30: {
        if (tag == 242) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tanh()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
      case 31: {
        if (tag == 250) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaledtanh()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationSigmoid sigmoid = 40;
      case 40: {
        if (tag == 322) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoid()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
      case 41: {
        if (tag == 330) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoidhard()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationELU ELU = 50;
      case 50: {
        if (tag == 402) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_elu()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationSoftsign softsign = 60;
      case 60: {
        if (tag == 482) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softsign()));
        } else {
          goto handle_unusual;
        }
        goto after_parametricsoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationSoftplus softplus = 70;
      case 70: {
        if (tag == 562) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softplus()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(570)) goto parse_parametricSoftplus;
        break;
      }

      // optional .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
      case 71: {
        if (tag == 570) {
         parse_parametricSoftplus:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_parametricsoftplus()));
        } else {
          goto handle_unusual;
        }
       after_parametricsoftplus:
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParams)
  return false;
#undef DO_
}

void ActivationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParams)
  // optional .CoreML.Specification.ActivationLinear linear = 5;
  if (has_linear()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *NonlinearityType_.linear_, output);
  }

  // optional .CoreML.Specification.ActivationReLU ReLU = 10;
  if (has_relu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *NonlinearityType_.relu_, output);
  }

  // optional .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  if (has_leakyrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *NonlinearityType_.leakyrelu_, output);
  }

  // optional .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  if (has_thresholdedrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *NonlinearityType_.thresholdedrelu_, output);
  }

  // optional .CoreML.Specification.ActivationPReLU PReLU = 25;
  if (has_prelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      25, *NonlinearityType_.prelu_, output);
  }

  // optional .CoreML.Specification.ActivationTanh tanh = 30;
  if (has_tanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *NonlinearityType_.tanh_, output);
  }

  // optional .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  if (has_scaledtanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *NonlinearityType_.scaledtanh_, output);
  }

  // optional .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  if (has_sigmoid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *NonlinearityType_.sigmoid_, output);
  }

  // optional .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  if (has_sigmoidhard()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *NonlinearityType_.sigmoidhard_, output);
  }

  // optional .CoreML.Specification.ActivationELU ELU = 50;
  if (has_elu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *NonlinearityType_.elu_, output);
  }

  // optional .CoreML.Specification.ActivationSoftsign softsign = 60;
  if (has_softsign()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *NonlinearityType_.softsign_, output);
  }

  // optional .CoreML.Specification.ActivationSoftplus softplus = 70;
  if (has_softplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *NonlinearityType_.softplus_, output);
  }

  // optional .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  if (has_parametricsoftplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *NonlinearityType_.parametricsoftplus_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParams)
}

size_t ActivationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParams)
  size_t total_size = 0;

  switch (NonlinearityType_case()) {
    // optional .CoreML.Specification.ActivationLinear linear = 5;
    case kLinear: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.linear_);
      break;
    }
    // optional .CoreML.Specification.ActivationReLU ReLU = 10;
    case kReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.relu_);
      break;
    }
    // optional .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
    case kLeakyReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.leakyrelu_);
      break;
    }
    // optional .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
    case kThresholdedReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.thresholdedrelu_);
      break;
    }
    // optional .CoreML.Specification.ActivationPReLU PReLU = 25;
    case kPReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.prelu_);
      break;
    }
    // optional .CoreML.Specification.ActivationTanh tanh = 30;
    case kTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.tanh_);
      break;
    }
    // optional .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
    case kScaledTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.scaledtanh_);
      break;
    }
    // optional .CoreML.Specification.ActivationSigmoid sigmoid = 40;
    case kSigmoid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoid_);
      break;
    }
    // optional .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
    case kSigmoidHard: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoidhard_);
      break;
    }
    // optional .CoreML.Specification.ActivationELU ELU = 50;
    case kELU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.elu_);
      break;
    }
    // optional .CoreML.Specification.ActivationSoftsign softsign = 60;
    case kSoftsign: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softsign_);
      break;
    }
    // optional .CoreML.Specification.ActivationSoftplus softplus = 70;
    case kSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softplus_);
      break;
    }
    // optional .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
    case kParametricSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.parametricsoftplus_);
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParams*>(&from));
}

void ActivationParams::MergeFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ActivationParams::UnsafeMergeFrom(const ActivationParams& from) {
  GOOGLE_DCHECK(&from != this);
  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
}

void ActivationParams::CopyFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ActivationParams::IsInitialized() const {

  return true;
}

void ActivationParams::Swap(ActivationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParams::InternalSwap(ActivationParams* other) {
  std::swap(NonlinearityType_, other->NonlinearityType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParams::GetTypeName() const {
  return "CoreML.Specification.ActivationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParams

// optional .CoreML.Specification.ActivationLinear linear = 5;
bool ActivationParams::has_linear() const {
  return NonlinearityType_case() == kLinear;
}
void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
void ActivationParams::clear_linear() {
  if (has_linear()) {
    delete NonlinearityType_.linear_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return has_linear()
      ? *NonlinearityType_.linear_
      : ::CoreML::Specification::ActivationLinear::default_instance();
}
::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  if (!has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = new ::CoreML::Specification::ActivationLinear;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return NonlinearityType_.linear_;
}
::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.linear)
}

// optional .CoreML.Specification.ActivationReLU ReLU = 10;
bool ActivationParams::has_relu() const {
  return NonlinearityType_case() == kReLU;
}
void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
void ActivationParams::clear_relu() {
  if (has_relu()) {
    delete NonlinearityType_.relu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return has_relu()
      ? *NonlinearityType_.relu_
      : ::CoreML::Specification::ActivationReLU::default_instance();
}
::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  if (!has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = new ::CoreML::Specification::ActivationReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return NonlinearityType_.relu_;
}
::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}

// optional .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
bool ActivationParams::has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
void ActivationParams::clear_leakyrelu() {
  if (has_leakyrelu()) {
    delete NonlinearityType_.leakyrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : ::CoreML::Specification::ActivationLeakyReLU::default_instance();
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  if (!has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = new ::CoreML::Specification::ActivationLeakyReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return NonlinearityType_.leakyrelu_;
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}

// optional .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
bool ActivationParams::has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
void ActivationParams::clear_thresholdedrelu() {
  if (has_thresholdedrelu()) {
    delete NonlinearityType_.thresholdedrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : ::CoreML::Specification::ActivationThresholdedReLU::default_instance();
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  if (!has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = new ::CoreML::Specification::ActivationThresholdedReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return NonlinearityType_.thresholdedrelu_;
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}

// optional .CoreML.Specification.ActivationPReLU PReLU = 25;
bool ActivationParams::has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
void ActivationParams::clear_prelu() {
  if (has_prelu()) {
    delete NonlinearityType_.prelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return has_prelu()
      ? *NonlinearityType_.prelu_
      : ::CoreML::Specification::ActivationPReLU::default_instance();
}
::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  if (!has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = new ::CoreML::Specification::ActivationPReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return NonlinearityType_.prelu_;
}
::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}

// optional .CoreML.Specification.ActivationTanh tanh = 30;
bool ActivationParams::has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
void ActivationParams::clear_tanh() {
  if (has_tanh()) {
    delete NonlinearityType_.tanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return has_tanh()
      ? *NonlinearityType_.tanh_
      : ::CoreML::Specification::ActivationTanh::default_instance();
}
::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  if (!has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = new ::CoreML::Specification::ActivationTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return NonlinearityType_.tanh_;
}
::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.tanh)
}

// optional .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
bool ActivationParams::has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
void ActivationParams::clear_scaledtanh() {
  if (has_scaledtanh()) {
    delete NonlinearityType_.scaledtanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : ::CoreML::Specification::ActivationScaledTanh::default_instance();
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  if (!has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = new ::CoreML::Specification::ActivationScaledTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return NonlinearityType_.scaledtanh_;
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}

// optional .CoreML.Specification.ActivationSigmoid sigmoid = 40;
bool ActivationParams::has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
void ActivationParams::clear_sigmoid() {
  if (has_sigmoid()) {
    delete NonlinearityType_.sigmoid_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : ::CoreML::Specification::ActivationSigmoid::default_instance();
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  if (!has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = new ::CoreML::Specification::ActivationSigmoid;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return NonlinearityType_.sigmoid_;
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}

// optional .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
bool ActivationParams::has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
void ActivationParams::clear_sigmoidhard() {
  if (has_sigmoidhard()) {
    delete NonlinearityType_.sigmoidhard_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : ::CoreML::Specification::ActivationSigmoidHard::default_instance();
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  if (!has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = new ::CoreML::Specification::ActivationSigmoidHard;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return NonlinearityType_.sigmoidhard_;
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}

// optional .CoreML.Specification.ActivationELU ELU = 50;
bool ActivationParams::has_elu() const {
  return NonlinearityType_case() == kELU;
}
void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
void ActivationParams::clear_elu() {
  if (has_elu()) {
    delete NonlinearityType_.elu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return has_elu()
      ? *NonlinearityType_.elu_
      : ::CoreML::Specification::ActivationELU::default_instance();
}
::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  if (!has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = new ::CoreML::Specification::ActivationELU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return NonlinearityType_.elu_;
}
::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ELU)
}

// optional .CoreML.Specification.ActivationSoftsign softsign = 60;
bool ActivationParams::has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
void ActivationParams::clear_softsign() {
  if (has_softsign()) {
    delete NonlinearityType_.softsign_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return has_softsign()
      ? *NonlinearityType_.softsign_
      : ::CoreML::Specification::ActivationSoftsign::default_instance();
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  if (!has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = new ::CoreML::Specification::ActivationSoftsign;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return NonlinearityType_.softsign_;
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softsign)
}

// optional .CoreML.Specification.ActivationSoftplus softplus = 70;
bool ActivationParams::has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
void ActivationParams::clear_softplus() {
  if (has_softplus()) {
    delete NonlinearityType_.softplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return has_softplus()
      ? *NonlinearityType_.softplus_
      : ::CoreML::Specification::ActivationSoftplus::default_instance();
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  if (!has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = new ::CoreML::Specification::ActivationSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return NonlinearityType_.softplus_;
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softplus)
}

// optional .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
bool ActivationParams::has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
void ActivationParams::clear_parametricsoftplus() {
  if (has_parametricsoftplus()) {
    delete NonlinearityType_.parametricsoftplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : ::CoreML::Specification::ActivationParametricSoftplus::default_instance();
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  if (!has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = new ::CoreML::Specification::ActivationParametricSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return NonlinearityType_.parametricsoftplus_;
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}

bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
inline const ActivationParams* ActivationParams::internal_default_instance() {
  return &ActivationParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkLayer::kNameFieldNumber;
const int NeuralNetworkLayer::kInputFieldNumber;
const int NeuralNetworkLayer::kOutputFieldNumber;
const int NeuralNetworkLayer::kConvolutionFieldNumber;
const int NeuralNetworkLayer::kPoolingFieldNumber;
const int NeuralNetworkLayer::kActivationFieldNumber;
const int NeuralNetworkLayer::kInnerProductFieldNumber;
const int NeuralNetworkLayer::kEmbeddingFieldNumber;
const int NeuralNetworkLayer::kBatchnormFieldNumber;
const int NeuralNetworkLayer::kMvnFieldNumber;
const int NeuralNetworkLayer::kL2NormalizeFieldNumber;
const int NeuralNetworkLayer::kSoftmaxFieldNumber;
const int NeuralNetworkLayer::kLrnFieldNumber;
const int NeuralNetworkLayer::kCropFieldNumber;
const int NeuralNetworkLayer::kPaddingFieldNumber;
const int NeuralNetworkLayer::kUpsampleFieldNumber;
const int NeuralNetworkLayer::kUnaryFieldNumber;
const int NeuralNetworkLayer::kAddFieldNumber;
const int NeuralNetworkLayer::kMultiplyFieldNumber;
const int NeuralNetworkLayer::kAverageFieldNumber;
const int NeuralNetworkLayer::kScaleFieldNumber;
const int NeuralNetworkLayer::kBiasFieldNumber;
const int NeuralNetworkLayer::kMaxFieldNumber;
const int NeuralNetworkLayer::kMinFieldNumber;
const int NeuralNetworkLayer::kDotFieldNumber;
const int NeuralNetworkLayer::kReduceFieldNumber;
const int NeuralNetworkLayer::kLoadConstantFieldNumber;
const int NeuralNetworkLayer::kReshapeFieldNumber;
const int NeuralNetworkLayer::kFlattenFieldNumber;
const int NeuralNetworkLayer::kPermuteFieldNumber;
const int NeuralNetworkLayer::kConcatFieldNumber;
const int NeuralNetworkLayer::kSplitFieldNumber;
const int NeuralNetworkLayer::kSequenceRepeatFieldNumber;
const int NeuralNetworkLayer::kSimpleRecurrentFieldNumber;
const int NeuralNetworkLayer::kGruFieldNumber;
const int NeuralNetworkLayer::kUniDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kBiDirectionalLSTMFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkLayer::NeuralNetworkLayer()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkLayer)
}

void NeuralNetworkLayer::InitAsDefaultInstance() {
}

NeuralNetworkLayer::NeuralNetworkLayer(const NeuralNetworkLayer& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkLayer)
}

void NeuralNetworkLayer::SharedCtor() {
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_layer();
  _cached_size_ = 0;
}

NeuralNetworkLayer::~NeuralNetworkLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkLayer)
  SharedDtor();
}

void NeuralNetworkLayer::SharedDtor() {
  name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_layer()) {
    clear_layer();
  }
}

void NeuralNetworkLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkLayer& NeuralNetworkLayer::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkLayer> NeuralNetworkLayer_default_instance_;

NeuralNetworkLayer* NeuralNetworkLayer::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkLayer* n = new NeuralNetworkLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkLayer::clear_layer() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkLayer)
  switch (layer_case()) {
    case kConvolution: {
      delete layer_.convolution_;
      break;
    }
    case kPooling: {
      delete layer_.pooling_;
      break;
    }
    case kActivation: {
      delete layer_.activation_;
      break;
    }
    case kInnerProduct: {
      delete layer_.innerproduct_;
      break;
    }
    case kEmbedding: {
      delete layer_.embedding_;
      break;
    }
    case kBatchnorm: {
      delete layer_.batchnorm_;
      break;
    }
    case kMvn: {
      delete layer_.mvn_;
      break;
    }
    case kL2Normalize: {
      delete layer_.l2normalize_;
      break;
    }
    case kSoftmax: {
      delete layer_.softmax_;
      break;
    }
    case kLrn: {
      delete layer_.lrn_;
      break;
    }
    case kCrop: {
      delete layer_.crop_;
      break;
    }
    case kPadding: {
      delete layer_.padding_;
      break;
    }
    case kUpsample: {
      delete layer_.upsample_;
      break;
    }
    case kUnary: {
      delete layer_.unary_;
      break;
    }
    case kAdd: {
      delete layer_.add_;
      break;
    }
    case kMultiply: {
      delete layer_.multiply_;
      break;
    }
    case kAverage: {
      delete layer_.average_;
      break;
    }
    case kScale: {
      delete layer_.scale_;
      break;
    }
    case kBias: {
      delete layer_.bias_;
      break;
    }
    case kMax: {
      delete layer_.max_;
      break;
    }
    case kMin: {
      delete layer_.min_;
      break;
    }
    case kDot: {
      delete layer_.dot_;
      break;
    }
    case kReduce: {
      delete layer_.reduce_;
      break;
    }
    case kLoadConstant: {
      delete layer_.loadconstant_;
      break;
    }
    case kReshape: {
      delete layer_.reshape_;
      break;
    }
    case kFlatten: {
      delete layer_.flatten_;
      break;
    }
    case kPermute: {
      delete layer_.permute_;
      break;
    }
    case kConcat: {
      delete layer_.concat_;
      break;
    }
    case kSplit: {
      delete layer_.split_;
      break;
    }
    case kSequenceRepeat: {
      delete layer_.sequencerepeat_;
      break;
    }
    case kSimpleRecurrent: {
      delete layer_.simplerecurrent_;
      break;
    }
    case kGru: {
      delete layer_.gru_;
      break;
    }
    case kUniDirectionalLSTM: {
      delete layer_.unidirectionallstm_;
      break;
    }
    case kBiDirectionalLSTM: {
      delete layer_.bidirectionallstm_;
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LAYER_NOT_SET;
}


void NeuralNetworkLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkLayer)
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  input_.Clear();
  output_.Clear();
  clear_layer();
}

bool NeuralNetworkLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional string name = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->name().data(), this->name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.name"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_input;
        break;
      }

      // repeated string input = 2;
      case 2: {
        if (tag == 18) {
         parse_input:
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input(this->input_size() - 1).data(),
            this->input(this->input_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.input"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_input;
        if (input->ExpectTag(26)) goto parse_output;
        break;
      }

      // repeated string output = 3;
      case 3: {
        if (tag == 26) {
         parse_output:
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_output()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->output(this->output_size() - 1).data(),
            this->output(this->output_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.output"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(26)) goto parse_output;
        if (input->ExpectTag(802)) goto parse_convolution;
        break;
      }

      // optional .CoreML.Specification.ConvolutionLayerParams convolution = 100;
      case 100: {
        if (tag == 802) {
         parse_convolution:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_convolution()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.PoolingLayerParams pooling = 120;
      case 120: {
        if (tag == 962) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_pooling()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.ActivationParams activation = 130;
      case 130: {
        if (tag == 1042) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
      case 140: {
        if (tag == 1122) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_innerproduct()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.EmbeddingLayerParams embedding = 150;
      case 150: {
        if (tag == 1202) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_embedding()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
      case 160: {
        if (tag == 1282) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_batchnorm()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
      case 165: {
        if (tag == 1322) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mvn()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
      case 170: {
        if (tag == 1362) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_l2normalize()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.SoftmaxLayerParams softmax = 175;
      case 175: {
        if (tag == 1402) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softmax()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.LRNLayerParams lrn = 180;
      case 180: {
        if (tag == 1442) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lrn()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.CropLayerParams crop = 190;
      case 190: {
        if (tag == 1522) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_crop()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.PaddingLayerParams padding = 200;
      case 200: {
        if (tag == 1602) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_padding()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.UpsampleLayerParams upsample = 210;
      case 210: {
        if (tag == 1682) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_upsample()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
      case 220: {
        if (tag == 1762) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unary()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.AddLayerParams add = 230;
      case 230: {
        if (tag == 1842) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_add()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.MultiplyLayerParams multiply = 231;
      case 231: {
        if (tag == 1850) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multiply()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.AverageLayerParams average = 240;
      case 240: {
        if (tag == 1922) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_average()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.ScaleLayerParams scale = 245;
      case 245: {
        if (tag == 1962) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.BiasLayerParams bias = 250;
      case 250: {
        if (tag == 2002) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.MaxLayerParams max = 260;
      case 260: {
        if (tag == 2082) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_max()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.MinLayerParams min = 261;
      case 261: {
        if (tag == 2090) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_min()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.DotProductLayerParams dot = 270;
      case 270: {
        if (tag == 2162) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_dot()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.ReduceLayerParams reduce = 280;
      case 280: {
        if (tag == 2242) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reduce()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
      case 290: {
        if (tag == 2322) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loadconstant()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.ReshapeLayerParams reshape = 300;
      case 300: {
        if (tag == 2402) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshape()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.FlattenLayerParams flatten = 301;
      case 301: {
        if (tag == 2410) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_flatten()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.PermuteLayerParams permute = 310;
      case 310: {
        if (tag == 2482) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_permute()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.ConcatLayerParams concat = 320;
      case 320: {
        if (tag == 2562) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_concat()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.SplitLayerParams split = 330;
      case 330: {
        if (tag == 2642) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_split()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
      case 340: {
        if (tag == 2722) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sequencerepeat()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
      case 400: {
        if (tag == 3202) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_simplerecurrent()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.GRULayerParams gru = 410;
      case 410: {
        if (tag == 3282) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gru()));
        } else {
          goto handle_unusual;
        }
        goto after_bidirectionallstm;
        break;
      }

      // optional .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
      case 420: {
        if (tag == 3362) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(3442)) goto parse_biDirectionalLSTM;
        break;
      }

      // optional .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
      case 430: {
        if (tag == 3442) {
         parse_biDirectionalLSTM:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bidirectionallstm()));
        } else {
          goto handle_unusual;
        }
       after_bidirectionallstm:
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkLayer)
  return false;
#undef DO_
}

void NeuralNetworkLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkLayer)
  // optional string name = 1;
  if (this->name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->name().data(), this->name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->name(), output);
  }

  // repeated string input = 2;
  for (int i = 0; i < this->input_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input(i).data(), this->input(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      2, this->input(i), output);
  }

  // repeated string output = 3;
  for (int i = 0; i < this->output_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output(i).data(), this->output(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.output");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      3, this->output(i), output);
  }

  // optional .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  if (has_convolution()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *layer_.convolution_, output);
  }

  // optional .CoreML.Specification.PoolingLayerParams pooling = 120;
  if (has_pooling()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      120, *layer_.pooling_, output);
  }

  // optional .CoreML.Specification.ActivationParams activation = 130;
  if (has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      130, *layer_.activation_, output);
  }

  // optional .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  if (has_innerproduct()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      140, *layer_.innerproduct_, output);
  }

  // optional .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  if (has_embedding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      150, *layer_.embedding_, output);
  }

  // optional .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  if (has_batchnorm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      160, *layer_.batchnorm_, output);
  }

  // optional .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  if (has_mvn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      165, *layer_.mvn_, output);
  }

  // optional .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  if (has_l2normalize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      170, *layer_.l2normalize_, output);
  }

  // optional .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  if (has_softmax()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      175, *layer_.softmax_, output);
  }

  // optional .CoreML.Specification.LRNLayerParams lrn = 180;
  if (has_lrn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      180, *layer_.lrn_, output);
  }

  // optional .CoreML.Specification.CropLayerParams crop = 190;
  if (has_crop()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      190, *layer_.crop_, output);
  }

  // optional .CoreML.Specification.PaddingLayerParams padding = 200;
  if (has_padding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      200, *layer_.padding_, output);
  }

  // optional .CoreML.Specification.UpsampleLayerParams upsample = 210;
  if (has_upsample()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      210, *layer_.upsample_, output);
  }

  // optional .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  if (has_unary()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      220, *layer_.unary_, output);
  }

  // optional .CoreML.Specification.AddLayerParams add = 230;
  if (has_add()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      230, *layer_.add_, output);
  }

  // optional .CoreML.Specification.MultiplyLayerParams multiply = 231;
  if (has_multiply()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      231, *layer_.multiply_, output);
  }

  // optional .CoreML.Specification.AverageLayerParams average = 240;
  if (has_average()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      240, *layer_.average_, output);
  }

  // optional .CoreML.Specification.ScaleLayerParams scale = 245;
  if (has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      245, *layer_.scale_, output);
  }

  // optional .CoreML.Specification.BiasLayerParams bias = 250;
  if (has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      250, *layer_.bias_, output);
  }

  // optional .CoreML.Specification.MaxLayerParams max = 260;
  if (has_max()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      260, *layer_.max_, output);
  }

  // optional .CoreML.Specification.MinLayerParams min = 261;
  if (has_min()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      261, *layer_.min_, output);
  }

  // optional .CoreML.Specification.DotProductLayerParams dot = 270;
  if (has_dot()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      270, *layer_.dot_, output);
  }

  // optional .CoreML.Specification.ReduceLayerParams reduce = 280;
  if (has_reduce()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      280, *layer_.reduce_, output);
  }

  // optional .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  if (has_loadconstant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      290, *layer_.loadconstant_, output);
  }

  // optional .CoreML.Specification.ReshapeLayerParams reshape = 300;
  if (has_reshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      300, *layer_.reshape_, output);
  }

  // optional .CoreML.Specification.FlattenLayerParams flatten = 301;
  if (has_flatten()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      301, *layer_.flatten_, output);
  }

  // optional .CoreML.Specification.PermuteLayerParams permute = 310;
  if (has_permute()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      310, *layer_.permute_, output);
  }

  // optional .CoreML.Specification.ConcatLayerParams concat = 320;
  if (has_concat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      320, *layer_.concat_, output);
  }

  // optional .CoreML.Specification.SplitLayerParams split = 330;
  if (has_split()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      330, *layer_.split_, output);
  }

  // optional .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  if (has_sequencerepeat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      340, *layer_.sequencerepeat_, output);
  }

  // optional .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  if (has_simplerecurrent()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      400, *layer_.simplerecurrent_, output);
  }

  // optional .CoreML.Specification.GRULayerParams gru = 410;
  if (has_gru()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      410, *layer_.gru_, output);
  }

  // optional .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  if (has_unidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      420, *layer_.unidirectionallstm_, output);
  }

  // optional .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  if (has_bidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      430, *layer_.bidirectionallstm_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkLayer)
}

size_t NeuralNetworkLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkLayer)
  size_t total_size = 0;

  // optional string name = 1;
  if (this->name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->name());
  }

  // repeated string input = 2;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->input_size());
  for (int i = 0; i < this->input_size(); i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->input(i));
  }

  // repeated string output = 3;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->output_size());
  for (int i = 0; i < this->output_size(); i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->output(i));
  }

  switch (layer_case()) {
    // optional .CoreML.Specification.ConvolutionLayerParams convolution = 100;
    case kConvolution: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.convolution_);
      break;
    }
    // optional .CoreML.Specification.PoolingLayerParams pooling = 120;
    case kPooling: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.pooling_);
      break;
    }
    // optional .CoreML.Specification.ActivationParams activation = 130;
    case kActivation: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.activation_);
      break;
    }
    // optional .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
    case kInnerProduct: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.innerproduct_);
      break;
    }
    // optional .CoreML.Specification.EmbeddingLayerParams embedding = 150;
    case kEmbedding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.embedding_);
      break;
    }
    // optional .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
    case kBatchnorm: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.batchnorm_);
      break;
    }
    // optional .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
    case kMvn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.mvn_);
      break;
    }
    // optional .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
    case kL2Normalize: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.l2normalize_);
      break;
    }
    // optional .CoreML.Specification.SoftmaxLayerParams softmax = 175;
    case kSoftmax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.softmax_);
      break;
    }
    // optional .CoreML.Specification.LRNLayerParams lrn = 180;
    case kLrn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lrn_);
      break;
    }
    // optional .CoreML.Specification.CropLayerParams crop = 190;
    case kCrop: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.crop_);
      break;
    }
    // optional .CoreML.Specification.PaddingLayerParams padding = 200;
    case kPadding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.padding_);
      break;
    }
    // optional .CoreML.Specification.UpsampleLayerParams upsample = 210;
    case kUpsample: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.upsample_);
      break;
    }
    // optional .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
    case kUnary: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unary_);
      break;
    }
    // optional .CoreML.Specification.AddLayerParams add = 230;
    case kAdd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.add_);
      break;
    }
    // optional .CoreML.Specification.MultiplyLayerParams multiply = 231;
    case kMultiply: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.multiply_);
      break;
    }
    // optional .CoreML.Specification.AverageLayerParams average = 240;
    case kAverage: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.average_);
      break;
    }
    // optional .CoreML.Specification.ScaleLayerParams scale = 245;
    case kScale: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scale_);
      break;
    }
    // optional .CoreML.Specification.BiasLayerParams bias = 250;
    case kBias: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bias_);
      break;
    }
    // optional .CoreML.Specification.MaxLayerParams max = 260;
    case kMax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.max_);
      break;
    }
    // optional .CoreML.Specification.MinLayerParams min = 261;
    case kMin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.min_);
      break;
    }
    // optional .CoreML.Specification.DotProductLayerParams dot = 270;
    case kDot: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.dot_);
      break;
    }
    // optional .CoreML.Specification.ReduceLayerParams reduce = 280;
    case kReduce: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reduce_);
      break;
    }
    // optional .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
    case kLoadConstant: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loadconstant_);
      break;
    }
    // optional .CoreML.Specification.ReshapeLayerParams reshape = 300;
    case kReshape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshape_);
      break;
    }
    // optional .CoreML.Specification.FlattenLayerParams flatten = 301;
    case kFlatten: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.flatten_);
      break;
    }
    // optional .CoreML.Specification.PermuteLayerParams permute = 310;
    case kPermute: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.permute_);
      break;
    }
    // optional .CoreML.Specification.ConcatLayerParams concat = 320;
    case kConcat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.concat_);
      break;
    }
    // optional .CoreML.Specification.SplitLayerParams split = 330;
    case kSplit: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.split_);
      break;
    }
    // optional .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
    case kSequenceRepeat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sequencerepeat_);
      break;
    }
    // optional .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
    case kSimpleRecurrent: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.simplerecurrent_);
      break;
    }
    // optional .CoreML.Specification.GRULayerParams gru = 410;
    case kGru: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gru_);
      break;
    }
    // optional .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
    case kUniDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unidirectionallstm_);
      break;
    }
    // optional .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
    case kBiDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bidirectionallstm_);
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkLayer*>(&from));
}

void NeuralNetworkLayer::MergeFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkLayer)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetworkLayer::UnsafeMergeFrom(const NeuralNetworkLayer& from) {
  GOOGLE_DCHECK(&from != this);
  input_.UnsafeMergeFrom(from.input_);
  output_.UnsafeMergeFrom(from.output_);
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  if (from.name().size() > 0) {

    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
}

void NeuralNetworkLayer::CopyFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkLayer)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetworkLayer::IsInitialized() const {

  return true;
}

void NeuralNetworkLayer::Swap(NeuralNetworkLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkLayer::InternalSwap(NeuralNetworkLayer* other) {
  name_.Swap(&other->name_);
  input_.UnsafeArenaSwap(&other->input_);
  output_.UnsafeArenaSwap(&other->output_);
  std::swap(layer_, other->layer_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkLayer::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkLayer

// optional string name = 1;
void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.GetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
void NeuralNetworkLayer::set_name(const char* value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.name)
}
void NeuralNetworkLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.name)
}
::std::string* NeuralNetworkLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
int NeuralNetworkLayer::input_size() const {
  return input_.size();
}
void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
const ::std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
void NeuralNetworkLayer::set_input(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(value);
}
void NeuralNetworkLayer::set_input(int index, const char* value) {
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
::std::string* NeuralNetworkLayer::add_input() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Add();
}
void NeuralNetworkLayer::add_input(const ::std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::add_input(const char* value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
int NeuralNetworkLayer::output_size() const {
  return output_.size();
}
void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
const ::std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
void NeuralNetworkLayer::set_output(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(value);
}
void NeuralNetworkLayer::set_output(int index, const char* value) {
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
::std::string* NeuralNetworkLayer::add_output() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Add();
}
void NeuralNetworkLayer::add_output(const ::std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::add_output(const char* value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// optional .CoreML.Specification.ConvolutionLayerParams convolution = 100;
bool NeuralNetworkLayer::has_convolution() const {
  return layer_case() == kConvolution;
}
void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
void NeuralNetworkLayer::clear_convolution() {
  if (has_convolution()) {
    delete layer_.convolution_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return has_convolution()
      ? *layer_.convolution_
      : ::CoreML::Specification::ConvolutionLayerParams::default_instance();
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  if (!has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = new ::CoreML::Specification::ConvolutionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return layer_.convolution_;
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}

// optional .CoreML.Specification.PoolingLayerParams pooling = 120;
bool NeuralNetworkLayer::has_pooling() const {
  return layer_case() == kPooling;
}
void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
void NeuralNetworkLayer::clear_pooling() {
  if (has_pooling()) {
    delete layer_.pooling_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return has_pooling()
      ? *layer_.pooling_
      : ::CoreML::Specification::PoolingLayerParams::default_instance();
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  if (!has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = new ::CoreML::Specification::PoolingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return layer_.pooling_;
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}

// optional .CoreML.Specification.ActivationParams activation = 130;
bool NeuralNetworkLayer::has_activation() const {
  return layer_case() == kActivation;
}
void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
void NeuralNetworkLayer::clear_activation() {
  if (has_activation()) {
    delete layer_.activation_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return has_activation()
      ? *layer_.activation_
      : ::CoreML::Specification::ActivationParams::default_instance();
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  if (!has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return layer_.activation_;
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}

// optional .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
bool NeuralNetworkLayer::has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
void NeuralNetworkLayer::clear_innerproduct() {
  if (has_innerproduct()) {
    delete layer_.innerproduct_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return has_innerproduct()
      ? *layer_.innerproduct_
      : ::CoreML::Specification::InnerProductLayerParams::default_instance();
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  if (!has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = new ::CoreML::Specification::InnerProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return layer_.innerproduct_;
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}

// optional .CoreML.Specification.EmbeddingLayerParams embedding = 150;
bool NeuralNetworkLayer::has_embedding() const {
  return layer_case() == kEmbedding;
}
void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
void NeuralNetworkLayer::clear_embedding() {
  if (has_embedding()) {
    delete layer_.embedding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return has_embedding()
      ? *layer_.embedding_
      : ::CoreML::Specification::EmbeddingLayerParams::default_instance();
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  if (!has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = new ::CoreML::Specification::EmbeddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return layer_.embedding_;
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}

// optional .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
bool NeuralNetworkLayer::has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
void NeuralNetworkLayer::clear_batchnorm() {
  if (has_batchnorm()) {
    delete layer_.batchnorm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return has_batchnorm()
      ? *layer_.batchnorm_
      : ::CoreML::Specification::BatchnormLayerParams::default_instance();
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  if (!has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = new ::CoreML::Specification::BatchnormLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return layer_.batchnorm_;
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}

// optional .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
bool NeuralNetworkLayer::has_mvn() const {
  return layer_case() == kMvn;
}
void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
void NeuralNetworkLayer::clear_mvn() {
  if (has_mvn()) {
    delete layer_.mvn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return has_mvn()
      ? *layer_.mvn_
      : ::CoreML::Specification::MeanVarianceNormalizeLayerParams::default_instance();
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  if (!has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = new ::CoreML::Specification::MeanVarianceNormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return layer_.mvn_;
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}

// optional .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
bool NeuralNetworkLayer::has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
void NeuralNetworkLayer::clear_l2normalize() {
  if (has_l2normalize()) {
    delete layer_.l2normalize_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return has_l2normalize()
      ? *layer_.l2normalize_
      : ::CoreML::Specification::L2NormalizeLayerParams::default_instance();
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  if (!has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = new ::CoreML::Specification::L2NormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return layer_.l2normalize_;
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}

// optional .CoreML.Specification.SoftmaxLayerParams softmax = 175;
bool NeuralNetworkLayer::has_softmax() const {
  return layer_case() == kSoftmax;
}
void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
void NeuralNetworkLayer::clear_softmax() {
  if (has_softmax()) {
    delete layer_.softmax_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return has_softmax()
      ? *layer_.softmax_
      : ::CoreML::Specification::SoftmaxLayerParams::default_instance();
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  if (!has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = new ::CoreML::Specification::SoftmaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return layer_.softmax_;
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}

// optional .CoreML.Specification.LRNLayerParams lrn = 180;
bool NeuralNetworkLayer::has_lrn() const {
  return layer_case() == kLrn;
}
void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
void NeuralNetworkLayer::clear_lrn() {
  if (has_lrn()) {
    delete layer_.lrn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return has_lrn()
      ? *layer_.lrn_
      : ::CoreML::Specification::LRNLayerParams::default_instance();
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  if (!has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = new ::CoreML::Specification::LRNLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return layer_.lrn_;
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}

// optional .CoreML.Specification.CropLayerParams crop = 190;
bool NeuralNetworkLayer::has_crop() const {
  return layer_case() == kCrop;
}
void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
void NeuralNetworkLayer::clear_crop() {
  if (has_crop()) {
    delete layer_.crop_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return has_crop()
      ? *layer_.crop_
      : ::CoreML::Specification::CropLayerParams::default_instance();
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  if (!has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = new ::CoreML::Specification::CropLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return layer_.crop_;
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}

// optional .CoreML.Specification.PaddingLayerParams padding = 200;
bool NeuralNetworkLayer::has_padding() const {
  return layer_case() == kPadding;
}
void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
void NeuralNetworkLayer::clear_padding() {
  if (has_padding()) {
    delete layer_.padding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return has_padding()
      ? *layer_.padding_
      : ::CoreML::Specification::PaddingLayerParams::default_instance();
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  if (!has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = new ::CoreML::Specification::PaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return layer_.padding_;
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}

// optional .CoreML.Specification.UpsampleLayerParams upsample = 210;
bool NeuralNetworkLayer::has_upsample() const {
  return layer_case() == kUpsample;
}
void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
void NeuralNetworkLayer::clear_upsample() {
  if (has_upsample()) {
    delete layer_.upsample_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return has_upsample()
      ? *layer_.upsample_
      : ::CoreML::Specification::UpsampleLayerParams::default_instance();
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  if (!has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = new ::CoreML::Specification::UpsampleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return layer_.upsample_;
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}

// optional .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
bool NeuralNetworkLayer::has_unary() const {
  return layer_case() == kUnary;
}
void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
void NeuralNetworkLayer::clear_unary() {
  if (has_unary()) {
    delete layer_.unary_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return has_unary()
      ? *layer_.unary_
      : ::CoreML::Specification::UnaryFunctionLayerParams::default_instance();
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  if (!has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = new ::CoreML::Specification::UnaryFunctionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return layer_.unary_;
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}

// optional .CoreML.Specification.AddLayerParams add = 230;
bool NeuralNetworkLayer::has_add() const {
  return layer_case() == kAdd;
}
void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
void NeuralNetworkLayer::clear_add() {
  if (has_add()) {
    delete layer_.add_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return has_add()
      ? *layer_.add_
      : ::CoreML::Specification::AddLayerParams::default_instance();
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  if (!has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = new ::CoreML::Specification::AddLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return layer_.add_;
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}

// optional .CoreML.Specification.MultiplyLayerParams multiply = 231;
bool NeuralNetworkLayer::has_multiply() const {
  return layer_case() == kMultiply;
}
void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
void NeuralNetworkLayer::clear_multiply() {
  if (has_multiply()) {
    delete layer_.multiply_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return has_multiply()
      ? *layer_.multiply_
      : ::CoreML::Specification::MultiplyLayerParams::default_instance();
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  if (!has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = new ::CoreML::Specification::MultiplyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return layer_.multiply_;
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}

// optional .CoreML.Specification.AverageLayerParams average = 240;
bool NeuralNetworkLayer::has_average() const {
  return layer_case() == kAverage;
}
void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
void NeuralNetworkLayer::clear_average() {
  if (has_average()) {
    delete layer_.average_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return has_average()
      ? *layer_.average_
      : ::CoreML::Specification::AverageLayerParams::default_instance();
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  if (!has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = new ::CoreML::Specification::AverageLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return layer_.average_;
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}

// optional .CoreML.Specification.ScaleLayerParams scale = 245;
bool NeuralNetworkLayer::has_scale() const {
  return layer_case() == kScale;
}
void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
void NeuralNetworkLayer::clear_scale() {
  if (has_scale()) {
    delete layer_.scale_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return has_scale()
      ? *layer_.scale_
      : ::CoreML::Specification::ScaleLayerParams::default_instance();
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  if (!has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = new ::CoreML::Specification::ScaleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return layer_.scale_;
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}

// optional .CoreML.Specification.BiasLayerParams bias = 250;
bool NeuralNetworkLayer::has_bias() const {
  return layer_case() == kBias;
}
void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
void NeuralNetworkLayer::clear_bias() {
  if (has_bias()) {
    delete layer_.bias_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return has_bias()
      ? *layer_.bias_
      : ::CoreML::Specification::BiasLayerParams::default_instance();
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  if (!has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = new ::CoreML::Specification::BiasLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return layer_.bias_;
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}

// optional .CoreML.Specification.MaxLayerParams max = 260;
bool NeuralNetworkLayer::has_max() const {
  return layer_case() == kMax;
}
void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
void NeuralNetworkLayer::clear_max() {
  if (has_max()) {
    delete layer_.max_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return has_max()
      ? *layer_.max_
      : ::CoreML::Specification::MaxLayerParams::default_instance();
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  if (!has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = new ::CoreML::Specification::MaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return layer_.max_;
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}

// optional .CoreML.Specification.MinLayerParams min = 261;
bool NeuralNetworkLayer::has_min() const {
  return layer_case() == kMin;
}
void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
void NeuralNetworkLayer::clear_min() {
  if (has_min()) {
    delete layer_.min_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return has_min()
      ? *layer_.min_
      : ::CoreML::Specification::MinLayerParams::default_instance();
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  if (!has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = new ::CoreML::Specification::MinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return layer_.min_;
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}

// optional .CoreML.Specification.DotProductLayerParams dot = 270;
bool NeuralNetworkLayer::has_dot() const {
  return layer_case() == kDot;
}
void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
void NeuralNetworkLayer::clear_dot() {
  if (has_dot()) {
    delete layer_.dot_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return has_dot()
      ? *layer_.dot_
      : ::CoreML::Specification::DotProductLayerParams::default_instance();
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  if (!has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = new ::CoreML::Specification::DotProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return layer_.dot_;
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}

// optional .CoreML.Specification.ReduceLayerParams reduce = 280;
bool NeuralNetworkLayer::has_reduce() const {
  return layer_case() == kReduce;
}
void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
void NeuralNetworkLayer::clear_reduce() {
  if (has_reduce()) {
    delete layer_.reduce_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return has_reduce()
      ? *layer_.reduce_
      : ::CoreML::Specification::ReduceLayerParams::default_instance();
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  if (!has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = new ::CoreML::Specification::ReduceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return layer_.reduce_;
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}

// optional .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
bool NeuralNetworkLayer::has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
void NeuralNetworkLayer::clear_loadconstant() {
  if (has_loadconstant()) {
    delete layer_.loadconstant_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return has_loadconstant()
      ? *layer_.loadconstant_
      : ::CoreML::Specification::LoadConstantLayerParams::default_instance();
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  if (!has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = new ::CoreML::Specification::LoadConstantLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return layer_.loadconstant_;
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}

// optional .CoreML.Specification.ReshapeLayerParams reshape = 300;
bool NeuralNetworkLayer::has_reshape() const {
  return layer_case() == kReshape;
}
void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
void NeuralNetworkLayer::clear_reshape() {
  if (has_reshape()) {
    delete layer_.reshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return has_reshape()
      ? *layer_.reshape_
      : ::CoreML::Specification::ReshapeLayerParams::default_instance();
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  if (!has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = new ::CoreML::Specification::ReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return layer_.reshape_;
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}

// optional .CoreML.Specification.FlattenLayerParams flatten = 301;
bool NeuralNetworkLayer::has_flatten() const {
  return layer_case() == kFlatten;
}
void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
void NeuralNetworkLayer::clear_flatten() {
  if (has_flatten()) {
    delete layer_.flatten_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return has_flatten()
      ? *layer_.flatten_
      : ::CoreML::Specification::FlattenLayerParams::default_instance();
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  if (!has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = new ::CoreML::Specification::FlattenLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return layer_.flatten_;
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}

// optional .CoreML.Specification.PermuteLayerParams permute = 310;
bool NeuralNetworkLayer::has_permute() const {
  return layer_case() == kPermute;
}
void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
void NeuralNetworkLayer::clear_permute() {
  if (has_permute()) {
    delete layer_.permute_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return has_permute()
      ? *layer_.permute_
      : ::CoreML::Specification::PermuteLayerParams::default_instance();
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  if (!has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = new ::CoreML::Specification::PermuteLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return layer_.permute_;
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}

// optional .CoreML.Specification.ConcatLayerParams concat = 320;
bool NeuralNetworkLayer::has_concat() const {
  return layer_case() == kConcat;
}
void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
void NeuralNetworkLayer::clear_concat() {
  if (has_concat()) {
    delete layer_.concat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return has_concat()
      ? *layer_.concat_
      : ::CoreML::Specification::ConcatLayerParams::default_instance();
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  if (!has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = new ::CoreML::Specification::ConcatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return layer_.concat_;
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}

// optional .CoreML.Specification.SplitLayerParams split = 330;
bool NeuralNetworkLayer::has_split() const {
  return layer_case() == kSplit;
}
void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
void NeuralNetworkLayer::clear_split() {
  if (has_split()) {
    delete layer_.split_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return has_split()
      ? *layer_.split_
      : ::CoreML::Specification::SplitLayerParams::default_instance();
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  if (!has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = new ::CoreML::Specification::SplitLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return layer_.split_;
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}

// optional .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
bool NeuralNetworkLayer::has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
void NeuralNetworkLayer::clear_sequencerepeat() {
  if (has_sequencerepeat()) {
    delete layer_.sequencerepeat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : ::CoreML::Specification::SequenceRepeatLayerParams::default_instance();
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  if (!has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = new ::CoreML::Specification::SequenceRepeatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return layer_.sequencerepeat_;
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}

// optional .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
bool NeuralNetworkLayer::has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
void NeuralNetworkLayer::clear_simplerecurrent() {
  if (has_simplerecurrent()) {
    delete layer_.simplerecurrent_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : ::CoreML::Specification::SimpleRecurrentLayerParams::default_instance();
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  if (!has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = new ::CoreML::Specification::SimpleRecurrentLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return layer_.simplerecurrent_;
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}

// optional .CoreML.Specification.GRULayerParams gru = 410;
bool NeuralNetworkLayer::has_gru() const {
  return layer_case() == kGru;
}
void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
void NeuralNetworkLayer::clear_gru() {
  if (has_gru()) {
    delete layer_.gru_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return has_gru()
      ? *layer_.gru_
      : ::CoreML::Specification::GRULayerParams::default_instance();
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  if (!has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = new ::CoreML::Specification::GRULayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return layer_.gru_;
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}

// optional .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (has_unidirectionallstm()) {
    delete layer_.unidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : ::CoreML::Specification::UniDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  if (!has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = new ::CoreML::Specification::UniDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return layer_.unidirectionallstm_;
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}

// optional .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (has_bidirectionallstm()) {
    delete layer_.bidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : ::CoreML::Specification::BiDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  if (!has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = new ::CoreML::Specification::BiDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return layer_.bidirectionallstm_;
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}

bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
inline const NeuralNetworkLayer* NeuralNetworkLayer::internal_default_instance() {
  return &NeuralNetworkLayer_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts_EdgeSizes::kStartEdgeSizeFieldNumber;
const int BorderAmounts_EdgeSizes::kEndEdgeSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}

void BorderAmounts_EdgeSizes::InitAsDefaultInstance() {
}

BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}

void BorderAmounts_EdgeSizes::SharedCtor() {
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  _cached_size_ = 0;
}

BorderAmounts_EdgeSizes::~BorderAmounts_EdgeSizes() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts.EdgeSizes)
  SharedDtor();
}

void BorderAmounts_EdgeSizes::SharedDtor() {
}

void BorderAmounts_EdgeSizes::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts_EdgeSizes& BorderAmounts_EdgeSizes::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts_EdgeSizes> BorderAmounts_EdgeSizes_default_instance_;

BorderAmounts_EdgeSizes* BorderAmounts_EdgeSizes::New(::google::protobuf::Arena* arena) const {
  BorderAmounts_EdgeSizes* n = new BorderAmounts_EdgeSizes;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts_EdgeSizes::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts.EdgeSizes)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(BorderAmounts_EdgeSizes, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<BorderAmounts_EdgeSizes*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(startedgesize_, endedgesize_);

#undef ZR_HELPER_
#undef ZR_

}

bool BorderAmounts_EdgeSizes::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 startEdgeSize = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &startedgesize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_endEdgeSize;
        break;
      }

      // optional uint64 endEdgeSize = 2;
      case 2: {
        if (tag == 16) {
         parse_endEdgeSize:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &endedgesize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts.EdgeSizes)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts.EdgeSizes)
  return false;
#undef DO_
}

void BorderAmounts_EdgeSizes::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  // optional uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->startedgesize(), output);
  }

  // optional uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->endedgesize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts.EdgeSizes)
}

size_t BorderAmounts_EdgeSizes::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  size_t total_size = 0;

  // optional uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->startedgesize());
  }

  // optional uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->endedgesize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts_EdgeSizes::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts_EdgeSizes*>(&from));
}

void BorderAmounts_EdgeSizes::MergeFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void BorderAmounts_EdgeSizes::UnsafeMergeFrom(const BorderAmounts_EdgeSizes& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.startedgesize() != 0) {
    set_startedgesize(from.startedgesize());
  }
  if (from.endedgesize() != 0) {
    set_endedgesize(from.endedgesize());
  }
}

void BorderAmounts_EdgeSizes::CopyFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool BorderAmounts_EdgeSizes::IsInitialized() const {

  return true;
}

void BorderAmounts_EdgeSizes::Swap(BorderAmounts_EdgeSizes* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts_EdgeSizes::InternalSwap(BorderAmounts_EdgeSizes* other) {
  std::swap(startedgesize_, other->startedgesize_);
  std::swap(endedgesize_, other->endedgesize_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts_EdgeSizes::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts.EdgeSizes";
}


// -------------------------------------------------------------------

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts::kBorderAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts::BorderAmounts()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts)
}

void BorderAmounts::InitAsDefaultInstance() {
}

BorderAmounts::BorderAmounts(const BorderAmounts& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts)
}

void BorderAmounts::SharedCtor() {
  _cached_size_ = 0;
}

BorderAmounts::~BorderAmounts() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts)
  SharedDtor();
}

void BorderAmounts::SharedDtor() {
}

void BorderAmounts::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts& BorderAmounts::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts> BorderAmounts_default_instance_;

BorderAmounts* BorderAmounts::New(::google::protobuf::Arena* arena) const {
  BorderAmounts* n = new BorderAmounts;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts)
  borderamounts_.Clear();
}

bool BorderAmounts::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
      case 10: {
        if (tag == 82) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_borderAmounts:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_borderamounts()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_loop_borderAmounts;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts)
  return false;
#undef DO_
}

void BorderAmounts::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts)
  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  for (unsigned int i = 0, n = this->borderamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->borderamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts)
}

size_t BorderAmounts::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts)
  size_t total_size = 0;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  {
    unsigned int count = this->borderamounts_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->borderamounts(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts*>(&from));
}

void BorderAmounts::MergeFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void BorderAmounts::UnsafeMergeFrom(const BorderAmounts& from) {
  GOOGLE_DCHECK(&from != this);
  borderamounts_.MergeFrom(from.borderamounts_);
}

void BorderAmounts::CopyFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool BorderAmounts::IsInitialized() const {

  return true;
}

void BorderAmounts::Swap(BorderAmounts* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts::InternalSwap(BorderAmounts* other) {
  borderamounts_.UnsafeArenaSwap(&other->borderamounts_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts_EdgeSizes

// optional uint64 startEdgeSize = 1;
void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return startedgesize_;
}
void BorderAmounts_EdgeSizes::set_startedgesize(::google::protobuf::uint64 value) {
  
  startedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// optional uint64 endEdgeSize = 2;
void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return endedgesize_;
}
void BorderAmounts_EdgeSizes::set_endedgesize(::google::protobuf::uint64 value) {
  
  endedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

inline const BorderAmounts_EdgeSizes* BorderAmounts_EdgeSizes::internal_default_instance() {
  return &BorderAmounts_EdgeSizes_default_instance_.get();
}
// -------------------------------------------------------------------

// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
int BorderAmounts::borderamounts_size() const {
  return borderamounts_.size();
}
void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Get(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

inline const BorderAmounts* BorderAmounts::internal_default_instance() {
  return &BorderAmounts_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ValidPadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ValidPadding::ValidPadding()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ValidPadding)
}

void ValidPadding::InitAsDefaultInstance() {
  paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
}

ValidPadding::ValidPadding(const ValidPadding& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ValidPadding)
}

void ValidPadding::SharedCtor() {
  paddingamounts_ = NULL;
  _cached_size_ = 0;
}

ValidPadding::~ValidPadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ValidPadding)
  SharedDtor();
}

void ValidPadding::SharedDtor() {
  if (this != &ValidPadding_default_instance_.get()) {
    delete paddingamounts_;
  }
}

void ValidPadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ValidPadding& ValidPadding::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ValidPadding> ValidPadding_default_instance_;

ValidPadding* ValidPadding::New(::google::protobuf::Arena* arena) const {
  ValidPadding* n = new ValidPadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ValidPadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ValidPadding)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}

bool ValidPadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ValidPadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.BorderAmounts paddingAmounts = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ValidPadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ValidPadding)
  return false;
#undef DO_
}

void ValidPadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ValidPadding)
  // optional .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ValidPadding)
}

size_t ValidPadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ValidPadding)
  size_t total_size = 0;

  // optional .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ValidPadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ValidPadding*>(&from));
}

void ValidPadding::MergeFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ValidPadding)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ValidPadding::UnsafeMergeFrom(const ValidPadding& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
}

void ValidPadding::CopyFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ValidPadding)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ValidPadding::IsInitialized() const {

  return true;
}

void ValidPadding::Swap(ValidPadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ValidPadding::InternalSwap(ValidPadding* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ValidPadding::GetTypeName() const {
  return "CoreML.Specification.ValidPadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ValidPadding

// optional .CoreML.Specification.BorderAmounts paddingAmounts = 1;
bool ValidPadding::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void ValidPadding::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

inline const ValidPadding* ValidPadding::internal_default_instance() {
  return &ValidPadding_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

bool SamePadding_SamePaddingMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SamePadding_SamePaddingMode SamePadding::BOTTOM_RIGHT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::TOP_LEFT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MIN;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MAX;
const int SamePadding::SamePaddingMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SamePadding::kAsymmetryModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SamePadding::SamePadding()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SamePadding)
}

void SamePadding::InitAsDefaultInstance() {
}

SamePadding::SamePadding(const SamePadding& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SamePadding)
}

void SamePadding::SharedCtor() {
  asymmetrymode_ = 0;
  _cached_size_ = 0;
}

SamePadding::~SamePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SamePadding)
  SharedDtor();
}

void SamePadding::SharedDtor() {
}

void SamePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SamePadding& SamePadding::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<SamePadding> SamePadding_default_instance_;

SamePadding* SamePadding::New(::google::protobuf::Arena* arena) const {
  SamePadding* n = new SamePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SamePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SamePadding)
  asymmetrymode_ = 0;
}

bool SamePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SamePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
      case 1: {
        if (tag == 8) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_asymmetrymode(static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(value));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SamePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SamePadding)
  return false;
#undef DO_
}

void SamePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SamePadding)
  // optional .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->asymmetrymode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SamePadding)
}

size_t SamePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SamePadding)
  size_t total_size = 0;

  // optional .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->asymmetrymode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SamePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SamePadding*>(&from));
}

void SamePadding::MergeFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SamePadding)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void SamePadding::UnsafeMergeFrom(const SamePadding& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.asymmetrymode() != 0) {
    set_asymmetrymode(from.asymmetrymode());
  }
}

void SamePadding::CopyFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SamePadding)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool SamePadding::IsInitialized() const {

  return true;
}

void SamePadding::Swap(SamePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SamePadding::InternalSwap(SamePadding* other) {
  std::swap(asymmetrymode_, other->asymmetrymode_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SamePadding::GetTypeName() const {
  return "CoreML.Specification.SamePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SamePadding

// optional .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  
  asymmetrymode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

inline const SamePadding* SamePadding::internal_default_instance() {
  return &SamePadding_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int WeightParams::kFloatValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WeightParams::WeightParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WeightParams)
}

void WeightParams::InitAsDefaultInstance() {
}

WeightParams::WeightParams(const WeightParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WeightParams)
}

void WeightParams::SharedCtor() {
  _cached_size_ = 0;
}

WeightParams::~WeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WeightParams)
  SharedDtor();
}

void WeightParams::SharedDtor() {
}

void WeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WeightParams& WeightParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<WeightParams> WeightParams_default_instance_;

WeightParams* WeightParams::New(::google::protobuf::Arena* arena) const {
  WeightParams* n = new WeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WeightParams)
  floatvalue_.Clear();
}

bool WeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (tag == 13) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WeightParams)
  return false;
#undef DO_
}

void WeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WeightParams)
  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
  }
  for (int i = 0; i < this->floatvalue_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteFloatNoTag(
      this->floatvalue(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WeightParams)
}

size_t WeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WeightParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->floatvalue_size();
    data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WeightParams*>(&from));
}

void WeightParams::MergeFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WeightParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void WeightParams::UnsafeMergeFrom(const WeightParams& from) {
  GOOGLE_DCHECK(&from != this);
  floatvalue_.UnsafeMergeFrom(from.floatvalue_);
}

void WeightParams::CopyFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WeightParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool WeightParams::IsInitialized() const {

  return true;
}

void WeightParams::Swap(WeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WeightParams::InternalSwap(WeightParams* other) {
  floatvalue_.UnsafeArenaSwap(&other->floatvalue_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WeightParams::GetTypeName() const {
  return "CoreML.Specification.WeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WeightParams

// repeated float floatValue = 1;
int WeightParams::floatvalue_size() const {
  return floatvalue_.size();
}
void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_.Get(index);
}
void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
void WeightParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return &floatvalue_;
}

inline const WeightParams* WeightParams::internal_default_instance() {
  return &WeightParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConvolutionLayerParams::kOutputChannelsFieldNumber;
const int ConvolutionLayerParams::kKernelChannelsFieldNumber;
const int ConvolutionLayerParams::kNGroupsFieldNumber;
const int ConvolutionLayerParams::kKernelSizeFieldNumber;
const int ConvolutionLayerParams::kStrideFieldNumber;
const int ConvolutionLayerParams::kDilationFactorFieldNumber;
const int ConvolutionLayerParams::kValidFieldNumber;
const int ConvolutionLayerParams::kSameFieldNumber;
const int ConvolutionLayerParams::kIsDeconvolutionFieldNumber;
const int ConvolutionLayerParams::kHasBiasFieldNumber;
const int ConvolutionLayerParams::kWeightsFieldNumber;
const int ConvolutionLayerParams::kBiasFieldNumber;
const int ConvolutionLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConvolutionLayerParams::ConvolutionLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConvolutionLayerParams)
}

void ConvolutionLayerParams::InitAsDefaultInstance() {
  weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

ConvolutionLayerParams::ConvolutionLayerParams(const ConvolutionLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConvolutionLayerParams)
}

void ConvolutionLayerParams::SharedCtor() {
  weights_ = NULL;
  bias_ = NULL;
  ::memset(&outputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  _cached_size_ = 0;
}

ConvolutionLayerParams::~ConvolutionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConvolutionLayerParams)
  SharedDtor();
}

void ConvolutionLayerParams::SharedDtor() {
  if (has_ConvolutionPaddingType()) {
    clear_ConvolutionPaddingType();
  }
  if (this != &ConvolutionLayerParams_default_instance_.get()) {
    delete weights_;
    delete bias_;
  }
}

void ConvolutionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConvolutionLayerParams& ConvolutionLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ConvolutionLayerParams> ConvolutionLayerParams_default_instance_;

ConvolutionLayerParams* ConvolutionLayerParams::New(::google::protobuf::Arena* arena) const {
  ConvolutionLayerParams* n = new ConvolutionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConvolutionLayerParams::clear_ConvolutionPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ConvolutionLayerParams)
  switch (ConvolutionPaddingType_case()) {
    case kValid: {
      delete ConvolutionPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete ConvolutionPaddingType_.same_;
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}


void ConvolutionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConvolutionLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(ConvolutionLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<ConvolutionLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(outputchannels_, ngroups_);
  ZR_(isdeconvolution_, hasbias_);
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;

#undef ZR_HELPER_
#undef ZR_

  kernelsize_.Clear();
  stride_.Clear();
  dilationfactor_.Clear();
  outputshape_.Clear();
  clear_ConvolutionPaddingType();
}

bool ConvolutionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConvolutionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 outputChannels = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_kernelChannels;
        break;
      }

      // optional uint64 kernelChannels = 2;
      case 2: {
        if (tag == 16) {
         parse_kernelChannels:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &kernelchannels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(80)) goto parse_nGroups;
        break;
      }

      // optional uint64 nGroups = 10;
      case 10: {
        if (tag == 80) {
         parse_nGroups:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &ngroups_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_kernelSize;
        break;
      }

      // repeated uint64 kernelSize = 20;
      case 20: {
        if (tag == 162) {
         parse_kernelSize:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (tag == 160) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(242)) goto parse_stride;
        break;
      }

      // repeated uint64 stride = 30;
      case 30: {
        if (tag == 242) {
         parse_stride:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (tag == 240) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 242, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(322)) goto parse_dilationFactor;
        break;
      }

      // repeated uint64 dilationFactor = 40;
      case 40: {
        if (tag == 322) {
         parse_dilationFactor:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_dilationfactor())));
        } else if (tag == 320) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 322, input, this->mutable_dilationfactor())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(402)) goto parse_valid;
        break;
      }

      // optional .CoreML.Specification.ValidPadding valid = 50;
      case 50: {
        if (tag == 402) {
         parse_valid:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        goto after_same;
        break;
      }

      // optional .CoreML.Specification.SamePadding same = 51;
      case 51: {
        if (tag == 410) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
       after_same:
        if (input->ExpectTag(480)) goto parse_isDeconvolution;
        break;
      }

      // optional bool isDeconvolution = 60;
      case 60: {
        if (tag == 480) {
         parse_isDeconvolution:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isdeconvolution_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(560)) goto parse_hasBias;
        break;
      }

      // optional bool hasBias = 70;
      case 70: {
        if (tag == 560) {
         parse_hasBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(722)) goto parse_weights;
        break;
      }

      // optional .CoreML.Specification.WeightParams weights = 90;
      case 90: {
        if (tag == 722) {
         parse_weights:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(730)) goto parse_bias;
        break;
      }

      // optional .CoreML.Specification.WeightParams bias = 91;
      case 91: {
        if (tag == 730) {
         parse_bias:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(802)) goto parse_outputShape;
        break;
      }

      // repeated uint64 outputShape = 100;
      case 100: {
        if (tag == 802) {
         parse_outputShape:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (tag == 800) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 802, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConvolutionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConvolutionLayerParams)
  return false;
#undef DO_
}

void ConvolutionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConvolutionLayerParams)
  // optional uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->outputchannels(), output);
  }

  // optional uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->kernelchannels(), output);
  }

  // optional uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(10, this->ngroups(), output);
  }

  // repeated uint64 kernelSize = 20;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0; i < this->kernelsize_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 30;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(30, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0; i < this->stride_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // repeated uint64 dilationFactor = 40;
  if (this->dilationfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(40, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_dilationfactor_cached_byte_size_);
  }
  for (int i = 0; i < this->dilationfactor_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->dilationfactor(i), output);
  }

  // optional .CoreML.Specification.ValidPadding valid = 50;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *ConvolutionPaddingType_.valid_, output);
  }

  // optional .CoreML.Specification.SamePadding same = 51;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *ConvolutionPaddingType_.same_, output);
  }

  // optional bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->isdeconvolution(), output);
  }

  // optional bool hasBias = 70;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(70, this->hasbias(), output);
  }

  // optional .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      90, *this->weights_, output);
  }

  // optional .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      91, *this->bias_, output);
  }

  // repeated uint64 outputShape = 100;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(100, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0; i < this->outputshape_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConvolutionLayerParams)
}

size_t ConvolutionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConvolutionLayerParams)
  size_t total_size = 0;

  // optional uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // optional uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->kernelchannels());
  }

  // optional uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->ngroups());
  }

  // optional bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    total_size += 2 + 1;
  }

  // optional bool hasBias = 70;
  if (this->hasbias() != 0) {
    total_size += 2 + 1;
  }

  // optional .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // optional .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // repeated uint64 kernelSize = 20;
  {
    size_t data_size = 0;
    unsigned int count = this->kernelsize_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->kernelsize(i));
    }
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 30;
  {
    size_t data_size = 0;
    unsigned int count = this->stride_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->stride(i));
    }
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 dilationFactor = 40;
  {
    size_t data_size = 0;
    unsigned int count = this->dilationfactor_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->dilationfactor(i));
    }
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _dilationfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 outputShape = 100;
  {
    size_t data_size = 0;
    unsigned int count = this->outputshape_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->outputshape(i));
    }
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  switch (ConvolutionPaddingType_case()) {
    // optional .CoreML.Specification.ValidPadding valid = 50;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.valid_);
      break;
    }
    // optional .CoreML.Specification.SamePadding same = 51;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.same_);
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConvolutionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConvolutionLayerParams*>(&from));
}

void ConvolutionLayerParams::MergeFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConvolutionLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ConvolutionLayerParams::UnsafeMergeFrom(const ConvolutionLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  kernelsize_.UnsafeMergeFrom(from.kernelsize_);
  stride_.UnsafeMergeFrom(from.stride_);
  dilationfactor_.UnsafeMergeFrom(from.dilationfactor_);
  outputshape_.UnsafeMergeFrom(from.outputshape_);
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.kernelchannels() != 0) {
    set_kernelchannels(from.kernelchannels());
  }
  if (from.ngroups() != 0) {
    set_ngroups(from.ngroups());
  }
  if (from.isdeconvolution() != 0) {
    set_isdeconvolution(from.isdeconvolution());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void ConvolutionLayerParams::CopyFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConvolutionLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ConvolutionLayerParams::IsInitialized() const {

  return true;
}

void ConvolutionLayerParams::Swap(ConvolutionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConvolutionLayerParams::InternalSwap(ConvolutionLayerParams* other) {
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(kernelchannels_, other->kernelchannels_);
  std::swap(ngroups_, other->ngroups_);
  kernelsize_.UnsafeArenaSwap(&other->kernelsize_);
  stride_.UnsafeArenaSwap(&other->stride_);
  dilationfactor_.UnsafeArenaSwap(&other->dilationfactor_);
  std::swap(isdeconvolution_, other->isdeconvolution_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  outputshape_.UnsafeArenaSwap(&other->outputshape_);
  std::swap(ConvolutionPaddingType_, other->ConvolutionPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConvolutionLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConvolutionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConvolutionLayerParams

// optional uint64 outputChannels = 1;
void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return outputchannels_;
}
void ConvolutionLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// optional uint64 kernelChannels = 2;
void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return kernelchannels_;
}
void ConvolutionLayerParams::set_kernelchannels(::google::protobuf::uint64 value) {
  
  kernelchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// optional uint64 nGroups = 10;
void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return ngroups_;
}
void ConvolutionLayerParams::set_ngroups(::google::protobuf::uint64 value) {
  
  ngroups_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
int ConvolutionLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void ConvolutionLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
void ConvolutionLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 30;
int ConvolutionLayerParams::stride_size() const {
  return stride_.size();
}
void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_.Get(index);
}
void ConvolutionLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
void ConvolutionLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return &stride_;
}

// repeated uint64 dilationFactor = 40;
int ConvolutionLayerParams::dilationfactor_size() const {
  return dilationfactor_.size();
}
void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_.Get(index);
}
void ConvolutionLayerParams::set_dilationfactor(int index, ::google::protobuf::uint64 value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
void ConvolutionLayerParams::add_dilationfactor(::google::protobuf::uint64 value) {
  dilationfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return &dilationfactor_;
}

// optional .CoreML.Specification.ValidPadding valid = 50;
bool ConvolutionLayerParams::has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void ConvolutionLayerParams::clear_valid() {
  if (has_valid()) {
    delete ConvolutionPaddingType_.valid_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return has_valid()
      ? *ConvolutionPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return ConvolutionPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}

// optional .CoreML.Specification.SamePadding same = 51;
bool ConvolutionLayerParams::has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void ConvolutionLayerParams::clear_same() {
  if (has_same()) {
    delete ConvolutionPaddingType_.same_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return has_same()
      ? *ConvolutionPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  if (!has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return ConvolutionPaddingType_.same_;
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}

// optional bool isDeconvolution = 60;
void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return isdeconvolution_;
}
void ConvolutionLayerParams::set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// optional bool hasBias = 70;
void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return hasbias_;
}
void ConvolutionLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// optional .CoreML.Specification.WeightParams weights = 90;
bool ConvolutionLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void ConvolutionLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// optional .CoreML.Specification.WeightParams bias = 91;
bool ConvolutionLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ConvolutionLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
int ConvolutionLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_.Get(index);
}
void ConvolutionLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
void ConvolutionLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return &outputshape_;
}

bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
inline const ConvolutionLayerParams* ConvolutionLayerParams::internal_default_instance() {
  return &ConvolutionLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InnerProductLayerParams::kInputChannelsFieldNumber;
const int InnerProductLayerParams::kOutputChannelsFieldNumber;
const int InnerProductLayerParams::kHasBiasFieldNumber;
const int InnerProductLayerParams::kWeightsFieldNumber;
const int InnerProductLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InnerProductLayerParams::InnerProductLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.InnerProductLayerParams)
}

void InnerProductLayerParams::InitAsDefaultInstance() {
  weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

InnerProductLayerParams::InnerProductLayerParams(const InnerProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.InnerProductLayerParams)
}

void InnerProductLayerParams::SharedCtor() {
  weights_ = NULL;
  bias_ = NULL;
  ::memset(&inputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

InnerProductLayerParams::~InnerProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.InnerProductLayerParams)
  SharedDtor();
}

void InnerProductLayerParams::SharedDtor() {
  if (this != &InnerProductLayerParams_default_instance_.get()) {
    delete weights_;
    delete bias_;
  }
}

void InnerProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const InnerProductLayerParams& InnerProductLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<InnerProductLayerParams> InnerProductLayerParams_default_instance_;

InnerProductLayerParams* InnerProductLayerParams::New(::google::protobuf::Arena* arena) const {
  InnerProductLayerParams* n = new InnerProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void InnerProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.InnerProductLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(InnerProductLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<InnerProductLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(inputchannels_, hasbias_);
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;

#undef ZR_HELPER_
#undef ZR_

}

bool InnerProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.InnerProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 inputChannels = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputchannels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_outputChannels;
        break;
      }

      // optional uint64 outputChannels = 2;
      case 2: {
        if (tag == 16) {
         parse_outputChannels:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(80)) goto parse_hasBias;
        break;
      }

      // optional bool hasBias = 10;
      case 10: {
        if (tag == 80) {
         parse_hasBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_weights;
        break;
      }

      // optional .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (tag == 162) {
         parse_weights:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(170)) goto parse_bias;
        break;
      }

      // optional .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (tag == 170) {
         parse_bias:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.InnerProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.InnerProductLayerParams)
  return false;
#undef DO_
}

void InnerProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.InnerProductLayerParams)
  // optional uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputchannels(), output);
  }

  // optional uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // optional bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // optional .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // optional .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.InnerProductLayerParams)
}

size_t InnerProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.InnerProductLayerParams)
  size_t total_size = 0;

  // optional uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputchannels());
  }

  // optional uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // optional bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  // optional .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // optional .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void InnerProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const InnerProductLayerParams*>(&from));
}

void InnerProductLayerParams::MergeFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.InnerProductLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void InnerProductLayerParams::UnsafeMergeFrom(const InnerProductLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.inputchannels() != 0) {
    set_inputchannels(from.inputchannels());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void InnerProductLayerParams::CopyFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.InnerProductLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool InnerProductLayerParams::IsInitialized() const {

  return true;
}

void InnerProductLayerParams::Swap(InnerProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void InnerProductLayerParams::InternalSwap(InnerProductLayerParams* other) {
  std::swap(inputchannels_, other->inputchannels_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string InnerProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.InnerProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// InnerProductLayerParams

// optional uint64 inputChannels = 1;
void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return inputchannels_;
}
void InnerProductLayerParams::set_inputchannels(::google::protobuf::uint64 value) {
  
  inputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// optional uint64 outputChannels = 2;
void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return outputchannels_;
}
void InnerProductLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// optional bool hasBias = 10;
void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return hasbias_;
}
void InnerProductLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// optional .CoreML.Specification.WeightParams weights = 20;
bool InnerProductLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void InnerProductLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// optional .CoreML.Specification.WeightParams bias = 21;
bool InnerProductLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void InnerProductLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

inline const InnerProductLayerParams* InnerProductLayerParams::internal_default_instance() {
  return &InnerProductLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EmbeddingLayerParams::kInputDimFieldNumber;
const int EmbeddingLayerParams::kOutputChannelsFieldNumber;
const int EmbeddingLayerParams::kHasBiasFieldNumber;
const int EmbeddingLayerParams::kWeightsFieldNumber;
const int EmbeddingLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EmbeddingLayerParams::EmbeddingLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EmbeddingLayerParams)
}

void EmbeddingLayerParams::InitAsDefaultInstance() {
  weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

EmbeddingLayerParams::EmbeddingLayerParams(const EmbeddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EmbeddingLayerParams)
}

void EmbeddingLayerParams::SharedCtor() {
  weights_ = NULL;
  bias_ = NULL;
  ::memset(&inputdim_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

EmbeddingLayerParams::~EmbeddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EmbeddingLayerParams)
  SharedDtor();
}

void EmbeddingLayerParams::SharedDtor() {
  if (this != &EmbeddingLayerParams_default_instance_.get()) {
    delete weights_;
    delete bias_;
  }
}

void EmbeddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EmbeddingLayerParams& EmbeddingLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<EmbeddingLayerParams> EmbeddingLayerParams_default_instance_;

EmbeddingLayerParams* EmbeddingLayerParams::New(::google::protobuf::Arena* arena) const {
  EmbeddingLayerParams* n = new EmbeddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EmbeddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EmbeddingLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(EmbeddingLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<EmbeddingLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(inputdim_, hasbias_);
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;

#undef ZR_HELPER_
#undef ZR_

}

bool EmbeddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EmbeddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 inputDim = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputdim_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_outputChannels;
        break;
      }

      // optional uint64 outputChannels = 2;
      case 2: {
        if (tag == 16) {
         parse_outputChannels:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(80)) goto parse_hasBias;
        break;
      }

      // optional bool hasBias = 10;
      case 10: {
        if (tag == 80) {
         parse_hasBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_weights;
        break;
      }

      // optional .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (tag == 162) {
         parse_weights:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(170)) goto parse_bias;
        break;
      }

      // optional .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (tag == 170) {
         parse_bias:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EmbeddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EmbeddingLayerParams)
  return false;
#undef DO_
}

void EmbeddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EmbeddingLayerParams)
  // optional uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputdim(), output);
  }

  // optional uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // optional bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // optional .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // optional .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EmbeddingLayerParams)
}

size_t EmbeddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EmbeddingLayerParams)
  size_t total_size = 0;

  // optional uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputdim());
  }

  // optional uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // optional bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  // optional .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // optional .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EmbeddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EmbeddingLayerParams*>(&from));
}

void EmbeddingLayerParams::MergeFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EmbeddingLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void EmbeddingLayerParams::UnsafeMergeFrom(const EmbeddingLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.inputdim() != 0) {
    set_inputdim(from.inputdim());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void EmbeddingLayerParams::CopyFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EmbeddingLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool EmbeddingLayerParams::IsInitialized() const {

  return true;
}

void EmbeddingLayerParams::Swap(EmbeddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EmbeddingLayerParams::InternalSwap(EmbeddingLayerParams* other) {
  std::swap(inputdim_, other->inputdim_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EmbeddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.EmbeddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EmbeddingLayerParams

// optional uint64 inputDim = 1;
void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return inputdim_;
}
void EmbeddingLayerParams::set_inputdim(::google::protobuf::uint64 value) {
  
  inputdim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// optional uint64 outputChannels = 2;
void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return outputchannels_;
}
void EmbeddingLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// optional bool hasBias = 10;
void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return hasbias_;
}
void EmbeddingLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// optional .CoreML.Specification.WeightParams weights = 20;
bool EmbeddingLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void EmbeddingLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// optional .CoreML.Specification.WeightParams bias = 21;
bool EmbeddingLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void EmbeddingLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

inline const EmbeddingLayerParams* EmbeddingLayerParams::internal_default_instance() {
  return &EmbeddingLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchnormLayerParams::kChannelsFieldNumber;
const int BatchnormLayerParams::kComputeMeanVarFieldNumber;
const int BatchnormLayerParams::kInstanceNormalizationFieldNumber;
const int BatchnormLayerParams::kEpsilonFieldNumber;
const int BatchnormLayerParams::kGammaFieldNumber;
const int BatchnormLayerParams::kBetaFieldNumber;
const int BatchnormLayerParams::kMeanFieldNumber;
const int BatchnormLayerParams::kVarianceFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchnormLayerParams::BatchnormLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BatchnormLayerParams)
}

void BatchnormLayerParams::InitAsDefaultInstance() {
  gamma_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  mean_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  variance_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

BatchnormLayerParams::BatchnormLayerParams(const BatchnormLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BatchnormLayerParams)
}

void BatchnormLayerParams::SharedCtor() {
  gamma_ = NULL;
  beta_ = NULL;
  mean_ = NULL;
  variance_ = NULL;
  ::memset(&channels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

BatchnormLayerParams::~BatchnormLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BatchnormLayerParams)
  SharedDtor();
}

void BatchnormLayerParams::SharedDtor() {
  if (this != &BatchnormLayerParams_default_instance_.get()) {
    delete gamma_;
    delete beta_;
    delete mean_;
    delete variance_;
  }
}

void BatchnormLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BatchnormLayerParams& BatchnormLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<BatchnormLayerParams> BatchnormLayerParams_default_instance_;

BatchnormLayerParams* BatchnormLayerParams::New(::google::protobuf::Arena* arena) const {
  BatchnormLayerParams* n = new BatchnormLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BatchnormLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BatchnormLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(BatchnormLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<BatchnormLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(channels_, epsilon_);
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;

#undef ZR_HELPER_
#undef ZR_

}

bool BatchnormLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BatchnormLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 channels = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &channels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(40)) goto parse_computeMeanVar;
        break;
      }

      // optional bool computeMeanVar = 5;
      case 5: {
        if (tag == 40) {
         parse_computeMeanVar:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &computemeanvar_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(48)) goto parse_instanceNormalization;
        break;
      }

      // optional bool instanceNormalization = 6;
      case 6: {
        if (tag == 48) {
         parse_instanceNormalization:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &instancenormalization_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(85)) goto parse_epsilon;
        break;
      }

      // optional float epsilon = 10;
      case 10: {
        if (tag == 85) {
         parse_epsilon:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(122)) goto parse_gamma;
        break;
      }

      // optional .CoreML.Specification.WeightParams gamma = 15;
      case 15: {
        if (tag == 122) {
         parse_gamma:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gamma()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(130)) goto parse_beta;
        break;
      }

      // optional .CoreML.Specification.WeightParams beta = 16;
      case 16: {
        if (tag == 130) {
         parse_beta:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(138)) goto parse_mean;
        break;
      }

      // optional .CoreML.Specification.WeightParams mean = 17;
      case 17: {
        if (tag == 138) {
         parse_mean:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mean()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(146)) goto parse_variance;
        break;
      }

      // optional .CoreML.Specification.WeightParams variance = 18;
      case 18: {
        if (tag == 146) {
         parse_variance:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_variance()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BatchnormLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BatchnormLayerParams)
  return false;
#undef DO_
}

void BatchnormLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BatchnormLayerParams)
  // optional uint64 channels = 1;
  if (this->channels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->channels(), output);
  }

  // optional bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(5, this->computemeanvar(), output);
  }

  // optional bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(6, this->instancenormalization(), output);
  }

  // optional float epsilon = 10;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->epsilon(), output);
  }

  // optional .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->gamma_, output);
  }

  // optional .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      16, *this->beta_, output);
  }

  // optional .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      17, *this->mean_, output);
  }

  // optional .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      18, *this->variance_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BatchnormLayerParams)
}

size_t BatchnormLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BatchnormLayerParams)
  size_t total_size = 0;

  // optional uint64 channels = 1;
  if (this->channels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->channels());
  }

  // optional bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    total_size += 1 + 1;
  }

  // optional bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    total_size += 1 + 1;
  }

  // optional float epsilon = 10;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // optional .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->gamma_);
  }

  // optional .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  // optional .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mean_);
  }

  // optional .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->variance_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BatchnormLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BatchnormLayerParams*>(&from));
}

void BatchnormLayerParams::MergeFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BatchnormLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void BatchnormLayerParams::UnsafeMergeFrom(const BatchnormLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.channels() != 0) {
    set_channels(from.channels());
  }
  if (from.computemeanvar() != 0) {
    set_computemeanvar(from.computemeanvar());
  }
  if (from.instancenormalization() != 0) {
    set_instancenormalization(from.instancenormalization());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.has_gamma()) {
    mutable_gamma()->::CoreML::Specification::WeightParams::MergeFrom(from.gamma());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
  if (from.has_mean()) {
    mutable_mean()->::CoreML::Specification::WeightParams::MergeFrom(from.mean());
  }
  if (from.has_variance()) {
    mutable_variance()->::CoreML::Specification::WeightParams::MergeFrom(from.variance());
  }
}

void BatchnormLayerParams::CopyFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BatchnormLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool BatchnormLayerParams::IsInitialized() const {

  return true;
}

void BatchnormLayerParams::Swap(BatchnormLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchnormLayerParams::InternalSwap(BatchnormLayerParams* other) {
  std::swap(channels_, other->channels_);
  std::swap(computemeanvar_, other->computemeanvar_);
  std::swap(instancenormalization_, other->instancenormalization_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(gamma_, other->gamma_);
  std::swap(beta_, other->beta_);
  std::swap(mean_, other->mean_);
  std::swap(variance_, other->variance_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BatchnormLayerParams::GetTypeName() const {
  return "CoreML.Specification.BatchnormLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BatchnormLayerParams

// optional uint64 channels = 1;
void BatchnormLayerParams::clear_channels() {
  channels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return channels_;
}
void BatchnormLayerParams::set_channels(::google::protobuf::uint64 value) {
  
  channels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// optional bool computeMeanVar = 5;
void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return computemeanvar_;
}
void BatchnormLayerParams::set_computemeanvar(bool value) {
  
  computemeanvar_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// optional bool instanceNormalization = 6;
void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return instancenormalization_;
}
void BatchnormLayerParams::set_instancenormalization(bool value) {
  
  instancenormalization_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// optional float epsilon = 10;
void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return epsilon_;
}
void BatchnormLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// optional .CoreML.Specification.WeightParams gamma = 15;
bool BatchnormLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
void BatchnormLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// optional .CoreML.Specification.WeightParams beta = 16;
bool BatchnormLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void BatchnormLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// optional .CoreML.Specification.WeightParams mean = 17;
bool BatchnormLayerParams::has_mean() const {
  return this != internal_default_instance() && mean_ != NULL;
}
void BatchnormLayerParams::clear_mean() {
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_ != NULL ? *mean_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {
  
  if (mean_ == NULL) {
    mean_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  delete mean_;
  mean_ = mean;
  if (mean) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// optional .CoreML.Specification.WeightParams variance = 18;
bool BatchnormLayerParams::has_variance() const {
  return this != internal_default_instance() && variance_ != NULL;
}
void BatchnormLayerParams::clear_variance() {
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_ != NULL ? *variance_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {
  
  if (variance_ == NULL) {
    variance_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  delete variance_;
  variance_ = variance;
  if (variance) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

inline const BatchnormLayerParams* BatchnormLayerParams::internal_default_instance() {
  return &BatchnormLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

bool PoolingLayerParams_PoolingType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const PoolingLayerParams_PoolingType PoolingLayerParams::MAX;
const PoolingLayerParams_PoolingType PoolingLayerParams::AVERAGE;
const PoolingLayerParams_PoolingType PoolingLayerParams::L2;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MIN;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MAX;
const int PoolingLayerParams::PoolingType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams_ValidCompletePadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

void PoolingLayerParams_ValidCompletePadding::InitAsDefaultInstance() {
}

PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

void PoolingLayerParams_ValidCompletePadding::SharedCtor() {
  _cached_size_ = 0;
}

PoolingLayerParams_ValidCompletePadding::~PoolingLayerParams_ValidCompletePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  SharedDtor();
}

void PoolingLayerParams_ValidCompletePadding::SharedDtor() {
}

void PoolingLayerParams_ValidCompletePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams_ValidCompletePadding& PoolingLayerParams_ValidCompletePadding::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams_ValidCompletePadding> PoolingLayerParams_ValidCompletePadding_default_instance_;

PoolingLayerParams_ValidCompletePadding* PoolingLayerParams_ValidCompletePadding::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams_ValidCompletePadding* n = new PoolingLayerParams_ValidCompletePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams_ValidCompletePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  paddingamounts_.Clear();
}

bool PoolingLayerParams_ValidCompletePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 paddingAmounts = 10;
      case 10: {
        if (tag == 82) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_paddingamounts())));
        } else if (tag == 80) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82, input, this->mutable_paddingamounts())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return false;
#undef DO_
}

void PoolingLayerParams_ValidCompletePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  // repeated uint64 paddingAmounts = 10;
  if (this->paddingamounts_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_paddingamounts_cached_byte_size_);
  }
  for (int i = 0; i < this->paddingamounts_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->paddingamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

size_t PoolingLayerParams_ValidCompletePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  size_t total_size = 0;

  // repeated uint64 paddingAmounts = 10;
  {
    size_t data_size = 0;
    unsigned int count = this->paddingamounts_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->paddingamounts(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _paddingamounts_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams_ValidCompletePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams_ValidCompletePadding*>(&from));
}

void PoolingLayerParams_ValidCompletePadding::MergeFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PoolingLayerParams_ValidCompletePadding::UnsafeMergeFrom(const PoolingLayerParams_ValidCompletePadding& from) {
  GOOGLE_DCHECK(&from != this);
  paddingamounts_.UnsafeMergeFrom(from.paddingamounts_);
}

void PoolingLayerParams_ValidCompletePadding::CopyFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PoolingLayerParams_ValidCompletePadding::IsInitialized() const {

  return true;
}

void PoolingLayerParams_ValidCompletePadding::Swap(PoolingLayerParams_ValidCompletePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams_ValidCompletePadding::InternalSwap(PoolingLayerParams_ValidCompletePadding* other) {
  paddingamounts_.UnsafeArenaSwap(&other->paddingamounts_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams_ValidCompletePadding::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams.ValidCompletePadding";
}


// -------------------------------------------------------------------

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams::kTypeFieldNumber;
const int PoolingLayerParams::kKernelSizeFieldNumber;
const int PoolingLayerParams::kStrideFieldNumber;
const int PoolingLayerParams::kValidFieldNumber;
const int PoolingLayerParams::kSameFieldNumber;
const int PoolingLayerParams::kIncludeLastPixelFieldNumber;
const int PoolingLayerParams::kAvgPoolExcludePaddingFieldNumber;
const int PoolingLayerParams::kGlobalPoolingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams::PoolingLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams)
}

void PoolingLayerParams::InitAsDefaultInstance() {
}

PoolingLayerParams::PoolingLayerParams(const PoolingLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams)
}

void PoolingLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  _cached_size_ = 0;
}

PoolingLayerParams::~PoolingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams)
  SharedDtor();
}

void PoolingLayerParams::SharedDtor() {
  if (has_PoolingPaddingType()) {
    clear_PoolingPaddingType();
  }
}

void PoolingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams& PoolingLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams> PoolingLayerParams_default_instance_;

PoolingLayerParams* PoolingLayerParams::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams* n = new PoolingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams::clear_PoolingPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PoolingLayerParams)
  switch (PoolingPaddingType_case()) {
    case kValid: {
      delete PoolingPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete PoolingPaddingType_.same_;
      break;
    }
    case kIncludeLastPixel: {
      delete PoolingPaddingType_.includelastpixel_;
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}


void PoolingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(PoolingLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<PoolingLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(type_, globalpooling_);

#undef ZR_HELPER_
#undef ZR_

  kernelsize_.Clear();
  stride_.Clear();
  clear_PoolingPaddingType();
}

bool PoolingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
      case 1: {
        if (tag == 8) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(value));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_kernelSize;
        break;
      }

      // repeated uint64 kernelSize = 10;
      case 10: {
        if (tag == 82) {
         parse_kernelSize:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (tag == 80) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_stride;
        break;
      }

      // repeated uint64 stride = 20;
      case 20: {
        if (tag == 162) {
         parse_stride:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (tag == 160) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(242)) goto parse_valid;
        break;
      }

      // optional .CoreML.Specification.ValidPadding valid = 30;
      case 30: {
        if (tag == 242) {
         parse_valid:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        goto after_includelastpixel;
        break;
      }

      // optional .CoreML.Specification.SamePadding same = 31;
      case 31: {
        if (tag == 250) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(258)) goto parse_includeLastPixel;
        break;
      }

      // optional .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
      case 32: {
        if (tag == 258) {
         parse_includeLastPixel:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_includelastpixel()));
        } else {
          goto handle_unusual;
        }
       after_includelastpixel:
        if (input->ExpectTag(400)) goto parse_avgPoolExcludePadding;
        break;
      }

      // optional bool avgPoolExcludePadding = 50;
      case 50: {
        if (tag == 400) {
         parse_avgPoolExcludePadding:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &avgpoolexcludepadding_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(480)) goto parse_globalPooling;
        break;
      }

      // optional bool globalPooling = 60;
      case 60: {
        if (tag == 480) {
         parse_globalPooling:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &globalpooling_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams)
  return false;
#undef DO_
}

void PoolingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams)
  // optional .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // repeated uint64 kernelSize = 10;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0; i < this->kernelsize_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 20;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0; i < this->stride_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // optional .CoreML.Specification.ValidPadding valid = 30;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *PoolingPaddingType_.valid_, output);
  }

  // optional .CoreML.Specification.SamePadding same = 31;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *PoolingPaddingType_.same_, output);
  }

  // optional .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  if (has_includelastpixel()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *PoolingPaddingType_.includelastpixel_, output);
  }

  // optional bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->avgpoolexcludepadding(), output);
  }

  // optional bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->globalpooling(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams)
}

size_t PoolingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // optional bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    total_size += 2 + 1;
  }

  // optional bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    total_size += 2 + 1;
  }

  // repeated uint64 kernelSize = 10;
  {
    size_t data_size = 0;
    unsigned int count = this->kernelsize_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->kernelsize(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 20;
  {
    size_t data_size = 0;
    unsigned int count = this->stride_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->stride(i));
    }
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  switch (PoolingPaddingType_case()) {
    // optional .CoreML.Specification.ValidPadding valid = 30;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.valid_);
      break;
    }
    // optional .CoreML.Specification.SamePadding same = 31;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.same_);
      break;
    }
    // optional .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
    case kIncludeLastPixel: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.includelastpixel_);
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams*>(&from));
}

void PoolingLayerParams::MergeFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PoolingLayerParams::UnsafeMergeFrom(const PoolingLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  kernelsize_.UnsafeMergeFrom(from.kernelsize_);
  stride_.UnsafeMergeFrom(from.stride_);
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.avgpoolexcludepadding() != 0) {
    set_avgpoolexcludepadding(from.avgpoolexcludepadding());
  }
  if (from.globalpooling() != 0) {
    set_globalpooling(from.globalpooling());
  }
}

void PoolingLayerParams::CopyFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PoolingLayerParams::IsInitialized() const {

  return true;
}

void PoolingLayerParams::Swap(PoolingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams::InternalSwap(PoolingLayerParams* other) {
  std::swap(type_, other->type_);
  kernelsize_.UnsafeArenaSwap(&other->kernelsize_);
  stride_.UnsafeArenaSwap(&other->stride_);
  std::swap(avgpoolexcludepadding_, other->avgpoolexcludepadding_);
  std::swap(globalpooling_, other->globalpooling_);
  std::swap(PoolingPaddingType_, other->PoolingPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return paddingamounts_.size();
}
void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_.Get(index);
}
void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, ::google::protobuf::uint64 value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(::google::protobuf::uint64 value) {
  paddingamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return &paddingamounts_;
}

inline const PoolingLayerParams_ValidCompletePadding* PoolingLayerParams_ValidCompletePadding::internal_default_instance() {
  return &PoolingLayerParams_ValidCompletePadding_default_instance_.get();
}
// -------------------------------------------------------------------

// PoolingLayerParams

// optional .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
void PoolingLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
int PoolingLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void PoolingLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
void PoolingLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 20;
int PoolingLayerParams::stride_size() const {
  return stride_.size();
}
void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return stride_.Get(index);
}
void PoolingLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
void PoolingLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return &stride_;
}

// optional .CoreML.Specification.ValidPadding valid = 30;
bool PoolingLayerParams::has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void PoolingLayerParams::clear_valid() {
  if (has_valid()) {
    delete PoolingPaddingType_.valid_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return has_valid()
      ? *PoolingPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return PoolingPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}

// optional .CoreML.Specification.SamePadding same = 31;
bool PoolingLayerParams::has_same() const {
  return PoolingPaddingType_case() == kSame;
}
void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void PoolingLayerParams::clear_same() {
  if (has_same()) {
    delete PoolingPaddingType_.same_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return has_same()
      ? *PoolingPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  if (!has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return PoolingPaddingType_.same_;
}
::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}

// optional .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
bool PoolingLayerParams::has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
void PoolingLayerParams::clear_includelastpixel() {
  if (has_includelastpixel()) {
    delete PoolingPaddingType_.includelastpixel_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::default_instance();
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  if (!has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = new ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return PoolingPaddingType_.includelastpixel_;
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}

// optional bool avgPoolExcludePadding = 50;
void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return avgpoolexcludepadding_;
}
void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {
  
  avgpoolexcludepadding_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// optional bool globalPooling = 60;
void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return globalpooling_;
}
void PoolingLayerParams::set_globalpooling(bool value) {
  
  globalpooling_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
inline const PoolingLayerParams* PoolingLayerParams::internal_default_instance() {
  return &PoolingLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams_PaddingConstant::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

void PaddingLayerParams_PaddingConstant::InitAsDefaultInstance() {
}

PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

void PaddingLayerParams_PaddingConstant::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingConstant::~PaddingLayerParams_PaddingConstant() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  SharedDtor();
}

void PaddingLayerParams_PaddingConstant::SharedDtor() {
}

void PaddingLayerParams_PaddingConstant::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingConstant& PaddingLayerParams_PaddingConstant::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingConstant> PaddingLayerParams_PaddingConstant_default_instance_;

PaddingLayerParams_PaddingConstant* PaddingLayerParams_PaddingConstant::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingConstant* n = new PaddingLayerParams_PaddingConstant;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingConstant::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  value_ = 0;
}

bool PaddingLayerParams_PaddingConstant::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float value = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingConstant::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  // optional float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

size_t PaddingLayerParams_PaddingConstant::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  size_t total_size = 0;

  // optional float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingConstant::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingConstant*>(&from));
}

void PaddingLayerParams_PaddingConstant::MergeFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PaddingLayerParams_PaddingConstant::UnsafeMergeFrom(const PaddingLayerParams_PaddingConstant& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.value() != 0) {
    set_value(from.value());
  }
}

void PaddingLayerParams_PaddingConstant::CopyFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PaddingLayerParams_PaddingConstant::IsInitialized() const {

  return true;
}

void PaddingLayerParams_PaddingConstant::Swap(PaddingLayerParams_PaddingConstant* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingConstant::InternalSwap(PaddingLayerParams_PaddingConstant* other) {
  std::swap(value_, other->value_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingConstant::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingConstant";
}


// -------------------------------------------------------------------

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

void PaddingLayerParams_PaddingReflection::InitAsDefaultInstance() {
}

PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

void PaddingLayerParams_PaddingReflection::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReflection::~PaddingLayerParams_PaddingReflection() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  SharedDtor();
}

void PaddingLayerParams_PaddingReflection::SharedDtor() {
}

void PaddingLayerParams_PaddingReflection::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReflection& PaddingLayerParams_PaddingReflection::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReflection> PaddingLayerParams_PaddingReflection_default_instance_;

PaddingLayerParams_PaddingReflection* PaddingLayerParams_PaddingReflection::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReflection* n = new PaddingLayerParams_PaddingReflection;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReflection::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

bool PaddingLayerParams_PaddingReflection::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReflection::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

size_t PaddingLayerParams_PaddingReflection::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReflection::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReflection*>(&from));
}

void PaddingLayerParams_PaddingReflection::MergeFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PaddingLayerParams_PaddingReflection::UnsafeMergeFrom(const PaddingLayerParams_PaddingReflection& from) {
  GOOGLE_DCHECK(&from != this);
}

void PaddingLayerParams_PaddingReflection::CopyFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PaddingLayerParams_PaddingReflection::IsInitialized() const {

  return true;
}

void PaddingLayerParams_PaddingReflection::Swap(PaddingLayerParams_PaddingReflection* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReflection::InternalSwap(PaddingLayerParams_PaddingReflection* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReflection::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReflection";
}


// -------------------------------------------------------------------

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

void PaddingLayerParams_PaddingReplication::InitAsDefaultInstance() {
}

PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

void PaddingLayerParams_PaddingReplication::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReplication::~PaddingLayerParams_PaddingReplication() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  SharedDtor();
}

void PaddingLayerParams_PaddingReplication::SharedDtor() {
}

void PaddingLayerParams_PaddingReplication::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReplication& PaddingLayerParams_PaddingReplication::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReplication> PaddingLayerParams_PaddingReplication_default_instance_;

PaddingLayerParams_PaddingReplication* PaddingLayerParams_PaddingReplication::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReplication* n = new PaddingLayerParams_PaddingReplication;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReplication::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

bool PaddingLayerParams_PaddingReplication::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReplication::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

size_t PaddingLayerParams_PaddingReplication::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReplication::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReplication*>(&from));
}

void PaddingLayerParams_PaddingReplication::MergeFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PaddingLayerParams_PaddingReplication::UnsafeMergeFrom(const PaddingLayerParams_PaddingReplication& from) {
  GOOGLE_DCHECK(&from != this);
}

void PaddingLayerParams_PaddingReplication::CopyFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PaddingLayerParams_PaddingReplication::IsInitialized() const {

  return true;
}

void PaddingLayerParams_PaddingReplication::Swap(PaddingLayerParams_PaddingReplication* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReplication::InternalSwap(PaddingLayerParams_PaddingReplication* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReplication::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReplication";
}


// -------------------------------------------------------------------

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams::kConstantFieldNumber;
const int PaddingLayerParams::kReflectionFieldNumber;
const int PaddingLayerParams::kReplicationFieldNumber;
const int PaddingLayerParams::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams::PaddingLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams)
}

void PaddingLayerParams::InitAsDefaultInstance() {
  paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
}

PaddingLayerParams::PaddingLayerParams(const PaddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams)
}

void PaddingLayerParams::SharedCtor() {
  paddingamounts_ = NULL;
  clear_has_PaddingType();
  _cached_size_ = 0;
}

PaddingLayerParams::~PaddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams)
  SharedDtor();
}

void PaddingLayerParams::SharedDtor() {
  if (has_PaddingType()) {
    clear_PaddingType();
  }
  if (this != &PaddingLayerParams_default_instance_.get()) {
    delete paddingamounts_;
  }
}

void PaddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams& PaddingLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams> PaddingLayerParams_default_instance_;

PaddingLayerParams* PaddingLayerParams::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams* n = new PaddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams::clear_PaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PaddingLayerParams)
  switch (PaddingType_case()) {
    case kConstant: {
      delete PaddingType_.constant_;
      break;
    }
    case kReflection: {
      delete PaddingType_.reflection_;
      break;
    }
    case kReplication: {
      delete PaddingType_.replication_;
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}


void PaddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
  clear_PaddingType();
}

bool PaddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_constant()));
        } else {
          goto handle_unusual;
        }
        goto after_replication;
        break;
      }

      // optional .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
      case 2: {
        if (tag == 18) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reflection()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(26)) goto parse_replication;
        break;
      }

      // optional .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
      case 3: {
        if (tag == 26) {
         parse_replication:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_replication()));
        } else {
          goto handle_unusual;
        }
       after_replication:
        if (input->ExpectTag(82)) goto parse_paddingAmounts;
        break;
      }

      // optional .CoreML.Specification.BorderAmounts paddingAmounts = 10;
      case 10: {
        if (tag == 82) {
         parse_paddingAmounts:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams)
  return false;
#undef DO_
}

void PaddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams)
  // optional .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  if (has_constant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *PaddingType_.constant_, output);
  }

  // optional .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  if (has_reflection()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *PaddingType_.reflection_, output);
  }

  // optional .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  if (has_replication()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *PaddingType_.replication_, output);
  }

  // optional .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams)
}

size_t PaddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  switch (PaddingType_case()) {
    // optional .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
    case kConstant: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.constant_);
      break;
    }
    // optional .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
    case kReflection: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.reflection_);
      break;
    }
    // optional .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
    case kReplication: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.replication_);
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams*>(&from));
}

void PaddingLayerParams::MergeFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PaddingLayerParams::UnsafeMergeFrom(const PaddingLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
}

void PaddingLayerParams::CopyFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PaddingLayerParams::IsInitialized() const {

  return true;
}

void PaddingLayerParams::Swap(PaddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams::InternalSwap(PaddingLayerParams* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(PaddingType_, other->PaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingConstant

// optional float value = 1;
void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return value_;
}
void PaddingLayerParams_PaddingConstant::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

inline const PaddingLayerParams_PaddingConstant* PaddingLayerParams_PaddingConstant::internal_default_instance() {
  return &PaddingLayerParams_PaddingConstant_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReflection

inline const PaddingLayerParams_PaddingReflection* PaddingLayerParams_PaddingReflection::internal_default_instance() {
  return &PaddingLayerParams_PaddingReflection_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReplication

inline const PaddingLayerParams_PaddingReplication* PaddingLayerParams_PaddingReplication::internal_default_instance() {
  return &PaddingLayerParams_PaddingReplication_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams

// optional .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
bool PaddingLayerParams::has_constant() const {
  return PaddingType_case() == kConstant;
}
void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
void PaddingLayerParams::clear_constant() {
  if (has_constant()) {
    delete PaddingType_.constant_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return has_constant()
      ? *PaddingType_.constant_
      : ::CoreML::Specification::PaddingLayerParams_PaddingConstant::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  if (!has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = new ::CoreML::Specification::PaddingLayerParams_PaddingConstant;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return PaddingType_.constant_;
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}

// optional .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
bool PaddingLayerParams::has_reflection() const {
  return PaddingType_case() == kReflection;
}
void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
void PaddingLayerParams::clear_reflection() {
  if (has_reflection()) {
    delete PaddingType_.reflection_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return has_reflection()
      ? *PaddingType_.reflection_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReflection::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  if (!has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReflection;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return PaddingType_.reflection_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}

// optional .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
bool PaddingLayerParams::has_replication() const {
  return PaddingType_case() == kReplication;
}
void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
void PaddingLayerParams::clear_replication() {
  if (has_replication()) {
    delete PaddingType_.replication_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return has_replication()
      ? *PaddingType_.replication_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReplication::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  if (!has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReplication;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return PaddingType_.replication_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}

// optional .CoreML.Specification.BorderAmounts paddingAmounts = 10;
bool PaddingLayerParams::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
inline const PaddingLayerParams* PaddingLayerParams::internal_default_instance() {
  return &PaddingLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConcatLayerParams::kSequenceConcatFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConcatLayerParams::ConcatLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConcatLayerParams)
}

void ConcatLayerParams::InitAsDefaultInstance() {
}

ConcatLayerParams::ConcatLayerParams(const ConcatLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConcatLayerParams)
}

void ConcatLayerParams::SharedCtor() {
  sequenceconcat_ = false;
  _cached_size_ = 0;
}

ConcatLayerParams::~ConcatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConcatLayerParams)
  SharedDtor();
}

void ConcatLayerParams::SharedDtor() {
}

void ConcatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConcatLayerParams& ConcatLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ConcatLayerParams> ConcatLayerParams_default_instance_;

ConcatLayerParams* ConcatLayerParams::New(::google::protobuf::Arena* arena) const {
  ConcatLayerParams* n = new ConcatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConcatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConcatLayerParams)
  sequenceconcat_ = false;
}

bool ConcatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConcatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional bool sequenceConcat = 100;
      case 100: {
        if (tag == 800) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceconcat_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConcatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConcatLayerParams)
  return false;
#undef DO_
}

void ConcatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConcatLayerParams)
  // optional bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->sequenceconcat(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConcatLayerParams)
}

size_t ConcatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConcatLayerParams)
  size_t total_size = 0;

  // optional bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConcatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConcatLayerParams*>(&from));
}

void ConcatLayerParams::MergeFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConcatLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ConcatLayerParams::UnsafeMergeFrom(const ConcatLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.sequenceconcat() != 0) {
    set_sequenceconcat(from.sequenceconcat());
  }
}

void ConcatLayerParams::CopyFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConcatLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ConcatLayerParams::IsInitialized() const {

  return true;
}

void ConcatLayerParams::Swap(ConcatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConcatLayerParams::InternalSwap(ConcatLayerParams* other) {
  std::swap(sequenceconcat_, other->sequenceconcat_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConcatLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConcatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConcatLayerParams

// optional bool sequenceConcat = 100;
void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return sequenceconcat_;
}
void ConcatLayerParams::set_sequenceconcat(bool value) {
  
  sequenceconcat_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

inline const ConcatLayerParams* ConcatLayerParams::internal_default_instance() {
  return &ConcatLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LRNLayerParams::kAlphaFieldNumber;
const int LRNLayerParams::kBetaFieldNumber;
const int LRNLayerParams::kLocalSizeFieldNumber;
const int LRNLayerParams::kKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LRNLayerParams::LRNLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LRNLayerParams)
}

void LRNLayerParams::InitAsDefaultInstance() {
}

LRNLayerParams::LRNLayerParams(const LRNLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LRNLayerParams)
}

void LRNLayerParams::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  _cached_size_ = 0;
}

LRNLayerParams::~LRNLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LRNLayerParams)
  SharedDtor();
}

void LRNLayerParams::SharedDtor() {
}

void LRNLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LRNLayerParams& LRNLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<LRNLayerParams> LRNLayerParams_default_instance_;

LRNLayerParams* LRNLayerParams::New(::google::protobuf::Arena* arena) const {
  LRNLayerParams* n = new LRNLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LRNLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LRNLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(LRNLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<LRNLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(alpha_, k_);

#undef ZR_HELPER_
#undef ZR_

}

bool LRNLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LRNLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(21)) goto parse_beta;
        break;
      }

      // optional float beta = 2;
      case 2: {
        if (tag == 21) {
         parse_beta:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(24)) goto parse_localSize;
        break;
      }

      // optional uint64 localSize = 3;
      case 3: {
        if (tag == 24) {
         parse_localSize:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &localsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(37)) goto parse_k;
        break;
      }

      // optional float k = 4;
      case 4: {
        if (tag == 37) {
         parse_k:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LRNLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LRNLayerParams)
  return false;
#undef DO_
}

void LRNLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LRNLayerParams)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // optional uint64 localSize = 3;
  if (this->localsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->localsize(), output);
  }

  // optional float k = 4;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->k(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LRNLayerParams)
}

size_t LRNLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LRNLayerParams)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // optional float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  // optional uint64 localSize = 3;
  if (this->localsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->localsize());
  }

  // optional float k = 4;
  if (this->k() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LRNLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LRNLayerParams*>(&from));
}

void LRNLayerParams::MergeFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LRNLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void LRNLayerParams::UnsafeMergeFrom(const LRNLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
  if (from.localsize() != 0) {
    set_localsize(from.localsize());
  }
  if (from.k() != 0) {
    set_k(from.k());
  }
}

void LRNLayerParams::CopyFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LRNLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool LRNLayerParams::IsInitialized() const {

  return true;
}

void LRNLayerParams::Swap(LRNLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LRNLayerParams::InternalSwap(LRNLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(localsize_, other->localsize_);
  std::swap(k_, other->k_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LRNLayerParams::GetTypeName() const {
  return "CoreML.Specification.LRNLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LRNLayerParams

// optional float alpha = 1;
void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return alpha_;
}
void LRNLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// optional float beta = 2;
void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return beta_;
}
void LRNLayerParams::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// optional uint64 localSize = 3;
void LRNLayerParams::clear_localsize() {
  localsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return localsize_;
}
void LRNLayerParams::set_localsize(::google::protobuf::uint64 value) {
  
  localsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// optional float k = 4;
void LRNLayerParams::clear_k() {
  k_ = 0;
}
float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return k_;
}
void LRNLayerParams::set_k(float value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

inline const LRNLayerParams* LRNLayerParams::internal_default_instance() {
  return &LRNLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SoftmaxLayerParams::SoftmaxLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SoftmaxLayerParams)
}

void SoftmaxLayerParams::InitAsDefaultInstance() {
}

SoftmaxLayerParams::SoftmaxLayerParams(const SoftmaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SoftmaxLayerParams)
}

void SoftmaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SoftmaxLayerParams::~SoftmaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SoftmaxLayerParams)
  SharedDtor();
}

void SoftmaxLayerParams::SharedDtor() {
}

void SoftmaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SoftmaxLayerParams& SoftmaxLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<SoftmaxLayerParams> SoftmaxLayerParams_default_instance_;

SoftmaxLayerParams* SoftmaxLayerParams::New(::google::protobuf::Arena* arena) const {
  SoftmaxLayerParams* n = new SoftmaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SoftmaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SoftmaxLayerParams)
}

bool SoftmaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SoftmaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SoftmaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SoftmaxLayerParams)
  return false;
#undef DO_
}

void SoftmaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SoftmaxLayerParams)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SoftmaxLayerParams)
}

size_t SoftmaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SoftmaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SoftmaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SoftmaxLayerParams*>(&from));
}

void SoftmaxLayerParams::MergeFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SoftmaxLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void SoftmaxLayerParams::UnsafeMergeFrom(const SoftmaxLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
}

void SoftmaxLayerParams::CopyFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SoftmaxLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool SoftmaxLayerParams::IsInitialized() const {

  return true;
}

void SoftmaxLayerParams::Swap(SoftmaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SoftmaxLayerParams::InternalSwap(SoftmaxLayerParams* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SoftmaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.SoftmaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SoftmaxLayerParams

inline const SoftmaxLayerParams* SoftmaxLayerParams::internal_default_instance() {
  return &SoftmaxLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SplitLayerParams::kNOutputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SplitLayerParams::SplitLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SplitLayerParams)
}

void SplitLayerParams::InitAsDefaultInstance() {
}

SplitLayerParams::SplitLayerParams(const SplitLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SplitLayerParams)
}

void SplitLayerParams::SharedCtor() {
  noutputs_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SplitLayerParams::~SplitLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SplitLayerParams)
  SharedDtor();
}

void SplitLayerParams::SharedDtor() {
}

void SplitLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SplitLayerParams& SplitLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<SplitLayerParams> SplitLayerParams_default_instance_;

SplitLayerParams* SplitLayerParams::New(::google::protobuf::Arena* arena) const {
  SplitLayerParams* n = new SplitLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SplitLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SplitLayerParams)
  noutputs_ = GOOGLE_ULONGLONG(0);
}

bool SplitLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SplitLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 nOutputs = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &noutputs_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SplitLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SplitLayerParams)
  return false;
#undef DO_
}

void SplitLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SplitLayerParams)
  // optional uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->noutputs(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SplitLayerParams)
}

size_t SplitLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SplitLayerParams)
  size_t total_size = 0;

  // optional uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->noutputs());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SplitLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SplitLayerParams*>(&from));
}

void SplitLayerParams::MergeFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SplitLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void SplitLayerParams::UnsafeMergeFrom(const SplitLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.noutputs() != 0) {
    set_noutputs(from.noutputs());
  }
}

void SplitLayerParams::CopyFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SplitLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool SplitLayerParams::IsInitialized() const {

  return true;
}

void SplitLayerParams::Swap(SplitLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SplitLayerParams::InternalSwap(SplitLayerParams* other) {
  std::swap(noutputs_, other->noutputs_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SplitLayerParams::GetTypeName() const {
  return "CoreML.Specification.SplitLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SplitLayerParams

// optional uint64 nOutputs = 1;
void SplitLayerParams::clear_noutputs() {
  noutputs_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return noutputs_;
}
void SplitLayerParams::set_noutputs(::google::protobuf::uint64 value) {
  
  noutputs_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

inline const SplitLayerParams* SplitLayerParams::internal_default_instance() {
  return &SplitLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int AddLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AddLayerParams::AddLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AddLayerParams)
}

void AddLayerParams::InitAsDefaultInstance() {
}

AddLayerParams::AddLayerParams(const AddLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AddLayerParams)
}

void AddLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

AddLayerParams::~AddLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AddLayerParams)
  SharedDtor();
}

void AddLayerParams::SharedDtor() {
}

void AddLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AddLayerParams& AddLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<AddLayerParams> AddLayerParams_default_instance_;

AddLayerParams* AddLayerParams::New(::google::protobuf::Arena* arena) const {
  AddLayerParams* n = new AddLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AddLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AddLayerParams)
  alpha_ = 0;
}

bool AddLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AddLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AddLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AddLayerParams)
  return false;
#undef DO_
}

void AddLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AddLayerParams)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AddLayerParams)
}

size_t AddLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AddLayerParams)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AddLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AddLayerParams*>(&from));
}

void AddLayerParams::MergeFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AddLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void AddLayerParams::UnsafeMergeFrom(const AddLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void AddLayerParams::CopyFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AddLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool AddLayerParams::IsInitialized() const {

  return true;
}

void AddLayerParams::Swap(AddLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AddLayerParams::InternalSwap(AddLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AddLayerParams::GetTypeName() const {
  return "CoreML.Specification.AddLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AddLayerParams

// optional float alpha = 1;
void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return alpha_;
}
void AddLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

inline const AddLayerParams* AddLayerParams::internal_default_instance() {
  return &AddLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiplyLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiplyLayerParams::MultiplyLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MultiplyLayerParams)
}

void MultiplyLayerParams::InitAsDefaultInstance() {
}

MultiplyLayerParams::MultiplyLayerParams(const MultiplyLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MultiplyLayerParams)
}

void MultiplyLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

MultiplyLayerParams::~MultiplyLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MultiplyLayerParams)
  SharedDtor();
}

void MultiplyLayerParams::SharedDtor() {
}

void MultiplyLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MultiplyLayerParams& MultiplyLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<MultiplyLayerParams> MultiplyLayerParams_default_instance_;

MultiplyLayerParams* MultiplyLayerParams::New(::google::protobuf::Arena* arena) const {
  MultiplyLayerParams* n = new MultiplyLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MultiplyLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MultiplyLayerParams)
  alpha_ = 0;
}

bool MultiplyLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MultiplyLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float alpha = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MultiplyLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MultiplyLayerParams)
  return false;
#undef DO_
}

void MultiplyLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MultiplyLayerParams)
  // optional float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MultiplyLayerParams)
}

size_t MultiplyLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MultiplyLayerParams)
  size_t total_size = 0;

  // optional float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiplyLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MultiplyLayerParams*>(&from));
}

void MultiplyLayerParams::MergeFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MultiplyLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void MultiplyLayerParams::UnsafeMergeFrom(const MultiplyLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void MultiplyLayerParams::CopyFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MultiplyLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool MultiplyLayerParams::IsInitialized() const {

  return true;
}

void MultiplyLayerParams::Swap(MultiplyLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MultiplyLayerParams::InternalSwap(MultiplyLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MultiplyLayerParams::GetTypeName() const {
  return "CoreML.Specification.MultiplyLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiplyLayerParams

// optional float alpha = 1;
void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return alpha_;
}
void MultiplyLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

inline const MultiplyLayerParams* MultiplyLayerParams::internal_default_instance() {
  return &MultiplyLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

bool UnaryFunctionLayerParams_Operation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::SQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::RSQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::INVERSE;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::POWER;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::EXP;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::LOG;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::ABS;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::THRESHOLD;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MIN;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MAX;
const int UnaryFunctionLayerParams::Operation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UnaryFunctionLayerParams::kTypeFieldNumber;
const int UnaryFunctionLayerParams::kAlphaFieldNumber;
const int UnaryFunctionLayerParams::kEpsilonFieldNumber;
const int UnaryFunctionLayerParams::kShiftFieldNumber;
const int UnaryFunctionLayerParams::kScaleFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UnaryFunctionLayerParams::UnaryFunctionLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UnaryFunctionLayerParams)
}

void UnaryFunctionLayerParams::InitAsDefaultInstance() {
}

UnaryFunctionLayerParams::UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UnaryFunctionLayerParams)
}

void UnaryFunctionLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  _cached_size_ = 0;
}

UnaryFunctionLayerParams::~UnaryFunctionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UnaryFunctionLayerParams)
  SharedDtor();
}

void UnaryFunctionLayerParams::SharedDtor() {
}

void UnaryFunctionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UnaryFunctionLayerParams& UnaryFunctionLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<UnaryFunctionLayerParams> UnaryFunctionLayerParams_default_instance_;

UnaryFunctionLayerParams* UnaryFunctionLayerParams::New(::google::protobuf::Arena* arena) const {
  UnaryFunctionLayerParams* n = new UnaryFunctionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UnaryFunctionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UnaryFunctionLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(UnaryFunctionLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<UnaryFunctionLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(type_, scale_);

#undef ZR_HELPER_
#undef ZR_

}

bool UnaryFunctionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UnaryFunctionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
      case 1: {
        if (tag == 8) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(value));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(21)) goto parse_alpha;
        break;
      }

      // optional float alpha = 2;
      case 2: {
        if (tag == 21) {
         parse_alpha:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(29)) goto parse_epsilon;
        break;
      }

      // optional float epsilon = 3;
      case 3: {
        if (tag == 29) {
         parse_epsilon:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(37)) goto parse_shift;
        break;
      }

      // optional float shift = 4;
      case 4: {
        if (tag == 37) {
         parse_shift:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &shift_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(45)) goto parse_scale;
        break;
      }

      // optional float scale = 5;
      case 5: {
        if (tag == 45) {
         parse_scale:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &scale_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UnaryFunctionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UnaryFunctionLayerParams)
  return false;
#undef DO_
}

void UnaryFunctionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UnaryFunctionLayerParams)
  // optional .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // optional float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // optional float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // optional float shift = 4;
  if (this->shift() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->shift(), output);
  }

  // optional float scale = 5;
  if (this->scale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->scale(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UnaryFunctionLayerParams)
}

size_t UnaryFunctionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UnaryFunctionLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // optional float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // optional float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // optional float shift = 4;
  if (this->shift() != 0) {
    total_size += 1 + 4;
  }

  // optional float scale = 5;
  if (this->scale() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UnaryFunctionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UnaryFunctionLayerParams*>(&from));
}

void UnaryFunctionLayerParams::MergeFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void UnaryFunctionLayerParams::UnsafeMergeFrom(const UnaryFunctionLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.shift() != 0) {
    set_shift(from.shift());
  }
  if (from.scale() != 0) {
    set_scale(from.scale());
  }
}

void UnaryFunctionLayerParams::CopyFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool UnaryFunctionLayerParams::IsInitialized() const {

  return true;
}

void UnaryFunctionLayerParams::Swap(UnaryFunctionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UnaryFunctionLayerParams::InternalSwap(UnaryFunctionLayerParams* other) {
  std::swap(type_, other->type_);
  std::swap(alpha_, other->alpha_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(shift_, other->shift_);
  std::swap(scale_, other->scale_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UnaryFunctionLayerParams::GetTypeName() const {
  return "CoreML.Specification.UnaryFunctionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UnaryFunctionLayerParams

// optional .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// optional float alpha = 2;
void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return alpha_;
}
void UnaryFunctionLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// optional float epsilon = 3;
void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return epsilon_;
}
void UnaryFunctionLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// optional float shift = 4;
void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return shift_;
}
void UnaryFunctionLayerParams::set_shift(float value) {
  
  shift_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// optional float scale = 5;
void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return scale_;
}
void UnaryFunctionLayerParams::set_scale(float value) {
  
  scale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

inline const UnaryFunctionLayerParams* UnaryFunctionLayerParams::internal_default_instance() {
  return &UnaryFunctionLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UpsampleLayerParams::kScalingFactorFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UpsampleLayerParams::UpsampleLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UpsampleLayerParams)
}

void UpsampleLayerParams::InitAsDefaultInstance() {
}

UpsampleLayerParams::UpsampleLayerParams(const UpsampleLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UpsampleLayerParams)
}

void UpsampleLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

UpsampleLayerParams::~UpsampleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UpsampleLayerParams)
  SharedDtor();
}

void UpsampleLayerParams::SharedDtor() {
}

void UpsampleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UpsampleLayerParams& UpsampleLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<UpsampleLayerParams> UpsampleLayerParams_default_instance_;

UpsampleLayerParams* UpsampleLayerParams::New(::google::protobuf::Arena* arena) const {
  UpsampleLayerParams* n = new UpsampleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UpsampleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UpsampleLayerParams)
  scalingfactor_.Clear();
}

bool UpsampleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UpsampleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 scalingFactor = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_scalingfactor())));
        } else if (tag == 8) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10, input, this->mutable_scalingfactor())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UpsampleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UpsampleLayerParams)
  return false;
#undef DO_
}

void UpsampleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UpsampleLayerParams)
  // repeated uint64 scalingFactor = 1;
  if (this->scalingfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scalingfactor_cached_byte_size_);
  }
  for (int i = 0; i < this->scalingfactor_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->scalingfactor(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UpsampleLayerParams)
}

size_t UpsampleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UpsampleLayerParams)
  size_t total_size = 0;

  // repeated uint64 scalingFactor = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->scalingfactor_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->scalingfactor(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scalingfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UpsampleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UpsampleLayerParams*>(&from));
}

void UpsampleLayerParams::MergeFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UpsampleLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void UpsampleLayerParams::UnsafeMergeFrom(const UpsampleLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  scalingfactor_.UnsafeMergeFrom(from.scalingfactor_);
}

void UpsampleLayerParams::CopyFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UpsampleLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool UpsampleLayerParams::IsInitialized() const {

  return true;
}

void UpsampleLayerParams::Swap(UpsampleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UpsampleLayerParams::InternalSwap(UpsampleLayerParams* other) {
  scalingfactor_.UnsafeArenaSwap(&other->scalingfactor_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UpsampleLayerParams::GetTypeName() const {
  return "CoreML.Specification.UpsampleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
int UpsampleLayerParams::scalingfactor_size() const {
  return scalingfactor_.size();
}
void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
::google::protobuf::uint64 UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_.Get(index);
}
void UpsampleLayerParams::set_scalingfactor(int index, ::google::protobuf::uint64 value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
void UpsampleLayerParams::add_scalingfactor(::google::protobuf::uint64 value) {
  scalingfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return &scalingfactor_;
}

inline const UpsampleLayerParams* UpsampleLayerParams::internal_default_instance() {
  return &UpsampleLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiasLayerParams::kShapeFieldNumber;
const int BiasLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiasLayerParams::BiasLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiasLayerParams)
}

void BiasLayerParams::InitAsDefaultInstance() {
  bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

BiasLayerParams::BiasLayerParams(const BiasLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiasLayerParams)
}

void BiasLayerParams::SharedCtor() {
  bias_ = NULL;
  _cached_size_ = 0;
}

BiasLayerParams::~BiasLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiasLayerParams)
  SharedDtor();
}

void BiasLayerParams::SharedDtor() {
  if (this != &BiasLayerParams_default_instance_.get()) {
    delete bias_;
  }
}

void BiasLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiasLayerParams& BiasLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<BiasLayerParams> BiasLayerParams_default_instance_;

BiasLayerParams* BiasLayerParams::New(::google::protobuf::Arena* arena) const {
  BiasLayerParams* n = new BiasLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiasLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiasLayerParams)
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
  shape_.Clear();
}

bool BiasLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiasLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (tag == 8) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_bias;
        break;
      }

      // optional .CoreML.Specification.WeightParams bias = 2;
      case 2: {
        if (tag == 18) {
         parse_bias:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiasLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiasLayerParams)
  return false;
#undef DO_
}

void BiasLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiasLayerParams)
  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0; i < this->shape_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // optional .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiasLayerParams)
}

size_t BiasLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiasLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // repeated uint64 shape = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->shape_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->shape(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiasLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiasLayerParams*>(&from));
}

void BiasLayerParams::MergeFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiasLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void BiasLayerParams::UnsafeMergeFrom(const BiasLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  shape_.UnsafeMergeFrom(from.shape_);
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void BiasLayerParams::CopyFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiasLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool BiasLayerParams::IsInitialized() const {

  return true;
}

void BiasLayerParams::Swap(BiasLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiasLayerParams::InternalSwap(BiasLayerParams* other) {
  shape_.UnsafeArenaSwap(&other->shape_);
  std::swap(bias_, other->bias_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiasLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiasLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiasLayerParams

// repeated uint64 shape = 1;
int BiasLayerParams::shape_size() const {
  return shape_.size();
}
void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return shape_.Get(index);
}
void BiasLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
void BiasLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return &shape_;
}

// optional .CoreML.Specification.WeightParams bias = 2;
bool BiasLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void BiasLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

inline const BiasLayerParams* BiasLayerParams::internal_default_instance() {
  return &BiasLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScaleLayerParams::kShapeScaleFieldNumber;
const int ScaleLayerParams::kScaleFieldNumber;
const int ScaleLayerParams::kHasBiasFieldNumber;
const int ScaleLayerParams::kShapeBiasFieldNumber;
const int ScaleLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScaleLayerParams::ScaleLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScaleLayerParams)
}

void ScaleLayerParams::InitAsDefaultInstance() {
  scale_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

ScaleLayerParams::ScaleLayerParams(const ScaleLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScaleLayerParams)
}

void ScaleLayerParams::SharedCtor() {
  scale_ = NULL;
  bias_ = NULL;
  hasbias_ = false;
  _cached_size_ = 0;
}

ScaleLayerParams::~ScaleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScaleLayerParams)
  SharedDtor();
}

void ScaleLayerParams::SharedDtor() {
  if (this != &ScaleLayerParams_default_instance_.get()) {
    delete scale_;
    delete bias_;
  }
}

void ScaleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScaleLayerParams& ScaleLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ScaleLayerParams> ScaleLayerParams_default_instance_;

ScaleLayerParams* ScaleLayerParams::New(::google::protobuf::Arena* arena) const {
  ScaleLayerParams* n = new ScaleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScaleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScaleLayerParams)
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
  hasbias_ = false;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
  shapescale_.Clear();
  shapebias_.Clear();
}

bool ScaleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScaleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shapeScale = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapescale())));
        } else if (tag == 8) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10, input, this->mutable_shapescale())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_scale;
        break;
      }

      // optional .CoreML.Specification.WeightParams scale = 2;
      case 2: {
        if (tag == 18) {
         parse_scale:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(24)) goto parse_hasBias;
        break;
      }

      // optional bool hasBias = 3;
      case 3: {
        if (tag == 24) {
         parse_hasBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(34)) goto parse_shapeBias;
        break;
      }

      // repeated uint64 shapeBias = 4;
      case 4: {
        if (tag == 34) {
         parse_shapeBias:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapebias())));
        } else if (tag == 32) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 34, input, this->mutable_shapebias())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(42)) goto parse_bias;
        break;
      }

      // optional .CoreML.Specification.WeightParams bias = 5;
      case 5: {
        if (tag == 42) {
         parse_bias:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScaleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScaleLayerParams)
  return false;
#undef DO_
}

void ScaleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScaleLayerParams)
  // repeated uint64 shapeScale = 1;
  if (this->shapescale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapescale_cached_byte_size_);
  }
  for (int i = 0; i < this->shapescale_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapescale(i), output);
  }

  // optional .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->scale_, output);
  }

  // optional bool hasBias = 3;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->hasbias(), output);
  }

  // repeated uint64 shapeBias = 4;
  if (this->shapebias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapebias_cached_byte_size_);
  }
  for (int i = 0; i < this->shapebias_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapebias(i), output);
  }

  // optional .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScaleLayerParams)
}

size_t ScaleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScaleLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->scale_);
  }

  // optional bool hasBias = 3;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  // optional .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // repeated uint64 shapeScale = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->shapescale_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->shapescale(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapescale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 shapeBias = 4;
  {
    size_t data_size = 0;
    unsigned int count = this->shapebias_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->shapebias(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapebias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScaleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScaleLayerParams*>(&from));
}

void ScaleLayerParams::MergeFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScaleLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ScaleLayerParams::UnsafeMergeFrom(const ScaleLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  shapescale_.UnsafeMergeFrom(from.shapescale_);
  shapebias_.UnsafeMergeFrom(from.shapebias_);
  if (from.has_scale()) {
    mutable_scale()->::CoreML::Specification::WeightParams::MergeFrom(from.scale());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void ScaleLayerParams::CopyFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScaleLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ScaleLayerParams::IsInitialized() const {

  return true;
}

void ScaleLayerParams::Swap(ScaleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScaleLayerParams::InternalSwap(ScaleLayerParams* other) {
  shapescale_.UnsafeArenaSwap(&other->shapescale_);
  std::swap(scale_, other->scale_);
  std::swap(hasbias_, other->hasbias_);
  shapebias_.UnsafeArenaSwap(&other->shapebias_);
  std::swap(bias_, other->bias_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScaleLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScaleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScaleLayerParams

// repeated uint64 shapeScale = 1;
int ScaleLayerParams::shapescale_size() const {
  return shapescale_.size();
}
void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_.Get(index);
}
void ScaleLayerParams::set_shapescale(int index, ::google::protobuf::uint64 value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
void ScaleLayerParams::add_shapescale(::google::protobuf::uint64 value) {
  shapescale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return &shapescale_;
}

// optional .CoreML.Specification.WeightParams scale = 2;
bool ScaleLayerParams::has_scale() const {
  return this != internal_default_instance() && scale_ != NULL;
}
void ScaleLayerParams::clear_scale() {
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return scale_ != NULL ? *scale_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {
  
  if (scale_ == NULL) {
    scale_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return scale_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  delete scale_;
  scale_ = scale;
  if (scale) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// optional bool hasBias = 3;
void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return hasbias_;
}
void ScaleLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
int ScaleLayerParams::shapebias_size() const {
  return shapebias_.size();
}
void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_.Get(index);
}
void ScaleLayerParams::set_shapebias(int index, ::google::protobuf::uint64 value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
void ScaleLayerParams::add_shapebias(::google::protobuf::uint64 value) {
  shapebias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return &shapebias_;
}

// optional .CoreML.Specification.WeightParams bias = 5;
bool ScaleLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ScaleLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

inline const ScaleLayerParams* ScaleLayerParams::internal_default_instance() {
  return &ScaleLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoadConstantLayerParams::kShapeFieldNumber;
const int LoadConstantLayerParams::kDataFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoadConstantLayerParams::LoadConstantLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoadConstantLayerParams)
}

void LoadConstantLayerParams::InitAsDefaultInstance() {
  data_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

LoadConstantLayerParams::LoadConstantLayerParams(const LoadConstantLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoadConstantLayerParams)
}

void LoadConstantLayerParams::SharedCtor() {
  data_ = NULL;
  _cached_size_ = 0;
}

LoadConstantLayerParams::~LoadConstantLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoadConstantLayerParams)
  SharedDtor();
}

void LoadConstantLayerParams::SharedDtor() {
  if (this != &LoadConstantLayerParams_default_instance_.get()) {
    delete data_;
  }
}

void LoadConstantLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoadConstantLayerParams& LoadConstantLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<LoadConstantLayerParams> LoadConstantLayerParams_default_instance_;

LoadConstantLayerParams* LoadConstantLayerParams::New(::google::protobuf::Arena* arena) const {
  LoadConstantLayerParams* n = new LoadConstantLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoadConstantLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoadConstantLayerParams)
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
  shape_.Clear();
}

bool LoadConstantLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoadConstantLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (tag == 8) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_data;
        break;
      }

      // optional .CoreML.Specification.WeightParams data = 2;
      case 2: {
        if (tag == 18) {
         parse_data:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_data()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoadConstantLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoadConstantLayerParams)
  return false;
#undef DO_
}

void LoadConstantLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoadConstantLayerParams)
  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0; i < this->shape_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // optional .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->data_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoadConstantLayerParams)
}

size_t LoadConstantLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoadConstantLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->data_);
  }

  // repeated uint64 shape = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->shape_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->shape(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoadConstantLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoadConstantLayerParams*>(&from));
}

void LoadConstantLayerParams::MergeFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoadConstantLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void LoadConstantLayerParams::UnsafeMergeFrom(const LoadConstantLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  shape_.UnsafeMergeFrom(from.shape_);
  if (from.has_data()) {
    mutable_data()->::CoreML::Specification::WeightParams::MergeFrom(from.data());
  }
}

void LoadConstantLayerParams::CopyFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoadConstantLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool LoadConstantLayerParams::IsInitialized() const {

  return true;
}

void LoadConstantLayerParams::Swap(LoadConstantLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoadConstantLayerParams::InternalSwap(LoadConstantLayerParams* other) {
  shape_.UnsafeArenaSwap(&other->shape_);
  std::swap(data_, other->data_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoadConstantLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoadConstantLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoadConstantLayerParams

// repeated uint64 shape = 1;
int LoadConstantLayerParams::shape_size() const {
  return shape_.size();
}
void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_.Get(index);
}
void LoadConstantLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
void LoadConstantLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return &shape_;
}

// optional .CoreML.Specification.WeightParams data = 2;
bool LoadConstantLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
void LoadConstantLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return data_;
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

inline const LoadConstantLayerParams* LoadConstantLayerParams::internal_default_instance() {
  return &LoadConstantLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int L2NormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

L2NormalizeLayerParams::L2NormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.L2NormalizeLayerParams)
}

void L2NormalizeLayerParams::InitAsDefaultInstance() {
}

L2NormalizeLayerParams::L2NormalizeLayerParams(const L2NormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.L2NormalizeLayerParams)
}

void L2NormalizeLayerParams::SharedCtor() {
  epsilon_ = 0;
  _cached_size_ = 0;
}

L2NormalizeLayerParams::~L2NormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.L2NormalizeLayerParams)
  SharedDtor();
}

void L2NormalizeLayerParams::SharedDtor() {
}

void L2NormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const L2NormalizeLayerParams& L2NormalizeLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<L2NormalizeLayerParams> L2NormalizeLayerParams_default_instance_;

L2NormalizeLayerParams* L2NormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  L2NormalizeLayerParams* n = new L2NormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void L2NormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.L2NormalizeLayerParams)
  epsilon_ = 0;
}

bool L2NormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.L2NormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float epsilon = 1;
      case 1: {
        if (tag == 13) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.L2NormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.L2NormalizeLayerParams)
  return false;
#undef DO_
}

void L2NormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.L2NormalizeLayerParams)
  // optional float epsilon = 1;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.L2NormalizeLayerParams)
}

size_t L2NormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.L2NormalizeLayerParams)
  size_t total_size = 0;

  // optional float epsilon = 1;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void L2NormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const L2NormalizeLayerParams*>(&from));
}

void L2NormalizeLayerParams::MergeFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.L2NormalizeLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void L2NormalizeLayerParams::UnsafeMergeFrom(const L2NormalizeLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void L2NormalizeLayerParams::CopyFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.L2NormalizeLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool L2NormalizeLayerParams::IsInitialized() const {

  return true;
}

void L2NormalizeLayerParams::Swap(L2NormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void L2NormalizeLayerParams::InternalSwap(L2NormalizeLayerParams* other) {
  std::swap(epsilon_, other->epsilon_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string L2NormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.L2NormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// L2NormalizeLayerParams

// optional float epsilon = 1;
void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return epsilon_;
}
void L2NormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

inline const L2NormalizeLayerParams* L2NormalizeLayerParams::internal_default_instance() {
  return &L2NormalizeLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

bool FlattenLayerParams_FlattenOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_FIRST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_LAST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MIN;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MAX;
const int FlattenLayerParams::FlattenOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FlattenLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FlattenLayerParams::FlattenLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FlattenLayerParams)
}

void FlattenLayerParams::InitAsDefaultInstance() {
}

FlattenLayerParams::FlattenLayerParams(const FlattenLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FlattenLayerParams)
}

void FlattenLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

FlattenLayerParams::~FlattenLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FlattenLayerParams)
  SharedDtor();
}

void FlattenLayerParams::SharedDtor() {
}

void FlattenLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FlattenLayerParams& FlattenLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<FlattenLayerParams> FlattenLayerParams_default_instance_;

FlattenLayerParams* FlattenLayerParams::New(::google::protobuf::Arena* arena) const {
  FlattenLayerParams* n = new FlattenLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FlattenLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FlattenLayerParams)
  mode_ = 0;
}

bool FlattenLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FlattenLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
      case 1: {
        if (tag == 8) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(value));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FlattenLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FlattenLayerParams)
  return false;
#undef DO_
}

void FlattenLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FlattenLayerParams)
  // optional .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FlattenLayerParams)
}

size_t FlattenLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FlattenLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FlattenLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FlattenLayerParams*>(&from));
}

void FlattenLayerParams::MergeFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FlattenLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void FlattenLayerParams::UnsafeMergeFrom(const FlattenLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void FlattenLayerParams::CopyFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FlattenLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool FlattenLayerParams::IsInitialized() const {

  return true;
}

void FlattenLayerParams::Swap(FlattenLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FlattenLayerParams::InternalSwap(FlattenLayerParams* other) {
  std::swap(mode_, other->mode_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FlattenLayerParams::GetTypeName() const {
  return "CoreML.Specification.FlattenLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FlattenLayerParams

// optional .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

inline const FlattenLayerParams* FlattenLayerParams::internal_default_instance() {
  return &FlattenLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

bool ReshapeLayerParams_ReshapeOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_FIRST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_LAST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MIN;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MAX;
const int ReshapeLayerParams::ReshapeOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReshapeLayerParams::kTargetShapeFieldNumber;
const int ReshapeLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeLayerParams::ReshapeLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeLayerParams)
}

void ReshapeLayerParams::InitAsDefaultInstance() {
}

ReshapeLayerParams::ReshapeLayerParams(const ReshapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeLayerParams)
}

void ReshapeLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

ReshapeLayerParams::~ReshapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeLayerParams)
  SharedDtor();
}

void ReshapeLayerParams::SharedDtor() {
}

void ReshapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeLayerParams& ReshapeLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ReshapeLayerParams> ReshapeLayerParams_default_instance_;

ReshapeLayerParams* ReshapeLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeLayerParams* n = new ReshapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeLayerParams)
  mode_ = 0;
  targetshape_.Clear();
}

bool ReshapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (tag == 8) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_mode;
        break;
      }

      // optional .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
      case 2: {
        if (tag == 16) {
         parse_mode:
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(value));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeLayerParams)
  return false;
#undef DO_
}

void ReshapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeLayerParams)
  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0; i < this->targetshape_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // optional .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      2, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeLayerParams)
}

size_t ReshapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  // repeated int64 targetShape = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->targetshape_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        Int64Size(this->targetshape(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeLayerParams*>(&from));
}

void ReshapeLayerParams::MergeFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ReshapeLayerParams::UnsafeMergeFrom(const ReshapeLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  targetshape_.UnsafeMergeFrom(from.targetshape_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReshapeLayerParams::CopyFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ReshapeLayerParams::IsInitialized() const {

  return true;
}

void ReshapeLayerParams::Swap(ReshapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeLayerParams::InternalSwap(ReshapeLayerParams* other) {
  targetshape_.UnsafeArenaSwap(&other->targetshape_);
  std::swap(mode_, other->mode_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeLayerParams

// repeated int64 targetShape = 1;
int ReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
void ReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
void ReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return &targetshape_;
}

// optional .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

inline const ReshapeLayerParams* ReshapeLayerParams::internal_default_instance() {
  return &ReshapeLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PermuteLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PermuteLayerParams::PermuteLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PermuteLayerParams)
}

void PermuteLayerParams::InitAsDefaultInstance() {
}

PermuteLayerParams::PermuteLayerParams(const PermuteLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PermuteLayerParams)
}

void PermuteLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

PermuteLayerParams::~PermuteLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PermuteLayerParams)
  SharedDtor();
}

void PermuteLayerParams::SharedDtor() {
}

void PermuteLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PermuteLayerParams& PermuteLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PermuteLayerParams> PermuteLayerParams_default_instance_;

PermuteLayerParams* PermuteLayerParams::New(::google::protobuf::Arena* arena) const {
  PermuteLayerParams* n = new PermuteLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PermuteLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PermuteLayerParams)
  axis_.Clear();
}

bool PermuteLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PermuteLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 axis = 1;
      case 1: {
        if (tag == 10) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_axis())));
        } else if (tag == 8) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10, input, this->mutable_axis())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PermuteLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PermuteLayerParams)
  return false;
#undef DO_
}

void PermuteLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PermuteLayerParams)
  // repeated uint64 axis = 1;
  if (this->axis_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axis_cached_byte_size_);
  }
  for (int i = 0; i < this->axis_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->axis(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PermuteLayerParams)
}

size_t PermuteLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PermuteLayerParams)
  size_t total_size = 0;

  // repeated uint64 axis = 1;
  {
    size_t data_size = 0;
    unsigned int count = this->axis_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->axis(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axis_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PermuteLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PermuteLayerParams*>(&from));
}

void PermuteLayerParams::MergeFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PermuteLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PermuteLayerParams::UnsafeMergeFrom(const PermuteLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  axis_.UnsafeMergeFrom(from.axis_);
}

void PermuteLayerParams::CopyFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PermuteLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PermuteLayerParams::IsInitialized() const {

  return true;
}

void PermuteLayerParams::Swap(PermuteLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PermuteLayerParams::InternalSwap(PermuteLayerParams* other) {
  axis_.UnsafeArenaSwap(&other->axis_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PermuteLayerParams::GetTypeName() const {
  return "CoreML.Specification.PermuteLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PermuteLayerParams

// repeated uint64 axis = 1;
int PermuteLayerParams::axis_size() const {
  return axis_.size();
}
void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
::google::protobuf::uint64 PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return axis_.Get(index);
}
void PermuteLayerParams::set_axis(int index, ::google::protobuf::uint64 value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
void PermuteLayerParams::add_axis(::google::protobuf::uint64 value) {
  axis_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return axis_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return &axis_;
}

inline const PermuteLayerParams* PermuteLayerParams::internal_default_instance() {
  return &PermuteLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

bool ReduceLayerParams_ReduceOperation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::AVG;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::PROD;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::LOGSUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUMSQUARE;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L1;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L2;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MAX;
const int ReduceLayerParams::ReduceOperation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceLayerParams::kModeFieldNumber;
const int ReduceLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceLayerParams::ReduceLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceLayerParams)
}

void ReduceLayerParams::InitAsDefaultInstance() {
}

ReduceLayerParams::ReduceLayerParams(const ReduceLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceLayerParams)
}

void ReduceLayerParams::SharedCtor() {
  ::memset(&mode_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&mode_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

ReduceLayerParams::~ReduceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceLayerParams)
  SharedDtor();
}

void ReduceLayerParams::SharedDtor() {
}

void ReduceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceLayerParams& ReduceLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<ReduceLayerParams> ReduceLayerParams_default_instance_;

ReduceLayerParams* ReduceLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceLayerParams* n = new ReduceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(ReduceLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<ReduceLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(mode_, epsilon_);

#undef ZR_HELPER_
#undef ZR_

}

bool ReduceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
      case 1: {
        if (tag == 8) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(value));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(21)) goto parse_epsilon;
        break;
      }

      // optional float epsilon = 2;
      case 2: {
        if (tag == 21) {
         parse_epsilon:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceLayerParams)
  return false;
#undef DO_
}

void ReduceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceLayerParams)
  // optional .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // optional float epsilon = 2;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceLayerParams)
}

size_t ReduceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  // optional float epsilon = 2;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceLayerParams*>(&from));
}

void ReduceLayerParams::MergeFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void ReduceLayerParams::UnsafeMergeFrom(const ReduceLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void ReduceLayerParams::CopyFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool ReduceLayerParams::IsInitialized() const {

  return true;
}

void ReduceLayerParams::Swap(ReduceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceLayerParams::InternalSwap(ReduceLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(epsilon_, other->epsilon_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceLayerParams

// optional .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// optional float epsilon = 2;
void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return epsilon_;
}
void ReduceLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

inline const ReduceLayerParams* ReduceLayerParams::internal_default_instance() {
  return &ReduceLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CropLayerParams::kCropAmountsFieldNumber;
const int CropLayerParams::kOffsetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CropLayerParams::CropLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CropLayerParams)
}

void CropLayerParams::InitAsDefaultInstance() {
  cropamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
}

CropLayerParams::CropLayerParams(const CropLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CropLayerParams)
}

void CropLayerParams::SharedCtor() {
  cropamounts_ = NULL;
  _cached_size_ = 0;
}

CropLayerParams::~CropLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CropLayerParams)
  SharedDtor();
}

void CropLayerParams::SharedDtor() {
  if (this != &CropLayerParams_default_instance_.get()) {
    delete cropamounts_;
  }
}

void CropLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CropLayerParams& CropLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<CropLayerParams> CropLayerParams_default_instance_;

CropLayerParams* CropLayerParams::New(::google::protobuf::Arena* arena) const {
  CropLayerParams* n = new CropLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CropLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CropLayerParams)
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
  offset_.Clear();
}

bool CropLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CropLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.BorderAmounts cropAmounts = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cropamounts()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(42)) goto parse_offset;
        break;
      }

      // repeated uint64 offset = 5;
      case 5: {
        if (tag == 42) {
         parse_offset:
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_offset())));
        } else if (tag == 40) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 42, input, this->mutable_offset())));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CropLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CropLayerParams)
  return false;
#undef DO_
}

void CropLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CropLayerParams)
  // optional .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->cropamounts_, output);
  }

  // repeated uint64 offset = 5;
  if (this->offset_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_offset_cached_byte_size_);
  }
  for (int i = 0; i < this->offset_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->offset(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CropLayerParams)
}

size_t CropLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CropLayerParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->cropamounts_);
  }

  // repeated uint64 offset = 5;
  {
    size_t data_size = 0;
    unsigned int count = this->offset_size();
    for (unsigned int i = 0; i < count; i++) {
      data_size += ::google::protobuf::internal::WireFormatLite::
        UInt64Size(this->offset(i));
    }
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _offset_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CropLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CropLayerParams*>(&from));
}

void CropLayerParams::MergeFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CropLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void CropLayerParams::UnsafeMergeFrom(const CropLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  offset_.UnsafeMergeFrom(from.offset_);
  if (from.has_cropamounts()) {
    mutable_cropamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.cropamounts());
  }
}

void CropLayerParams::CopyFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CropLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool CropLayerParams::IsInitialized() const {

  return true;
}

void CropLayerParams::Swap(CropLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CropLayerParams::InternalSwap(CropLayerParams* other) {
  std::swap(cropamounts_, other->cropamounts_);
  offset_.UnsafeArenaSwap(&other->offset_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CropLayerParams::GetTypeName() const {
  return "CoreML.Specification.CropLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CropLayerParams

// optional .CoreML.Specification.BorderAmounts cropAmounts = 1;
bool CropLayerParams::has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != NULL;
}
void CropLayerParams::clear_cropamounts() {
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_ != NULL ? *cropamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {
  
  if (cropamounts_ == NULL) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_;
}
::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = NULL;
  return temp;
}
void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  delete cropamounts_;
  cropamounts_ = cropamounts;
  if (cropamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
int CropLayerParams::offset_size() const {
  return offset_.size();
}
void CropLayerParams::clear_offset() {
  offset_.Clear();
}
::google::protobuf::uint64 CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return offset_.Get(index);
}
void CropLayerParams::set_offset(int index, ::google::protobuf::uint64 value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
void CropLayerParams::add_offset(::google::protobuf::uint64 value) {
  offset_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return offset_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return &offset_;
}

inline const CropLayerParams* CropLayerParams::internal_default_instance() {
  return &CropLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AverageLayerParams::AverageLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AverageLayerParams)
}

void AverageLayerParams::InitAsDefaultInstance() {
}

AverageLayerParams::AverageLayerParams(const AverageLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AverageLayerParams)
}

void AverageLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AverageLayerParams::~AverageLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AverageLayerParams)
  SharedDtor();
}

void AverageLayerParams::SharedDtor() {
}

void AverageLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AverageLayerParams& AverageLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<AverageLayerParams> AverageLayerParams_default_instance_;

AverageLayerParams* AverageLayerParams::New(::google::protobuf::Arena* arena) const {
  AverageLayerParams* n = new AverageLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AverageLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AverageLayerParams)
}

bool AverageLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AverageLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AverageLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AverageLayerParams)
  return false;
#undef DO_
}

void AverageLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AverageLayerParams)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AverageLayerParams)
}

size_t AverageLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AverageLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AverageLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AverageLayerParams*>(&from));
}

void AverageLayerParams::MergeFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AverageLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void AverageLayerParams::UnsafeMergeFrom(const AverageLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
}

void AverageLayerParams::CopyFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AverageLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool AverageLayerParams::IsInitialized() const {

  return true;
}

void AverageLayerParams::Swap(AverageLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AverageLayerParams::InternalSwap(AverageLayerParams* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AverageLayerParams::GetTypeName() const {
  return "CoreML.Specification.AverageLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AverageLayerParams

inline const AverageLayerParams* AverageLayerParams::internal_default_instance() {
  return &AverageLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MaxLayerParams::MaxLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MaxLayerParams)
}

void MaxLayerParams::InitAsDefaultInstance() {
}

MaxLayerParams::MaxLayerParams(const MaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MaxLayerParams)
}

void MaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MaxLayerParams::~MaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MaxLayerParams)
  SharedDtor();
}

void MaxLayerParams::SharedDtor() {
}

void MaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MaxLayerParams& MaxLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<MaxLayerParams> MaxLayerParams_default_instance_;

MaxLayerParams* MaxLayerParams::New(::google::protobuf::Arena* arena) const {
  MaxLayerParams* n = new MaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MaxLayerParams)
}

bool MaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MaxLayerParams)
  return false;
#undef DO_
}

void MaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MaxLayerParams)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MaxLayerParams)
}

size_t MaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MaxLayerParams*>(&from));
}

void MaxLayerParams::MergeFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MaxLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void MaxLayerParams::UnsafeMergeFrom(const MaxLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
}

void MaxLayerParams::CopyFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MaxLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool MaxLayerParams::IsInitialized() const {

  return true;
}

void MaxLayerParams::Swap(MaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MaxLayerParams::InternalSwap(MaxLayerParams* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.MaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MaxLayerParams

inline const MaxLayerParams* MaxLayerParams::internal_default_instance() {
  return &MaxLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MinLayerParams::MinLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MinLayerParams)
}

void MinLayerParams::InitAsDefaultInstance() {
}

MinLayerParams::MinLayerParams(const MinLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MinLayerParams)
}

void MinLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MinLayerParams::~MinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MinLayerParams)
  SharedDtor();
}

void MinLayerParams::SharedDtor() {
}

void MinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MinLayerParams& MinLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<MinLayerParams> MinLayerParams_default_instance_;

MinLayerParams* MinLayerParams::New(::google::protobuf::Arena* arena) const {
  MinLayerParams* n = new MinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MinLayerParams)
}

bool MinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MinLayerParams)
  return false;
#undef DO_
}

void MinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MinLayerParams)
  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MinLayerParams)
}

size_t MinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MinLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MinLayerParams*>(&from));
}

void MinLayerParams::MergeFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MinLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void MinLayerParams::UnsafeMergeFrom(const MinLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
}

void MinLayerParams::CopyFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MinLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool MinLayerParams::IsInitialized() const {

  return true;
}

void MinLayerParams::Swap(MinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MinLayerParams::InternalSwap(MinLayerParams* other) {
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MinLayerParams::GetTypeName() const {
  return "CoreML.Specification.MinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MinLayerParams

inline const MinLayerParams* MinLayerParams::internal_default_instance() {
  return &MinLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int DotProductLayerParams::kCosineSimilarityFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DotProductLayerParams::DotProductLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.DotProductLayerParams)
}

void DotProductLayerParams::InitAsDefaultInstance() {
}

DotProductLayerParams::DotProductLayerParams(const DotProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.DotProductLayerParams)
}

void DotProductLayerParams::SharedCtor() {
  cosinesimilarity_ = false;
  _cached_size_ = 0;
}

DotProductLayerParams::~DotProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.DotProductLayerParams)
  SharedDtor();
}

void DotProductLayerParams::SharedDtor() {
}

void DotProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const DotProductLayerParams& DotProductLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<DotProductLayerParams> DotProductLayerParams_default_instance_;

DotProductLayerParams* DotProductLayerParams::New(::google::protobuf::Arena* arena) const {
  DotProductLayerParams* n = new DotProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void DotProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.DotProductLayerParams)
  cosinesimilarity_ = false;
}

bool DotProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.DotProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional bool cosineSimilarity = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &cosinesimilarity_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.DotProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.DotProductLayerParams)
  return false;
#undef DO_
}

void DotProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.DotProductLayerParams)
  // optional bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->cosinesimilarity(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.DotProductLayerParams)
}

size_t DotProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.DotProductLayerParams)
  size_t total_size = 0;

  // optional bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void DotProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const DotProductLayerParams*>(&from));
}

void DotProductLayerParams::MergeFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.DotProductLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void DotProductLayerParams::UnsafeMergeFrom(const DotProductLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.cosinesimilarity() != 0) {
    set_cosinesimilarity(from.cosinesimilarity());
  }
}

void DotProductLayerParams::CopyFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.DotProductLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool DotProductLayerParams::IsInitialized() const {

  return true;
}

void DotProductLayerParams::Swap(DotProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DotProductLayerParams::InternalSwap(DotProductLayerParams* other) {
  std::swap(cosinesimilarity_, other->cosinesimilarity_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string DotProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.DotProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// DotProductLayerParams

// optional bool cosineSimilarity = 1;
void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return cosinesimilarity_;
}
void DotProductLayerParams::set_cosinesimilarity(bool value) {
  
  cosinesimilarity_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

inline const DotProductLayerParams* DotProductLayerParams::internal_default_instance() {
  return &DotProductLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MeanVarianceNormalizeLayerParams::kAcrossChannelsFieldNumber;
const int MeanVarianceNormalizeLayerParams::kNormalizeVarianceFieldNumber;
const int MeanVarianceNormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

void MeanVarianceNormalizeLayerParams::InitAsDefaultInstance() {
}

MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

void MeanVarianceNormalizeLayerParams::SharedCtor() {
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

MeanVarianceNormalizeLayerParams::~MeanVarianceNormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  SharedDtor();
}

void MeanVarianceNormalizeLayerParams::SharedDtor() {
}

void MeanVarianceNormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MeanVarianceNormalizeLayerParams& MeanVarianceNormalizeLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<MeanVarianceNormalizeLayerParams> MeanVarianceNormalizeLayerParams_default_instance_;

MeanVarianceNormalizeLayerParams* MeanVarianceNormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  MeanVarianceNormalizeLayerParams* n = new MeanVarianceNormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MeanVarianceNormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(MeanVarianceNormalizeLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<MeanVarianceNormalizeLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(acrosschannels_, epsilon_);

#undef ZR_HELPER_
#undef ZR_

}

bool MeanVarianceNormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional bool acrossChannels = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &acrosschannels_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_normalizeVariance;
        break;
      }

      // optional bool normalizeVariance = 2;
      case 2: {
        if (tag == 16) {
         parse_normalizeVariance:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &normalizevariance_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(29)) goto parse_epsilon;
        break;
      }

      // optional float epsilon = 3;
      case 3: {
        if (tag == 29) {
         parse_epsilon:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return false;
#undef DO_
}

void MeanVarianceNormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  // optional bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->acrosschannels(), output);
  }

  // optional bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->normalizevariance(), output);
  }

  // optional float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

size_t MeanVarianceNormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  size_t total_size = 0;

  // optional bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    total_size += 1 + 1;
  }

  // optional bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    total_size += 1 + 1;
  }

  // optional float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MeanVarianceNormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MeanVarianceNormalizeLayerParams*>(&from));
}

void MeanVarianceNormalizeLayerParams::MergeFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void MeanVarianceNormalizeLayerParams::UnsafeMergeFrom(const MeanVarianceNormalizeLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.acrosschannels() != 0) {
    set_acrosschannels(from.acrosschannels());
  }
  if (from.normalizevariance() != 0) {
    set_normalizevariance(from.normalizevariance());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void MeanVarianceNormalizeLayerParams::CopyFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool MeanVarianceNormalizeLayerParams::IsInitialized() const {

  return true;
}

void MeanVarianceNormalizeLayerParams::Swap(MeanVarianceNormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MeanVarianceNormalizeLayerParams::InternalSwap(MeanVarianceNormalizeLayerParams* other) {
  std::swap(acrosschannels_, other->acrosschannels_);
  std::swap(normalizevariance_, other->normalizevariance_);
  std::swap(epsilon_, other->epsilon_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MeanVarianceNormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.MeanVarianceNormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MeanVarianceNormalizeLayerParams

// optional bool acrossChannels = 1;
void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return acrosschannels_;
}
void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {
  
  acrosschannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// optional bool normalizeVariance = 2;
void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return normalizevariance_;
}
void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {
  
  normalizevariance_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// optional float epsilon = 3;
void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return epsilon_;
}
void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

inline const MeanVarianceNormalizeLayerParams* MeanVarianceNormalizeLayerParams::internal_default_instance() {
  return &MeanVarianceNormalizeLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SequenceRepeatLayerParams::kNRepetitionsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SequenceRepeatLayerParams::SequenceRepeatLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SequenceRepeatLayerParams)
}

void SequenceRepeatLayerParams::InitAsDefaultInstance() {
}

SequenceRepeatLayerParams::SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SequenceRepeatLayerParams)
}

void SequenceRepeatLayerParams::SharedCtor() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SequenceRepeatLayerParams::~SequenceRepeatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SequenceRepeatLayerParams)
  SharedDtor();
}

void SequenceRepeatLayerParams::SharedDtor() {
}

void SequenceRepeatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SequenceRepeatLayerParams& SequenceRepeatLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<SequenceRepeatLayerParams> SequenceRepeatLayerParams_default_instance_;

SequenceRepeatLayerParams* SequenceRepeatLayerParams::New(::google::protobuf::Arena* arena) const {
  SequenceRepeatLayerParams* n = new SequenceRepeatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SequenceRepeatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SequenceRepeatLayerParams)
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}

bool SequenceRepeatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SequenceRepeatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 nRepetitions = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &nrepetitions_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SequenceRepeatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SequenceRepeatLayerParams)
  return false;
#undef DO_
}

void SequenceRepeatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SequenceRepeatLayerParams)
  // optional uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->nrepetitions(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SequenceRepeatLayerParams)
}

size_t SequenceRepeatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SequenceRepeatLayerParams)
  size_t total_size = 0;

  // optional uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->nrepetitions());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SequenceRepeatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SequenceRepeatLayerParams*>(&from));
}

void SequenceRepeatLayerParams::MergeFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void SequenceRepeatLayerParams::UnsafeMergeFrom(const SequenceRepeatLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.nrepetitions() != 0) {
    set_nrepetitions(from.nrepetitions());
  }
}

void SequenceRepeatLayerParams::CopyFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool SequenceRepeatLayerParams::IsInitialized() const {

  return true;
}

void SequenceRepeatLayerParams::Swap(SequenceRepeatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SequenceRepeatLayerParams::InternalSwap(SequenceRepeatLayerParams* other) {
  std::swap(nrepetitions_, other->nrepetitions_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SequenceRepeatLayerParams::GetTypeName() const {
  return "CoreML.Specification.SequenceRepeatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SequenceRepeatLayerParams

// optional uint64 nRepetitions = 1;
void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return nrepetitions_;
}
void SequenceRepeatLayerParams::set_nrepetitions(::google::protobuf::uint64 value) {
  
  nrepetitions_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

inline const SequenceRepeatLayerParams* SequenceRepeatLayerParams::internal_default_instance() {
  return &SequenceRepeatLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SimpleRecurrentLayerParams::kInputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kOutputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kActivationFieldNumber;
const int SimpleRecurrentLayerParams::kSequenceOutputFieldNumber;
const int SimpleRecurrentLayerParams::kHasBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kWeightMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kRecursionMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SimpleRecurrentLayerParams::SimpleRecurrentLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}

void SimpleRecurrentLayerParams::InitAsDefaultInstance() {
  activation_ = const_cast< ::CoreML::Specification::ActivationParams*>(
      ::CoreML::Specification::ActivationParams::internal_default_instance());
  weightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  recursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  biasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

SimpleRecurrentLayerParams::SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}

void SimpleRecurrentLayerParams::SharedCtor() {
  activation_ = NULL;
  weightmatrix_ = NULL;
  recursionmatrix_ = NULL;
  biasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

SimpleRecurrentLayerParams::~SimpleRecurrentLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SimpleRecurrentLayerParams)
  SharedDtor();
}

void SimpleRecurrentLayerParams::SharedDtor() {
  if (this != &SimpleRecurrentLayerParams_default_instance_.get()) {
    delete activation_;
    delete weightmatrix_;
    delete recursionmatrix_;
    delete biasvector_;
  }
}

void SimpleRecurrentLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SimpleRecurrentLayerParams& SimpleRecurrentLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<SimpleRecurrentLayerParams> SimpleRecurrentLayerParams_default_instance_;

SimpleRecurrentLayerParams* SimpleRecurrentLayerParams::New(::google::protobuf::Arena* arena) const {
  SimpleRecurrentLayerParams* n = new SimpleRecurrentLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SimpleRecurrentLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SimpleRecurrentLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(SimpleRecurrentLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<SimpleRecurrentLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(inputvectorsize_, hasbiasvector_);
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
  reverseinput_ = false;

#undef ZR_HELPER_
#undef ZR_

}

bool SimpleRecurrentLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SimpleRecurrentLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 inputVectorSize = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_outputVectorSize;
        break;
      }

      // optional uint64 outputVectorSize = 2;
      case 2: {
        if (tag == 16) {
         parse_outputVectorSize:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_activation;
        break;
      }

      // optional .CoreML.Specification.ActivationParams activation = 10;
      case 10: {
        if (tag == 82) {
         parse_activation:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(120)) goto parse_sequenceOutput;
        break;
      }

      // optional bool sequenceOutput = 15;
      case 15: {
        if (tag == 120) {
         parse_sequenceOutput:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(160)) goto parse_hasBiasVector;
        break;
      }

      // optional bool hasBiasVector = 20;
      case 20: {
        if (tag == 160) {
         parse_hasBiasVector:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvector_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(242)) goto parse_weightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams weightMatrix = 30;
      case 30: {
        if (tag == 242) {
         parse_weightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(250)) goto parse_recursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams recursionMatrix = 31;
      case 31: {
        if (tag == 250) {
         parse_recursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_recursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(258)) goto parse_biasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams biasVector = 32;
      case 32: {
        if (tag == 258) {
         parse_biasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_biasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(800)) goto parse_reverseInput;
        break;
      }

      // optional bool reverseInput = 100;
      case 100: {
        if (tag == 800) {
         parse_reverseInput:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SimpleRecurrentLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SimpleRecurrentLayerParams)
  return false;
#undef DO_
}

void SimpleRecurrentLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SimpleRecurrentLayerParams)
  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // optional .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->activation_, output);
  }

  // optional bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // optional bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvector(), output);
  }

  // optional .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->weightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->recursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->biasvector_, output);
  }

  // optional bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SimpleRecurrentLayerParams)
}

size_t SimpleRecurrentLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SimpleRecurrentLayerParams)
  size_t total_size = 0;

  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // optional .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->activation_);
  }

  // optional bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // optional bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    total_size += 2 + 1;
  }

  // optional .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->recursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->biasvector_);
  }

  // optional bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SimpleRecurrentLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SimpleRecurrentLayerParams*>(&from));
}

void SimpleRecurrentLayerParams::MergeFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void SimpleRecurrentLayerParams::UnsafeMergeFrom(const SimpleRecurrentLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.has_activation()) {
    mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvector() != 0) {
    set_hasbiasvector(from.hasbiasvector());
  }
  if (from.has_weightmatrix()) {
    mutable_weightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.weightmatrix());
  }
  if (from.has_recursionmatrix()) {
    mutable_recursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.recursionmatrix());
  }
  if (from.has_biasvector()) {
    mutable_biasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.biasvector());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void SimpleRecurrentLayerParams::CopyFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool SimpleRecurrentLayerParams::IsInitialized() const {

  return true;
}

void SimpleRecurrentLayerParams::Swap(SimpleRecurrentLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SimpleRecurrentLayerParams::InternalSwap(SimpleRecurrentLayerParams* other) {
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(activation_, other->activation_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvector_, other->hasbiasvector_);
  std::swap(weightmatrix_, other->weightmatrix_);
  std::swap(recursionmatrix_, other->recursionmatrix_);
  std::swap(biasvector_, other->biasvector_);
  std::swap(reverseinput_, other->reverseinput_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SimpleRecurrentLayerParams::GetTypeName() const {
  return "CoreML.Specification.SimpleRecurrentLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SimpleRecurrentLayerParams

// optional uint64 inputVectorSize = 1;
void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void SimpleRecurrentLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void SimpleRecurrentLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// optional .CoreML.Specification.ActivationParams activation = 10;
bool SimpleRecurrentLayerParams::has_activation() const {
  return this != internal_default_instance() && activation_ != NULL;
}
void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
}
const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_ != NULL ? *activation_
                         : *::CoreML::Specification::ActivationParams::internal_default_instance();
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {
  
  if (activation_ == NULL) {
    activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_;
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  delete activation_;
  activation_ = activation;
  if (activation) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// optional bool sequenceOutput = 15;
void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return sequenceoutput_;
}
void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// optional bool hasBiasVector = 20;
void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return hasbiasvector_;
}
void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {
  
  hasbiasvector_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// optional .CoreML.Specification.WeightParams weightMatrix = 30;
bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_ != NULL ? *weightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {
  
  if (weightmatrix_ == NULL) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  delete weightmatrix_;
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// optional .CoreML.Specification.WeightParams recursionMatrix = 31;
bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_ != NULL ? *recursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {
  
  if (recursionmatrix_ == NULL) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  delete recursionmatrix_;
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// optional .CoreML.Specification.WeightParams biasVector = 32;
bool SimpleRecurrentLayerParams::has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != NULL;
}
void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_ != NULL ? *biasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {
  
  if (biasvector_ == NULL) {
    biasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  delete biasvector_;
  biasvector_ = biasvector;
  if (biasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// optional bool reverseInput = 100;
void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return reverseinput_;
}
void SimpleRecurrentLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

inline const SimpleRecurrentLayerParams* SimpleRecurrentLayerParams::internal_default_instance() {
  return &SimpleRecurrentLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GRULayerParams::kInputVectorSizeFieldNumber;
const int GRULayerParams::kOutputVectorSizeFieldNumber;
const int GRULayerParams::kActivationsFieldNumber;
const int GRULayerParams::kSequenceOutputFieldNumber;
const int GRULayerParams::kHasBiasVectorsFieldNumber;
const int GRULayerParams::kUpdateGateWeightMatrixFieldNumber;
const int GRULayerParams::kResetGateWeightMatrixFieldNumber;
const int GRULayerParams::kOutputGateWeightMatrixFieldNumber;
const int GRULayerParams::kUpdateGateRecursionMatrixFieldNumber;
const int GRULayerParams::kResetGateRecursionMatrixFieldNumber;
const int GRULayerParams::kOutputGateRecursionMatrixFieldNumber;
const int GRULayerParams::kUpdateGateBiasVectorFieldNumber;
const int GRULayerParams::kResetGateBiasVectorFieldNumber;
const int GRULayerParams::kOutputGateBiasVectorFieldNumber;
const int GRULayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GRULayerParams::GRULayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GRULayerParams)
}

void GRULayerParams::InitAsDefaultInstance() {
  updategateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  resetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  updategaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  resetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  updategatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  resetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

GRULayerParams::GRULayerParams(const GRULayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GRULayerParams)
}

void GRULayerParams::SharedCtor() {
  updategateweightmatrix_ = NULL;
  resetgateweightmatrix_ = NULL;
  outputgateweightmatrix_ = NULL;
  updategaterecursionmatrix_ = NULL;
  resetgaterecursionmatrix_ = NULL;
  outputgaterecursionmatrix_ = NULL;
  updategatebiasvector_ = NULL;
  resetgatebiasvector_ = NULL;
  outputgatebiasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

GRULayerParams::~GRULayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GRULayerParams)
  SharedDtor();
}

void GRULayerParams::SharedDtor() {
  if (this != &GRULayerParams_default_instance_.get()) {
    delete updategateweightmatrix_;
    delete resetgateweightmatrix_;
    delete outputgateweightmatrix_;
    delete updategaterecursionmatrix_;
    delete resetgaterecursionmatrix_;
    delete outputgaterecursionmatrix_;
    delete updategatebiasvector_;
    delete resetgatebiasvector_;
    delete outputgatebiasvector_;
  }
}

void GRULayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GRULayerParams& GRULayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<GRULayerParams> GRULayerParams_default_instance_;

GRULayerParams* GRULayerParams::New(::google::protobuf::Arena* arena) const {
  GRULayerParams* n = new GRULayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GRULayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GRULayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(GRULayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<GRULayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(inputvectorsize_, hasbiasvectors_);
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  reverseinput_ = false;

#undef ZR_HELPER_
#undef ZR_

  activations_.Clear();
}

bool GRULayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GRULayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 inputVectorSize = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_outputVectorSize;
        break;
      }

      // optional uint64 outputVectorSize = 2;
      case 2: {
        if (tag == 16) {
         parse_outputVectorSize:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_activations;
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (tag == 82) {
         parse_activations:
          DO_(input->IncrementRecursionDepth());
         parse_loop_activations:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_loop_activations;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectTag(120)) goto parse_sequenceOutput;
        break;
      }

      // optional bool sequenceOutput = 15;
      case 15: {
        if (tag == 120) {
         parse_sequenceOutput:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(160)) goto parse_hasBiasVectors;
        break;
      }

      // optional bool hasBiasVectors = 20;
      case 20: {
        if (tag == 160) {
         parse_hasBiasVectors:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(242)) goto parse_updateGateWeightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
      case 30: {
        if (tag == 242) {
         parse_updateGateWeightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(250)) goto parse_resetGateWeightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
      case 31: {
        if (tag == 250) {
         parse_resetGateWeightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(258)) goto parse_outputGateWeightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
      case 32: {
        if (tag == 258) {
         parse_outputGateWeightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(402)) goto parse_updateGateRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
      case 50: {
        if (tag == 402) {
         parse_updateGateRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(410)) goto parse_resetGateRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
      case 51: {
        if (tag == 410) {
         parse_resetGateRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(418)) goto parse_outputGateRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
      case 52: {
        if (tag == 418) {
         parse_outputGateRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(562)) goto parse_updateGateBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams updateGateBiasVector = 70;
      case 70: {
        if (tag == 562) {
         parse_updateGateBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategatebiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(570)) goto parse_resetGateBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams resetGateBiasVector = 71;
      case 71: {
        if (tag == 570) {
         parse_resetGateBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(578)) goto parse_outputGateBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGateBiasVector = 72;
      case 72: {
        if (tag == 578) {
         parse_outputGateBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(800)) goto parse_reverseInput;
        break;
      }

      // optional bool reverseInput = 100;
      case 100: {
        if (tag == 800) {
         parse_reverseInput:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GRULayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GRULayerParams)
  return false;
#undef DO_
}

void GRULayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GRULayerParams)
  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // optional bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // optional bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // optional .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->updategateweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->resetgateweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->outputgateweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *this->updategaterecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *this->resetgaterecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      52, *this->outputgaterecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *this->updategatebiasvector_, output);
  }

  // optional .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *this->resetgatebiasvector_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      72, *this->outputgatebiasvector_, output);
  }

  // optional bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GRULayerParams)
}

size_t GRULayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GRULayerParams)
  size_t total_size = 0;

  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // optional bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // optional bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // optional .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategateweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgateweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategaterecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgaterecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategatebiasvector_);
  }

  // optional .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgatebiasvector_);
  }

  // optional .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // optional bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GRULayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GRULayerParams*>(&from));
}

void GRULayerParams::MergeFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GRULayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void GRULayerParams::UnsafeMergeFrom(const GRULayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  activations_.MergeFrom(from.activations_);
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.has_updategateweightmatrix()) {
    mutable_updategateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategateweightmatrix());
  }
  if (from.has_resetgateweightmatrix()) {
    mutable_resetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgateweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_updategaterecursionmatrix()) {
    mutable_updategaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategaterecursionmatrix());
  }
  if (from.has_resetgaterecursionmatrix()) {
    mutable_resetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgaterecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_updategatebiasvector()) {
    mutable_updategatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.updategatebiasvector());
  }
  if (from.has_resetgatebiasvector()) {
    mutable_resetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgatebiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void GRULayerParams::CopyFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GRULayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool GRULayerParams::IsInitialized() const {

  return true;
}

void GRULayerParams::Swap(GRULayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GRULayerParams::InternalSwap(GRULayerParams* other) {
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  activations_.UnsafeArenaSwap(&other->activations_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(updategateweightmatrix_, other->updategateweightmatrix_);
  std::swap(resetgateweightmatrix_, other->resetgateweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(updategaterecursionmatrix_, other->updategaterecursionmatrix_);
  std::swap(resetgaterecursionmatrix_, other->resetgaterecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(updategatebiasvector_, other->updategatebiasvector_);
  std::swap(resetgatebiasvector_, other->resetgatebiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(reverseinput_, other->reverseinput_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GRULayerParams::GetTypeName() const {
  return "CoreML.Specification.GRULayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GRULayerParams

// optional uint64 inputVectorSize = 1;
void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return inputvectorsize_;
}
void GRULayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return outputvectorsize_;
}
void GRULayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int GRULayerParams::activations_size() const {
  return activations_.size();
}
void GRULayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// optional bool sequenceOutput = 15;
void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return sequenceoutput_;
}
void GRULayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// optional bool hasBiasVectors = 20;
void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return hasbiasvectors_;
}
void GRULayerParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// optional .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
bool GRULayerParams::has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != NULL;
}
void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_ != NULL ? *updategateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {
  
  if (updategateweightmatrix_ == NULL) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  delete updategateweightmatrix_;
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
bool GRULayerParams::has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_ != NULL ? *resetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {
  
  if (resetgateweightmatrix_ == NULL) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  delete resetgateweightmatrix_;
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
bool GRULayerParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
bool GRULayerParams::has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_ != NULL ? *updategaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {
  
  if (updategaterecursionmatrix_ == NULL) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_ != NULL ? *resetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {
  
  if (resetgaterecursionmatrix_ == NULL) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams updateGateBiasVector = 70;
bool GRULayerParams::has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != NULL;
}
void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_ != NULL ? *updategatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {
  
  if (updategatebiasvector_ == NULL) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  delete updategatebiasvector_;
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// optional .CoreML.Specification.WeightParams resetGateBiasVector = 71;
bool GRULayerParams::has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != NULL;
}
void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_ != NULL ? *resetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {
  
  if (resetgatebiasvector_ == NULL) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  delete resetgatebiasvector_;
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// optional .CoreML.Specification.WeightParams outputGateBiasVector = 72;
bool GRULayerParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// optional bool reverseInput = 100;
void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return reverseinput_;
}
void GRULayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

inline const GRULayerParams* GRULayerParams::internal_default_instance() {
  return &GRULayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMParams::kSequenceOutputFieldNumber;
const int LSTMParams::kHasBiasVectorsFieldNumber;
const int LSTMParams::kForgetBiasFieldNumber;
const int LSTMParams::kHasPeepholeVectorsFieldNumber;
const int LSTMParams::kCoupledInputAndForgetGateFieldNumber;
const int LSTMParams::kCellClipThresholdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMParams::LSTMParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMParams)
}

void LSTMParams::InitAsDefaultInstance() {
}

LSTMParams::LSTMParams(const LSTMParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMParams)
}

void LSTMParams::SharedCtor() {
  ::memset(&sequenceoutput_, 0, reinterpret_cast<char*>(&cellclipthreshold_) -
    reinterpret_cast<char*>(&sequenceoutput_) + sizeof(cellclipthreshold_));
  _cached_size_ = 0;
}

LSTMParams::~LSTMParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMParams)
  SharedDtor();
}

void LSTMParams::SharedDtor() {
}

void LSTMParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMParams& LSTMParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<LSTMParams> LSTMParams_default_instance_;

LSTMParams* LSTMParams::New(::google::protobuf::Arena* arena) const {
  LSTMParams* n = new LSTMParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(LSTMParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<LSTMParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(sequenceoutput_, cellclipthreshold_);

#undef ZR_HELPER_
#undef ZR_

}

bool LSTMParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional bool sequenceOutput = 10;
      case 10: {
        if (tag == 80) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(160)) goto parse_hasBiasVectors;
        break;
      }

      // optional bool hasBiasVectors = 20;
      case 20: {
        if (tag == 160) {
         parse_hasBiasVectors:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(240)) goto parse_forgetBias;
        break;
      }

      // optional bool forgetBias = 30;
      case 30: {
        if (tag == 240) {
         parse_forgetBias:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &forgetbias_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(320)) goto parse_hasPeepholeVectors;
        break;
      }

      // optional bool hasPeepholeVectors = 40;
      case 40: {
        if (tag == 320) {
         parse_hasPeepholeVectors:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &haspeepholevectors_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(400)) goto parse_coupledInputAndForgetGate;
        break;
      }

      // optional bool coupledInputAndForgetGate = 50;
      case 50: {
        if (tag == 400) {
         parse_coupledInputAndForgetGate:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &coupledinputandforgetgate_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(485)) goto parse_cellClipThreshold;
        break;
      }

      // optional float cellClipThreshold = 60;
      case 60: {
        if (tag == 485) {
         parse_cellClipThreshold:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &cellclipthreshold_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMParams)
  return false;
#undef DO_
}

void LSTMParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMParams)
  // optional bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->sequenceoutput(), output);
  }

  // optional bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // optional bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(30, this->forgetbias(), output);
  }

  // optional bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(40, this->haspeepholevectors(), output);
  }

  // optional bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->coupledinputandforgetgate(), output);
  }

  // optional float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(60, this->cellclipthreshold(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMParams)
}

size_t LSTMParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMParams)
  size_t total_size = 0;

  // optional bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // optional bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // optional bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    total_size += 2 + 1;
  }

  // optional bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    total_size += 2 + 1;
  }

  // optional bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    total_size += 2 + 1;
  }

  // optional float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    total_size += 2 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMParams*>(&from));
}

void LSTMParams::MergeFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void LSTMParams::UnsafeMergeFrom(const LSTMParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.forgetbias() != 0) {
    set_forgetbias(from.forgetbias());
  }
  if (from.haspeepholevectors() != 0) {
    set_haspeepholevectors(from.haspeepholevectors());
  }
  if (from.coupledinputandforgetgate() != 0) {
    set_coupledinputandforgetgate(from.coupledinputandforgetgate());
  }
  if (from.cellclipthreshold() != 0) {
    set_cellclipthreshold(from.cellclipthreshold());
  }
}

void LSTMParams::CopyFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool LSTMParams::IsInitialized() const {

  return true;
}

void LSTMParams::Swap(LSTMParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMParams::InternalSwap(LSTMParams* other) {
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(forgetbias_, other->forgetbias_);
  std::swap(haspeepholevectors_, other->haspeepholevectors_);
  std::swap(coupledinputandforgetgate_, other->coupledinputandforgetgate_);
  std::swap(cellclipthreshold_, other->cellclipthreshold_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMParams::GetTypeName() const {
  return "CoreML.Specification.LSTMParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMParams

// optional bool sequenceOutput = 10;
void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return sequenceoutput_;
}
void LSTMParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// optional bool hasBiasVectors = 20;
void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return hasbiasvectors_;
}
void LSTMParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// optional bool forgetBias = 30;
void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return forgetbias_;
}
void LSTMParams::set_forgetbias(bool value) {
  
  forgetbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// optional bool hasPeepholeVectors = 40;
void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return haspeepholevectors_;
}
void LSTMParams::set_haspeepholevectors(bool value) {
  
  haspeepholevectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// optional bool coupledInputAndForgetGate = 50;
void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return coupledinputandforgetgate_;
}
void LSTMParams::set_coupledinputandforgetgate(bool value) {
  
  coupledinputandforgetgate_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// optional float cellClipThreshold = 60;
void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return cellclipthreshold_;
}
void LSTMParams::set_cellclipthreshold(float value) {
  
  cellclipthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

inline const LSTMParams* LSTMParams::internal_default_instance() {
  return &LSTMParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMWeightParams::kInputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputWeightMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kInputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputRecursionMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kInputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kForgetGateBiasVectorFieldNumber;
const int LSTMWeightParams::kBlockInputBiasVectorFieldNumber;
const int LSTMWeightParams::kOutputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kInputGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kForgetGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kOutputGatePeepholeVectorFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMWeightParams::LSTMWeightParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMWeightParams)
}

void LSTMWeightParams::InitAsDefaultInstance() {
  inputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  forgetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  blockinputweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  inputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  forgetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  blockinputrecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  inputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  forgetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  blockinputbiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  inputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  forgetgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  outputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
}

LSTMWeightParams::LSTMWeightParams(const LSTMWeightParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMWeightParams)
}

void LSTMWeightParams::SharedCtor() {
  inputgateweightmatrix_ = NULL;
  forgetgateweightmatrix_ = NULL;
  blockinputweightmatrix_ = NULL;
  outputgateweightmatrix_ = NULL;
  inputgaterecursionmatrix_ = NULL;
  forgetgaterecursionmatrix_ = NULL;
  blockinputrecursionmatrix_ = NULL;
  outputgaterecursionmatrix_ = NULL;
  inputgatebiasvector_ = NULL;
  forgetgatebiasvector_ = NULL;
  blockinputbiasvector_ = NULL;
  outputgatebiasvector_ = NULL;
  inputgatepeepholevector_ = NULL;
  forgetgatepeepholevector_ = NULL;
  outputgatepeepholevector_ = NULL;
  _cached_size_ = 0;
}

LSTMWeightParams::~LSTMWeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMWeightParams)
  SharedDtor();
}

void LSTMWeightParams::SharedDtor() {
  if (this != &LSTMWeightParams_default_instance_.get()) {
    delete inputgateweightmatrix_;
    delete forgetgateweightmatrix_;
    delete blockinputweightmatrix_;
    delete outputgateweightmatrix_;
    delete inputgaterecursionmatrix_;
    delete forgetgaterecursionmatrix_;
    delete blockinputrecursionmatrix_;
    delete outputgaterecursionmatrix_;
    delete inputgatebiasvector_;
    delete forgetgatebiasvector_;
    delete blockinputbiasvector_;
    delete outputgatebiasvector_;
    delete inputgatepeepholevector_;
    delete forgetgatepeepholevector_;
    delete outputgatepeepholevector_;
  }
}

void LSTMWeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMWeightParams& LSTMWeightParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<LSTMWeightParams> LSTMWeightParams_default_instance_;

LSTMWeightParams* LSTMWeightParams::New(::google::protobuf::Arena* arena) const {
  LSTMWeightParams* n = new LSTMWeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMWeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMWeightParams)
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}

bool LSTMWeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMWeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_forgetGateWeightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
      case 2: {
        if (tag == 18) {
         parse_forgetGateWeightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(26)) goto parse_blockInputWeightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
      case 3: {
        if (tag == 26) {
         parse_blockInputWeightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(34)) goto parse_outputGateWeightMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
      case 4: {
        if (tag == 34) {
         parse_outputGateWeightMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_inputGateRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
      case 20: {
        if (tag == 162) {
         parse_inputGateRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(170)) goto parse_forgetGateRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
      case 21: {
        if (tag == 170) {
         parse_forgetGateRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(178)) goto parse_blockInputRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
      case 22: {
        if (tag == 178) {
         parse_blockInputRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputrecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(186)) goto parse_outputGateRecursionMatrix;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
      case 23: {
        if (tag == 186) {
         parse_outputGateRecursionMatrix:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(322)) goto parse_inputGateBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams inputGateBiasVector = 40;
      case 40: {
        if (tag == 322) {
         parse_inputGateBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(330)) goto parse_forgetGateBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
      case 41: {
        if (tag == 330) {
         parse_forgetGateBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(338)) goto parse_blockInputBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams blockInputBiasVector = 42;
      case 42: {
        if (tag == 338) {
         parse_blockInputBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputbiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(346)) goto parse_outputGateBiasVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGateBiasVector = 43;
      case 43: {
        if (tag == 346) {
         parse_outputGateBiasVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(482)) goto parse_inputGatePeepholeVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
      case 60: {
        if (tag == 482) {
         parse_inputGatePeepholeVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(490)) goto parse_forgetGatePeepholeVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
      case 61: {
        if (tag == 490) {
         parse_forgetGatePeepholeVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(498)) goto parse_outputGatePeepholeVector;
        break;
      }

      // optional .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
      case 62: {
        if (tag == 498) {
         parse_outputGatePeepholeVector:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMWeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMWeightParams)
  return false;
#undef DO_
}

void LSTMWeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMWeightParams)
  // optional .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->inputgateweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->forgetgateweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->blockinputweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->outputgateweightmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->inputgaterecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->forgetgaterecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      22, *this->blockinputrecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      23, *this->outputgaterecursionmatrix_, output);
  }

  // optional .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->inputgatebiasvector_, output);
  }

  // optional .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *this->forgetgatebiasvector_, output);
  }

  // optional .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      42, *this->blockinputbiasvector_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      43, *this->outputgatebiasvector_, output);
  }

  // optional .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *this->inputgatepeepholevector_, output);
  }

  // optional .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      61, *this->forgetgatepeepholevector_, output);
  }

  // optional .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      62, *this->outputgatepeepholevector_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMWeightParams)
}

size_t LSTMWeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMWeightParams)
  size_t total_size = 0;

  // optional .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgateweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgateweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // optional .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgaterecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgaterecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputrecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // optional .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatebiasvector_);
  }

  // optional .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatebiasvector_);
  }

  // optional .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputbiasvector_);
  }

  // optional .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // optional .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatepeepholevector_);
  }

  // optional .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatepeepholevector_);
  }

  // optional .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatepeepholevector_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMWeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMWeightParams*>(&from));
}

void LSTMWeightParams::MergeFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMWeightParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void LSTMWeightParams::UnsafeMergeFrom(const LSTMWeightParams& from) {
  GOOGLE_DCHECK(&from != this);
  if (from.has_inputgateweightmatrix()) {
    mutable_inputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgateweightmatrix());
  }
  if (from.has_forgetgateweightmatrix()) {
    mutable_forgetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgateweightmatrix());
  }
  if (from.has_blockinputweightmatrix()) {
    mutable_blockinputweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_inputgaterecursionmatrix()) {
    mutable_inputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgaterecursionmatrix());
  }
  if (from.has_forgetgaterecursionmatrix()) {
    mutable_forgetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgaterecursionmatrix());
  }
  if (from.has_blockinputrecursionmatrix()) {
    mutable_blockinputrecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputrecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_inputgatebiasvector()) {
    mutable_inputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatebiasvector());
  }
  if (from.has_forgetgatebiasvector()) {
    mutable_forgetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatebiasvector());
  }
  if (from.has_blockinputbiasvector()) {
    mutable_blockinputbiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputbiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.has_inputgatepeepholevector()) {
    mutable_inputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatepeepholevector());
  }
  if (from.has_forgetgatepeepholevector()) {
    mutable_forgetgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatepeepholevector());
  }
  if (from.has_outputgatepeepholevector()) {
    mutable_outputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatepeepholevector());
  }
}

void LSTMWeightParams::CopyFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMWeightParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool LSTMWeightParams::IsInitialized() const {

  return true;
}

void LSTMWeightParams::Swap(LSTMWeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMWeightParams::InternalSwap(LSTMWeightParams* other) {
  std::swap(inputgateweightmatrix_, other->inputgateweightmatrix_);
  std::swap(forgetgateweightmatrix_, other->forgetgateweightmatrix_);
  std::swap(blockinputweightmatrix_, other->blockinputweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(inputgaterecursionmatrix_, other->inputgaterecursionmatrix_);
  std::swap(forgetgaterecursionmatrix_, other->forgetgaterecursionmatrix_);
  std::swap(blockinputrecursionmatrix_, other->blockinputrecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(inputgatebiasvector_, other->inputgatebiasvector_);
  std::swap(forgetgatebiasvector_, other->forgetgatebiasvector_);
  std::swap(blockinputbiasvector_, other->blockinputbiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputgatepeepholevector_, other->inputgatepeepholevector_);
  std::swap(forgetgatepeepholevector_, other->forgetgatepeepholevector_);
  std::swap(outputgatepeepholevector_, other->outputgatepeepholevector_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMWeightParams::GetTypeName() const {
  return "CoreML.Specification.LSTMWeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMWeightParams

// optional .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_ != NULL ? *inputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {
  
  if (inputgateweightmatrix_ == NULL) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  delete inputgateweightmatrix_;
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_ != NULL ? *forgetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {
  
  if (forgetgateweightmatrix_ == NULL) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_ != NULL ? *blockinputweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {
  
  if (blockinputweightmatrix_ == NULL) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  delete blockinputweightmatrix_;
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_ != NULL ? *inputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {
  
  if (inputgaterecursionmatrix_ == NULL) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_ != NULL ? *forgetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {
  
  if (forgetgaterecursionmatrix_ == NULL) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_ != NULL ? *blockinputrecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {
  
  if (blockinputrecursionmatrix_ == NULL) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams inputGateBiasVector = 40;
bool LSTMWeightParams::has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_ != NULL ? *inputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {
  
  if (inputgatebiasvector_ == NULL) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  delete inputgatebiasvector_;
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// optional .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_ != NULL ? *forgetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {
  
  if (forgetgatebiasvector_ == NULL) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  delete forgetgatebiasvector_;
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// optional .CoreML.Specification.WeightParams blockInputBiasVector = 42;
bool LSTMWeightParams::has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != NULL;
}
void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_ != NULL ? *blockinputbiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {
  
  if (blockinputbiasvector_ == NULL) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  delete blockinputbiasvector_;
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// optional .CoreML.Specification.WeightParams outputGateBiasVector = 43;
bool LSTMWeightParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// optional .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_ != NULL ? *inputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {
  
  if (inputgatepeepholevector_ == NULL) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  delete inputgatepeepholevector_;
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// optional .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_ != NULL ? *forgetgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {
  
  if (forgetgatepeepholevector_ == NULL) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// optional .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_ != NULL ? *outputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {
  
  if (outputgatepeepholevector_ == NULL) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  delete outputgatepeepholevector_;
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

inline const LSTMWeightParams* LSTMWeightParams::internal_default_instance() {
  return &LSTMWeightParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UniDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kActivationsFieldNumber;
const int UniDirectionalLSTMLayerParams::kParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

void UniDirectionalLSTMLayerParams::InitAsDefaultInstance() {
  params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  weightparams_ = const_cast< ::CoreML::Specification::LSTMWeightParams*>(
      ::CoreML::Specification::LSTMWeightParams::internal_default_instance());
}

UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

void UniDirectionalLSTMLayerParams::SharedCtor() {
  params_ = NULL;
  weightparams_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

UniDirectionalLSTMLayerParams::~UniDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
  SharedDtor();
}

void UniDirectionalLSTMLayerParams::SharedDtor() {
  if (this != &UniDirectionalLSTMLayerParams_default_instance_.get()) {
    delete params_;
    delete weightparams_;
  }
}

void UniDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UniDirectionalLSTMLayerParams& UniDirectionalLSTMLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<UniDirectionalLSTMLayerParams> UniDirectionalLSTMLayerParams_default_instance_;

UniDirectionalLSTMLayerParams* UniDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  UniDirectionalLSTMLayerParams* n = new UniDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UniDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(UniDirectionalLSTMLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<UniDirectionalLSTMLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(inputvectorsize_, reverseinput_);
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;

#undef ZR_HELPER_
#undef ZR_

  activations_.Clear();
}

bool UniDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 inputVectorSize = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_outputVectorSize;
        break;
      }

      // optional uint64 outputVectorSize = 2;
      case 2: {
        if (tag == 16) {
         parse_outputVectorSize:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_activations;
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (tag == 82) {
         parse_activations:
          DO_(input->IncrementRecursionDepth());
         parse_loop_activations:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_loop_activations;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectTag(122)) goto parse_params;
        break;
      }

      // optional .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (tag == 122) {
         parse_params:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_weightParams;
        break;
      }

      // optional .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (tag == 162) {
         parse_weightParams:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightparams()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(800)) goto parse_reverseInput;
        break;
      }

      // optional bool reverseInput = 100;
      case 100: {
        if (tag == 800) {
         parse_reverseInput:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void UniDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // optional .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // optional .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weightparams_, output);
  }

  // optional bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

size_t UniDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // optional .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // optional .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightparams_);
  }

  // optional bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UniDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UniDirectionalLSTMLayerParams*>(&from));
}

void UniDirectionalLSTMLayerParams::MergeFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void UniDirectionalLSTMLayerParams::UnsafeMergeFrom(const UniDirectionalLSTMLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  activations_.MergeFrom(from.activations_);
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.has_weightparams()) {
    mutable_weightparams()->::CoreML::Specification::LSTMWeightParams::MergeFrom(from.weightparams());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void UniDirectionalLSTMLayerParams::CopyFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool UniDirectionalLSTMLayerParams::IsInitialized() const {

  return true;
}

void UniDirectionalLSTMLayerParams::Swap(UniDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UniDirectionalLSTMLayerParams::InternalSwap(UniDirectionalLSTMLayerParams* other) {
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  activations_.UnsafeArenaSwap(&other->activations_);
  std::swap(params_, other->params_);
  std::swap(weightparams_, other->weightparams_);
  std::swap(reverseinput_, other->reverseinput_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UniDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.UniDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UniDirectionalLSTMLayerParams

// optional uint64 inputVectorSize = 1;
void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int UniDirectionalLSTMLayerParams::activations_size() const {
  return activations_.size();
}
void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// optional .CoreML.Specification.LSTMParams params = 15;
bool UniDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// optional .CoreML.Specification.LSTMWeightParams weightParams = 20;
bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;
}
const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_ != NULL ? *weightparams_
                         : *::CoreML::Specification::LSTMWeightParams::internal_default_instance();
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {
  
  if (weightparams_ == NULL) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  delete weightparams_;
  weightparams_ = weightparams;
  if (weightparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// optional bool reverseInput = 100;
void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return reverseinput_;
}
void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

inline const UniDirectionalLSTMLayerParams* UniDirectionalLSTMLayerParams::internal_default_instance() {
  return &UniDirectionalLSTMLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsForwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsBackwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kParamsFieldNumber;
const int BiDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

void BiDirectionalLSTMLayerParams::InitAsDefaultInstance() {
  params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
}

BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

void BiDirectionalLSTMLayerParams::SharedCtor() {
  params_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
  _cached_size_ = 0;
}

BiDirectionalLSTMLayerParams::~BiDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
  SharedDtor();
}

void BiDirectionalLSTMLayerParams::SharedDtor() {
  if (this != &BiDirectionalLSTMLayerParams_default_instance_.get()) {
    delete params_;
  }
}

void BiDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiDirectionalLSTMLayerParams& BiDirectionalLSTMLayerParams::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<BiDirectionalLSTMLayerParams> BiDirectionalLSTMLayerParams_default_instance_;

BiDirectionalLSTMLayerParams* BiDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  BiDirectionalLSTMLayerParams* n = new BiDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
#if defined(__clang__)
#define ZR_HELPER_(f) \
  _Pragma("clang diagnostic push") \
  _Pragma("clang diagnostic ignored \"-Winvalid-offsetof\"") \
  __builtin_offsetof(BiDirectionalLSTMLayerParams, f) \
  _Pragma("clang diagnostic pop")
#else
#define ZR_HELPER_(f) reinterpret_cast<char*>(\
  &reinterpret_cast<BiDirectionalLSTMLayerParams*>(16)->f)
#endif

#define ZR_(first, last) do {\
  ::memset(&(first), 0,\
           ZR_HELPER_(last) - ZR_HELPER_(first) + sizeof(last));\
} while (0)

  ZR_(inputvectorsize_, outputvectorsize_);
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;

#undef ZR_HELPER_
#undef ZR_

  activationsforwardlstm_.Clear();
  activationsbackwardlstm_.Clear();
  weightparams_.Clear();
}

bool BiDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional uint64 inputVectorSize = 1;
      case 1: {
        if (tag == 8) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(16)) goto parse_outputVectorSize;
        break;
      }

      // optional uint64 outputVectorSize = 2;
      case 2: {
        if (tag == 16) {
         parse_outputVectorSize:

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_activationsForwardLSTM;
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
      case 10: {
        if (tag == 82) {
         parse_activationsForwardLSTM:
          DO_(input->IncrementRecursionDepth());
         parse_loop_activationsForwardLSTM:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_activationsforwardlstm()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(82)) goto parse_loop_activationsForwardLSTM;
        if (input->ExpectTag(90)) goto parse_loop_activationsBackwardLSTM;
        input->UnsafeDecrementRecursionDepth();
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
      case 11: {
        if (tag == 90) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_activationsBackwardLSTM:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_activationsbackwardlstm()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(90)) goto parse_loop_activationsBackwardLSTM;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectTag(122)) goto parse_params;
        break;
      }

      // optional .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (tag == 122) {
         parse_params:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_weightParams;
        break;
      }

      // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (tag == 162) {
         parse_weightParams:
          DO_(input->IncrementRecursionDepth());
         parse_loop_weightParams:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_weightparams()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(162)) goto parse_loop_weightParams;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void BiDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  for (unsigned int i = 0, n = this->activationsforwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activationsforwardlstm(i), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  for (unsigned int i = 0, n = this->activationsbackwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, this->activationsbackwardlstm(i), output);
  }

  // optional .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  for (unsigned int i = 0, n = this->weightparams_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weightparams(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

size_t BiDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // optional uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // optional uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // optional .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  {
    unsigned int count = this->activationsforwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsforwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  {
    unsigned int count = this->activationsbackwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsbackwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  {
    unsigned int count = this->weightparams_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weightparams(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiDirectionalLSTMLayerParams*>(&from));
}

void BiDirectionalLSTMLayerParams::MergeFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void BiDirectionalLSTMLayerParams::UnsafeMergeFrom(const BiDirectionalLSTMLayerParams& from) {
  GOOGLE_DCHECK(&from != this);
  activationsforwardlstm_.MergeFrom(from.activationsforwardlstm_);
  activationsbackwardlstm_.MergeFrom(from.activationsbackwardlstm_);
  weightparams_.MergeFrom(from.weightparams_);
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
}

void BiDirectionalLSTMLayerParams::CopyFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool BiDirectionalLSTMLayerParams::IsInitialized() const {

  return true;
}

void BiDirectionalLSTMLayerParams::Swap(BiDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiDirectionalLSTMLayerParams::InternalSwap(BiDirectionalLSTMLayerParams* other) {
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  activationsforwardlstm_.UnsafeArenaSwap(&other->activationsforwardlstm_);
  activationsbackwardlstm_.UnsafeArenaSwap(&other->activationsbackwardlstm_);
  std::swap(params_, other->params_);
  weightparams_.UnsafeArenaSwap(&other->weightparams_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiDirectionalLSTMLayerParams

// optional uint64 inputVectorSize = 1;
void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// optional .CoreML.Specification.LSTMParams params = 15;
bool BiDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return weightparams_.size();
}
void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Get(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

inline const BiDirectionalLSTMLayerParams* BiDirectionalLSTMLayerParams::internal_default_instance() {
  return &BiDirectionalLSTMLayerParams_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkClassifier::kLayersFieldNumber;
const int NeuralNetworkClassifier::kPreprocessingFieldNumber;
const int NeuralNetworkClassifier::kStringClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kInt64ClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kLabelProbabilityLayerNameFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkClassifier::NeuralNetworkClassifier()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkClassifier)
}

void NeuralNetworkClassifier::InitAsDefaultInstance() {
}

NeuralNetworkClassifier::NeuralNetworkClassifier(const NeuralNetworkClassifier& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkClassifier)
}

void NeuralNetworkClassifier::SharedCtor() {
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_ClassLabels();
  _cached_size_ = 0;
}

NeuralNetworkClassifier::~NeuralNetworkClassifier() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkClassifier)
  SharedDtor();
}

void NeuralNetworkClassifier::SharedDtor() {
  labelprobabilitylayername_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_ClassLabels()) {
    clear_ClassLabels();
  }
}

void NeuralNetworkClassifier::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkClassifier& NeuralNetworkClassifier::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkClassifier> NeuralNetworkClassifier_default_instance_;

NeuralNetworkClassifier* NeuralNetworkClassifier::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkClassifier* n = new NeuralNetworkClassifier;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkClassifier::clear_ClassLabels() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  switch (ClassLabels_case()) {
    case kStringClassLabels: {
      delete ClassLabels_.stringclasslabels_;
      break;
    }
    case kInt64ClassLabels: {
      delete ClassLabels_.int64classlabels_;
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}


void NeuralNetworkClassifier::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  layers_.Clear();
  preprocessing_.Clear();
  clear_ClassLabels();
}

bool NeuralNetworkClassifier::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkClassifier)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(16383);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_layers:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_layers;
        if (input->ExpectTag(18)) goto parse_loop_preprocessing;
        input->UnsafeDecrementRecursionDepth();
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (tag == 18) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_preprocessing:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_loop_preprocessing;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectTag(802)) goto parse_stringClassLabels;
        break;
      }

      // optional .CoreML.Specification.StringVector stringClassLabels = 100;
      case 100: {
        if (tag == 802) {
         parse_stringClassLabels:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_stringclasslabels()));
        } else {
          goto handle_unusual;
        }
        goto after_int64classlabels;
        break;
      }

      // optional .CoreML.Specification.Int64Vector int64ClassLabels = 101;
      case 101: {
        if (tag == 810) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_int64classlabels()));
        } else {
          goto handle_unusual;
        }
       after_int64classlabels:
        if (input->ExpectTag(1602)) goto parse_labelProbabilityLayerName;
        break;
      }

      // optional string labelProbabilityLayerName = 200;
      case 200: {
        if (tag == 1602) {
         parse_labelProbabilityLayerName:
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_labelprobabilitylayername()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkClassifier)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkClassifier)
  return false;
#undef DO_
}

void NeuralNetworkClassifier::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkClassifier)
  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // optional .CoreML.Specification.StringVector stringClassLabels = 100;
  if (has_stringclasslabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *ClassLabels_.stringclasslabels_, output);
  }

  // optional .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  if (has_int64classlabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *ClassLabels_.int64classlabels_, output);
  }

  // optional string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      200, this->labelprobabilitylayername(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkClassifier)
}

size_t NeuralNetworkClassifier::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkClassifier)
  size_t total_size = 0;

  // optional string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->labelprobabilitylayername());
  }

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  switch (ClassLabels_case()) {
    // optional .CoreML.Specification.StringVector stringClassLabels = 100;
    case kStringClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.stringclasslabels_);
      break;
    }
    // optional .CoreML.Specification.Int64Vector int64ClassLabels = 101;
    case kInt64ClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.int64classlabels_);
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkClassifier::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkClassifier*>(&from));
}

void NeuralNetworkClassifier::MergeFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkClassifier)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetworkClassifier::UnsafeMergeFrom(const NeuralNetworkClassifier& from) {
  GOOGLE_DCHECK(&from != this);
  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  if (from.labelprobabilitylayername().size() > 0) {

    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
}

void NeuralNetworkClassifier::CopyFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkClassifier)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetworkClassifier::IsInitialized() const {

  return true;
}

void NeuralNetworkClassifier::Swap(NeuralNetworkClassifier* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkClassifier::InternalSwap(NeuralNetworkClassifier* other) {
  layers_.UnsafeArenaSwap(&other->layers_);
  preprocessing_.UnsafeArenaSwap(&other->preprocessing_);
  labelprobabilitylayername_.Swap(&other->labelprobabilitylayername_);
  std::swap(ClassLabels_, other->ClassLabels_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkClassifier::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkClassifier";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkClassifier::layers_size() const {
  return layers_.size();
}
void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkClassifier::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// optional .CoreML.Specification.StringVector stringClassLabels = 100;
bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
void NeuralNetworkClassifier::clear_stringclasslabels() {
  if (has_stringclasslabels()) {
    delete ClassLabels_.stringclasslabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : ::CoreML::Specification::StringVector::default_instance();
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  if (!has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = new ::CoreML::Specification::StringVector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return ClassLabels_.stringclasslabels_;
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}

// optional .CoreML.Specification.Int64Vector int64ClassLabels = 101;
bool NeuralNetworkClassifier::has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
void NeuralNetworkClassifier::clear_int64classlabels() {
  if (has_int64classlabels()) {
    delete ClassLabels_.int64classlabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : ::CoreML::Specification::Int64Vector::default_instance();
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  if (!has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = new ::CoreML::Specification::Int64Vector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return ClassLabels_.int64classlabels_;
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}

// optional string labelProbabilityLayerName = 200;
void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.GetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const ::std::string& value) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value, size_t size) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
::std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  
  return labelprobabilitylayername_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != NULL) {
    
  } else {
    
  }
  labelprobabilitylayername_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
inline const NeuralNetworkClassifier* NeuralNetworkClassifier::internal_default_instance() {
  return &NeuralNetworkClassifier_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkRegressor::kLayersFieldNumber;
const int NeuralNetworkRegressor::kPreprocessingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkRegressor::NeuralNetworkRegressor()
  : ::google::protobuf::MessageLite(), _arena_ptr_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_NeuralNetwork_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkRegressor)
}

void NeuralNetworkRegressor::InitAsDefaultInstance() {
}

NeuralNetworkRegressor::NeuralNetworkRegressor(const NeuralNetworkRegressor& from)
  : ::google::protobuf::MessageLite(),
    _arena_ptr_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkRegressor)
}

void NeuralNetworkRegressor::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetworkRegressor::~NeuralNetworkRegressor() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkRegressor)
  SharedDtor();
}

void NeuralNetworkRegressor::SharedDtor() {
}

void NeuralNetworkRegressor::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkRegressor& NeuralNetworkRegressor::default_instance() {
  protobuf_InitDefaults_NeuralNetwork_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkRegressor> NeuralNetworkRegressor_default_instance_;

NeuralNetworkRegressor* NeuralNetworkRegressor::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkRegressor* n = new NeuralNetworkRegressor;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkRegressor::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkRegressor)
  layers_.Clear();
  preprocessing_.Clear();
}

bool NeuralNetworkRegressor::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkRegressor)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_layers:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_layers;
        if (input->ExpectTag(18)) goto parse_loop_preprocessing;
        input->UnsafeDecrementRecursionDepth();
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (tag == 18) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_preprocessing:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_loop_preprocessing;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkRegressor)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkRegressor)
  return false;
#undef DO_
}

void NeuralNetworkRegressor::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkRegressor)
  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkRegressor)
}

size_t NeuralNetworkRegressor::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkRegressor)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkRegressor::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkRegressor*>(&from));
}

void NeuralNetworkRegressor::MergeFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkRegressor)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void NeuralNetworkRegressor::UnsafeMergeFrom(const NeuralNetworkRegressor& from) {
  GOOGLE_DCHECK(&from != this);
  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
}

void NeuralNetworkRegressor::CopyFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkRegressor)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool NeuralNetworkRegressor::IsInitialized() const {

  return true;
}

void NeuralNetworkRegressor::Swap(NeuralNetworkRegressor* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkRegressor::InternalSwap(NeuralNetworkRegressor* other) {
  layers_.UnsafeArenaSwap(&other->layers_);
  preprocessing_.UnsafeArenaSwap(&other->preprocessing_);
  _unknown_fields_.Swap(&other->_unknown_fields_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkRegressor::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkRegressor";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkRegressor::layers_size() const {
  return layers_.size();
}
void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkRegressor::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

inline const NeuralNetworkRegressor* NeuralNetworkRegressor::internal_default_instance() {
  return &NeuralNetworkRegressor_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace Specification
}  // namespace CoreML

// @@protoc_insertion_point(global_scope)
