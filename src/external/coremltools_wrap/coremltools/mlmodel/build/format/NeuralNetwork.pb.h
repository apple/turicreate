// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#ifndef PROTOBUF_NeuralNetwork_2eproto__INCLUDED
#define PROTOBUF_NeuralNetwork_2eproto__INCLUDED

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3003000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3003000 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/message_lite.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_field_lite.h>
#include <google/protobuf/generated_enum_util.h>
#include "DataStructures.pb.h"  // IWYU pragma: export
#include "Parameters.pb.h"  // IWYU pragma: export
// @@protoc_insertion_point(includes)
namespace CoreML {
namespace Specification {
class AcosLayerParams;
class AcosLayerParamsDefaultTypeInternal;
extern AcosLayerParamsDefaultTypeInternal _AcosLayerParams_default_instance_;
class AcoshLayerParams;
class AcoshLayerParamsDefaultTypeInternal;
extern AcoshLayerParamsDefaultTypeInternal _AcoshLayerParams_default_instance_;
class ActivationELU;
class ActivationELUDefaultTypeInternal;
extern ActivationELUDefaultTypeInternal _ActivationELU_default_instance_;
class ActivationLeakyReLU;
class ActivationLeakyReLUDefaultTypeInternal;
extern ActivationLeakyReLUDefaultTypeInternal _ActivationLeakyReLU_default_instance_;
class ActivationLinear;
class ActivationLinearDefaultTypeInternal;
extern ActivationLinearDefaultTypeInternal _ActivationLinear_default_instance_;
class ActivationPReLU;
class ActivationPReLUDefaultTypeInternal;
extern ActivationPReLUDefaultTypeInternal _ActivationPReLU_default_instance_;
class ActivationParametricSoftplus;
class ActivationParametricSoftplusDefaultTypeInternal;
extern ActivationParametricSoftplusDefaultTypeInternal _ActivationParametricSoftplus_default_instance_;
class ActivationParams;
class ActivationParamsDefaultTypeInternal;
extern ActivationParamsDefaultTypeInternal _ActivationParams_default_instance_;
class ActivationReLU;
class ActivationReLUDefaultTypeInternal;
extern ActivationReLUDefaultTypeInternal _ActivationReLU_default_instance_;
class ActivationScaledTanh;
class ActivationScaledTanhDefaultTypeInternal;
extern ActivationScaledTanhDefaultTypeInternal _ActivationScaledTanh_default_instance_;
class ActivationSigmoid;
class ActivationSigmoidDefaultTypeInternal;
extern ActivationSigmoidDefaultTypeInternal _ActivationSigmoid_default_instance_;
class ActivationSigmoidHard;
class ActivationSigmoidHardDefaultTypeInternal;
extern ActivationSigmoidHardDefaultTypeInternal _ActivationSigmoidHard_default_instance_;
class ActivationSoftplus;
class ActivationSoftplusDefaultTypeInternal;
extern ActivationSoftplusDefaultTypeInternal _ActivationSoftplus_default_instance_;
class ActivationSoftsign;
class ActivationSoftsignDefaultTypeInternal;
extern ActivationSoftsignDefaultTypeInternal _ActivationSoftsign_default_instance_;
class ActivationTanh;
class ActivationTanhDefaultTypeInternal;
extern ActivationTanhDefaultTypeInternal _ActivationTanh_default_instance_;
class ActivationThresholdedReLU;
class ActivationThresholdedReLUDefaultTypeInternal;
extern ActivationThresholdedReLUDefaultTypeInternal _ActivationThresholdedReLU_default_instance_;
class AdamOptimizer;
class AdamOptimizerDefaultTypeInternal;
extern AdamOptimizerDefaultTypeInternal _AdamOptimizer_default_instance_;
class AddBroadcastableLayerParams;
class AddBroadcastableLayerParamsDefaultTypeInternal;
extern AddBroadcastableLayerParamsDefaultTypeInternal _AddBroadcastableLayerParams_default_instance_;
class AddLayerParams;
class AddLayerParamsDefaultTypeInternal;
extern AddLayerParamsDefaultTypeInternal _AddLayerParams_default_instance_;
class ArgMaxLayerParams;
class ArgMaxLayerParamsDefaultTypeInternal;
extern ArgMaxLayerParamsDefaultTypeInternal _ArgMaxLayerParams_default_instance_;
class ArgMinLayerParams;
class ArgMinLayerParamsDefaultTypeInternal;
extern ArgMinLayerParamsDefaultTypeInternal _ArgMinLayerParams_default_instance_;
class ArrayFeatureType;
class ArrayFeatureTypeDefaultTypeInternal;
extern ArrayFeatureTypeDefaultTypeInternal _ArrayFeatureType_default_instance_;
class ArrayFeatureType_EnumeratedShapes;
class ArrayFeatureType_EnumeratedShapesDefaultTypeInternal;
extern ArrayFeatureType_EnumeratedShapesDefaultTypeInternal _ArrayFeatureType_EnumeratedShapes_default_instance_;
class ArrayFeatureType_Shape;
class ArrayFeatureType_ShapeDefaultTypeInternal;
extern ArrayFeatureType_ShapeDefaultTypeInternal _ArrayFeatureType_Shape_default_instance_;
class ArrayFeatureType_ShapeRange;
class ArrayFeatureType_ShapeRangeDefaultTypeInternal;
extern ArrayFeatureType_ShapeRangeDefaultTypeInternal _ArrayFeatureType_ShapeRange_default_instance_;
class AsinLayerParams;
class AsinLayerParamsDefaultTypeInternal;
extern AsinLayerParamsDefaultTypeInternal _AsinLayerParams_default_instance_;
class AsinhLayerParams;
class AsinhLayerParamsDefaultTypeInternal;
extern AsinhLayerParamsDefaultTypeInternal _AsinhLayerParams_default_instance_;
class AtanLayerParams;
class AtanLayerParamsDefaultTypeInternal;
extern AtanLayerParamsDefaultTypeInternal _AtanLayerParams_default_instance_;
class AtanhLayerParams;
class AtanhLayerParamsDefaultTypeInternal;
extern AtanhLayerParamsDefaultTypeInternal _AtanhLayerParams_default_instance_;
class AverageLayerParams;
class AverageLayerParamsDefaultTypeInternal;
extern AverageLayerParamsDefaultTypeInternal _AverageLayerParams_default_instance_;
class BatchedMatMulLayerParams;
class BatchedMatMulLayerParamsDefaultTypeInternal;
extern BatchedMatMulLayerParamsDefaultTypeInternal _BatchedMatMulLayerParams_default_instance_;
class BatchnormLayerParams;
class BatchnormLayerParamsDefaultTypeInternal;
extern BatchnormLayerParamsDefaultTypeInternal _BatchnormLayerParams_default_instance_;
class BiDirectionalLSTMLayerParams;
class BiDirectionalLSTMLayerParamsDefaultTypeInternal;
extern BiDirectionalLSTMLayerParamsDefaultTypeInternal _BiDirectionalLSTMLayerParams_default_instance_;
class BiasLayerParams;
class BiasLayerParamsDefaultTypeInternal;
extern BiasLayerParamsDefaultTypeInternal _BiasLayerParams_default_instance_;
class BoolParameter;
class BoolParameterDefaultTypeInternal;
extern BoolParameterDefaultTypeInternal _BoolParameter_default_instance_;
class BorderAmounts;
class BorderAmountsDefaultTypeInternal;
extern BorderAmountsDefaultTypeInternal _BorderAmounts_default_instance_;
class BorderAmounts_EdgeSizes;
class BorderAmounts_EdgeSizesDefaultTypeInternal;
extern BorderAmounts_EdgeSizesDefaultTypeInternal _BorderAmounts_EdgeSizes_default_instance_;
class BoxCoordinatesMode;
class BoxCoordinatesModeDefaultTypeInternal;
extern BoxCoordinatesModeDefaultTypeInternal _BoxCoordinatesMode_default_instance_;
class BranchLayerParams;
class BranchLayerParamsDefaultTypeInternal;
extern BranchLayerParamsDefaultTypeInternal _BranchLayerParams_default_instance_;
class BroadcastToDynamicLayerParams;
class BroadcastToDynamicLayerParamsDefaultTypeInternal;
extern BroadcastToDynamicLayerParamsDefaultTypeInternal _BroadcastToDynamicLayerParams_default_instance_;
class BroadcastToLikeLayerParams;
class BroadcastToLikeLayerParamsDefaultTypeInternal;
extern BroadcastToLikeLayerParamsDefaultTypeInternal _BroadcastToLikeLayerParams_default_instance_;
class BroadcastToStaticLayerParams;
class BroadcastToStaticLayerParamsDefaultTypeInternal;
extern BroadcastToStaticLayerParamsDefaultTypeInternal _BroadcastToStaticLayerParams_default_instance_;
class CategoricalCrossEntropyLossLayer;
class CategoricalCrossEntropyLossLayerDefaultTypeInternal;
extern CategoricalCrossEntropyLossLayerDefaultTypeInternal _CategoricalCrossEntropyLossLayer_default_instance_;
class CategoricalDistributionLayerParams;
class CategoricalDistributionLayerParamsDefaultTypeInternal;
extern CategoricalDistributionLayerParamsDefaultTypeInternal _CategoricalDistributionLayerParams_default_instance_;
class CeilLayerParams;
class CeilLayerParamsDefaultTypeInternal;
extern CeilLayerParamsDefaultTypeInternal _CeilLayerParams_default_instance_;
class ClipLayerParams;
class ClipLayerParamsDefaultTypeInternal;
extern ClipLayerParamsDefaultTypeInternal _ClipLayerParams_default_instance_;
class ConcatLayerParams;
class ConcatLayerParamsDefaultTypeInternal;
extern ConcatLayerParamsDefaultTypeInternal _ConcatLayerParams_default_instance_;
class ConcatNDLayerParams;
class ConcatNDLayerParamsDefaultTypeInternal;
extern ConcatNDLayerParamsDefaultTypeInternal _ConcatNDLayerParams_default_instance_;
class ConstantPaddingLayerParams;
class ConstantPaddingLayerParamsDefaultTypeInternal;
extern ConstantPaddingLayerParamsDefaultTypeInternal _ConstantPaddingLayerParams_default_instance_;
class ConvolutionLayerParams;
class ConvolutionLayerParamsDefaultTypeInternal;
extern ConvolutionLayerParamsDefaultTypeInternal _ConvolutionLayerParams_default_instance_;
class CopyLayerParams;
class CopyLayerParamsDefaultTypeInternal;
extern CopyLayerParamsDefaultTypeInternal _CopyLayerParams_default_instance_;
class CosLayerParams;
class CosLayerParamsDefaultTypeInternal;
extern CosLayerParamsDefaultTypeInternal _CosLayerParams_default_instance_;
class CoshLayerParams;
class CoshLayerParamsDefaultTypeInternal;
extern CoshLayerParamsDefaultTypeInternal _CoshLayerParams_default_instance_;
class CropLayerParams;
class CropLayerParamsDefaultTypeInternal;
extern CropLayerParamsDefaultTypeInternal _CropLayerParams_default_instance_;
class CropResizeLayerParams;
class CropResizeLayerParamsDefaultTypeInternal;
extern CropResizeLayerParamsDefaultTypeInternal _CropResizeLayerParams_default_instance_;
class CustomLayerParams;
class CustomLayerParamsDefaultTypeInternal;
extern CustomLayerParamsDefaultTypeInternal _CustomLayerParams_default_instance_;
class CustomLayerParams_CustomLayerParamValue;
class CustomLayerParams_CustomLayerParamValueDefaultTypeInternal;
extern CustomLayerParams_CustomLayerParamValueDefaultTypeInternal _CustomLayerParams_CustomLayerParamValue_default_instance_;
class CustomLayerParams_ParametersEntry;
class CustomLayerParams_ParametersEntryDefaultTypeInternal;
extern CustomLayerParams_ParametersEntryDefaultTypeInternal _CustomLayerParams_ParametersEntry_default_instance_;
class DictionaryFeatureType;
class DictionaryFeatureTypeDefaultTypeInternal;
extern DictionaryFeatureTypeDefaultTypeInternal _DictionaryFeatureType_default_instance_;
class DivideBroadcastableLayerParams;
class DivideBroadcastableLayerParamsDefaultTypeInternal;
extern DivideBroadcastableLayerParamsDefaultTypeInternal _DivideBroadcastableLayerParams_default_instance_;
class DotProductLayerParams;
class DotProductLayerParamsDefaultTypeInternal;
extern DotProductLayerParamsDefaultTypeInternal _DotProductLayerParams_default_instance_;
class DoubleFeatureType;
class DoubleFeatureTypeDefaultTypeInternal;
extern DoubleFeatureTypeDefaultTypeInternal _DoubleFeatureType_default_instance_;
class DoubleParameter;
class DoubleParameterDefaultTypeInternal;
extern DoubleParameterDefaultTypeInternal _DoubleParameter_default_instance_;
class DoubleRange;
class DoubleRangeDefaultTypeInternal;
extern DoubleRangeDefaultTypeInternal _DoubleRange_default_instance_;
class DoubleVector;
class DoubleVectorDefaultTypeInternal;
extern DoubleVectorDefaultTypeInternal _DoubleVector_default_instance_;
class EmbeddingLayerParams;
class EmbeddingLayerParamsDefaultTypeInternal;
extern EmbeddingLayerParamsDefaultTypeInternal _EmbeddingLayerParams_default_instance_;
class EmbeddingNDLayerParams;
class EmbeddingNDLayerParamsDefaultTypeInternal;
extern EmbeddingNDLayerParamsDefaultTypeInternal _EmbeddingNDLayerParams_default_instance_;
class EqualLayerParams;
class EqualLayerParamsDefaultTypeInternal;
extern EqualLayerParamsDefaultTypeInternal _EqualLayerParams_default_instance_;
class ErfLayerParams;
class ErfLayerParamsDefaultTypeInternal;
extern ErfLayerParamsDefaultTypeInternal _ErfLayerParams_default_instance_;
class Exp2LayerParams;
class Exp2LayerParamsDefaultTypeInternal;
extern Exp2LayerParamsDefaultTypeInternal _Exp2LayerParams_default_instance_;
class ExpandDimsLayerParams;
class ExpandDimsLayerParamsDefaultTypeInternal;
extern ExpandDimsLayerParamsDefaultTypeInternal _ExpandDimsLayerParams_default_instance_;
class FeatureType;
class FeatureTypeDefaultTypeInternal;
extern FeatureTypeDefaultTypeInternal _FeatureType_default_instance_;
class FillDynamicLayerParams;
class FillDynamicLayerParamsDefaultTypeInternal;
extern FillDynamicLayerParamsDefaultTypeInternal _FillDynamicLayerParams_default_instance_;
class FillLikeLayerParams;
class FillLikeLayerParamsDefaultTypeInternal;
extern FillLikeLayerParamsDefaultTypeInternal _FillLikeLayerParams_default_instance_;
class FillStaticLayerParams;
class FillStaticLayerParamsDefaultTypeInternal;
extern FillStaticLayerParamsDefaultTypeInternal _FillStaticLayerParams_default_instance_;
class FlattenLayerParams;
class FlattenLayerParamsDefaultTypeInternal;
extern FlattenLayerParamsDefaultTypeInternal _FlattenLayerParams_default_instance_;
class FlattenTo2DLayerParams;
class FlattenTo2DLayerParamsDefaultTypeInternal;
extern FlattenTo2DLayerParamsDefaultTypeInternal _FlattenTo2DLayerParams_default_instance_;
class FloatVector;
class FloatVectorDefaultTypeInternal;
extern FloatVectorDefaultTypeInternal _FloatVector_default_instance_;
class FloorDivBroadcastableLayerParams;
class FloorDivBroadcastableLayerParamsDefaultTypeInternal;
extern FloorDivBroadcastableLayerParamsDefaultTypeInternal _FloorDivBroadcastableLayerParams_default_instance_;
class FloorLayerParams;
class FloorLayerParamsDefaultTypeInternal;
extern FloorLayerParamsDefaultTypeInternal _FloorLayerParams_default_instance_;
class GRULayerParams;
class GRULayerParamsDefaultTypeInternal;
extern GRULayerParamsDefaultTypeInternal _GRULayerParams_default_instance_;
class GatherAlongAxisLayerParams;
class GatherAlongAxisLayerParamsDefaultTypeInternal;
extern GatherAlongAxisLayerParamsDefaultTypeInternal _GatherAlongAxisLayerParams_default_instance_;
class GatherLayerParams;
class GatherLayerParamsDefaultTypeInternal;
extern GatherLayerParamsDefaultTypeInternal _GatherLayerParams_default_instance_;
class GatherNDLayerParams;
class GatherNDLayerParamsDefaultTypeInternal;
extern GatherNDLayerParamsDefaultTypeInternal _GatherNDLayerParams_default_instance_;
class GeluLayerParams;
class GeluLayerParamsDefaultTypeInternal;
extern GeluLayerParamsDefaultTypeInternal _GeluLayerParams_default_instance_;
class GetShapeLayerParams;
class GetShapeLayerParamsDefaultTypeInternal;
extern GetShapeLayerParamsDefaultTypeInternal _GetShapeLayerParams_default_instance_;
class GreaterEqualLayerParams;
class GreaterEqualLayerParamsDefaultTypeInternal;
extern GreaterEqualLayerParamsDefaultTypeInternal _GreaterEqualLayerParams_default_instance_;
class GreaterThanLayerParams;
class GreaterThanLayerParamsDefaultTypeInternal;
extern GreaterThanLayerParamsDefaultTypeInternal _GreaterThanLayerParams_default_instance_;
class ImageFeatureType;
class ImageFeatureTypeDefaultTypeInternal;
extern ImageFeatureTypeDefaultTypeInternal _ImageFeatureType_default_instance_;
class ImageFeatureType_EnumeratedImageSizes;
class ImageFeatureType_EnumeratedImageSizesDefaultTypeInternal;
extern ImageFeatureType_EnumeratedImageSizesDefaultTypeInternal _ImageFeatureType_EnumeratedImageSizes_default_instance_;
class ImageFeatureType_ImageSize;
class ImageFeatureType_ImageSizeDefaultTypeInternal;
extern ImageFeatureType_ImageSizeDefaultTypeInternal _ImageFeatureType_ImageSize_default_instance_;
class ImageFeatureType_ImageSizeRange;
class ImageFeatureType_ImageSizeRangeDefaultTypeInternal;
extern ImageFeatureType_ImageSizeRangeDefaultTypeInternal _ImageFeatureType_ImageSizeRange_default_instance_;
class InnerProductLayerParams;
class InnerProductLayerParamsDefaultTypeInternal;
extern InnerProductLayerParamsDefaultTypeInternal _InnerProductLayerParams_default_instance_;
class Int64FeatureType;
class Int64FeatureTypeDefaultTypeInternal;
extern Int64FeatureTypeDefaultTypeInternal _Int64FeatureType_default_instance_;
class Int64Parameter;
class Int64ParameterDefaultTypeInternal;
extern Int64ParameterDefaultTypeInternal _Int64Parameter_default_instance_;
class Int64Range;
class Int64RangeDefaultTypeInternal;
extern Int64RangeDefaultTypeInternal _Int64Range_default_instance_;
class Int64Set;
class Int64SetDefaultTypeInternal;
extern Int64SetDefaultTypeInternal _Int64Set_default_instance_;
class Int64ToDoubleMap;
class Int64ToDoubleMapDefaultTypeInternal;
extern Int64ToDoubleMapDefaultTypeInternal _Int64ToDoubleMap_default_instance_;
class Int64ToDoubleMap_MapEntry;
class Int64ToDoubleMap_MapEntryDefaultTypeInternal;
extern Int64ToDoubleMap_MapEntryDefaultTypeInternal _Int64ToDoubleMap_MapEntry_default_instance_;
class Int64ToStringMap;
class Int64ToStringMapDefaultTypeInternal;
extern Int64ToStringMapDefaultTypeInternal _Int64ToStringMap_default_instance_;
class Int64ToStringMap_MapEntry;
class Int64ToStringMap_MapEntryDefaultTypeInternal;
extern Int64ToStringMap_MapEntryDefaultTypeInternal _Int64ToStringMap_MapEntry_default_instance_;
class Int64Vector;
class Int64VectorDefaultTypeInternal;
extern Int64VectorDefaultTypeInternal _Int64Vector_default_instance_;
class L2NormalizeLayerParams;
class L2NormalizeLayerParamsDefaultTypeInternal;
extern L2NormalizeLayerParamsDefaultTypeInternal _L2NormalizeLayerParams_default_instance_;
class LRNLayerParams;
class LRNLayerParamsDefaultTypeInternal;
extern LRNLayerParamsDefaultTypeInternal _LRNLayerParams_default_instance_;
class LSTMParams;
class LSTMParamsDefaultTypeInternal;
extern LSTMParamsDefaultTypeInternal _LSTMParams_default_instance_;
class LSTMWeightParams;
class LSTMWeightParamsDefaultTypeInternal;
extern LSTMWeightParamsDefaultTypeInternal _LSTMWeightParams_default_instance_;
class LayerNormalizationLayerParams;
class LayerNormalizationLayerParamsDefaultTypeInternal;
extern LayerNormalizationLayerParamsDefaultTypeInternal _LayerNormalizationLayerParams_default_instance_;
class LessEqualLayerParams;
class LessEqualLayerParamsDefaultTypeInternal;
extern LessEqualLayerParamsDefaultTypeInternal _LessEqualLayerParams_default_instance_;
class LessThanLayerParams;
class LessThanLayerParamsDefaultTypeInternal;
extern LessThanLayerParamsDefaultTypeInternal _LessThanLayerParams_default_instance_;
class LinearQuantizationParams;
class LinearQuantizationParamsDefaultTypeInternal;
extern LinearQuantizationParamsDefaultTypeInternal _LinearQuantizationParams_default_instance_;
class LoadConstantLayerParams;
class LoadConstantLayerParamsDefaultTypeInternal;
extern LoadConstantLayerParamsDefaultTypeInternal _LoadConstantLayerParams_default_instance_;
class LoadConstantNDLayerParams;
class LoadConstantNDLayerParamsDefaultTypeInternal;
extern LoadConstantNDLayerParamsDefaultTypeInternal _LoadConstantNDLayerParams_default_instance_;
class LogicalAndLayerParams;
class LogicalAndLayerParamsDefaultTypeInternal;
extern LogicalAndLayerParamsDefaultTypeInternal _LogicalAndLayerParams_default_instance_;
class LogicalNotLayerParams;
class LogicalNotLayerParamsDefaultTypeInternal;
extern LogicalNotLayerParamsDefaultTypeInternal _LogicalNotLayerParams_default_instance_;
class LogicalOrLayerParams;
class LogicalOrLayerParamsDefaultTypeInternal;
extern LogicalOrLayerParamsDefaultTypeInternal _LogicalOrLayerParams_default_instance_;
class LogicalXorLayerParams;
class LogicalXorLayerParamsDefaultTypeInternal;
extern LogicalXorLayerParamsDefaultTypeInternal _LogicalXorLayerParams_default_instance_;
class LookUpTableQuantizationParams;
class LookUpTableQuantizationParamsDefaultTypeInternal;
extern LookUpTableQuantizationParamsDefaultTypeInternal _LookUpTableQuantizationParams_default_instance_;
class LoopBreakLayerParams;
class LoopBreakLayerParamsDefaultTypeInternal;
extern LoopBreakLayerParamsDefaultTypeInternal _LoopBreakLayerParams_default_instance_;
class LoopContinueLayerParams;
class LoopContinueLayerParamsDefaultTypeInternal;
extern LoopContinueLayerParamsDefaultTypeInternal _LoopContinueLayerParams_default_instance_;
class LoopLayerParams;
class LoopLayerParamsDefaultTypeInternal;
extern LoopLayerParamsDefaultTypeInternal _LoopLayerParams_default_instance_;
class LossLayer;
class LossLayerDefaultTypeInternal;
extern LossLayerDefaultTypeInternal _LossLayer_default_instance_;
class LowerTriangularLayerParams;
class LowerTriangularLayerParamsDefaultTypeInternal;
extern LowerTriangularLayerParamsDefaultTypeInternal _LowerTriangularLayerParams_default_instance_;
class MatrixBandPartLayerParams;
class MatrixBandPartLayerParamsDefaultTypeInternal;
extern MatrixBandPartLayerParamsDefaultTypeInternal _MatrixBandPartLayerParams_default_instance_;
class MaxBroadcastableLayerParams;
class MaxBroadcastableLayerParamsDefaultTypeInternal;
extern MaxBroadcastableLayerParamsDefaultTypeInternal _MaxBroadcastableLayerParams_default_instance_;
class MaxLayerParams;
class MaxLayerParamsDefaultTypeInternal;
extern MaxLayerParamsDefaultTypeInternal _MaxLayerParams_default_instance_;
class MeanSquaredErrorLossLayer;
class MeanSquaredErrorLossLayerDefaultTypeInternal;
extern MeanSquaredErrorLossLayerDefaultTypeInternal _MeanSquaredErrorLossLayer_default_instance_;
class MeanVarianceNormalizeLayerParams;
class MeanVarianceNormalizeLayerParamsDefaultTypeInternal;
extern MeanVarianceNormalizeLayerParamsDefaultTypeInternal _MeanVarianceNormalizeLayerParams_default_instance_;
class MinBroadcastableLayerParams;
class MinBroadcastableLayerParamsDefaultTypeInternal;
extern MinBroadcastableLayerParamsDefaultTypeInternal _MinBroadcastableLayerParams_default_instance_;
class MinLayerParams;
class MinLayerParamsDefaultTypeInternal;
extern MinLayerParamsDefaultTypeInternal _MinLayerParams_default_instance_;
class ModBroadcastableLayerParams;
class ModBroadcastableLayerParamsDefaultTypeInternal;
extern ModBroadcastableLayerParamsDefaultTypeInternal _ModBroadcastableLayerParams_default_instance_;
class MultiplyBroadcastableLayerParams;
class MultiplyBroadcastableLayerParamsDefaultTypeInternal;
extern MultiplyBroadcastableLayerParamsDefaultTypeInternal _MultiplyBroadcastableLayerParams_default_instance_;
class MultiplyLayerParams;
class MultiplyLayerParamsDefaultTypeInternal;
extern MultiplyLayerParamsDefaultTypeInternal _MultiplyLayerParams_default_instance_;
class NetworkUpdateParameters;
class NetworkUpdateParametersDefaultTypeInternal;
extern NetworkUpdateParametersDefaultTypeInternal _NetworkUpdateParameters_default_instance_;
class NeuralNetwork;
class NeuralNetworkDefaultTypeInternal;
extern NeuralNetworkDefaultTypeInternal _NeuralNetwork_default_instance_;
class NeuralNetworkClassifier;
class NeuralNetworkClassifierDefaultTypeInternal;
extern NeuralNetworkClassifierDefaultTypeInternal _NeuralNetworkClassifier_default_instance_;
class NeuralNetworkImageScaler;
class NeuralNetworkImageScalerDefaultTypeInternal;
extern NeuralNetworkImageScalerDefaultTypeInternal _NeuralNetworkImageScaler_default_instance_;
class NeuralNetworkLayer;
class NeuralNetworkLayerDefaultTypeInternal;
extern NeuralNetworkLayerDefaultTypeInternal _NeuralNetworkLayer_default_instance_;
class NeuralNetworkMeanImage;
class NeuralNetworkMeanImageDefaultTypeInternal;
extern NeuralNetworkMeanImageDefaultTypeInternal _NeuralNetworkMeanImage_default_instance_;
class NeuralNetworkPreprocessing;
class NeuralNetworkPreprocessingDefaultTypeInternal;
extern NeuralNetworkPreprocessingDefaultTypeInternal _NeuralNetworkPreprocessing_default_instance_;
class NeuralNetworkRegressor;
class NeuralNetworkRegressorDefaultTypeInternal;
extern NeuralNetworkRegressorDefaultTypeInternal _NeuralNetworkRegressor_default_instance_;
class NonMaximumSuppressionLayerParams;
class NonMaximumSuppressionLayerParamsDefaultTypeInternal;
extern NonMaximumSuppressionLayerParamsDefaultTypeInternal _NonMaximumSuppressionLayerParams_default_instance_;
class NotEqualLayerParams;
class NotEqualLayerParamsDefaultTypeInternal;
extern NotEqualLayerParamsDefaultTypeInternal _NotEqualLayerParams_default_instance_;
class Optimizer;
class OptimizerDefaultTypeInternal;
extern OptimizerDefaultTypeInternal _Optimizer_default_instance_;
class PaddingLayerParams;
class PaddingLayerParamsDefaultTypeInternal;
extern PaddingLayerParamsDefaultTypeInternal _PaddingLayerParams_default_instance_;
class PaddingLayerParams_PaddingConstant;
class PaddingLayerParams_PaddingConstantDefaultTypeInternal;
extern PaddingLayerParams_PaddingConstantDefaultTypeInternal _PaddingLayerParams_PaddingConstant_default_instance_;
class PaddingLayerParams_PaddingReflection;
class PaddingLayerParams_PaddingReflectionDefaultTypeInternal;
extern PaddingLayerParams_PaddingReflectionDefaultTypeInternal _PaddingLayerParams_PaddingReflection_default_instance_;
class PaddingLayerParams_PaddingReplication;
class PaddingLayerParams_PaddingReplicationDefaultTypeInternal;
extern PaddingLayerParams_PaddingReplicationDefaultTypeInternal _PaddingLayerParams_PaddingReplication_default_instance_;
class PermuteLayerParams;
class PermuteLayerParamsDefaultTypeInternal;
extern PermuteLayerParamsDefaultTypeInternal _PermuteLayerParams_default_instance_;
class PoolingLayerParams;
class PoolingLayerParamsDefaultTypeInternal;
extern PoolingLayerParamsDefaultTypeInternal _PoolingLayerParams_default_instance_;
class PoolingLayerParams_ValidCompletePadding;
class PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal;
extern PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal _PoolingLayerParams_ValidCompletePadding_default_instance_;
class PowBroadcastableLayerParams;
class PowBroadcastableLayerParamsDefaultTypeInternal;
extern PowBroadcastableLayerParamsDefaultTypeInternal _PowBroadcastableLayerParams_default_instance_;
class QuantizationParams;
class QuantizationParamsDefaultTypeInternal;
extern QuantizationParamsDefaultTypeInternal _QuantizationParams_default_instance_;
class RandomBernoulliDynamicLayerParams;
class RandomBernoulliDynamicLayerParamsDefaultTypeInternal;
extern RandomBernoulliDynamicLayerParamsDefaultTypeInternal _RandomBernoulliDynamicLayerParams_default_instance_;
class RandomBernoulliLikeLayerParams;
class RandomBernoulliLikeLayerParamsDefaultTypeInternal;
extern RandomBernoulliLikeLayerParamsDefaultTypeInternal _RandomBernoulliLikeLayerParams_default_instance_;
class RandomBernoulliStaticLayerParams;
class RandomBernoulliStaticLayerParamsDefaultTypeInternal;
extern RandomBernoulliStaticLayerParamsDefaultTypeInternal _RandomBernoulliStaticLayerParams_default_instance_;
class RandomNormalDynamicLayerParams;
class RandomNormalDynamicLayerParamsDefaultTypeInternal;
extern RandomNormalDynamicLayerParamsDefaultTypeInternal _RandomNormalDynamicLayerParams_default_instance_;
class RandomNormalLikeLayerParams;
class RandomNormalLikeLayerParamsDefaultTypeInternal;
extern RandomNormalLikeLayerParamsDefaultTypeInternal _RandomNormalLikeLayerParams_default_instance_;
class RandomNormalStaticLayerParams;
class RandomNormalStaticLayerParamsDefaultTypeInternal;
extern RandomNormalStaticLayerParamsDefaultTypeInternal _RandomNormalStaticLayerParams_default_instance_;
class RandomUniformDynamicLayerParams;
class RandomUniformDynamicLayerParamsDefaultTypeInternal;
extern RandomUniformDynamicLayerParamsDefaultTypeInternal _RandomUniformDynamicLayerParams_default_instance_;
class RandomUniformLikeLayerParams;
class RandomUniformLikeLayerParamsDefaultTypeInternal;
extern RandomUniformLikeLayerParamsDefaultTypeInternal _RandomUniformLikeLayerParams_default_instance_;
class RandomUniformStaticLayerParams;
class RandomUniformStaticLayerParamsDefaultTypeInternal;
extern RandomUniformStaticLayerParamsDefaultTypeInternal _RandomUniformStaticLayerParams_default_instance_;
class RangeDynamicLayerParams;
class RangeDynamicLayerParamsDefaultTypeInternal;
extern RangeDynamicLayerParamsDefaultTypeInternal _RangeDynamicLayerParams_default_instance_;
class RangeStaticLayerParams;
class RangeStaticLayerParamsDefaultTypeInternal;
extern RangeStaticLayerParamsDefaultTypeInternal _RangeStaticLayerParams_default_instance_;
class RankPreservingReshapeLayerParams;
class RankPreservingReshapeLayerParamsDefaultTypeInternal;
extern RankPreservingReshapeLayerParamsDefaultTypeInternal _RankPreservingReshapeLayerParams_default_instance_;
class ReduceL1LayerParams;
class ReduceL1LayerParamsDefaultTypeInternal;
extern ReduceL1LayerParamsDefaultTypeInternal _ReduceL1LayerParams_default_instance_;
class ReduceL2LayerParams;
class ReduceL2LayerParamsDefaultTypeInternal;
extern ReduceL2LayerParamsDefaultTypeInternal _ReduceL2LayerParams_default_instance_;
class ReduceLayerParams;
class ReduceLayerParamsDefaultTypeInternal;
extern ReduceLayerParamsDefaultTypeInternal _ReduceLayerParams_default_instance_;
class ReduceLogSumExpLayerParams;
class ReduceLogSumExpLayerParamsDefaultTypeInternal;
extern ReduceLogSumExpLayerParamsDefaultTypeInternal _ReduceLogSumExpLayerParams_default_instance_;
class ReduceLogSumLayerParams;
class ReduceLogSumLayerParamsDefaultTypeInternal;
extern ReduceLogSumLayerParamsDefaultTypeInternal _ReduceLogSumLayerParams_default_instance_;
class ReduceMaxLayerParams;
class ReduceMaxLayerParamsDefaultTypeInternal;
extern ReduceMaxLayerParamsDefaultTypeInternal _ReduceMaxLayerParams_default_instance_;
class ReduceMeanLayerParams;
class ReduceMeanLayerParamsDefaultTypeInternal;
extern ReduceMeanLayerParamsDefaultTypeInternal _ReduceMeanLayerParams_default_instance_;
class ReduceMinLayerParams;
class ReduceMinLayerParamsDefaultTypeInternal;
extern ReduceMinLayerParamsDefaultTypeInternal _ReduceMinLayerParams_default_instance_;
class ReduceProdLayerParams;
class ReduceProdLayerParamsDefaultTypeInternal;
extern ReduceProdLayerParamsDefaultTypeInternal _ReduceProdLayerParams_default_instance_;
class ReduceSumLayerParams;
class ReduceSumLayerParamsDefaultTypeInternal;
extern ReduceSumLayerParamsDefaultTypeInternal _ReduceSumLayerParams_default_instance_;
class ReduceSumSquareLayerParams;
class ReduceSumSquareLayerParamsDefaultTypeInternal;
extern ReduceSumSquareLayerParamsDefaultTypeInternal _ReduceSumSquareLayerParams_default_instance_;
class ReorganizeDataLayerParams;
class ReorganizeDataLayerParamsDefaultTypeInternal;
extern ReorganizeDataLayerParamsDefaultTypeInternal _ReorganizeDataLayerParams_default_instance_;
class ReshapeDynamicLayerParams;
class ReshapeDynamicLayerParamsDefaultTypeInternal;
extern ReshapeDynamicLayerParamsDefaultTypeInternal _ReshapeDynamicLayerParams_default_instance_;
class ReshapeLayerParams;
class ReshapeLayerParamsDefaultTypeInternal;
extern ReshapeLayerParamsDefaultTypeInternal _ReshapeLayerParams_default_instance_;
class ReshapeLikeLayerParams;
class ReshapeLikeLayerParamsDefaultTypeInternal;
extern ReshapeLikeLayerParamsDefaultTypeInternal _ReshapeLikeLayerParams_default_instance_;
class ReshapeStaticLayerParams;
class ReshapeStaticLayerParamsDefaultTypeInternal;
extern ReshapeStaticLayerParamsDefaultTypeInternal _ReshapeStaticLayerParams_default_instance_;
class ResizeBilinearLayerParams;
class ResizeBilinearLayerParamsDefaultTypeInternal;
extern ResizeBilinearLayerParamsDefaultTypeInternal _ResizeBilinearLayerParams_default_instance_;
class ReverseLayerParams;
class ReverseLayerParamsDefaultTypeInternal;
extern ReverseLayerParamsDefaultTypeInternal _ReverseLayerParams_default_instance_;
class ReverseSeqLayerParams;
class ReverseSeqLayerParamsDefaultTypeInternal;
extern ReverseSeqLayerParamsDefaultTypeInternal _ReverseSeqLayerParams_default_instance_;
class RoundLayerParams;
class RoundLayerParamsDefaultTypeInternal;
extern RoundLayerParamsDefaultTypeInternal _RoundLayerParams_default_instance_;
class SGDOptimizer;
class SGDOptimizerDefaultTypeInternal;
extern SGDOptimizerDefaultTypeInternal _SGDOptimizer_default_instance_;
class SamePadding;
class SamePaddingDefaultTypeInternal;
extern SamePaddingDefaultTypeInternal _SamePadding_default_instance_;
class SamplingMode;
class SamplingModeDefaultTypeInternal;
extern SamplingModeDefaultTypeInternal _SamplingMode_default_instance_;
class ScaleLayerParams;
class ScaleLayerParamsDefaultTypeInternal;
extern ScaleLayerParamsDefaultTypeInternal _ScaleLayerParams_default_instance_;
class ScatterAlongAxisLayerParams;
class ScatterAlongAxisLayerParamsDefaultTypeInternal;
extern ScatterAlongAxisLayerParamsDefaultTypeInternal _ScatterAlongAxisLayerParams_default_instance_;
class ScatterLayerParams;
class ScatterLayerParamsDefaultTypeInternal;
extern ScatterLayerParamsDefaultTypeInternal _ScatterLayerParams_default_instance_;
class ScatterNDLayerParams;
class ScatterNDLayerParamsDefaultTypeInternal;
extern ScatterNDLayerParamsDefaultTypeInternal _ScatterNDLayerParams_default_instance_;
class SequenceFeatureType;
class SequenceFeatureTypeDefaultTypeInternal;
extern SequenceFeatureTypeDefaultTypeInternal _SequenceFeatureType_default_instance_;
class SequenceRepeatLayerParams;
class SequenceRepeatLayerParamsDefaultTypeInternal;
extern SequenceRepeatLayerParamsDefaultTypeInternal _SequenceRepeatLayerParams_default_instance_;
class SignLayerParams;
class SignLayerParamsDefaultTypeInternal;
extern SignLayerParamsDefaultTypeInternal _SignLayerParams_default_instance_;
class SimpleRecurrentLayerParams;
class SimpleRecurrentLayerParamsDefaultTypeInternal;
extern SimpleRecurrentLayerParamsDefaultTypeInternal _SimpleRecurrentLayerParams_default_instance_;
class SinLayerParams;
class SinLayerParamsDefaultTypeInternal;
extern SinLayerParamsDefaultTypeInternal _SinLayerParams_default_instance_;
class SinhLayerParams;
class SinhLayerParamsDefaultTypeInternal;
extern SinhLayerParamsDefaultTypeInternal _SinhLayerParams_default_instance_;
class SizeRange;
class SizeRangeDefaultTypeInternal;
extern SizeRangeDefaultTypeInternal _SizeRange_default_instance_;
class SliceDynamicLayerParams;
class SliceDynamicLayerParamsDefaultTypeInternal;
extern SliceDynamicLayerParamsDefaultTypeInternal _SliceDynamicLayerParams_default_instance_;
class SliceLayerParams;
class SliceLayerParamsDefaultTypeInternal;
extern SliceLayerParamsDefaultTypeInternal _SliceLayerParams_default_instance_;
class SliceStaticLayerParams;
class SliceStaticLayerParamsDefaultTypeInternal;
extern SliceStaticLayerParamsDefaultTypeInternal _SliceStaticLayerParams_default_instance_;
class SlidingWindowsLayerParams;
class SlidingWindowsLayerParamsDefaultTypeInternal;
extern SlidingWindowsLayerParamsDefaultTypeInternal _SlidingWindowsLayerParams_default_instance_;
class SoftmaxLayerParams;
class SoftmaxLayerParamsDefaultTypeInternal;
extern SoftmaxLayerParamsDefaultTypeInternal _SoftmaxLayerParams_default_instance_;
class SoftmaxNDLayerParams;
class SoftmaxNDLayerParamsDefaultTypeInternal;
extern SoftmaxNDLayerParamsDefaultTypeInternal _SoftmaxNDLayerParams_default_instance_;
class SplitLayerParams;
class SplitLayerParamsDefaultTypeInternal;
extern SplitLayerParamsDefaultTypeInternal _SplitLayerParams_default_instance_;
class SplitNDLayerParams;
class SplitNDLayerParamsDefaultTypeInternal;
extern SplitNDLayerParamsDefaultTypeInternal _SplitNDLayerParams_default_instance_;
class SqueezeLayerParams;
class SqueezeLayerParamsDefaultTypeInternal;
extern SqueezeLayerParamsDefaultTypeInternal _SqueezeLayerParams_default_instance_;
class StackLayerParams;
class StackLayerParamsDefaultTypeInternal;
extern StackLayerParamsDefaultTypeInternal _StackLayerParams_default_instance_;
class StringFeatureType;
class StringFeatureTypeDefaultTypeInternal;
extern StringFeatureTypeDefaultTypeInternal _StringFeatureType_default_instance_;
class StringParameter;
class StringParameterDefaultTypeInternal;
extern StringParameterDefaultTypeInternal _StringParameter_default_instance_;
class StringToDoubleMap;
class StringToDoubleMapDefaultTypeInternal;
extern StringToDoubleMapDefaultTypeInternal _StringToDoubleMap_default_instance_;
class StringToDoubleMap_MapEntry;
class StringToDoubleMap_MapEntryDefaultTypeInternal;
extern StringToDoubleMap_MapEntryDefaultTypeInternal _StringToDoubleMap_MapEntry_default_instance_;
class StringToInt64Map;
class StringToInt64MapDefaultTypeInternal;
extern StringToInt64MapDefaultTypeInternal _StringToInt64Map_default_instance_;
class StringToInt64Map_MapEntry;
class StringToInt64Map_MapEntryDefaultTypeInternal;
extern StringToInt64Map_MapEntryDefaultTypeInternal _StringToInt64Map_MapEntry_default_instance_;
class StringVector;
class StringVectorDefaultTypeInternal;
extern StringVectorDefaultTypeInternal _StringVector_default_instance_;
class SubtractBroadcastableLayerParams;
class SubtractBroadcastableLayerParamsDefaultTypeInternal;
extern SubtractBroadcastableLayerParamsDefaultTypeInternal _SubtractBroadcastableLayerParams_default_instance_;
class TanLayerParams;
class TanLayerParamsDefaultTypeInternal;
extern TanLayerParamsDefaultTypeInternal _TanLayerParams_default_instance_;
class TanhLayerParams;
class TanhLayerParamsDefaultTypeInternal;
extern TanhLayerParamsDefaultTypeInternal _TanhLayerParams_default_instance_;
class Tensor;
class TensorDefaultTypeInternal;
extern TensorDefaultTypeInternal _Tensor_default_instance_;
class TileLayerParams;
class TileLayerParamsDefaultTypeInternal;
extern TileLayerParamsDefaultTypeInternal _TileLayerParams_default_instance_;
class TopKLayerParams;
class TopKLayerParamsDefaultTypeInternal;
extern TopKLayerParamsDefaultTypeInternal _TopKLayerParams_default_instance_;
class TransposeLayerParams;
class TransposeLayerParamsDefaultTypeInternal;
extern TransposeLayerParamsDefaultTypeInternal _TransposeLayerParams_default_instance_;
class UnaryFunctionLayerParams;
class UnaryFunctionLayerParamsDefaultTypeInternal;
extern UnaryFunctionLayerParamsDefaultTypeInternal _UnaryFunctionLayerParams_default_instance_;
class UniDirectionalLSTMLayerParams;
class UniDirectionalLSTMLayerParamsDefaultTypeInternal;
extern UniDirectionalLSTMLayerParamsDefaultTypeInternal _UniDirectionalLSTMLayerParams_default_instance_;
class UpperTriangularLayerParams;
class UpperTriangularLayerParamsDefaultTypeInternal;
extern UpperTriangularLayerParamsDefaultTypeInternal _UpperTriangularLayerParams_default_instance_;
class UpsampleLayerParams;
class UpsampleLayerParamsDefaultTypeInternal;
extern UpsampleLayerParamsDefaultTypeInternal _UpsampleLayerParams_default_instance_;
class ValidPadding;
class ValidPaddingDefaultTypeInternal;
extern ValidPaddingDefaultTypeInternal _ValidPadding_default_instance_;
class WeightParams;
class WeightParamsDefaultTypeInternal;
extern WeightParamsDefaultTypeInternal _WeightParams_default_instance_;
class WhereBroadcastableLayerParams;
class WhereBroadcastableLayerParamsDefaultTypeInternal;
extern WhereBroadcastableLayerParamsDefaultTypeInternal _WhereBroadcastableLayerParams_default_instance_;
class WhereNonZeroLayerParams;
class WhereNonZeroLayerParamsDefaultTypeInternal;
extern WhereNonZeroLayerParamsDefaultTypeInternal _WhereNonZeroLayerParams_default_instance_;
}  // namespace Specification
}  // namespace CoreML

namespace CoreML {
namespace Specification {

namespace protobuf_NeuralNetwork_2eproto {
// Internal implementation detail -- do not call these.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[];
  static const ::google::protobuf::uint32 offsets[];
  static void InitDefaultsImpl();
  static void Shutdown();
};
void AddDescriptors();
void InitDefaults();
}  // namespace protobuf_NeuralNetwork_2eproto

enum SamePadding_SamePaddingMode {
  SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY = 0,
  SamePadding_SamePaddingMode_TOP_LEFT_HEAVY = 1,
  SamePadding_SamePaddingMode_SamePadding_SamePaddingMode_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  SamePadding_SamePaddingMode_SamePadding_SamePaddingMode_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool SamePadding_SamePaddingMode_IsValid(int value);
const SamePadding_SamePaddingMode SamePadding_SamePaddingMode_SamePaddingMode_MIN = SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY;
const SamePadding_SamePaddingMode SamePadding_SamePaddingMode_SamePaddingMode_MAX = SamePadding_SamePaddingMode_TOP_LEFT_HEAVY;
const int SamePadding_SamePaddingMode_SamePaddingMode_ARRAYSIZE = SamePadding_SamePaddingMode_SamePaddingMode_MAX + 1;

enum SamplingMode_Method {
  SamplingMode_Method_STRICT_ALIGN_ENDPOINTS_MODE = 0,
  SamplingMode_Method_ALIGN_ENDPOINTS_MODE = 1,
  SamplingMode_Method_UPSAMPLE_MODE = 2,
  SamplingMode_Method_ROI_ALIGN_MODE = 3,
  SamplingMode_Method_SamplingMode_Method_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  SamplingMode_Method_SamplingMode_Method_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool SamplingMode_Method_IsValid(int value);
const SamplingMode_Method SamplingMode_Method_Method_MIN = SamplingMode_Method_STRICT_ALIGN_ENDPOINTS_MODE;
const SamplingMode_Method SamplingMode_Method_Method_MAX = SamplingMode_Method_ROI_ALIGN_MODE;
const int SamplingMode_Method_Method_ARRAYSIZE = SamplingMode_Method_Method_MAX + 1;

enum BoxCoordinatesMode_Coordinates {
  BoxCoordinatesMode_Coordinates_CORNERS_HEIGHT_FIRST = 0,
  BoxCoordinatesMode_Coordinates_CORNERS_WIDTH_FIRST = 1,
  BoxCoordinatesMode_Coordinates_CENTER_SIZE_HEIGHT_FIRST = 2,
  BoxCoordinatesMode_Coordinates_CENTER_SIZE_WIDTH_FIRST = 3,
  BoxCoordinatesMode_Coordinates_BoxCoordinatesMode_Coordinates_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  BoxCoordinatesMode_Coordinates_BoxCoordinatesMode_Coordinates_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool BoxCoordinatesMode_Coordinates_IsValid(int value);
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode_Coordinates_Coordinates_MIN = BoxCoordinatesMode_Coordinates_CORNERS_HEIGHT_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode_Coordinates_Coordinates_MAX = BoxCoordinatesMode_Coordinates_CENTER_SIZE_WIDTH_FIRST;
const int BoxCoordinatesMode_Coordinates_Coordinates_ARRAYSIZE = BoxCoordinatesMode_Coordinates_Coordinates_MAX + 1;

enum PoolingLayerParams_PoolingType {
  PoolingLayerParams_PoolingType_MAX = 0,
  PoolingLayerParams_PoolingType_AVERAGE = 1,
  PoolingLayerParams_PoolingType_L2 = 2,
  PoolingLayerParams_PoolingType_PoolingLayerParams_PoolingType_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  PoolingLayerParams_PoolingType_PoolingLayerParams_PoolingType_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool PoolingLayerParams_PoolingType_IsValid(int value);
const PoolingLayerParams_PoolingType PoolingLayerParams_PoolingType_PoolingType_MIN = PoolingLayerParams_PoolingType_MAX;
const PoolingLayerParams_PoolingType PoolingLayerParams_PoolingType_PoolingType_MAX = PoolingLayerParams_PoolingType_L2;
const int PoolingLayerParams_PoolingType_PoolingType_ARRAYSIZE = PoolingLayerParams_PoolingType_PoolingType_MAX + 1;

enum UnaryFunctionLayerParams_Operation {
  UnaryFunctionLayerParams_Operation_SQRT = 0,
  UnaryFunctionLayerParams_Operation_RSQRT = 1,
  UnaryFunctionLayerParams_Operation_INVERSE = 2,
  UnaryFunctionLayerParams_Operation_POWER = 3,
  UnaryFunctionLayerParams_Operation_EXP = 4,
  UnaryFunctionLayerParams_Operation_LOG = 5,
  UnaryFunctionLayerParams_Operation_ABS = 6,
  UnaryFunctionLayerParams_Operation_THRESHOLD = 7,
  UnaryFunctionLayerParams_Operation_UnaryFunctionLayerParams_Operation_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  UnaryFunctionLayerParams_Operation_UnaryFunctionLayerParams_Operation_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool UnaryFunctionLayerParams_Operation_IsValid(int value);
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams_Operation_Operation_MIN = UnaryFunctionLayerParams_Operation_SQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams_Operation_Operation_MAX = UnaryFunctionLayerParams_Operation_THRESHOLD;
const int UnaryFunctionLayerParams_Operation_Operation_ARRAYSIZE = UnaryFunctionLayerParams_Operation_Operation_MAX + 1;

enum UpsampleLayerParams_InterpolationMode {
  UpsampleLayerParams_InterpolationMode_NN = 0,
  UpsampleLayerParams_InterpolationMode_BILINEAR = 1,
  UpsampleLayerParams_InterpolationMode_UpsampleLayerParams_InterpolationMode_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  UpsampleLayerParams_InterpolationMode_UpsampleLayerParams_InterpolationMode_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool UpsampleLayerParams_InterpolationMode_IsValid(int value);
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams_InterpolationMode_InterpolationMode_MIN = UpsampleLayerParams_InterpolationMode_NN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams_InterpolationMode_InterpolationMode_MAX = UpsampleLayerParams_InterpolationMode_BILINEAR;
const int UpsampleLayerParams_InterpolationMode_InterpolationMode_ARRAYSIZE = UpsampleLayerParams_InterpolationMode_InterpolationMode_MAX + 1;

enum FlattenLayerParams_FlattenOrder {
  FlattenLayerParams_FlattenOrder_CHANNEL_FIRST = 0,
  FlattenLayerParams_FlattenOrder_CHANNEL_LAST = 1,
  FlattenLayerParams_FlattenOrder_FlattenLayerParams_FlattenOrder_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  FlattenLayerParams_FlattenOrder_FlattenLayerParams_FlattenOrder_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool FlattenLayerParams_FlattenOrder_IsValid(int value);
const FlattenLayerParams_FlattenOrder FlattenLayerParams_FlattenOrder_FlattenOrder_MIN = FlattenLayerParams_FlattenOrder_CHANNEL_FIRST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams_FlattenOrder_FlattenOrder_MAX = FlattenLayerParams_FlattenOrder_CHANNEL_LAST;
const int FlattenLayerParams_FlattenOrder_FlattenOrder_ARRAYSIZE = FlattenLayerParams_FlattenOrder_FlattenOrder_MAX + 1;

enum ReshapeLayerParams_ReshapeOrder {
  ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST = 0,
  ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST = 1,
  ReshapeLayerParams_ReshapeOrder_ReshapeLayerParams_ReshapeOrder_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ReshapeLayerParams_ReshapeOrder_ReshapeLayerParams_ReshapeOrder_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ReshapeLayerParams_ReshapeOrder_IsValid(int value);
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MIN = ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX = ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST;
const int ReshapeLayerParams_ReshapeOrder_ReshapeOrder_ARRAYSIZE = ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX + 1;

enum ReorganizeDataLayerParams_ReorganizationType {
  ReorganizeDataLayerParams_ReorganizationType_SPACE_TO_DEPTH = 0,
  ReorganizeDataLayerParams_ReorganizationType_DEPTH_TO_SPACE = 1,
  ReorganizeDataLayerParams_ReorganizationType_ReorganizeDataLayerParams_ReorganizationType_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ReorganizeDataLayerParams_ReorganizationType_ReorganizeDataLayerParams_ReorganizationType_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ReorganizeDataLayerParams_ReorganizationType_IsValid(int value);
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MIN = ReorganizeDataLayerParams_ReorganizationType_SPACE_TO_DEPTH;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MAX = ReorganizeDataLayerParams_ReorganizationType_DEPTH_TO_SPACE;
const int ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_ARRAYSIZE = ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MAX + 1;

enum SliceLayerParams_SliceAxis {
  SliceLayerParams_SliceAxis_CHANNEL_AXIS = 0,
  SliceLayerParams_SliceAxis_HEIGHT_AXIS = 1,
  SliceLayerParams_SliceAxis_WIDTH_AXIS = 2,
  SliceLayerParams_SliceAxis_SliceLayerParams_SliceAxis_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  SliceLayerParams_SliceAxis_SliceLayerParams_SliceAxis_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool SliceLayerParams_SliceAxis_IsValid(int value);
const SliceLayerParams_SliceAxis SliceLayerParams_SliceAxis_SliceAxis_MIN = SliceLayerParams_SliceAxis_CHANNEL_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams_SliceAxis_SliceAxis_MAX = SliceLayerParams_SliceAxis_WIDTH_AXIS;
const int SliceLayerParams_SliceAxis_SliceAxis_ARRAYSIZE = SliceLayerParams_SliceAxis_SliceAxis_MAX + 1;

enum ReduceLayerParams_ReduceOperation {
  ReduceLayerParams_ReduceOperation_SUM = 0,
  ReduceLayerParams_ReduceOperation_AVG = 1,
  ReduceLayerParams_ReduceOperation_PROD = 2,
  ReduceLayerParams_ReduceOperation_LOGSUM = 3,
  ReduceLayerParams_ReduceOperation_SUMSQUARE = 4,
  ReduceLayerParams_ReduceOperation_L1 = 5,
  ReduceLayerParams_ReduceOperation_L2 = 6,
  ReduceLayerParams_ReduceOperation_MAX = 7,
  ReduceLayerParams_ReduceOperation_MIN = 8,
  ReduceLayerParams_ReduceOperation_ARGMAX = 9,
  ReduceLayerParams_ReduceOperation_ReduceLayerParams_ReduceOperation_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ReduceLayerParams_ReduceOperation_ReduceLayerParams_ReduceOperation_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ReduceLayerParams_ReduceOperation_IsValid(int value);
const ReduceLayerParams_ReduceOperation ReduceLayerParams_ReduceOperation_ReduceOperation_MIN = ReduceLayerParams_ReduceOperation_SUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams_ReduceOperation_ReduceOperation_MAX = ReduceLayerParams_ReduceOperation_ARGMAX;
const int ReduceLayerParams_ReduceOperation_ReduceOperation_ARRAYSIZE = ReduceLayerParams_ReduceOperation_ReduceOperation_MAX + 1;

enum ReduceLayerParams_ReduceAxis {
  ReduceLayerParams_ReduceAxis_CHW = 0,
  ReduceLayerParams_ReduceAxis_HW = 1,
  ReduceLayerParams_ReduceAxis_C = 2,
  ReduceLayerParams_ReduceAxis_H = 3,
  ReduceLayerParams_ReduceAxis_W = 4,
  ReduceLayerParams_ReduceAxis_ReduceLayerParams_ReduceAxis_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ReduceLayerParams_ReduceAxis_ReduceLayerParams_ReduceAxis_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ReduceLayerParams_ReduceAxis_IsValid(int value);
const ReduceLayerParams_ReduceAxis ReduceLayerParams_ReduceAxis_ReduceAxis_MIN = ReduceLayerParams_ReduceAxis_CHW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams_ReduceAxis_ReduceAxis_MAX = ReduceLayerParams_ReduceAxis_W;
const int ReduceLayerParams_ReduceAxis_ReduceAxis_ARRAYSIZE = ReduceLayerParams_ReduceAxis_ReduceAxis_MAX + 1;

enum GeluLayerParams_GeluMode {
  GeluLayerParams_GeluMode_EXACT = 0,
  GeluLayerParams_GeluMode_TANH_APPROXIMATION = 1,
  GeluLayerParams_GeluMode_SIGMOID_APPROXIMATION = 2,
  GeluLayerParams_GeluMode_GeluLayerParams_GeluMode_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  GeluLayerParams_GeluMode_GeluLayerParams_GeluMode_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool GeluLayerParams_GeluMode_IsValid(int value);
const GeluLayerParams_GeluMode GeluLayerParams_GeluMode_GeluMode_MIN = GeluLayerParams_GeluMode_EXACT;
const GeluLayerParams_GeluMode GeluLayerParams_GeluMode_GeluMode_MAX = GeluLayerParams_GeluMode_SIGMOID_APPROXIMATION;
const int GeluLayerParams_GeluMode_GeluMode_ARRAYSIZE = GeluLayerParams_GeluMode_GeluMode_MAX + 1;

enum NeuralNetworkMultiArrayShapeMapping {
  RANK5_ARRAY_MAPPING = 0,
  EXACT_ARRAY_MAPPING = 1,
  NeuralNetworkMultiArrayShapeMapping_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  NeuralNetworkMultiArrayShapeMapping_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool NeuralNetworkMultiArrayShapeMapping_IsValid(int value);
const NeuralNetworkMultiArrayShapeMapping NeuralNetworkMultiArrayShapeMapping_MIN = RANK5_ARRAY_MAPPING;
const NeuralNetworkMultiArrayShapeMapping NeuralNetworkMultiArrayShapeMapping_MAX = EXACT_ARRAY_MAPPING;
const int NeuralNetworkMultiArrayShapeMapping_ARRAYSIZE = NeuralNetworkMultiArrayShapeMapping_MAX + 1;

enum NeuralNetworkImageShapeMapping {
  RANK5_IMAGE_MAPPING = 0,
  RANK4_IMAGE_MAPPING = 1,
  NeuralNetworkImageShapeMapping_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  NeuralNetworkImageShapeMapping_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool NeuralNetworkImageShapeMapping_IsValid(int value);
const NeuralNetworkImageShapeMapping NeuralNetworkImageShapeMapping_MIN = RANK5_IMAGE_MAPPING;
const NeuralNetworkImageShapeMapping NeuralNetworkImageShapeMapping_MAX = RANK4_IMAGE_MAPPING;
const int NeuralNetworkImageShapeMapping_ARRAYSIZE = NeuralNetworkImageShapeMapping_MAX + 1;

enum ScatterMode {
  SCATTER_UPDATE = 0,
  SCATTER_ADD = 1,
  SCATTER_SUB = 2,
  SCATTER_MUL = 3,
  SCATTER_DIV = 4,
  SCATTER_MAX = 5,
  SCATTER_MIN = 6,
  ScatterMode_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ScatterMode_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ScatterMode_IsValid(int value);
const ScatterMode ScatterMode_MIN = SCATTER_UPDATE;
const ScatterMode ScatterMode_MAX = SCATTER_MIN;
const int ScatterMode_ARRAYSIZE = ScatterMode_MAX + 1;

// ===================================================================

class NeuralNetwork : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetwork) */ {
 public:
  NeuralNetwork();
  virtual ~NeuralNetwork();

  NeuralNetwork(const NeuralNetwork& from);

  inline NeuralNetwork& operator=(const NeuralNetwork& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetwork& default_instance();

  static inline const NeuralNetwork* internal_default_instance() {
    return reinterpret_cast<const NeuralNetwork*>(
               &_NeuralNetwork_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    0;

  void Swap(NeuralNetwork* other);

  // implements Message ----------------------------------------------

  inline NeuralNetwork* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetwork* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetwork& from);
  void MergeFrom(const NeuralNetwork& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetwork* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  void clear_layers();
  static const int kLayersFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  void clear_preprocessing();
  static const int kPreprocessingFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  bool has_updateparams() const;
  void clear_updateparams();
  static const int kUpdateParamsFieldNumber = 10;
  const ::CoreML::Specification::NetworkUpdateParameters& updateparams() const;
  ::CoreML::Specification::NetworkUpdateParameters* mutable_updateparams();
  ::CoreML::Specification::NetworkUpdateParameters* release_updateparams();
  void set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams);

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  void clear_arrayinputshapemapping();
  static const int kArrayInputShapeMappingFieldNumber = 5;
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping arrayinputshapemapping() const;
  void set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  void clear_imageinputshapemapping();
  static const int kImageInputShapeMappingFieldNumber = 6;
  ::CoreML::Specification::NeuralNetworkImageShapeMapping imageinputshapemapping() const;
  void set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetwork)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::CoreML::Specification::NetworkUpdateParameters* updateparams_;
  int arrayinputshapemapping_;
  int imageinputshapemapping_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NeuralNetworkImageScaler : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkImageScaler) */ {
 public:
  NeuralNetworkImageScaler();
  virtual ~NeuralNetworkImageScaler();

  NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from);

  inline NeuralNetworkImageScaler& operator=(const NeuralNetworkImageScaler& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkImageScaler& default_instance();

  static inline const NeuralNetworkImageScaler* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkImageScaler*>(
               &_NeuralNetworkImageScaler_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    1;

  void Swap(NeuralNetworkImageScaler* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkImageScaler* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetworkImageScaler* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetworkImageScaler& from);
  void MergeFrom(const NeuralNetworkImageScaler& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkImageScaler* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float grayBias = 30;
  void clear_graybias();
  static const int kGrayBiasFieldNumber = 30;
  float graybias() const;
  void set_graybias(float value);

  // float channelScale = 10;
  void clear_channelscale();
  static const int kChannelScaleFieldNumber = 10;
  float channelscale() const;
  void set_channelscale(float value);

  // float blueBias = 20;
  void clear_bluebias();
  static const int kBlueBiasFieldNumber = 20;
  float bluebias() const;
  void set_bluebias(float value);

  // float greenBias = 21;
  void clear_greenbias();
  static const int kGreenBiasFieldNumber = 21;
  float greenbias() const;
  void set_greenbias(float value);

  // float redBias = 22;
  void clear_redbias();
  static const int kRedBiasFieldNumber = 22;
  float redbias() const;
  void set_redbias(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkImageScaler)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float graybias_;
  float channelscale_;
  float bluebias_;
  float greenbias_;
  float redbias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NeuralNetworkMeanImage : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkMeanImage) */ {
 public:
  NeuralNetworkMeanImage();
  virtual ~NeuralNetworkMeanImage();

  NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from);

  inline NeuralNetworkMeanImage& operator=(const NeuralNetworkMeanImage& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkMeanImage& default_instance();

  static inline const NeuralNetworkMeanImage* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkMeanImage*>(
               &_NeuralNetworkMeanImage_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    2;

  void Swap(NeuralNetworkMeanImage* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkMeanImage* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetworkMeanImage* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetworkMeanImage& from);
  void MergeFrom(const NeuralNetworkMeanImage& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkMeanImage* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated float meanImage = 1;
  int meanimage_size() const;
  void clear_meanimage();
  static const int kMeanImageFieldNumber = 1;
  float meanimage(int index) const;
  void set_meanimage(int index, float value);
  void add_meanimage(float value);
  const ::google::protobuf::RepeatedField< float >&
      meanimage() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_meanimage();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkMeanImage)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< float > meanimage_;
  mutable int _meanimage_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NeuralNetworkPreprocessing : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkPreprocessing) */ {
 public:
  NeuralNetworkPreprocessing();
  virtual ~NeuralNetworkPreprocessing();

  NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from);

  inline NeuralNetworkPreprocessing& operator=(const NeuralNetworkPreprocessing& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkPreprocessing& default_instance();

  enum PreprocessorCase {
    kScaler = 10,
    kMeanImage = 11,
    PREPROCESSOR_NOT_SET = 0,
  };

  static inline const NeuralNetworkPreprocessing* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkPreprocessing*>(
               &_NeuralNetworkPreprocessing_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    3;

  void Swap(NeuralNetworkPreprocessing* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkPreprocessing* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetworkPreprocessing* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetworkPreprocessing& from);
  void MergeFrom(const NeuralNetworkPreprocessing& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkPreprocessing* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string featureName = 1;
  void clear_featurename();
  static const int kFeatureNameFieldNumber = 1;
  const ::std::string& featurename() const;
  void set_featurename(const ::std::string& value);
  #if LANG_CXX11
  void set_featurename(::std::string&& value);
  #endif
  void set_featurename(const char* value);
  void set_featurename(const char* value, size_t size);
  ::std::string* mutable_featurename();
  ::std::string* release_featurename();
  void set_allocated_featurename(::std::string* featurename);

  // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  bool has_scaler() const;
  void clear_scaler();
  static const int kScalerFieldNumber = 10;
  const ::CoreML::Specification::NeuralNetworkImageScaler& scaler() const;
  ::CoreML::Specification::NeuralNetworkImageScaler* mutable_scaler();
  ::CoreML::Specification::NeuralNetworkImageScaler* release_scaler();
  void set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler);

  // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  bool has_meanimage() const;
  void clear_meanimage();
  static const int kMeanImageFieldNumber = 11;
  const ::CoreML::Specification::NeuralNetworkMeanImage& meanimage() const;
  ::CoreML::Specification::NeuralNetworkMeanImage* mutable_meanimage();
  ::CoreML::Specification::NeuralNetworkMeanImage* release_meanimage();
  void set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage);

  PreprocessorCase preprocessor_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkPreprocessing)
 private:
  void set_has_scaler();
  void set_has_meanimage();

  inline bool has_preprocessor() const;
  void clear_preprocessor();
  inline void clear_has_preprocessor();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr featurename_;
  union PreprocessorUnion {
    PreprocessorUnion() {}
    ::CoreML::Specification::NeuralNetworkImageScaler* scaler_;
    ::CoreML::Specification::NeuralNetworkMeanImage* meanimage_;
  } preprocessor_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationReLU) */ {
 public:
  ActivationReLU();
  virtual ~ActivationReLU();

  ActivationReLU(const ActivationReLU& from);

  inline ActivationReLU& operator=(const ActivationReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationReLU& default_instance();

  static inline const ActivationReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationReLU*>(
               &_ActivationReLU_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    4;

  void Swap(ActivationReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationReLU* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationReLU* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationReLU& from);
  void MergeFrom(const ActivationReLU& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationReLU* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationReLU)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationLeakyReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationLeakyReLU) */ {
 public:
  ActivationLeakyReLU();
  virtual ~ActivationLeakyReLU();

  ActivationLeakyReLU(const ActivationLeakyReLU& from);

  inline ActivationLeakyReLU& operator=(const ActivationLeakyReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationLeakyReLU& default_instance();

  static inline const ActivationLeakyReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationLeakyReLU*>(
               &_ActivationLeakyReLU_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    5;

  void Swap(ActivationLeakyReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationLeakyReLU* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationLeakyReLU* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationLeakyReLU& from);
  void MergeFrom(const ActivationLeakyReLU& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationLeakyReLU* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationLeakyReLU)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationTanh : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationTanh) */ {
 public:
  ActivationTanh();
  virtual ~ActivationTanh();

  ActivationTanh(const ActivationTanh& from);

  inline ActivationTanh& operator=(const ActivationTanh& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationTanh& default_instance();

  static inline const ActivationTanh* internal_default_instance() {
    return reinterpret_cast<const ActivationTanh*>(
               &_ActivationTanh_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    6;

  void Swap(ActivationTanh* other);

  // implements Message ----------------------------------------------

  inline ActivationTanh* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationTanh* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationTanh& from);
  void MergeFrom(const ActivationTanh& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationTanh* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationTanh)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationScaledTanh : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationScaledTanh) */ {
 public:
  ActivationScaledTanh();
  virtual ~ActivationScaledTanh();

  ActivationScaledTanh(const ActivationScaledTanh& from);

  inline ActivationScaledTanh& operator=(const ActivationScaledTanh& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationScaledTanh& default_instance();

  static inline const ActivationScaledTanh* internal_default_instance() {
    return reinterpret_cast<const ActivationScaledTanh*>(
               &_ActivationScaledTanh_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    7;

  void Swap(ActivationScaledTanh* other);

  // implements Message ----------------------------------------------

  inline ActivationScaledTanh* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationScaledTanh* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationScaledTanh& from);
  void MergeFrom(const ActivationScaledTanh& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationScaledTanh* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationScaledTanh)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  float beta_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationSigmoid : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSigmoid) */ {
 public:
  ActivationSigmoid();
  virtual ~ActivationSigmoid();

  ActivationSigmoid(const ActivationSigmoid& from);

  inline ActivationSigmoid& operator=(const ActivationSigmoid& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSigmoid& default_instance();

  static inline const ActivationSigmoid* internal_default_instance() {
    return reinterpret_cast<const ActivationSigmoid*>(
               &_ActivationSigmoid_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    8;

  void Swap(ActivationSigmoid* other);

  // implements Message ----------------------------------------------

  inline ActivationSigmoid* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationSigmoid* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationSigmoid& from);
  void MergeFrom(const ActivationSigmoid& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSigmoid* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSigmoid)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationLinear : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationLinear) */ {
 public:
  ActivationLinear();
  virtual ~ActivationLinear();

  ActivationLinear(const ActivationLinear& from);

  inline ActivationLinear& operator=(const ActivationLinear& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationLinear& default_instance();

  static inline const ActivationLinear* internal_default_instance() {
    return reinterpret_cast<const ActivationLinear*>(
               &_ActivationLinear_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    9;

  void Swap(ActivationLinear* other);

  // implements Message ----------------------------------------------

  inline ActivationLinear* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationLinear* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationLinear& from);
  void MergeFrom(const ActivationLinear& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationLinear* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationLinear)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  float beta_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationSigmoidHard : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSigmoidHard) */ {
 public:
  ActivationSigmoidHard();
  virtual ~ActivationSigmoidHard();

  ActivationSigmoidHard(const ActivationSigmoidHard& from);

  inline ActivationSigmoidHard& operator=(const ActivationSigmoidHard& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSigmoidHard& default_instance();

  static inline const ActivationSigmoidHard* internal_default_instance() {
    return reinterpret_cast<const ActivationSigmoidHard*>(
               &_ActivationSigmoidHard_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    10;

  void Swap(ActivationSigmoidHard* other);

  // implements Message ----------------------------------------------

  inline ActivationSigmoidHard* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationSigmoidHard* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationSigmoidHard& from);
  void MergeFrom(const ActivationSigmoidHard& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSigmoidHard* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSigmoidHard)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  float beta_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationPReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationPReLU) */ {
 public:
  ActivationPReLU();
  virtual ~ActivationPReLU();

  ActivationPReLU(const ActivationPReLU& from);

  inline ActivationPReLU& operator=(const ActivationPReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationPReLU& default_instance();

  static inline const ActivationPReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationPReLU*>(
               &_ActivationPReLU_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    11;

  void Swap(ActivationPReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationPReLU* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationPReLU* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationPReLU& from);
  void MergeFrom(const ActivationPReLU& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationPReLU* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams alpha = 1;
  bool has_alpha() const;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  const ::CoreML::Specification::WeightParams& alpha() const;
  ::CoreML::Specification::WeightParams* mutable_alpha();
  ::CoreML::Specification::WeightParams* release_alpha();
  void set_allocated_alpha(::CoreML::Specification::WeightParams* alpha);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationPReLU)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationELU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationELU) */ {
 public:
  ActivationELU();
  virtual ~ActivationELU();

  ActivationELU(const ActivationELU& from);

  inline ActivationELU& operator=(const ActivationELU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationELU& default_instance();

  static inline const ActivationELU* internal_default_instance() {
    return reinterpret_cast<const ActivationELU*>(
               &_ActivationELU_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    12;

  void Swap(ActivationELU* other);

  // implements Message ----------------------------------------------

  inline ActivationELU* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationELU* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationELU& from);
  void MergeFrom(const ActivationELU& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationELU* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationELU)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationThresholdedReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationThresholdedReLU) */ {
 public:
  ActivationThresholdedReLU();
  virtual ~ActivationThresholdedReLU();

  ActivationThresholdedReLU(const ActivationThresholdedReLU& from);

  inline ActivationThresholdedReLU& operator=(const ActivationThresholdedReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationThresholdedReLU& default_instance();

  static inline const ActivationThresholdedReLU* internal_default_instance() {
    return reinterpret_cast<const ActivationThresholdedReLU*>(
               &_ActivationThresholdedReLU_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    13;

  void Swap(ActivationThresholdedReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationThresholdedReLU* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationThresholdedReLU* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationThresholdedReLU& from);
  void MergeFrom(const ActivationThresholdedReLU& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationThresholdedReLU* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationThresholdedReLU)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationSoftsign : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSoftsign) */ {
 public:
  ActivationSoftsign();
  virtual ~ActivationSoftsign();

  ActivationSoftsign(const ActivationSoftsign& from);

  inline ActivationSoftsign& operator=(const ActivationSoftsign& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSoftsign& default_instance();

  static inline const ActivationSoftsign* internal_default_instance() {
    return reinterpret_cast<const ActivationSoftsign*>(
               &_ActivationSoftsign_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    14;

  void Swap(ActivationSoftsign* other);

  // implements Message ----------------------------------------------

  inline ActivationSoftsign* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationSoftsign* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationSoftsign& from);
  void MergeFrom(const ActivationSoftsign& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSoftsign* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSoftsign)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationSoftplus : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSoftplus) */ {
 public:
  ActivationSoftplus();
  virtual ~ActivationSoftplus();

  ActivationSoftplus(const ActivationSoftplus& from);

  inline ActivationSoftplus& operator=(const ActivationSoftplus& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSoftplus& default_instance();

  static inline const ActivationSoftplus* internal_default_instance() {
    return reinterpret_cast<const ActivationSoftplus*>(
               &_ActivationSoftplus_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    15;

  void Swap(ActivationSoftplus* other);

  // implements Message ----------------------------------------------

  inline ActivationSoftplus* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationSoftplus* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationSoftplus& from);
  void MergeFrom(const ActivationSoftplus& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSoftplus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSoftplus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationParametricSoftplus : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationParametricSoftplus) */ {
 public:
  ActivationParametricSoftplus();
  virtual ~ActivationParametricSoftplus();

  ActivationParametricSoftplus(const ActivationParametricSoftplus& from);

  inline ActivationParametricSoftplus& operator=(const ActivationParametricSoftplus& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationParametricSoftplus& default_instance();

  static inline const ActivationParametricSoftplus* internal_default_instance() {
    return reinterpret_cast<const ActivationParametricSoftplus*>(
               &_ActivationParametricSoftplus_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    16;

  void Swap(ActivationParametricSoftplus* other);

  // implements Message ----------------------------------------------

  inline ActivationParametricSoftplus* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationParametricSoftplus* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationParametricSoftplus& from);
  void MergeFrom(const ActivationParametricSoftplus& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationParametricSoftplus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams alpha = 1;
  bool has_alpha() const;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  const ::CoreML::Specification::WeightParams& alpha() const;
  ::CoreML::Specification::WeightParams* mutable_alpha();
  ::CoreML::Specification::WeightParams* release_alpha();
  void set_allocated_alpha(::CoreML::Specification::WeightParams* alpha);

  // .CoreML.Specification.WeightParams beta = 2;
  bool has_beta() const;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& beta() const;
  ::CoreML::Specification::WeightParams* mutable_beta();
  ::CoreML::Specification::WeightParams* release_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationParametricSoftplus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* alpha_;
  ::CoreML::Specification::WeightParams* beta_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ActivationParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationParams) */ {
 public:
  ActivationParams();
  virtual ~ActivationParams();

  ActivationParams(const ActivationParams& from);

  inline ActivationParams& operator=(const ActivationParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationParams& default_instance();

  enum NonlinearityTypeCase {
    kLinear = 5,
    kReLU = 10,
    kLeakyReLU = 15,
    kThresholdedReLU = 20,
    kPReLU = 25,
    kTanh = 30,
    kScaledTanh = 31,
    kSigmoid = 40,
    kSigmoidHard = 41,
    kELU = 50,
    kSoftsign = 60,
    kSoftplus = 70,
    kParametricSoftplus = 71,
    NONLINEARITYTYPE_NOT_SET = 0,
  };

  static inline const ActivationParams* internal_default_instance() {
    return reinterpret_cast<const ActivationParams*>(
               &_ActivationParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    17;

  void Swap(ActivationParams* other);

  // implements Message ----------------------------------------------

  inline ActivationParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ActivationParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ActivationParams& from);
  void MergeFrom(const ActivationParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.ActivationLinear linear = 5;
  bool has_linear() const;
  void clear_linear();
  static const int kLinearFieldNumber = 5;
  const ::CoreML::Specification::ActivationLinear& linear() const;
  ::CoreML::Specification::ActivationLinear* mutable_linear();
  ::CoreML::Specification::ActivationLinear* release_linear();
  void set_allocated_linear(::CoreML::Specification::ActivationLinear* linear);

  // .CoreML.Specification.ActivationReLU ReLU = 10;
  bool has_relu() const;
  void clear_relu();
  static const int kReLUFieldNumber = 10;
  const ::CoreML::Specification::ActivationReLU& relu() const;
  ::CoreML::Specification::ActivationReLU* mutable_relu();
  ::CoreML::Specification::ActivationReLU* release_relu();
  void set_allocated_relu(::CoreML::Specification::ActivationReLU* relu);

  // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  bool has_leakyrelu() const;
  void clear_leakyrelu();
  static const int kLeakyReLUFieldNumber = 15;
  const ::CoreML::Specification::ActivationLeakyReLU& leakyrelu() const;
  ::CoreML::Specification::ActivationLeakyReLU* mutable_leakyrelu();
  ::CoreML::Specification::ActivationLeakyReLU* release_leakyrelu();
  void set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu);

  // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  bool has_thresholdedrelu() const;
  void clear_thresholdedrelu();
  static const int kThresholdedReLUFieldNumber = 20;
  const ::CoreML::Specification::ActivationThresholdedReLU& thresholdedrelu() const;
  ::CoreML::Specification::ActivationThresholdedReLU* mutable_thresholdedrelu();
  ::CoreML::Specification::ActivationThresholdedReLU* release_thresholdedrelu();
  void set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu);

  // .CoreML.Specification.ActivationPReLU PReLU = 25;
  bool has_prelu() const;
  void clear_prelu();
  static const int kPReLUFieldNumber = 25;
  const ::CoreML::Specification::ActivationPReLU& prelu() const;
  ::CoreML::Specification::ActivationPReLU* mutable_prelu();
  ::CoreML::Specification::ActivationPReLU* release_prelu();
  void set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu);

  // .CoreML.Specification.ActivationTanh tanh = 30;
  bool has_tanh() const;
  void clear_tanh();
  static const int kTanhFieldNumber = 30;
  const ::CoreML::Specification::ActivationTanh& tanh() const;
  ::CoreML::Specification::ActivationTanh* mutable_tanh();
  ::CoreML::Specification::ActivationTanh* release_tanh();
  void set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh);

  // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  bool has_scaledtanh() const;
  void clear_scaledtanh();
  static const int kScaledTanhFieldNumber = 31;
  const ::CoreML::Specification::ActivationScaledTanh& scaledtanh() const;
  ::CoreML::Specification::ActivationScaledTanh* mutable_scaledtanh();
  ::CoreML::Specification::ActivationScaledTanh* release_scaledtanh();
  void set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh);

  // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  bool has_sigmoid() const;
  void clear_sigmoid();
  static const int kSigmoidFieldNumber = 40;
  const ::CoreML::Specification::ActivationSigmoid& sigmoid() const;
  ::CoreML::Specification::ActivationSigmoid* mutable_sigmoid();
  ::CoreML::Specification::ActivationSigmoid* release_sigmoid();
  void set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid);

  // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  bool has_sigmoidhard() const;
  void clear_sigmoidhard();
  static const int kSigmoidHardFieldNumber = 41;
  const ::CoreML::Specification::ActivationSigmoidHard& sigmoidhard() const;
  ::CoreML::Specification::ActivationSigmoidHard* mutable_sigmoidhard();
  ::CoreML::Specification::ActivationSigmoidHard* release_sigmoidhard();
  void set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard);

  // .CoreML.Specification.ActivationELU ELU = 50;
  bool has_elu() const;
  void clear_elu();
  static const int kELUFieldNumber = 50;
  const ::CoreML::Specification::ActivationELU& elu() const;
  ::CoreML::Specification::ActivationELU* mutable_elu();
  ::CoreML::Specification::ActivationELU* release_elu();
  void set_allocated_elu(::CoreML::Specification::ActivationELU* elu);

  // .CoreML.Specification.ActivationSoftsign softsign = 60;
  bool has_softsign() const;
  void clear_softsign();
  static const int kSoftsignFieldNumber = 60;
  const ::CoreML::Specification::ActivationSoftsign& softsign() const;
  ::CoreML::Specification::ActivationSoftsign* mutable_softsign();
  ::CoreML::Specification::ActivationSoftsign* release_softsign();
  void set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign);

  // .CoreML.Specification.ActivationSoftplus softplus = 70;
  bool has_softplus() const;
  void clear_softplus();
  static const int kSoftplusFieldNumber = 70;
  const ::CoreML::Specification::ActivationSoftplus& softplus() const;
  ::CoreML::Specification::ActivationSoftplus* mutable_softplus();
  ::CoreML::Specification::ActivationSoftplus* release_softplus();
  void set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus);

  // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  bool has_parametricsoftplus() const;
  void clear_parametricsoftplus();
  static const int kParametricSoftplusFieldNumber = 71;
  const ::CoreML::Specification::ActivationParametricSoftplus& parametricsoftplus() const;
  ::CoreML::Specification::ActivationParametricSoftplus* mutable_parametricsoftplus();
  ::CoreML::Specification::ActivationParametricSoftplus* release_parametricsoftplus();
  void set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus);

  NonlinearityTypeCase NonlinearityType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationParams)
 private:
  void set_has_linear();
  void set_has_relu();
  void set_has_leakyrelu();
  void set_has_thresholdedrelu();
  void set_has_prelu();
  void set_has_tanh();
  void set_has_scaledtanh();
  void set_has_sigmoid();
  void set_has_sigmoidhard();
  void set_has_elu();
  void set_has_softsign();
  void set_has_softplus();
  void set_has_parametricsoftplus();

  inline bool has_NonlinearityType() const;
  void clear_NonlinearityType();
  inline void clear_has_NonlinearityType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  union NonlinearityTypeUnion {
    NonlinearityTypeUnion() {}
    ::CoreML::Specification::ActivationLinear* linear_;
    ::CoreML::Specification::ActivationReLU* relu_;
    ::CoreML::Specification::ActivationLeakyReLU* leakyrelu_;
    ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu_;
    ::CoreML::Specification::ActivationPReLU* prelu_;
    ::CoreML::Specification::ActivationTanh* tanh_;
    ::CoreML::Specification::ActivationScaledTanh* scaledtanh_;
    ::CoreML::Specification::ActivationSigmoid* sigmoid_;
    ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard_;
    ::CoreML::Specification::ActivationELU* elu_;
    ::CoreML::Specification::ActivationSoftsign* softsign_;
    ::CoreML::Specification::ActivationSoftplus* softplus_;
    ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus_;
  } NonlinearityType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class Tensor : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Tensor) */ {
 public:
  Tensor();
  virtual ~Tensor();

  Tensor(const Tensor& from);

  inline Tensor& operator=(const Tensor& from) {
    CopyFrom(from);
    return *this;
  }

  static const Tensor& default_instance();

  static inline const Tensor* internal_default_instance() {
    return reinterpret_cast<const Tensor*>(
               &_Tensor_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    18;

  void Swap(Tensor* other);

  // implements Message ----------------------------------------------

  inline Tensor* New() const PROTOBUF_FINAL { return New(NULL); }

  Tensor* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const Tensor& from);
  void MergeFrom(const Tensor& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Tensor* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 dimValue = 2;
  int dimvalue_size() const;
  void clear_dimvalue();
  static const int kDimValueFieldNumber = 2;
  ::google::protobuf::int64 dimvalue(int index) const;
  void set_dimvalue(int index, ::google::protobuf::int64 value);
  void add_dimvalue(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      dimvalue() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_dimvalue();

  // uint32 rank = 1;
  void clear_rank();
  static const int kRankFieldNumber = 1;
  ::google::protobuf::uint32 rank() const;
  void set_rank(::google::protobuf::uint32 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.Tensor)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > dimvalue_;
  mutable int _dimvalue_cached_byte_size_;
  ::google::protobuf::uint32 rank_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NeuralNetworkLayer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkLayer) */ {
 public:
  NeuralNetworkLayer();
  virtual ~NeuralNetworkLayer();

  NeuralNetworkLayer(const NeuralNetworkLayer& from);

  inline NeuralNetworkLayer& operator=(const NeuralNetworkLayer& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkLayer& default_instance();

  enum LayerCase {
    kConvolution = 100,
    kPooling = 120,
    kActivation = 130,
    kInnerProduct = 140,
    kEmbedding = 150,
    kBatchnorm = 160,
    kMvn = 165,
    kL2Normalize = 170,
    kSoftmax = 175,
    kLrn = 180,
    kCrop = 190,
    kPadding = 200,
    kUpsample = 210,
    kResizeBilinear = 211,
    kCropResize = 212,
    kUnary = 220,
    kAdd = 230,
    kMultiply = 231,
    kAverage = 240,
    kScale = 245,
    kBias = 250,
    kMax = 260,
    kMin = 261,
    kDot = 270,
    kReduce = 280,
    kLoadConstant = 290,
    kReshape = 300,
    kFlatten = 301,
    kPermute = 310,
    kConcat = 320,
    kSplit = 330,
    kSequenceRepeat = 340,
    kReorganizeData = 345,
    kSlice = 350,
    kSimpleRecurrent = 400,
    kGru = 410,
    kUniDirectionalLSTM = 420,
    kBiDirectionalLSTM = 430,
    kCustom = 500,
    kCopy = 600,
    kBranch = 605,
    kLoop = 615,
    kLoopBreak = 620,
    kLoopContinue = 625,
    kRangeStatic = 635,
    kRangeDynamic = 640,
    kClip = 660,
    kCeil = 665,
    kFloor = 670,
    kSign = 680,
    kRound = 685,
    kExp2 = 700,
    kSin = 710,
    kCos = 715,
    kTan = 720,
    kAsin = 730,
    kAcos = 735,
    kAtan = 740,
    kSinh = 750,
    kCosh = 755,
    kTanh = 760,
    kAsinh = 770,
    kAcosh = 775,
    kAtanh = 780,
    kErf = 790,
    kGelu = 795,
    kEqual = 815,
    kNotEqual = 820,
    kLessThan = 825,
    kLessEqual = 827,
    kGreaterThan = 830,
    kGreaterEqual = 832,
    kLogicalOr = 840,
    kLogicalXor = 845,
    kLogicalNot = 850,
    kLogicalAnd = 855,
    kModBroadcastable = 865,
    kMinBroadcastable = 870,
    kMaxBroadcastable = 875,
    kAddBroadcastable = 880,
    kPowBroadcastable = 885,
    kDivideBroadcastable = 890,
    kFloorDivBroadcastable = 895,
    kMultiplyBroadcastable = 900,
    kSubtractBroadcastable = 905,
    kTile = 920,
    kStack = 925,
    kGather = 930,
    kScatter = 935,
    kGatherND = 940,
    kScatterND = 945,
    kSoftmaxND = 950,
    kGatherAlongAxis = 952,
    kScatterAlongAxis = 954,
    kReverse = 960,
    kReverseSeq = 965,
    kSplitND = 975,
    kConcatND = 980,
    kTranspose = 985,
    kSliceStatic = 995,
    kSliceDynamic = 1000,
    kSlidingWindows = 1005,
    kTopK = 1015,
    kArgMin = 1020,
    kArgMax = 1025,
    kEmbeddingND = 1040,
    kBatchedMatmul = 1045,
    kGetShape = 1065,
    kLoadConstantND = 1070,
    kFillLike = 1080,
    kFillStatic = 1085,
    kFillDynamic = 1090,
    kBroadcastToLike = 1100,
    kBroadcastToStatic = 1105,
    kBroadcastToDynamic = 1110,
    kSqueeze = 1120,
    kExpandDims = 1125,
    kFlattenTo2D = 1130,
    kReshapeLike = 1135,
    kReshapeStatic = 1140,
    kReshapeDynamic = 1145,
    kRankPreservingReshape = 1150,
    kConstantPad = 1155,
    kRandomNormalLike = 1170,
    kRandomNormalStatic = 1175,
    kRandomNormalDynamic = 1180,
    kRandomUniformLike = 1190,
    kRandomUniformStatic = 1195,
    kRandomUniformDynamic = 1200,
    kRandomBernoulliLike = 1210,
    kRandomBernoulliStatic = 1215,
    kRandomBernoulliDynamic = 1220,
    kCategoricalDistribution = 1230,
    kReduceL1 = 1250,
    kReduceL2 = 1255,
    kReduceMax = 1260,
    kReduceMin = 1265,
    kReduceSum = 1270,
    kReduceProd = 1275,
    kReduceMean = 1280,
    kReduceLogSum = 1285,
    kReduceSumSquare = 1290,
    kReduceLogSumExp = 1295,
    kWhereNonZero = 1313,
    kMatrixBandPart = 1315,
    kLowerTriangular = 1320,
    kUpperTriangular = 1325,
    kWhereBroadcastable = 1330,
    kLayerNormalization = 1350,
    kNonMaximumSuppression = 1400,
    LAYER_NOT_SET = 0,
  };

  static inline const NeuralNetworkLayer* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkLayer*>(
               &_NeuralNetworkLayer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    19;

  void Swap(NeuralNetworkLayer* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkLayer* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetworkLayer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetworkLayer& from);
  void MergeFrom(const NeuralNetworkLayer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkLayer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated string input = 2;
  int input_size() const;
  void clear_input();
  static const int kInputFieldNumber = 2;
  const ::std::string& input(int index) const;
  ::std::string* mutable_input(int index);
  void set_input(int index, const ::std::string& value);
  #if LANG_CXX11
  void set_input(int index, ::std::string&& value);
  #endif
  void set_input(int index, const char* value);
  void set_input(int index, const char* value, size_t size);
  ::std::string* add_input();
  void add_input(const ::std::string& value);
  #if LANG_CXX11
  void add_input(::std::string&& value);
  #endif
  void add_input(const char* value);
  void add_input(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& input() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_input();

  // repeated string output = 3;
  int output_size() const;
  void clear_output();
  static const int kOutputFieldNumber = 3;
  const ::std::string& output(int index) const;
  ::std::string* mutable_output(int index);
  void set_output(int index, const ::std::string& value);
  #if LANG_CXX11
  void set_output(int index, ::std::string&& value);
  #endif
  void set_output(int index, const char* value);
  void set_output(int index, const char* value, size_t size);
  ::std::string* add_output();
  void add_output(const ::std::string& value);
  #if LANG_CXX11
  void add_output(::std::string&& value);
  #endif
  void add_output(const char* value);
  void add_output(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& output() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_output();

  // repeated .CoreML.Specification.Tensor inputTensor = 4;
  int inputtensor_size() const;
  void clear_inputtensor();
  static const int kInputTensorFieldNumber = 4;
  const ::CoreML::Specification::Tensor& inputtensor(int index) const;
  ::CoreML::Specification::Tensor* mutable_inputtensor(int index);
  ::CoreML::Specification::Tensor* add_inputtensor();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
      mutable_inputtensor();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
      inputtensor() const;

  // repeated .CoreML.Specification.Tensor outputTensor = 5;
  int outputtensor_size() const;
  void clear_outputtensor();
  static const int kOutputTensorFieldNumber = 5;
  const ::CoreML::Specification::Tensor& outputtensor(int index) const;
  ::CoreML::Specification::Tensor* mutable_outputtensor(int index);
  ::CoreML::Specification::Tensor* add_outputtensor();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
      mutable_outputtensor();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
      outputtensor() const;

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // bool isUpdatable = 10;
  void clear_isupdatable();
  static const int kIsUpdatableFieldNumber = 10;
  bool isupdatable() const;
  void set_isupdatable(bool value);

  // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  bool has_convolution() const;
  void clear_convolution();
  static const int kConvolutionFieldNumber = 100;
  const ::CoreML::Specification::ConvolutionLayerParams& convolution() const;
  ::CoreML::Specification::ConvolutionLayerParams* mutable_convolution();
  ::CoreML::Specification::ConvolutionLayerParams* release_convolution();
  void set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution);

  // .CoreML.Specification.PoolingLayerParams pooling = 120;
  bool has_pooling() const;
  void clear_pooling();
  static const int kPoolingFieldNumber = 120;
  const ::CoreML::Specification::PoolingLayerParams& pooling() const;
  ::CoreML::Specification::PoolingLayerParams* mutable_pooling();
  ::CoreML::Specification::PoolingLayerParams* release_pooling();
  void set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling);

  // .CoreML.Specification.ActivationParams activation = 130;
  bool has_activation() const;
  void clear_activation();
  static const int kActivationFieldNumber = 130;
  const ::CoreML::Specification::ActivationParams& activation() const;
  ::CoreML::Specification::ActivationParams* mutable_activation();
  ::CoreML::Specification::ActivationParams* release_activation();
  void set_allocated_activation(::CoreML::Specification::ActivationParams* activation);

  // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  bool has_innerproduct() const;
  void clear_innerproduct();
  static const int kInnerProductFieldNumber = 140;
  const ::CoreML::Specification::InnerProductLayerParams& innerproduct() const;
  ::CoreML::Specification::InnerProductLayerParams* mutable_innerproduct();
  ::CoreML::Specification::InnerProductLayerParams* release_innerproduct();
  void set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct);

  // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  bool has_embedding() const;
  void clear_embedding();
  static const int kEmbeddingFieldNumber = 150;
  const ::CoreML::Specification::EmbeddingLayerParams& embedding() const;
  ::CoreML::Specification::EmbeddingLayerParams* mutable_embedding();
  ::CoreML::Specification::EmbeddingLayerParams* release_embedding();
  void set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding);

  // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  bool has_batchnorm() const;
  void clear_batchnorm();
  static const int kBatchnormFieldNumber = 160;
  const ::CoreML::Specification::BatchnormLayerParams& batchnorm() const;
  ::CoreML::Specification::BatchnormLayerParams* mutable_batchnorm();
  ::CoreML::Specification::BatchnormLayerParams* release_batchnorm();
  void set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm);

  // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  bool has_mvn() const;
  void clear_mvn();
  static const int kMvnFieldNumber = 165;
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& mvn() const;
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mutable_mvn();
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* release_mvn();
  void set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn);

  // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  bool has_l2normalize() const;
  void clear_l2normalize();
  static const int kL2NormalizeFieldNumber = 170;
  const ::CoreML::Specification::L2NormalizeLayerParams& l2normalize() const;
  ::CoreML::Specification::L2NormalizeLayerParams* mutable_l2normalize();
  ::CoreML::Specification::L2NormalizeLayerParams* release_l2normalize();
  void set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize);

  // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  bool has_softmax() const;
  void clear_softmax();
  static const int kSoftmaxFieldNumber = 175;
  const ::CoreML::Specification::SoftmaxLayerParams& softmax() const;
  ::CoreML::Specification::SoftmaxLayerParams* mutable_softmax();
  ::CoreML::Specification::SoftmaxLayerParams* release_softmax();
  void set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax);

  // .CoreML.Specification.LRNLayerParams lrn = 180;
  bool has_lrn() const;
  void clear_lrn();
  static const int kLrnFieldNumber = 180;
  const ::CoreML::Specification::LRNLayerParams& lrn() const;
  ::CoreML::Specification::LRNLayerParams* mutable_lrn();
  ::CoreML::Specification::LRNLayerParams* release_lrn();
  void set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn);

  // .CoreML.Specification.CropLayerParams crop = 190;
  bool has_crop() const;
  void clear_crop();
  static const int kCropFieldNumber = 190;
  const ::CoreML::Specification::CropLayerParams& crop() const;
  ::CoreML::Specification::CropLayerParams* mutable_crop();
  ::CoreML::Specification::CropLayerParams* release_crop();
  void set_allocated_crop(::CoreML::Specification::CropLayerParams* crop);

  // .CoreML.Specification.PaddingLayerParams padding = 200;
  bool has_padding() const;
  void clear_padding();
  static const int kPaddingFieldNumber = 200;
  const ::CoreML::Specification::PaddingLayerParams& padding() const;
  ::CoreML::Specification::PaddingLayerParams* mutable_padding();
  ::CoreML::Specification::PaddingLayerParams* release_padding();
  void set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding);

  // .CoreML.Specification.UpsampleLayerParams upsample = 210;
  bool has_upsample() const;
  void clear_upsample();
  static const int kUpsampleFieldNumber = 210;
  const ::CoreML::Specification::UpsampleLayerParams& upsample() const;
  ::CoreML::Specification::UpsampleLayerParams* mutable_upsample();
  ::CoreML::Specification::UpsampleLayerParams* release_upsample();
  void set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample);

  // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
  bool has_resizebilinear() const;
  void clear_resizebilinear();
  static const int kResizeBilinearFieldNumber = 211;
  const ::CoreML::Specification::ResizeBilinearLayerParams& resizebilinear() const;
  ::CoreML::Specification::ResizeBilinearLayerParams* mutable_resizebilinear();
  ::CoreML::Specification::ResizeBilinearLayerParams* release_resizebilinear();
  void set_allocated_resizebilinear(::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear);

  // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
  bool has_cropresize() const;
  void clear_cropresize();
  static const int kCropResizeFieldNumber = 212;
  const ::CoreML::Specification::CropResizeLayerParams& cropresize() const;
  ::CoreML::Specification::CropResizeLayerParams* mutable_cropresize();
  ::CoreML::Specification::CropResizeLayerParams* release_cropresize();
  void set_allocated_cropresize(::CoreML::Specification::CropResizeLayerParams* cropresize);

  // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  bool has_unary() const;
  void clear_unary();
  static const int kUnaryFieldNumber = 220;
  const ::CoreML::Specification::UnaryFunctionLayerParams& unary() const;
  ::CoreML::Specification::UnaryFunctionLayerParams* mutable_unary();
  ::CoreML::Specification::UnaryFunctionLayerParams* release_unary();
  void set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary);

  // .CoreML.Specification.AddLayerParams add = 230;
  bool has_add() const;
  void clear_add();
  static const int kAddFieldNumber = 230;
  const ::CoreML::Specification::AddLayerParams& add() const;
  ::CoreML::Specification::AddLayerParams* mutable_add();
  ::CoreML::Specification::AddLayerParams* release_add();
  void set_allocated_add(::CoreML::Specification::AddLayerParams* add);

  // .CoreML.Specification.MultiplyLayerParams multiply = 231;
  bool has_multiply() const;
  void clear_multiply();
  static const int kMultiplyFieldNumber = 231;
  const ::CoreML::Specification::MultiplyLayerParams& multiply() const;
  ::CoreML::Specification::MultiplyLayerParams* mutable_multiply();
  ::CoreML::Specification::MultiplyLayerParams* release_multiply();
  void set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply);

  // .CoreML.Specification.AverageLayerParams average = 240;
  bool has_average() const;
  void clear_average();
  static const int kAverageFieldNumber = 240;
  const ::CoreML::Specification::AverageLayerParams& average() const;
  ::CoreML::Specification::AverageLayerParams* mutable_average();
  ::CoreML::Specification::AverageLayerParams* release_average();
  void set_allocated_average(::CoreML::Specification::AverageLayerParams* average);

  // .CoreML.Specification.ScaleLayerParams scale = 245;
  bool has_scale() const;
  void clear_scale();
  static const int kScaleFieldNumber = 245;
  const ::CoreML::Specification::ScaleLayerParams& scale() const;
  ::CoreML::Specification::ScaleLayerParams* mutable_scale();
  ::CoreML::Specification::ScaleLayerParams* release_scale();
  void set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale);

  // .CoreML.Specification.BiasLayerParams bias = 250;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 250;
  const ::CoreML::Specification::BiasLayerParams& bias() const;
  ::CoreML::Specification::BiasLayerParams* mutable_bias();
  ::CoreML::Specification::BiasLayerParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias);

  // .CoreML.Specification.MaxLayerParams max = 260;
  bool has_max() const;
  void clear_max();
  static const int kMaxFieldNumber = 260;
  const ::CoreML::Specification::MaxLayerParams& max() const;
  ::CoreML::Specification::MaxLayerParams* mutable_max();
  ::CoreML::Specification::MaxLayerParams* release_max();
  void set_allocated_max(::CoreML::Specification::MaxLayerParams* max);

  // .CoreML.Specification.MinLayerParams min = 261;
  bool has_min() const;
  void clear_min();
  static const int kMinFieldNumber = 261;
  const ::CoreML::Specification::MinLayerParams& min() const;
  ::CoreML::Specification::MinLayerParams* mutable_min();
  ::CoreML::Specification::MinLayerParams* release_min();
  void set_allocated_min(::CoreML::Specification::MinLayerParams* min);

  // .CoreML.Specification.DotProductLayerParams dot = 270;
  bool has_dot() const;
  void clear_dot();
  static const int kDotFieldNumber = 270;
  const ::CoreML::Specification::DotProductLayerParams& dot() const;
  ::CoreML::Specification::DotProductLayerParams* mutable_dot();
  ::CoreML::Specification::DotProductLayerParams* release_dot();
  void set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot);

  // .CoreML.Specification.ReduceLayerParams reduce = 280;
  bool has_reduce() const;
  void clear_reduce();
  static const int kReduceFieldNumber = 280;
  const ::CoreML::Specification::ReduceLayerParams& reduce() const;
  ::CoreML::Specification::ReduceLayerParams* mutable_reduce();
  ::CoreML::Specification::ReduceLayerParams* release_reduce();
  void set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce);

  // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  bool has_loadconstant() const;
  void clear_loadconstant();
  static const int kLoadConstantFieldNumber = 290;
  const ::CoreML::Specification::LoadConstantLayerParams& loadconstant() const;
  ::CoreML::Specification::LoadConstantLayerParams* mutable_loadconstant();
  ::CoreML::Specification::LoadConstantLayerParams* release_loadconstant();
  void set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant);

  // .CoreML.Specification.ReshapeLayerParams reshape = 300;
  bool has_reshape() const;
  void clear_reshape();
  static const int kReshapeFieldNumber = 300;
  const ::CoreML::Specification::ReshapeLayerParams& reshape() const;
  ::CoreML::Specification::ReshapeLayerParams* mutable_reshape();
  ::CoreML::Specification::ReshapeLayerParams* release_reshape();
  void set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape);

  // .CoreML.Specification.FlattenLayerParams flatten = 301;
  bool has_flatten() const;
  void clear_flatten();
  static const int kFlattenFieldNumber = 301;
  const ::CoreML::Specification::FlattenLayerParams& flatten() const;
  ::CoreML::Specification::FlattenLayerParams* mutable_flatten();
  ::CoreML::Specification::FlattenLayerParams* release_flatten();
  void set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten);

  // .CoreML.Specification.PermuteLayerParams permute = 310;
  bool has_permute() const;
  void clear_permute();
  static const int kPermuteFieldNumber = 310;
  const ::CoreML::Specification::PermuteLayerParams& permute() const;
  ::CoreML::Specification::PermuteLayerParams* mutable_permute();
  ::CoreML::Specification::PermuteLayerParams* release_permute();
  void set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute);

  // .CoreML.Specification.ConcatLayerParams concat = 320;
  bool has_concat() const;
  void clear_concat();
  static const int kConcatFieldNumber = 320;
  const ::CoreML::Specification::ConcatLayerParams& concat() const;
  ::CoreML::Specification::ConcatLayerParams* mutable_concat();
  ::CoreML::Specification::ConcatLayerParams* release_concat();
  void set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat);

  // .CoreML.Specification.SplitLayerParams split = 330;
  bool has_split() const;
  void clear_split();
  static const int kSplitFieldNumber = 330;
  const ::CoreML::Specification::SplitLayerParams& split() const;
  ::CoreML::Specification::SplitLayerParams* mutable_split();
  ::CoreML::Specification::SplitLayerParams* release_split();
  void set_allocated_split(::CoreML::Specification::SplitLayerParams* split);

  // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  bool has_sequencerepeat() const;
  void clear_sequencerepeat();
  static const int kSequenceRepeatFieldNumber = 340;
  const ::CoreML::Specification::SequenceRepeatLayerParams& sequencerepeat() const;
  ::CoreML::Specification::SequenceRepeatLayerParams* mutable_sequencerepeat();
  ::CoreML::Specification::SequenceRepeatLayerParams* release_sequencerepeat();
  void set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat);

  // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
  bool has_reorganizedata() const;
  void clear_reorganizedata();
  static const int kReorganizeDataFieldNumber = 345;
  const ::CoreML::Specification::ReorganizeDataLayerParams& reorganizedata() const;
  ::CoreML::Specification::ReorganizeDataLayerParams* mutable_reorganizedata();
  ::CoreML::Specification::ReorganizeDataLayerParams* release_reorganizedata();
  void set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata);

  // .CoreML.Specification.SliceLayerParams slice = 350;
  bool has_slice() const;
  void clear_slice();
  static const int kSliceFieldNumber = 350;
  const ::CoreML::Specification::SliceLayerParams& slice() const;
  ::CoreML::Specification::SliceLayerParams* mutable_slice();
  ::CoreML::Specification::SliceLayerParams* release_slice();
  void set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice);

  // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  bool has_simplerecurrent() const;
  void clear_simplerecurrent();
  static const int kSimpleRecurrentFieldNumber = 400;
  const ::CoreML::Specification::SimpleRecurrentLayerParams& simplerecurrent() const;
  ::CoreML::Specification::SimpleRecurrentLayerParams* mutable_simplerecurrent();
  ::CoreML::Specification::SimpleRecurrentLayerParams* release_simplerecurrent();
  void set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent);

  // .CoreML.Specification.GRULayerParams gru = 410;
  bool has_gru() const;
  void clear_gru();
  static const int kGruFieldNumber = 410;
  const ::CoreML::Specification::GRULayerParams& gru() const;
  ::CoreML::Specification::GRULayerParams* mutable_gru();
  ::CoreML::Specification::GRULayerParams* release_gru();
  void set_allocated_gru(::CoreML::Specification::GRULayerParams* gru);

  // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  bool has_unidirectionallstm() const;
  void clear_unidirectionallstm();
  static const int kUniDirectionalLSTMFieldNumber = 420;
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams& unidirectionallstm() const;
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* mutable_unidirectionallstm();
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* release_unidirectionallstm();
  void set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm);

  // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  bool has_bidirectionallstm() const;
  void clear_bidirectionallstm();
  static const int kBiDirectionalLSTMFieldNumber = 430;
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams& bidirectionallstm() const;
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* mutable_bidirectionallstm();
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* release_bidirectionallstm();
  void set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm);

  // .CoreML.Specification.CustomLayerParams custom = 500;
  bool has_custom() const;
  void clear_custom();
  static const int kCustomFieldNumber = 500;
  const ::CoreML::Specification::CustomLayerParams& custom() const;
  ::CoreML::Specification::CustomLayerParams* mutable_custom();
  ::CoreML::Specification::CustomLayerParams* release_custom();
  void set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom);

  // .CoreML.Specification.CopyLayerParams copy = 600;
  bool has_copy() const;
  void clear_copy();
  static const int kCopyFieldNumber = 600;
  const ::CoreML::Specification::CopyLayerParams& copy() const;
  ::CoreML::Specification::CopyLayerParams* mutable_copy();
  ::CoreML::Specification::CopyLayerParams* release_copy();
  void set_allocated_copy(::CoreML::Specification::CopyLayerParams* copy);

  // .CoreML.Specification.BranchLayerParams branch = 605;
  bool has_branch() const;
  void clear_branch();
  static const int kBranchFieldNumber = 605;
  const ::CoreML::Specification::BranchLayerParams& branch() const;
  ::CoreML::Specification::BranchLayerParams* mutable_branch();
  ::CoreML::Specification::BranchLayerParams* release_branch();
  void set_allocated_branch(::CoreML::Specification::BranchLayerParams* branch);

  // .CoreML.Specification.LoopLayerParams loop = 615;
  bool has_loop() const;
  void clear_loop();
  static const int kLoopFieldNumber = 615;
  const ::CoreML::Specification::LoopLayerParams& loop() const;
  ::CoreML::Specification::LoopLayerParams* mutable_loop();
  ::CoreML::Specification::LoopLayerParams* release_loop();
  void set_allocated_loop(::CoreML::Specification::LoopLayerParams* loop);

  // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
  bool has_loopbreak() const;
  void clear_loopbreak();
  static const int kLoopBreakFieldNumber = 620;
  const ::CoreML::Specification::LoopBreakLayerParams& loopbreak() const;
  ::CoreML::Specification::LoopBreakLayerParams* mutable_loopbreak();
  ::CoreML::Specification::LoopBreakLayerParams* release_loopbreak();
  void set_allocated_loopbreak(::CoreML::Specification::LoopBreakLayerParams* loopbreak);

  // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
  bool has_loopcontinue() const;
  void clear_loopcontinue();
  static const int kLoopContinueFieldNumber = 625;
  const ::CoreML::Specification::LoopContinueLayerParams& loopcontinue() const;
  ::CoreML::Specification::LoopContinueLayerParams* mutable_loopcontinue();
  ::CoreML::Specification::LoopContinueLayerParams* release_loopcontinue();
  void set_allocated_loopcontinue(::CoreML::Specification::LoopContinueLayerParams* loopcontinue);

  // .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
  bool has_rangestatic() const;
  void clear_rangestatic();
  static const int kRangeStaticFieldNumber = 635;
  const ::CoreML::Specification::RangeStaticLayerParams& rangestatic() const;
  ::CoreML::Specification::RangeStaticLayerParams* mutable_rangestatic();
  ::CoreML::Specification::RangeStaticLayerParams* release_rangestatic();
  void set_allocated_rangestatic(::CoreML::Specification::RangeStaticLayerParams* rangestatic);

  // .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
  bool has_rangedynamic() const;
  void clear_rangedynamic();
  static const int kRangeDynamicFieldNumber = 640;
  const ::CoreML::Specification::RangeDynamicLayerParams& rangedynamic() const;
  ::CoreML::Specification::RangeDynamicLayerParams* mutable_rangedynamic();
  ::CoreML::Specification::RangeDynamicLayerParams* release_rangedynamic();
  void set_allocated_rangedynamic(::CoreML::Specification::RangeDynamicLayerParams* rangedynamic);

  // .CoreML.Specification.ClipLayerParams clip = 660;
  bool has_clip() const;
  void clear_clip();
  static const int kClipFieldNumber = 660;
  const ::CoreML::Specification::ClipLayerParams& clip() const;
  ::CoreML::Specification::ClipLayerParams* mutable_clip();
  ::CoreML::Specification::ClipLayerParams* release_clip();
  void set_allocated_clip(::CoreML::Specification::ClipLayerParams* clip);

  // .CoreML.Specification.CeilLayerParams ceil = 665;
  bool has_ceil() const;
  void clear_ceil();
  static const int kCeilFieldNumber = 665;
  const ::CoreML::Specification::CeilLayerParams& ceil() const;
  ::CoreML::Specification::CeilLayerParams* mutable_ceil();
  ::CoreML::Specification::CeilLayerParams* release_ceil();
  void set_allocated_ceil(::CoreML::Specification::CeilLayerParams* ceil);

  // .CoreML.Specification.FloorLayerParams floor = 670;
  bool has_floor() const;
  void clear_floor();
  static const int kFloorFieldNumber = 670;
  const ::CoreML::Specification::FloorLayerParams& floor() const;
  ::CoreML::Specification::FloorLayerParams* mutable_floor();
  ::CoreML::Specification::FloorLayerParams* release_floor();
  void set_allocated_floor(::CoreML::Specification::FloorLayerParams* floor);

  // .CoreML.Specification.SignLayerParams sign = 680;
  bool has_sign() const;
  void clear_sign();
  static const int kSignFieldNumber = 680;
  const ::CoreML::Specification::SignLayerParams& sign() const;
  ::CoreML::Specification::SignLayerParams* mutable_sign();
  ::CoreML::Specification::SignLayerParams* release_sign();
  void set_allocated_sign(::CoreML::Specification::SignLayerParams* sign);

  // .CoreML.Specification.RoundLayerParams round = 685;
  bool has_round() const;
  void clear_round();
  static const int kRoundFieldNumber = 685;
  const ::CoreML::Specification::RoundLayerParams& round() const;
  ::CoreML::Specification::RoundLayerParams* mutable_round();
  ::CoreML::Specification::RoundLayerParams* release_round();
  void set_allocated_round(::CoreML::Specification::RoundLayerParams* round);

  // .CoreML.Specification.Exp2LayerParams exp2 = 700;
  bool has_exp2() const;
  void clear_exp2();
  static const int kExp2FieldNumber = 700;
  const ::CoreML::Specification::Exp2LayerParams& exp2() const;
  ::CoreML::Specification::Exp2LayerParams* mutable_exp2();
  ::CoreML::Specification::Exp2LayerParams* release_exp2();
  void set_allocated_exp2(::CoreML::Specification::Exp2LayerParams* exp2);

  // .CoreML.Specification.SinLayerParams sin = 710;
  bool has_sin() const;
  void clear_sin();
  static const int kSinFieldNumber = 710;
  const ::CoreML::Specification::SinLayerParams& sin() const;
  ::CoreML::Specification::SinLayerParams* mutable_sin();
  ::CoreML::Specification::SinLayerParams* release_sin();
  void set_allocated_sin(::CoreML::Specification::SinLayerParams* sin);

  // .CoreML.Specification.CosLayerParams cos = 715;
  bool has_cos() const;
  void clear_cos();
  static const int kCosFieldNumber = 715;
  const ::CoreML::Specification::CosLayerParams& cos() const;
  ::CoreML::Specification::CosLayerParams* mutable_cos();
  ::CoreML::Specification::CosLayerParams* release_cos();
  void set_allocated_cos(::CoreML::Specification::CosLayerParams* cos);

  // .CoreML.Specification.TanLayerParams tan = 720;
  bool has_tan() const;
  void clear_tan();
  static const int kTanFieldNumber = 720;
  const ::CoreML::Specification::TanLayerParams& tan() const;
  ::CoreML::Specification::TanLayerParams* mutable_tan();
  ::CoreML::Specification::TanLayerParams* release_tan();
  void set_allocated_tan(::CoreML::Specification::TanLayerParams* tan);

  // .CoreML.Specification.AsinLayerParams asin = 730;
  bool has_asin() const;
  void clear_asin();
  static const int kAsinFieldNumber = 730;
  const ::CoreML::Specification::AsinLayerParams& asin() const;
  ::CoreML::Specification::AsinLayerParams* mutable_asin();
  ::CoreML::Specification::AsinLayerParams* release_asin();
  void set_allocated_asin(::CoreML::Specification::AsinLayerParams* asin);

  // .CoreML.Specification.AcosLayerParams acos = 735;
  bool has_acos() const;
  void clear_acos();
  static const int kAcosFieldNumber = 735;
  const ::CoreML::Specification::AcosLayerParams& acos() const;
  ::CoreML::Specification::AcosLayerParams* mutable_acos();
  ::CoreML::Specification::AcosLayerParams* release_acos();
  void set_allocated_acos(::CoreML::Specification::AcosLayerParams* acos);

  // .CoreML.Specification.AtanLayerParams atan = 740;
  bool has_atan() const;
  void clear_atan();
  static const int kAtanFieldNumber = 740;
  const ::CoreML::Specification::AtanLayerParams& atan() const;
  ::CoreML::Specification::AtanLayerParams* mutable_atan();
  ::CoreML::Specification::AtanLayerParams* release_atan();
  void set_allocated_atan(::CoreML::Specification::AtanLayerParams* atan);

  // .CoreML.Specification.SinhLayerParams sinh = 750;
  bool has_sinh() const;
  void clear_sinh();
  static const int kSinhFieldNumber = 750;
  const ::CoreML::Specification::SinhLayerParams& sinh() const;
  ::CoreML::Specification::SinhLayerParams* mutable_sinh();
  ::CoreML::Specification::SinhLayerParams* release_sinh();
  void set_allocated_sinh(::CoreML::Specification::SinhLayerParams* sinh);

  // .CoreML.Specification.CoshLayerParams cosh = 755;
  bool has_cosh() const;
  void clear_cosh();
  static const int kCoshFieldNumber = 755;
  const ::CoreML::Specification::CoshLayerParams& cosh() const;
  ::CoreML::Specification::CoshLayerParams* mutable_cosh();
  ::CoreML::Specification::CoshLayerParams* release_cosh();
  void set_allocated_cosh(::CoreML::Specification::CoshLayerParams* cosh);

  // .CoreML.Specification.TanhLayerParams tanh = 760;
  bool has_tanh() const;
  void clear_tanh();
  static const int kTanhFieldNumber = 760;
  const ::CoreML::Specification::TanhLayerParams& tanh() const;
  ::CoreML::Specification::TanhLayerParams* mutable_tanh();
  ::CoreML::Specification::TanhLayerParams* release_tanh();
  void set_allocated_tanh(::CoreML::Specification::TanhLayerParams* tanh);

  // .CoreML.Specification.AsinhLayerParams asinh = 770;
  bool has_asinh() const;
  void clear_asinh();
  static const int kAsinhFieldNumber = 770;
  const ::CoreML::Specification::AsinhLayerParams& asinh() const;
  ::CoreML::Specification::AsinhLayerParams* mutable_asinh();
  ::CoreML::Specification::AsinhLayerParams* release_asinh();
  void set_allocated_asinh(::CoreML::Specification::AsinhLayerParams* asinh);

  // .CoreML.Specification.AcoshLayerParams acosh = 775;
  bool has_acosh() const;
  void clear_acosh();
  static const int kAcoshFieldNumber = 775;
  const ::CoreML::Specification::AcoshLayerParams& acosh() const;
  ::CoreML::Specification::AcoshLayerParams* mutable_acosh();
  ::CoreML::Specification::AcoshLayerParams* release_acosh();
  void set_allocated_acosh(::CoreML::Specification::AcoshLayerParams* acosh);

  // .CoreML.Specification.AtanhLayerParams atanh = 780;
  bool has_atanh() const;
  void clear_atanh();
  static const int kAtanhFieldNumber = 780;
  const ::CoreML::Specification::AtanhLayerParams& atanh() const;
  ::CoreML::Specification::AtanhLayerParams* mutable_atanh();
  ::CoreML::Specification::AtanhLayerParams* release_atanh();
  void set_allocated_atanh(::CoreML::Specification::AtanhLayerParams* atanh);

  // .CoreML.Specification.ErfLayerParams erf = 790;
  bool has_erf() const;
  void clear_erf();
  static const int kErfFieldNumber = 790;
  const ::CoreML::Specification::ErfLayerParams& erf() const;
  ::CoreML::Specification::ErfLayerParams* mutable_erf();
  ::CoreML::Specification::ErfLayerParams* release_erf();
  void set_allocated_erf(::CoreML::Specification::ErfLayerParams* erf);

  // .CoreML.Specification.GeluLayerParams gelu = 795;
  bool has_gelu() const;
  void clear_gelu();
  static const int kGeluFieldNumber = 795;
  const ::CoreML::Specification::GeluLayerParams& gelu() const;
  ::CoreML::Specification::GeluLayerParams* mutable_gelu();
  ::CoreML::Specification::GeluLayerParams* release_gelu();
  void set_allocated_gelu(::CoreML::Specification::GeluLayerParams* gelu);

  // .CoreML.Specification.EqualLayerParams equal = 815;
  bool has_equal() const;
  void clear_equal();
  static const int kEqualFieldNumber = 815;
  const ::CoreML::Specification::EqualLayerParams& equal() const;
  ::CoreML::Specification::EqualLayerParams* mutable_equal();
  ::CoreML::Specification::EqualLayerParams* release_equal();
  void set_allocated_equal(::CoreML::Specification::EqualLayerParams* equal);

  // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
  bool has_notequal() const;
  void clear_notequal();
  static const int kNotEqualFieldNumber = 820;
  const ::CoreML::Specification::NotEqualLayerParams& notequal() const;
  ::CoreML::Specification::NotEqualLayerParams* mutable_notequal();
  ::CoreML::Specification::NotEqualLayerParams* release_notequal();
  void set_allocated_notequal(::CoreML::Specification::NotEqualLayerParams* notequal);

  // .CoreML.Specification.LessThanLayerParams lessThan = 825;
  bool has_lessthan() const;
  void clear_lessthan();
  static const int kLessThanFieldNumber = 825;
  const ::CoreML::Specification::LessThanLayerParams& lessthan() const;
  ::CoreML::Specification::LessThanLayerParams* mutable_lessthan();
  ::CoreML::Specification::LessThanLayerParams* release_lessthan();
  void set_allocated_lessthan(::CoreML::Specification::LessThanLayerParams* lessthan);

  // .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
  bool has_lessequal() const;
  void clear_lessequal();
  static const int kLessEqualFieldNumber = 827;
  const ::CoreML::Specification::LessEqualLayerParams& lessequal() const;
  ::CoreML::Specification::LessEqualLayerParams* mutable_lessequal();
  ::CoreML::Specification::LessEqualLayerParams* release_lessequal();
  void set_allocated_lessequal(::CoreML::Specification::LessEqualLayerParams* lessequal);

  // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
  bool has_greaterthan() const;
  void clear_greaterthan();
  static const int kGreaterThanFieldNumber = 830;
  const ::CoreML::Specification::GreaterThanLayerParams& greaterthan() const;
  ::CoreML::Specification::GreaterThanLayerParams* mutable_greaterthan();
  ::CoreML::Specification::GreaterThanLayerParams* release_greaterthan();
  void set_allocated_greaterthan(::CoreML::Specification::GreaterThanLayerParams* greaterthan);

  // .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
  bool has_greaterequal() const;
  void clear_greaterequal();
  static const int kGreaterEqualFieldNumber = 832;
  const ::CoreML::Specification::GreaterEqualLayerParams& greaterequal() const;
  ::CoreML::Specification::GreaterEqualLayerParams* mutable_greaterequal();
  ::CoreML::Specification::GreaterEqualLayerParams* release_greaterequal();
  void set_allocated_greaterequal(::CoreML::Specification::GreaterEqualLayerParams* greaterequal);

  // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
  bool has_logicalor() const;
  void clear_logicalor();
  static const int kLogicalOrFieldNumber = 840;
  const ::CoreML::Specification::LogicalOrLayerParams& logicalor() const;
  ::CoreML::Specification::LogicalOrLayerParams* mutable_logicalor();
  ::CoreML::Specification::LogicalOrLayerParams* release_logicalor();
  void set_allocated_logicalor(::CoreML::Specification::LogicalOrLayerParams* logicalor);

  // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
  bool has_logicalxor() const;
  void clear_logicalxor();
  static const int kLogicalXorFieldNumber = 845;
  const ::CoreML::Specification::LogicalXorLayerParams& logicalxor() const;
  ::CoreML::Specification::LogicalXorLayerParams* mutable_logicalxor();
  ::CoreML::Specification::LogicalXorLayerParams* release_logicalxor();
  void set_allocated_logicalxor(::CoreML::Specification::LogicalXorLayerParams* logicalxor);

  // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
  bool has_logicalnot() const;
  void clear_logicalnot();
  static const int kLogicalNotFieldNumber = 850;
  const ::CoreML::Specification::LogicalNotLayerParams& logicalnot() const;
  ::CoreML::Specification::LogicalNotLayerParams* mutable_logicalnot();
  ::CoreML::Specification::LogicalNotLayerParams* release_logicalnot();
  void set_allocated_logicalnot(::CoreML::Specification::LogicalNotLayerParams* logicalnot);

  // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
  bool has_logicaland() const;
  void clear_logicaland();
  static const int kLogicalAndFieldNumber = 855;
  const ::CoreML::Specification::LogicalAndLayerParams& logicaland() const;
  ::CoreML::Specification::LogicalAndLayerParams* mutable_logicaland();
  ::CoreML::Specification::LogicalAndLayerParams* release_logicaland();
  void set_allocated_logicaland(::CoreML::Specification::LogicalAndLayerParams* logicaland);

  // .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
  bool has_modbroadcastable() const;
  void clear_modbroadcastable();
  static const int kModBroadcastableFieldNumber = 865;
  const ::CoreML::Specification::ModBroadcastableLayerParams& modbroadcastable() const;
  ::CoreML::Specification::ModBroadcastableLayerParams* mutable_modbroadcastable();
  ::CoreML::Specification::ModBroadcastableLayerParams* release_modbroadcastable();
  void set_allocated_modbroadcastable(::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable);

  // .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
  bool has_minbroadcastable() const;
  void clear_minbroadcastable();
  static const int kMinBroadcastableFieldNumber = 870;
  const ::CoreML::Specification::MinBroadcastableLayerParams& minbroadcastable() const;
  ::CoreML::Specification::MinBroadcastableLayerParams* mutable_minbroadcastable();
  ::CoreML::Specification::MinBroadcastableLayerParams* release_minbroadcastable();
  void set_allocated_minbroadcastable(::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable);

  // .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
  bool has_maxbroadcastable() const;
  void clear_maxbroadcastable();
  static const int kMaxBroadcastableFieldNumber = 875;
  const ::CoreML::Specification::MaxBroadcastableLayerParams& maxbroadcastable() const;
  ::CoreML::Specification::MaxBroadcastableLayerParams* mutable_maxbroadcastable();
  ::CoreML::Specification::MaxBroadcastableLayerParams* release_maxbroadcastable();
  void set_allocated_maxbroadcastable(::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable);

  // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
  bool has_addbroadcastable() const;
  void clear_addbroadcastable();
  static const int kAddBroadcastableFieldNumber = 880;
  const ::CoreML::Specification::AddBroadcastableLayerParams& addbroadcastable() const;
  ::CoreML::Specification::AddBroadcastableLayerParams* mutable_addbroadcastable();
  ::CoreML::Specification::AddBroadcastableLayerParams* release_addbroadcastable();
  void set_allocated_addbroadcastable(::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable);

  // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
  bool has_powbroadcastable() const;
  void clear_powbroadcastable();
  static const int kPowBroadcastableFieldNumber = 885;
  const ::CoreML::Specification::PowBroadcastableLayerParams& powbroadcastable() const;
  ::CoreML::Specification::PowBroadcastableLayerParams* mutable_powbroadcastable();
  ::CoreML::Specification::PowBroadcastableLayerParams* release_powbroadcastable();
  void set_allocated_powbroadcastable(::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable);

  // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
  bool has_dividebroadcastable() const;
  void clear_dividebroadcastable();
  static const int kDivideBroadcastableFieldNumber = 890;
  const ::CoreML::Specification::DivideBroadcastableLayerParams& dividebroadcastable() const;
  ::CoreML::Specification::DivideBroadcastableLayerParams* mutable_dividebroadcastable();
  ::CoreML::Specification::DivideBroadcastableLayerParams* release_dividebroadcastable();
  void set_allocated_dividebroadcastable(::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable);

  // .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
  bool has_floordivbroadcastable() const;
  void clear_floordivbroadcastable();
  static const int kFloorDivBroadcastableFieldNumber = 895;
  const ::CoreML::Specification::FloorDivBroadcastableLayerParams& floordivbroadcastable() const;
  ::CoreML::Specification::FloorDivBroadcastableLayerParams* mutable_floordivbroadcastable();
  ::CoreML::Specification::FloorDivBroadcastableLayerParams* release_floordivbroadcastable();
  void set_allocated_floordivbroadcastable(::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable);

  // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
  bool has_multiplybroadcastable() const;
  void clear_multiplybroadcastable();
  static const int kMultiplyBroadcastableFieldNumber = 900;
  const ::CoreML::Specification::MultiplyBroadcastableLayerParams& multiplybroadcastable() const;
  ::CoreML::Specification::MultiplyBroadcastableLayerParams* mutable_multiplybroadcastable();
  ::CoreML::Specification::MultiplyBroadcastableLayerParams* release_multiplybroadcastable();
  void set_allocated_multiplybroadcastable(::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable);

  // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
  bool has_subtractbroadcastable() const;
  void clear_subtractbroadcastable();
  static const int kSubtractBroadcastableFieldNumber = 905;
  const ::CoreML::Specification::SubtractBroadcastableLayerParams& subtractbroadcastable() const;
  ::CoreML::Specification::SubtractBroadcastableLayerParams* mutable_subtractbroadcastable();
  ::CoreML::Specification::SubtractBroadcastableLayerParams* release_subtractbroadcastable();
  void set_allocated_subtractbroadcastable(::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable);

  // .CoreML.Specification.TileLayerParams tile = 920;
  bool has_tile() const;
  void clear_tile();
  static const int kTileFieldNumber = 920;
  const ::CoreML::Specification::TileLayerParams& tile() const;
  ::CoreML::Specification::TileLayerParams* mutable_tile();
  ::CoreML::Specification::TileLayerParams* release_tile();
  void set_allocated_tile(::CoreML::Specification::TileLayerParams* tile);

  // .CoreML.Specification.StackLayerParams stack = 925;
  bool has_stack() const;
  void clear_stack();
  static const int kStackFieldNumber = 925;
  const ::CoreML::Specification::StackLayerParams& stack() const;
  ::CoreML::Specification::StackLayerParams* mutable_stack();
  ::CoreML::Specification::StackLayerParams* release_stack();
  void set_allocated_stack(::CoreML::Specification::StackLayerParams* stack);

  // .CoreML.Specification.GatherLayerParams gather = 930;
  bool has_gather() const;
  void clear_gather();
  static const int kGatherFieldNumber = 930;
  const ::CoreML::Specification::GatherLayerParams& gather() const;
  ::CoreML::Specification::GatherLayerParams* mutable_gather();
  ::CoreML::Specification::GatherLayerParams* release_gather();
  void set_allocated_gather(::CoreML::Specification::GatherLayerParams* gather);

  // .CoreML.Specification.ScatterLayerParams scatter = 935;
  bool has_scatter() const;
  void clear_scatter();
  static const int kScatterFieldNumber = 935;
  const ::CoreML::Specification::ScatterLayerParams& scatter() const;
  ::CoreML::Specification::ScatterLayerParams* mutable_scatter();
  ::CoreML::Specification::ScatterLayerParams* release_scatter();
  void set_allocated_scatter(::CoreML::Specification::ScatterLayerParams* scatter);

  // .CoreML.Specification.GatherNDLayerParams gatherND = 940;
  bool has_gathernd() const;
  void clear_gathernd();
  static const int kGatherNDFieldNumber = 940;
  const ::CoreML::Specification::GatherNDLayerParams& gathernd() const;
  ::CoreML::Specification::GatherNDLayerParams* mutable_gathernd();
  ::CoreML::Specification::GatherNDLayerParams* release_gathernd();
  void set_allocated_gathernd(::CoreML::Specification::GatherNDLayerParams* gathernd);

  // .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
  bool has_scatternd() const;
  void clear_scatternd();
  static const int kScatterNDFieldNumber = 945;
  const ::CoreML::Specification::ScatterNDLayerParams& scatternd() const;
  ::CoreML::Specification::ScatterNDLayerParams* mutable_scatternd();
  ::CoreML::Specification::ScatterNDLayerParams* release_scatternd();
  void set_allocated_scatternd(::CoreML::Specification::ScatterNDLayerParams* scatternd);

  // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
  bool has_softmaxnd() const;
  void clear_softmaxnd();
  static const int kSoftmaxNDFieldNumber = 950;
  const ::CoreML::Specification::SoftmaxNDLayerParams& softmaxnd() const;
  ::CoreML::Specification::SoftmaxNDLayerParams* mutable_softmaxnd();
  ::CoreML::Specification::SoftmaxNDLayerParams* release_softmaxnd();
  void set_allocated_softmaxnd(::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd);

  // .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
  bool has_gatheralongaxis() const;
  void clear_gatheralongaxis();
  static const int kGatherAlongAxisFieldNumber = 952;
  const ::CoreML::Specification::GatherAlongAxisLayerParams& gatheralongaxis() const;
  ::CoreML::Specification::GatherAlongAxisLayerParams* mutable_gatheralongaxis();
  ::CoreML::Specification::GatherAlongAxisLayerParams* release_gatheralongaxis();
  void set_allocated_gatheralongaxis(::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis);

  // .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
  bool has_scatteralongaxis() const;
  void clear_scatteralongaxis();
  static const int kScatterAlongAxisFieldNumber = 954;
  const ::CoreML::Specification::ScatterAlongAxisLayerParams& scatteralongaxis() const;
  ::CoreML::Specification::ScatterAlongAxisLayerParams* mutable_scatteralongaxis();
  ::CoreML::Specification::ScatterAlongAxisLayerParams* release_scatteralongaxis();
  void set_allocated_scatteralongaxis(::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis);

  // .CoreML.Specification.ReverseLayerParams reverse = 960;
  bool has_reverse() const;
  void clear_reverse();
  static const int kReverseFieldNumber = 960;
  const ::CoreML::Specification::ReverseLayerParams& reverse() const;
  ::CoreML::Specification::ReverseLayerParams* mutable_reverse();
  ::CoreML::Specification::ReverseLayerParams* release_reverse();
  void set_allocated_reverse(::CoreML::Specification::ReverseLayerParams* reverse);

  // .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
  bool has_reverseseq() const;
  void clear_reverseseq();
  static const int kReverseSeqFieldNumber = 965;
  const ::CoreML::Specification::ReverseSeqLayerParams& reverseseq() const;
  ::CoreML::Specification::ReverseSeqLayerParams* mutable_reverseseq();
  ::CoreML::Specification::ReverseSeqLayerParams* release_reverseseq();
  void set_allocated_reverseseq(::CoreML::Specification::ReverseSeqLayerParams* reverseseq);

  // .CoreML.Specification.SplitNDLayerParams splitND = 975;
  bool has_splitnd() const;
  void clear_splitnd();
  static const int kSplitNDFieldNumber = 975;
  const ::CoreML::Specification::SplitNDLayerParams& splitnd() const;
  ::CoreML::Specification::SplitNDLayerParams* mutable_splitnd();
  ::CoreML::Specification::SplitNDLayerParams* release_splitnd();
  void set_allocated_splitnd(::CoreML::Specification::SplitNDLayerParams* splitnd);

  // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
  bool has_concatnd() const;
  void clear_concatnd();
  static const int kConcatNDFieldNumber = 980;
  const ::CoreML::Specification::ConcatNDLayerParams& concatnd() const;
  ::CoreML::Specification::ConcatNDLayerParams* mutable_concatnd();
  ::CoreML::Specification::ConcatNDLayerParams* release_concatnd();
  void set_allocated_concatnd(::CoreML::Specification::ConcatNDLayerParams* concatnd);

  // .CoreML.Specification.TransposeLayerParams transpose = 985;
  bool has_transpose() const;
  void clear_transpose();
  static const int kTransposeFieldNumber = 985;
  const ::CoreML::Specification::TransposeLayerParams& transpose() const;
  ::CoreML::Specification::TransposeLayerParams* mutable_transpose();
  ::CoreML::Specification::TransposeLayerParams* release_transpose();
  void set_allocated_transpose(::CoreML::Specification::TransposeLayerParams* transpose);

  // .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
  bool has_slicestatic() const;
  void clear_slicestatic();
  static const int kSliceStaticFieldNumber = 995;
  const ::CoreML::Specification::SliceStaticLayerParams& slicestatic() const;
  ::CoreML::Specification::SliceStaticLayerParams* mutable_slicestatic();
  ::CoreML::Specification::SliceStaticLayerParams* release_slicestatic();
  void set_allocated_slicestatic(::CoreML::Specification::SliceStaticLayerParams* slicestatic);

  // .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
  bool has_slicedynamic() const;
  void clear_slicedynamic();
  static const int kSliceDynamicFieldNumber = 1000;
  const ::CoreML::Specification::SliceDynamicLayerParams& slicedynamic() const;
  ::CoreML::Specification::SliceDynamicLayerParams* mutable_slicedynamic();
  ::CoreML::Specification::SliceDynamicLayerParams* release_slicedynamic();
  void set_allocated_slicedynamic(::CoreML::Specification::SliceDynamicLayerParams* slicedynamic);

  // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
  bool has_slidingwindows() const;
  void clear_slidingwindows();
  static const int kSlidingWindowsFieldNumber = 1005;
  const ::CoreML::Specification::SlidingWindowsLayerParams& slidingwindows() const;
  ::CoreML::Specification::SlidingWindowsLayerParams* mutable_slidingwindows();
  ::CoreML::Specification::SlidingWindowsLayerParams* release_slidingwindows();
  void set_allocated_slidingwindows(::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows);

  // .CoreML.Specification.TopKLayerParams topK = 1015;
  bool has_topk() const;
  void clear_topk();
  static const int kTopKFieldNumber = 1015;
  const ::CoreML::Specification::TopKLayerParams& topk() const;
  ::CoreML::Specification::TopKLayerParams* mutable_topk();
  ::CoreML::Specification::TopKLayerParams* release_topk();
  void set_allocated_topk(::CoreML::Specification::TopKLayerParams* topk);

  // .CoreML.Specification.ArgMinLayerParams argMin = 1020;
  bool has_argmin() const;
  void clear_argmin();
  static const int kArgMinFieldNumber = 1020;
  const ::CoreML::Specification::ArgMinLayerParams& argmin() const;
  ::CoreML::Specification::ArgMinLayerParams* mutable_argmin();
  ::CoreML::Specification::ArgMinLayerParams* release_argmin();
  void set_allocated_argmin(::CoreML::Specification::ArgMinLayerParams* argmin);

  // .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
  bool has_argmax() const;
  void clear_argmax();
  static const int kArgMaxFieldNumber = 1025;
  const ::CoreML::Specification::ArgMaxLayerParams& argmax() const;
  ::CoreML::Specification::ArgMaxLayerParams* mutable_argmax();
  ::CoreML::Specification::ArgMaxLayerParams* release_argmax();
  void set_allocated_argmax(::CoreML::Specification::ArgMaxLayerParams* argmax);

  // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
  bool has_embeddingnd() const;
  void clear_embeddingnd();
  static const int kEmbeddingNDFieldNumber = 1040;
  const ::CoreML::Specification::EmbeddingNDLayerParams& embeddingnd() const;
  ::CoreML::Specification::EmbeddingNDLayerParams* mutable_embeddingnd();
  ::CoreML::Specification::EmbeddingNDLayerParams* release_embeddingnd();
  void set_allocated_embeddingnd(::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd);

  // .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
  bool has_batchedmatmul() const;
  void clear_batchedmatmul();
  static const int kBatchedMatmulFieldNumber = 1045;
  const ::CoreML::Specification::BatchedMatMulLayerParams& batchedmatmul() const;
  ::CoreML::Specification::BatchedMatMulLayerParams* mutable_batchedmatmul();
  ::CoreML::Specification::BatchedMatMulLayerParams* release_batchedmatmul();
  void set_allocated_batchedmatmul(::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul);

  // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
  bool has_getshape() const;
  void clear_getshape();
  static const int kGetShapeFieldNumber = 1065;
  const ::CoreML::Specification::GetShapeLayerParams& getshape() const;
  ::CoreML::Specification::GetShapeLayerParams* mutable_getshape();
  ::CoreML::Specification::GetShapeLayerParams* release_getshape();
  void set_allocated_getshape(::CoreML::Specification::GetShapeLayerParams* getshape);

  // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
  bool has_loadconstantnd() const;
  void clear_loadconstantnd();
  static const int kLoadConstantNDFieldNumber = 1070;
  const ::CoreML::Specification::LoadConstantNDLayerParams& loadconstantnd() const;
  ::CoreML::Specification::LoadConstantNDLayerParams* mutable_loadconstantnd();
  ::CoreML::Specification::LoadConstantNDLayerParams* release_loadconstantnd();
  void set_allocated_loadconstantnd(::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd);

  // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
  bool has_filllike() const;
  void clear_filllike();
  static const int kFillLikeFieldNumber = 1080;
  const ::CoreML::Specification::FillLikeLayerParams& filllike() const;
  ::CoreML::Specification::FillLikeLayerParams* mutable_filllike();
  ::CoreML::Specification::FillLikeLayerParams* release_filllike();
  void set_allocated_filllike(::CoreML::Specification::FillLikeLayerParams* filllike);

  // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
  bool has_fillstatic() const;
  void clear_fillstatic();
  static const int kFillStaticFieldNumber = 1085;
  const ::CoreML::Specification::FillStaticLayerParams& fillstatic() const;
  ::CoreML::Specification::FillStaticLayerParams* mutable_fillstatic();
  ::CoreML::Specification::FillStaticLayerParams* release_fillstatic();
  void set_allocated_fillstatic(::CoreML::Specification::FillStaticLayerParams* fillstatic);

  // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
  bool has_filldynamic() const;
  void clear_filldynamic();
  static const int kFillDynamicFieldNumber = 1090;
  const ::CoreML::Specification::FillDynamicLayerParams& filldynamic() const;
  ::CoreML::Specification::FillDynamicLayerParams* mutable_filldynamic();
  ::CoreML::Specification::FillDynamicLayerParams* release_filldynamic();
  void set_allocated_filldynamic(::CoreML::Specification::FillDynamicLayerParams* filldynamic);

  // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
  bool has_broadcasttolike() const;
  void clear_broadcasttolike();
  static const int kBroadcastToLikeFieldNumber = 1100;
  const ::CoreML::Specification::BroadcastToLikeLayerParams& broadcasttolike() const;
  ::CoreML::Specification::BroadcastToLikeLayerParams* mutable_broadcasttolike();
  ::CoreML::Specification::BroadcastToLikeLayerParams* release_broadcasttolike();
  void set_allocated_broadcasttolike(::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike);

  // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
  bool has_broadcasttostatic() const;
  void clear_broadcasttostatic();
  static const int kBroadcastToStaticFieldNumber = 1105;
  const ::CoreML::Specification::BroadcastToStaticLayerParams& broadcasttostatic() const;
  ::CoreML::Specification::BroadcastToStaticLayerParams* mutable_broadcasttostatic();
  ::CoreML::Specification::BroadcastToStaticLayerParams* release_broadcasttostatic();
  void set_allocated_broadcasttostatic(::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic);

  // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
  bool has_broadcasttodynamic() const;
  void clear_broadcasttodynamic();
  static const int kBroadcastToDynamicFieldNumber = 1110;
  const ::CoreML::Specification::BroadcastToDynamicLayerParams& broadcasttodynamic() const;
  ::CoreML::Specification::BroadcastToDynamicLayerParams* mutable_broadcasttodynamic();
  ::CoreML::Specification::BroadcastToDynamicLayerParams* release_broadcasttodynamic();
  void set_allocated_broadcasttodynamic(::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic);

  // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
  bool has_squeeze() const;
  void clear_squeeze();
  static const int kSqueezeFieldNumber = 1120;
  const ::CoreML::Specification::SqueezeLayerParams& squeeze() const;
  ::CoreML::Specification::SqueezeLayerParams* mutable_squeeze();
  ::CoreML::Specification::SqueezeLayerParams* release_squeeze();
  void set_allocated_squeeze(::CoreML::Specification::SqueezeLayerParams* squeeze);

  // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
  bool has_expanddims() const;
  void clear_expanddims();
  static const int kExpandDimsFieldNumber = 1125;
  const ::CoreML::Specification::ExpandDimsLayerParams& expanddims() const;
  ::CoreML::Specification::ExpandDimsLayerParams* mutable_expanddims();
  ::CoreML::Specification::ExpandDimsLayerParams* release_expanddims();
  void set_allocated_expanddims(::CoreML::Specification::ExpandDimsLayerParams* expanddims);

  // .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
  bool has_flattento2d() const;
  void clear_flattento2d();
  static const int kFlattenTo2DFieldNumber = 1130;
  const ::CoreML::Specification::FlattenTo2DLayerParams& flattento2d() const;
  ::CoreML::Specification::FlattenTo2DLayerParams* mutable_flattento2d();
  ::CoreML::Specification::FlattenTo2DLayerParams* release_flattento2d();
  void set_allocated_flattento2d(::CoreML::Specification::FlattenTo2DLayerParams* flattento2d);

  // .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
  bool has_reshapelike() const;
  void clear_reshapelike();
  static const int kReshapeLikeFieldNumber = 1135;
  const ::CoreML::Specification::ReshapeLikeLayerParams& reshapelike() const;
  ::CoreML::Specification::ReshapeLikeLayerParams* mutable_reshapelike();
  ::CoreML::Specification::ReshapeLikeLayerParams* release_reshapelike();
  void set_allocated_reshapelike(::CoreML::Specification::ReshapeLikeLayerParams* reshapelike);

  // .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
  bool has_reshapestatic() const;
  void clear_reshapestatic();
  static const int kReshapeStaticFieldNumber = 1140;
  const ::CoreML::Specification::ReshapeStaticLayerParams& reshapestatic() const;
  ::CoreML::Specification::ReshapeStaticLayerParams* mutable_reshapestatic();
  ::CoreML::Specification::ReshapeStaticLayerParams* release_reshapestatic();
  void set_allocated_reshapestatic(::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic);

  // .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
  bool has_reshapedynamic() const;
  void clear_reshapedynamic();
  static const int kReshapeDynamicFieldNumber = 1145;
  const ::CoreML::Specification::ReshapeDynamicLayerParams& reshapedynamic() const;
  ::CoreML::Specification::ReshapeDynamicLayerParams* mutable_reshapedynamic();
  ::CoreML::Specification::ReshapeDynamicLayerParams* release_reshapedynamic();
  void set_allocated_reshapedynamic(::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic);

  // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
  bool has_rankpreservingreshape() const;
  void clear_rankpreservingreshape();
  static const int kRankPreservingReshapeFieldNumber = 1150;
  const ::CoreML::Specification::RankPreservingReshapeLayerParams& rankpreservingreshape() const;
  ::CoreML::Specification::RankPreservingReshapeLayerParams* mutable_rankpreservingreshape();
  ::CoreML::Specification::RankPreservingReshapeLayerParams* release_rankpreservingreshape();
  void set_allocated_rankpreservingreshape(::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape);

  // .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
  bool has_constantpad() const;
  void clear_constantpad();
  static const int kConstantPadFieldNumber = 1155;
  const ::CoreML::Specification::ConstantPaddingLayerParams& constantpad() const;
  ::CoreML::Specification::ConstantPaddingLayerParams* mutable_constantpad();
  ::CoreML::Specification::ConstantPaddingLayerParams* release_constantpad();
  void set_allocated_constantpad(::CoreML::Specification::ConstantPaddingLayerParams* constantpad);

  // .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
  bool has_randomnormallike() const;
  void clear_randomnormallike();
  static const int kRandomNormalLikeFieldNumber = 1170;
  const ::CoreML::Specification::RandomNormalLikeLayerParams& randomnormallike() const;
  ::CoreML::Specification::RandomNormalLikeLayerParams* mutable_randomnormallike();
  ::CoreML::Specification::RandomNormalLikeLayerParams* release_randomnormallike();
  void set_allocated_randomnormallike(::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike);

  // .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
  bool has_randomnormalstatic() const;
  void clear_randomnormalstatic();
  static const int kRandomNormalStaticFieldNumber = 1175;
  const ::CoreML::Specification::RandomNormalStaticLayerParams& randomnormalstatic() const;
  ::CoreML::Specification::RandomNormalStaticLayerParams* mutable_randomnormalstatic();
  ::CoreML::Specification::RandomNormalStaticLayerParams* release_randomnormalstatic();
  void set_allocated_randomnormalstatic(::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic);

  // .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
  bool has_randomnormaldynamic() const;
  void clear_randomnormaldynamic();
  static const int kRandomNormalDynamicFieldNumber = 1180;
  const ::CoreML::Specification::RandomNormalDynamicLayerParams& randomnormaldynamic() const;
  ::CoreML::Specification::RandomNormalDynamicLayerParams* mutable_randomnormaldynamic();
  ::CoreML::Specification::RandomNormalDynamicLayerParams* release_randomnormaldynamic();
  void set_allocated_randomnormaldynamic(::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic);

  // .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
  bool has_randomuniformlike() const;
  void clear_randomuniformlike();
  static const int kRandomUniformLikeFieldNumber = 1190;
  const ::CoreML::Specification::RandomUniformLikeLayerParams& randomuniformlike() const;
  ::CoreML::Specification::RandomUniformLikeLayerParams* mutable_randomuniformlike();
  ::CoreML::Specification::RandomUniformLikeLayerParams* release_randomuniformlike();
  void set_allocated_randomuniformlike(::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike);

  // .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
  bool has_randomuniformstatic() const;
  void clear_randomuniformstatic();
  static const int kRandomUniformStaticFieldNumber = 1195;
  const ::CoreML::Specification::RandomUniformStaticLayerParams& randomuniformstatic() const;
  ::CoreML::Specification::RandomUniformStaticLayerParams* mutable_randomuniformstatic();
  ::CoreML::Specification::RandomUniformStaticLayerParams* release_randomuniformstatic();
  void set_allocated_randomuniformstatic(::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic);

  // .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
  bool has_randomuniformdynamic() const;
  void clear_randomuniformdynamic();
  static const int kRandomUniformDynamicFieldNumber = 1200;
  const ::CoreML::Specification::RandomUniformDynamicLayerParams& randomuniformdynamic() const;
  ::CoreML::Specification::RandomUniformDynamicLayerParams* mutable_randomuniformdynamic();
  ::CoreML::Specification::RandomUniformDynamicLayerParams* release_randomuniformdynamic();
  void set_allocated_randomuniformdynamic(::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic);

  // .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
  bool has_randombernoullilike() const;
  void clear_randombernoullilike();
  static const int kRandomBernoulliLikeFieldNumber = 1210;
  const ::CoreML::Specification::RandomBernoulliLikeLayerParams& randombernoullilike() const;
  ::CoreML::Specification::RandomBernoulliLikeLayerParams* mutable_randombernoullilike();
  ::CoreML::Specification::RandomBernoulliLikeLayerParams* release_randombernoullilike();
  void set_allocated_randombernoullilike(::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike);

  // .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
  bool has_randombernoullistatic() const;
  void clear_randombernoullistatic();
  static const int kRandomBernoulliStaticFieldNumber = 1215;
  const ::CoreML::Specification::RandomBernoulliStaticLayerParams& randombernoullistatic() const;
  ::CoreML::Specification::RandomBernoulliStaticLayerParams* mutable_randombernoullistatic();
  ::CoreML::Specification::RandomBernoulliStaticLayerParams* release_randombernoullistatic();
  void set_allocated_randombernoullistatic(::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic);

  // .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
  bool has_randombernoullidynamic() const;
  void clear_randombernoullidynamic();
  static const int kRandomBernoulliDynamicFieldNumber = 1220;
  const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& randombernoullidynamic() const;
  ::CoreML::Specification::RandomBernoulliDynamicLayerParams* mutable_randombernoullidynamic();
  ::CoreML::Specification::RandomBernoulliDynamicLayerParams* release_randombernoullidynamic();
  void set_allocated_randombernoullidynamic(::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic);

  // .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
  bool has_categoricaldistribution() const;
  void clear_categoricaldistribution();
  static const int kCategoricalDistributionFieldNumber = 1230;
  const ::CoreML::Specification::CategoricalDistributionLayerParams& categoricaldistribution() const;
  ::CoreML::Specification::CategoricalDistributionLayerParams* mutable_categoricaldistribution();
  ::CoreML::Specification::CategoricalDistributionLayerParams* release_categoricaldistribution();
  void set_allocated_categoricaldistribution(::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution);

  // .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
  bool has_reducel1() const;
  void clear_reducel1();
  static const int kReduceL1FieldNumber = 1250;
  const ::CoreML::Specification::ReduceL1LayerParams& reducel1() const;
  ::CoreML::Specification::ReduceL1LayerParams* mutable_reducel1();
  ::CoreML::Specification::ReduceL1LayerParams* release_reducel1();
  void set_allocated_reducel1(::CoreML::Specification::ReduceL1LayerParams* reducel1);

  // .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
  bool has_reducel2() const;
  void clear_reducel2();
  static const int kReduceL2FieldNumber = 1255;
  const ::CoreML::Specification::ReduceL2LayerParams& reducel2() const;
  ::CoreML::Specification::ReduceL2LayerParams* mutable_reducel2();
  ::CoreML::Specification::ReduceL2LayerParams* release_reducel2();
  void set_allocated_reducel2(::CoreML::Specification::ReduceL2LayerParams* reducel2);

  // .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
  bool has_reducemax() const;
  void clear_reducemax();
  static const int kReduceMaxFieldNumber = 1260;
  const ::CoreML::Specification::ReduceMaxLayerParams& reducemax() const;
  ::CoreML::Specification::ReduceMaxLayerParams* mutable_reducemax();
  ::CoreML::Specification::ReduceMaxLayerParams* release_reducemax();
  void set_allocated_reducemax(::CoreML::Specification::ReduceMaxLayerParams* reducemax);

  // .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
  bool has_reducemin() const;
  void clear_reducemin();
  static const int kReduceMinFieldNumber = 1265;
  const ::CoreML::Specification::ReduceMinLayerParams& reducemin() const;
  ::CoreML::Specification::ReduceMinLayerParams* mutable_reducemin();
  ::CoreML::Specification::ReduceMinLayerParams* release_reducemin();
  void set_allocated_reducemin(::CoreML::Specification::ReduceMinLayerParams* reducemin);

  // .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
  bool has_reducesum() const;
  void clear_reducesum();
  static const int kReduceSumFieldNumber = 1270;
  const ::CoreML::Specification::ReduceSumLayerParams& reducesum() const;
  ::CoreML::Specification::ReduceSumLayerParams* mutable_reducesum();
  ::CoreML::Specification::ReduceSumLayerParams* release_reducesum();
  void set_allocated_reducesum(::CoreML::Specification::ReduceSumLayerParams* reducesum);

  // .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
  bool has_reduceprod() const;
  void clear_reduceprod();
  static const int kReduceProdFieldNumber = 1275;
  const ::CoreML::Specification::ReduceProdLayerParams& reduceprod() const;
  ::CoreML::Specification::ReduceProdLayerParams* mutable_reduceprod();
  ::CoreML::Specification::ReduceProdLayerParams* release_reduceprod();
  void set_allocated_reduceprod(::CoreML::Specification::ReduceProdLayerParams* reduceprod);

  // .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
  bool has_reducemean() const;
  void clear_reducemean();
  static const int kReduceMeanFieldNumber = 1280;
  const ::CoreML::Specification::ReduceMeanLayerParams& reducemean() const;
  ::CoreML::Specification::ReduceMeanLayerParams* mutable_reducemean();
  ::CoreML::Specification::ReduceMeanLayerParams* release_reducemean();
  void set_allocated_reducemean(::CoreML::Specification::ReduceMeanLayerParams* reducemean);

  // .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
  bool has_reducelogsum() const;
  void clear_reducelogsum();
  static const int kReduceLogSumFieldNumber = 1285;
  const ::CoreML::Specification::ReduceLogSumLayerParams& reducelogsum() const;
  ::CoreML::Specification::ReduceLogSumLayerParams* mutable_reducelogsum();
  ::CoreML::Specification::ReduceLogSumLayerParams* release_reducelogsum();
  void set_allocated_reducelogsum(::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum);

  // .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
  bool has_reducesumsquare() const;
  void clear_reducesumsquare();
  static const int kReduceSumSquareFieldNumber = 1290;
  const ::CoreML::Specification::ReduceSumSquareLayerParams& reducesumsquare() const;
  ::CoreML::Specification::ReduceSumSquareLayerParams* mutable_reducesumsquare();
  ::CoreML::Specification::ReduceSumSquareLayerParams* release_reducesumsquare();
  void set_allocated_reducesumsquare(::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare);

  // .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
  bool has_reducelogsumexp() const;
  void clear_reducelogsumexp();
  static const int kReduceLogSumExpFieldNumber = 1295;
  const ::CoreML::Specification::ReduceLogSumExpLayerParams& reducelogsumexp() const;
  ::CoreML::Specification::ReduceLogSumExpLayerParams* mutable_reducelogsumexp();
  ::CoreML::Specification::ReduceLogSumExpLayerParams* release_reducelogsumexp();
  void set_allocated_reducelogsumexp(::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp);

  // .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
  bool has_wherenonzero() const;
  void clear_wherenonzero();
  static const int kWhereNonZeroFieldNumber = 1313;
  const ::CoreML::Specification::WhereNonZeroLayerParams& wherenonzero() const;
  ::CoreML::Specification::WhereNonZeroLayerParams* mutable_wherenonzero();
  ::CoreML::Specification::WhereNonZeroLayerParams* release_wherenonzero();
  void set_allocated_wherenonzero(::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero);

  // .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
  bool has_matrixbandpart() const;
  void clear_matrixbandpart();
  static const int kMatrixBandPartFieldNumber = 1315;
  const ::CoreML::Specification::MatrixBandPartLayerParams& matrixbandpart() const;
  ::CoreML::Specification::MatrixBandPartLayerParams* mutable_matrixbandpart();
  ::CoreML::Specification::MatrixBandPartLayerParams* release_matrixbandpart();
  void set_allocated_matrixbandpart(::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart);

  // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
  bool has_lowertriangular() const;
  void clear_lowertriangular();
  static const int kLowerTriangularFieldNumber = 1320;
  const ::CoreML::Specification::LowerTriangularLayerParams& lowertriangular() const;
  ::CoreML::Specification::LowerTriangularLayerParams* mutable_lowertriangular();
  ::CoreML::Specification::LowerTriangularLayerParams* release_lowertriangular();
  void set_allocated_lowertriangular(::CoreML::Specification::LowerTriangularLayerParams* lowertriangular);

  // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
  bool has_uppertriangular() const;
  void clear_uppertriangular();
  static const int kUpperTriangularFieldNumber = 1325;
  const ::CoreML::Specification::UpperTriangularLayerParams& uppertriangular() const;
  ::CoreML::Specification::UpperTriangularLayerParams* mutable_uppertriangular();
  ::CoreML::Specification::UpperTriangularLayerParams* release_uppertriangular();
  void set_allocated_uppertriangular(::CoreML::Specification::UpperTriangularLayerParams* uppertriangular);

  // .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
  bool has_wherebroadcastable() const;
  void clear_wherebroadcastable();
  static const int kWhereBroadcastableFieldNumber = 1330;
  const ::CoreML::Specification::WhereBroadcastableLayerParams& wherebroadcastable() const;
  ::CoreML::Specification::WhereBroadcastableLayerParams* mutable_wherebroadcastable();
  ::CoreML::Specification::WhereBroadcastableLayerParams* release_wherebroadcastable();
  void set_allocated_wherebroadcastable(::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable);

  // .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
  bool has_layernormalization() const;
  void clear_layernormalization();
  static const int kLayerNormalizationFieldNumber = 1350;
  const ::CoreML::Specification::LayerNormalizationLayerParams& layernormalization() const;
  ::CoreML::Specification::LayerNormalizationLayerParams* mutable_layernormalization();
  ::CoreML::Specification::LayerNormalizationLayerParams* release_layernormalization();
  void set_allocated_layernormalization(::CoreML::Specification::LayerNormalizationLayerParams* layernormalization);

  // .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
  bool has_nonmaximumsuppression() const;
  void clear_nonmaximumsuppression();
  static const int kNonMaximumSuppressionFieldNumber = 1400;
  const ::CoreML::Specification::NonMaximumSuppressionLayerParams& nonmaximumsuppression() const;
  ::CoreML::Specification::NonMaximumSuppressionLayerParams* mutable_nonmaximumsuppression();
  ::CoreML::Specification::NonMaximumSuppressionLayerParams* release_nonmaximumsuppression();
  void set_allocated_nonmaximumsuppression(::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression);

  LayerCase layer_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkLayer)
 private:
  void set_has_convolution();
  void set_has_pooling();
  void set_has_activation();
  void set_has_innerproduct();
  void set_has_embedding();
  void set_has_batchnorm();
  void set_has_mvn();
  void set_has_l2normalize();
  void set_has_softmax();
  void set_has_lrn();
  void set_has_crop();
  void set_has_padding();
  void set_has_upsample();
  void set_has_resizebilinear();
  void set_has_cropresize();
  void set_has_unary();
  void set_has_add();
  void set_has_multiply();
  void set_has_average();
  void set_has_scale();
  void set_has_bias();
  void set_has_max();
  void set_has_min();
  void set_has_dot();
  void set_has_reduce();
  void set_has_loadconstant();
  void set_has_reshape();
  void set_has_flatten();
  void set_has_permute();
  void set_has_concat();
  void set_has_split();
  void set_has_sequencerepeat();
  void set_has_reorganizedata();
  void set_has_slice();
  void set_has_simplerecurrent();
  void set_has_gru();
  void set_has_unidirectionallstm();
  void set_has_bidirectionallstm();
  void set_has_custom();
  void set_has_copy();
  void set_has_branch();
  void set_has_loop();
  void set_has_loopbreak();
  void set_has_loopcontinue();
  void set_has_rangestatic();
  void set_has_rangedynamic();
  void set_has_clip();
  void set_has_ceil();
  void set_has_floor();
  void set_has_sign();
  void set_has_round();
  void set_has_exp2();
  void set_has_sin();
  void set_has_cos();
  void set_has_tan();
  void set_has_asin();
  void set_has_acos();
  void set_has_atan();
  void set_has_sinh();
  void set_has_cosh();
  void set_has_tanh();
  void set_has_asinh();
  void set_has_acosh();
  void set_has_atanh();
  void set_has_erf();
  void set_has_gelu();
  void set_has_equal();
  void set_has_notequal();
  void set_has_lessthan();
  void set_has_lessequal();
  void set_has_greaterthan();
  void set_has_greaterequal();
  void set_has_logicalor();
  void set_has_logicalxor();
  void set_has_logicalnot();
  void set_has_logicaland();
  void set_has_modbroadcastable();
  void set_has_minbroadcastable();
  void set_has_maxbroadcastable();
  void set_has_addbroadcastable();
  void set_has_powbroadcastable();
  void set_has_dividebroadcastable();
  void set_has_floordivbroadcastable();
  void set_has_multiplybroadcastable();
  void set_has_subtractbroadcastable();
  void set_has_tile();
  void set_has_stack();
  void set_has_gather();
  void set_has_scatter();
  void set_has_gathernd();
  void set_has_scatternd();
  void set_has_softmaxnd();
  void set_has_gatheralongaxis();
  void set_has_scatteralongaxis();
  void set_has_reverse();
  void set_has_reverseseq();
  void set_has_splitnd();
  void set_has_concatnd();
  void set_has_transpose();
  void set_has_slicestatic();
  void set_has_slicedynamic();
  void set_has_slidingwindows();
  void set_has_topk();
  void set_has_argmin();
  void set_has_argmax();
  void set_has_embeddingnd();
  void set_has_batchedmatmul();
  void set_has_getshape();
  void set_has_loadconstantnd();
  void set_has_filllike();
  void set_has_fillstatic();
  void set_has_filldynamic();
  void set_has_broadcasttolike();
  void set_has_broadcasttostatic();
  void set_has_broadcasttodynamic();
  void set_has_squeeze();
  void set_has_expanddims();
  void set_has_flattento2d();
  void set_has_reshapelike();
  void set_has_reshapestatic();
  void set_has_reshapedynamic();
  void set_has_rankpreservingreshape();
  void set_has_constantpad();
  void set_has_randomnormallike();
  void set_has_randomnormalstatic();
  void set_has_randomnormaldynamic();
  void set_has_randomuniformlike();
  void set_has_randomuniformstatic();
  void set_has_randomuniformdynamic();
  void set_has_randombernoullilike();
  void set_has_randombernoullistatic();
  void set_has_randombernoullidynamic();
  void set_has_categoricaldistribution();
  void set_has_reducel1();
  void set_has_reducel2();
  void set_has_reducemax();
  void set_has_reducemin();
  void set_has_reducesum();
  void set_has_reduceprod();
  void set_has_reducemean();
  void set_has_reducelogsum();
  void set_has_reducesumsquare();
  void set_has_reducelogsumexp();
  void set_has_wherenonzero();
  void set_has_matrixbandpart();
  void set_has_lowertriangular();
  void set_has_uppertriangular();
  void set_has_wherebroadcastable();
  void set_has_layernormalization();
  void set_has_nonmaximumsuppression();

  inline bool has_layer() const;
  void clear_layer();
  inline void clear_has_layer();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::std::string> input_;
  ::google::protobuf::RepeatedPtrField< ::std::string> output_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor > inputtensor_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor > outputtensor_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  bool isupdatable_;
  union LayerUnion {
    LayerUnion() {}
    ::CoreML::Specification::ConvolutionLayerParams* convolution_;
    ::CoreML::Specification::PoolingLayerParams* pooling_;
    ::CoreML::Specification::ActivationParams* activation_;
    ::CoreML::Specification::InnerProductLayerParams* innerproduct_;
    ::CoreML::Specification::EmbeddingLayerParams* embedding_;
    ::CoreML::Specification::BatchnormLayerParams* batchnorm_;
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn_;
    ::CoreML::Specification::L2NormalizeLayerParams* l2normalize_;
    ::CoreML::Specification::SoftmaxLayerParams* softmax_;
    ::CoreML::Specification::LRNLayerParams* lrn_;
    ::CoreML::Specification::CropLayerParams* crop_;
    ::CoreML::Specification::PaddingLayerParams* padding_;
    ::CoreML::Specification::UpsampleLayerParams* upsample_;
    ::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear_;
    ::CoreML::Specification::CropResizeLayerParams* cropresize_;
    ::CoreML::Specification::UnaryFunctionLayerParams* unary_;
    ::CoreML::Specification::AddLayerParams* add_;
    ::CoreML::Specification::MultiplyLayerParams* multiply_;
    ::CoreML::Specification::AverageLayerParams* average_;
    ::CoreML::Specification::ScaleLayerParams* scale_;
    ::CoreML::Specification::BiasLayerParams* bias_;
    ::CoreML::Specification::MaxLayerParams* max_;
    ::CoreML::Specification::MinLayerParams* min_;
    ::CoreML::Specification::DotProductLayerParams* dot_;
    ::CoreML::Specification::ReduceLayerParams* reduce_;
    ::CoreML::Specification::LoadConstantLayerParams* loadconstant_;
    ::CoreML::Specification::ReshapeLayerParams* reshape_;
    ::CoreML::Specification::FlattenLayerParams* flatten_;
    ::CoreML::Specification::PermuteLayerParams* permute_;
    ::CoreML::Specification::ConcatLayerParams* concat_;
    ::CoreML::Specification::SplitLayerParams* split_;
    ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat_;
    ::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata_;
    ::CoreML::Specification::SliceLayerParams* slice_;
    ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent_;
    ::CoreML::Specification::GRULayerParams* gru_;
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm_;
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm_;
    ::CoreML::Specification::CustomLayerParams* custom_;
    ::CoreML::Specification::CopyLayerParams* copy_;
    ::CoreML::Specification::BranchLayerParams* branch_;
    ::CoreML::Specification::LoopLayerParams* loop_;
    ::CoreML::Specification::LoopBreakLayerParams* loopbreak_;
    ::CoreML::Specification::LoopContinueLayerParams* loopcontinue_;
    ::CoreML::Specification::RangeStaticLayerParams* rangestatic_;
    ::CoreML::Specification::RangeDynamicLayerParams* rangedynamic_;
    ::CoreML::Specification::ClipLayerParams* clip_;
    ::CoreML::Specification::CeilLayerParams* ceil_;
    ::CoreML::Specification::FloorLayerParams* floor_;
    ::CoreML::Specification::SignLayerParams* sign_;
    ::CoreML::Specification::RoundLayerParams* round_;
    ::CoreML::Specification::Exp2LayerParams* exp2_;
    ::CoreML::Specification::SinLayerParams* sin_;
    ::CoreML::Specification::CosLayerParams* cos_;
    ::CoreML::Specification::TanLayerParams* tan_;
    ::CoreML::Specification::AsinLayerParams* asin_;
    ::CoreML::Specification::AcosLayerParams* acos_;
    ::CoreML::Specification::AtanLayerParams* atan_;
    ::CoreML::Specification::SinhLayerParams* sinh_;
    ::CoreML::Specification::CoshLayerParams* cosh_;
    ::CoreML::Specification::TanhLayerParams* tanh_;
    ::CoreML::Specification::AsinhLayerParams* asinh_;
    ::CoreML::Specification::AcoshLayerParams* acosh_;
    ::CoreML::Specification::AtanhLayerParams* atanh_;
    ::CoreML::Specification::ErfLayerParams* erf_;
    ::CoreML::Specification::GeluLayerParams* gelu_;
    ::CoreML::Specification::EqualLayerParams* equal_;
    ::CoreML::Specification::NotEqualLayerParams* notequal_;
    ::CoreML::Specification::LessThanLayerParams* lessthan_;
    ::CoreML::Specification::LessEqualLayerParams* lessequal_;
    ::CoreML::Specification::GreaterThanLayerParams* greaterthan_;
    ::CoreML::Specification::GreaterEqualLayerParams* greaterequal_;
    ::CoreML::Specification::LogicalOrLayerParams* logicalor_;
    ::CoreML::Specification::LogicalXorLayerParams* logicalxor_;
    ::CoreML::Specification::LogicalNotLayerParams* logicalnot_;
    ::CoreML::Specification::LogicalAndLayerParams* logicaland_;
    ::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable_;
    ::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable_;
    ::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable_;
    ::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable_;
    ::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable_;
    ::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable_;
    ::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable_;
    ::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable_;
    ::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable_;
    ::CoreML::Specification::TileLayerParams* tile_;
    ::CoreML::Specification::StackLayerParams* stack_;
    ::CoreML::Specification::GatherLayerParams* gather_;
    ::CoreML::Specification::ScatterLayerParams* scatter_;
    ::CoreML::Specification::GatherNDLayerParams* gathernd_;
    ::CoreML::Specification::ScatterNDLayerParams* scatternd_;
    ::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd_;
    ::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis_;
    ::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis_;
    ::CoreML::Specification::ReverseLayerParams* reverse_;
    ::CoreML::Specification::ReverseSeqLayerParams* reverseseq_;
    ::CoreML::Specification::SplitNDLayerParams* splitnd_;
    ::CoreML::Specification::ConcatNDLayerParams* concatnd_;
    ::CoreML::Specification::TransposeLayerParams* transpose_;
    ::CoreML::Specification::SliceStaticLayerParams* slicestatic_;
    ::CoreML::Specification::SliceDynamicLayerParams* slicedynamic_;
    ::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows_;
    ::CoreML::Specification::TopKLayerParams* topk_;
    ::CoreML::Specification::ArgMinLayerParams* argmin_;
    ::CoreML::Specification::ArgMaxLayerParams* argmax_;
    ::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd_;
    ::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul_;
    ::CoreML::Specification::GetShapeLayerParams* getshape_;
    ::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd_;
    ::CoreML::Specification::FillLikeLayerParams* filllike_;
    ::CoreML::Specification::FillStaticLayerParams* fillstatic_;
    ::CoreML::Specification::FillDynamicLayerParams* filldynamic_;
    ::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike_;
    ::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic_;
    ::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic_;
    ::CoreML::Specification::SqueezeLayerParams* squeeze_;
    ::CoreML::Specification::ExpandDimsLayerParams* expanddims_;
    ::CoreML::Specification::FlattenTo2DLayerParams* flattento2d_;
    ::CoreML::Specification::ReshapeLikeLayerParams* reshapelike_;
    ::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic_;
    ::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic_;
    ::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape_;
    ::CoreML::Specification::ConstantPaddingLayerParams* constantpad_;
    ::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike_;
    ::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic_;
    ::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic_;
    ::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike_;
    ::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic_;
    ::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic_;
    ::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike_;
    ::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic_;
    ::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic_;
    ::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution_;
    ::CoreML::Specification::ReduceL1LayerParams* reducel1_;
    ::CoreML::Specification::ReduceL2LayerParams* reducel2_;
    ::CoreML::Specification::ReduceMaxLayerParams* reducemax_;
    ::CoreML::Specification::ReduceMinLayerParams* reducemin_;
    ::CoreML::Specification::ReduceSumLayerParams* reducesum_;
    ::CoreML::Specification::ReduceProdLayerParams* reduceprod_;
    ::CoreML::Specification::ReduceMeanLayerParams* reducemean_;
    ::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum_;
    ::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare_;
    ::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp_;
    ::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero_;
    ::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart_;
    ::CoreML::Specification::LowerTriangularLayerParams* lowertriangular_;
    ::CoreML::Specification::UpperTriangularLayerParams* uppertriangular_;
    ::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable_;
    ::CoreML::Specification::LayerNormalizationLayerParams* layernormalization_;
    ::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression_;
  } layer_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BranchLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BranchLayerParams) */ {
 public:
  BranchLayerParams();
  virtual ~BranchLayerParams();

  BranchLayerParams(const BranchLayerParams& from);

  inline BranchLayerParams& operator=(const BranchLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BranchLayerParams& default_instance();

  static inline const BranchLayerParams* internal_default_instance() {
    return reinterpret_cast<const BranchLayerParams*>(
               &_BranchLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    20;

  void Swap(BranchLayerParams* other);

  // implements Message ----------------------------------------------

  inline BranchLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BranchLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BranchLayerParams& from);
  void MergeFrom(const BranchLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BranchLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.NeuralNetwork ifBranch = 1;
  bool has_ifbranch() const;
  void clear_ifbranch();
  static const int kIfBranchFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetwork& ifbranch() const;
  ::CoreML::Specification::NeuralNetwork* mutable_ifbranch();
  ::CoreML::Specification::NeuralNetwork* release_ifbranch();
  void set_allocated_ifbranch(::CoreML::Specification::NeuralNetwork* ifbranch);

  // .CoreML.Specification.NeuralNetwork elseBranch = 2;
  bool has_elsebranch() const;
  void clear_elsebranch();
  static const int kElseBranchFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetwork& elsebranch() const;
  ::CoreML::Specification::NeuralNetwork* mutable_elsebranch();
  ::CoreML::Specification::NeuralNetwork* release_elsebranch();
  void set_allocated_elsebranch(::CoreML::Specification::NeuralNetwork* elsebranch);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BranchLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::NeuralNetwork* ifbranch_;
  ::CoreML::Specification::NeuralNetwork* elsebranch_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LoopLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoopLayerParams) */ {
 public:
  LoopLayerParams();
  virtual ~LoopLayerParams();

  LoopLayerParams(const LoopLayerParams& from);

  inline LoopLayerParams& operator=(const LoopLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LoopLayerParams& default_instance();

  static inline const LoopLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoopLayerParams*>(
               &_LoopLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    21;

  void Swap(LoopLayerParams* other);

  // implements Message ----------------------------------------------

  inline LoopLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LoopLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LoopLayerParams& from);
  void MergeFrom(const LoopLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoopLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string conditionVar = 2;
  void clear_conditionvar();
  static const int kConditionVarFieldNumber = 2;
  const ::std::string& conditionvar() const;
  void set_conditionvar(const ::std::string& value);
  #if LANG_CXX11
  void set_conditionvar(::std::string&& value);
  #endif
  void set_conditionvar(const char* value);
  void set_conditionvar(const char* value, size_t size);
  ::std::string* mutable_conditionvar();
  ::std::string* release_conditionvar();
  void set_allocated_conditionvar(::std::string* conditionvar);

  // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
  bool has_conditionnetwork() const;
  void clear_conditionnetwork();
  static const int kConditionNetworkFieldNumber = 3;
  const ::CoreML::Specification::NeuralNetwork& conditionnetwork() const;
  ::CoreML::Specification::NeuralNetwork* mutable_conditionnetwork();
  ::CoreML::Specification::NeuralNetwork* release_conditionnetwork();
  void set_allocated_conditionnetwork(::CoreML::Specification::NeuralNetwork* conditionnetwork);

  // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
  bool has_bodynetwork() const;
  void clear_bodynetwork();
  static const int kBodyNetworkFieldNumber = 4;
  const ::CoreML::Specification::NeuralNetwork& bodynetwork() const;
  ::CoreML::Specification::NeuralNetwork* mutable_bodynetwork();
  ::CoreML::Specification::NeuralNetwork* release_bodynetwork();
  void set_allocated_bodynetwork(::CoreML::Specification::NeuralNetwork* bodynetwork);

  // uint64 maxLoopIterations = 1;
  void clear_maxloopiterations();
  static const int kMaxLoopIterationsFieldNumber = 1;
  ::google::protobuf::uint64 maxloopiterations() const;
  void set_maxloopiterations(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoopLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr conditionvar_;
  ::CoreML::Specification::NeuralNetwork* conditionnetwork_;
  ::CoreML::Specification::NeuralNetwork* bodynetwork_;
  ::google::protobuf::uint64 maxloopiterations_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LoopBreakLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoopBreakLayerParams) */ {
 public:
  LoopBreakLayerParams();
  virtual ~LoopBreakLayerParams();

  LoopBreakLayerParams(const LoopBreakLayerParams& from);

  inline LoopBreakLayerParams& operator=(const LoopBreakLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LoopBreakLayerParams& default_instance();

  static inline const LoopBreakLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoopBreakLayerParams*>(
               &_LoopBreakLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    22;

  void Swap(LoopBreakLayerParams* other);

  // implements Message ----------------------------------------------

  inline LoopBreakLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LoopBreakLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LoopBreakLayerParams& from);
  void MergeFrom(const LoopBreakLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoopBreakLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoopBreakLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LoopContinueLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoopContinueLayerParams) */ {
 public:
  LoopContinueLayerParams();
  virtual ~LoopContinueLayerParams();

  LoopContinueLayerParams(const LoopContinueLayerParams& from);

  inline LoopContinueLayerParams& operator=(const LoopContinueLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LoopContinueLayerParams& default_instance();

  static inline const LoopContinueLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoopContinueLayerParams*>(
               &_LoopContinueLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    23;

  void Swap(LoopContinueLayerParams* other);

  // implements Message ----------------------------------------------

  inline LoopContinueLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LoopContinueLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LoopContinueLayerParams& from);
  void MergeFrom(const LoopContinueLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoopContinueLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoopContinueLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CopyLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CopyLayerParams) */ {
 public:
  CopyLayerParams();
  virtual ~CopyLayerParams();

  CopyLayerParams(const CopyLayerParams& from);

  inline CopyLayerParams& operator=(const CopyLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CopyLayerParams& default_instance();

  static inline const CopyLayerParams* internal_default_instance() {
    return reinterpret_cast<const CopyLayerParams*>(
               &_CopyLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    24;

  void Swap(CopyLayerParams* other);

  // implements Message ----------------------------------------------

  inline CopyLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CopyLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CopyLayerParams& from);
  void MergeFrom(const CopyLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CopyLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CopyLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GreaterThanLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GreaterThanLayerParams) */ {
 public:
  GreaterThanLayerParams();
  virtual ~GreaterThanLayerParams();

  GreaterThanLayerParams(const GreaterThanLayerParams& from);

  inline GreaterThanLayerParams& operator=(const GreaterThanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GreaterThanLayerParams& default_instance();

  static inline const GreaterThanLayerParams* internal_default_instance() {
    return reinterpret_cast<const GreaterThanLayerParams*>(
               &_GreaterThanLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    25;

  void Swap(GreaterThanLayerParams* other);

  // implements Message ----------------------------------------------

  inline GreaterThanLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GreaterThanLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GreaterThanLayerParams& from);
  void MergeFrom(const GreaterThanLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GreaterThanLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 2;
  void clear_alpha();
  static const int kAlphaFieldNumber = 2;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GreaterThanLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GreaterEqualLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GreaterEqualLayerParams) */ {
 public:
  GreaterEqualLayerParams();
  virtual ~GreaterEqualLayerParams();

  GreaterEqualLayerParams(const GreaterEqualLayerParams& from);

  inline GreaterEqualLayerParams& operator=(const GreaterEqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GreaterEqualLayerParams& default_instance();

  static inline const GreaterEqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const GreaterEqualLayerParams*>(
               &_GreaterEqualLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    26;

  void Swap(GreaterEqualLayerParams* other);

  // implements Message ----------------------------------------------

  inline GreaterEqualLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GreaterEqualLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GreaterEqualLayerParams& from);
  void MergeFrom(const GreaterEqualLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GreaterEqualLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 2;
  void clear_alpha();
  static const int kAlphaFieldNumber = 2;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GreaterEqualLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LessThanLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LessThanLayerParams) */ {
 public:
  LessThanLayerParams();
  virtual ~LessThanLayerParams();

  LessThanLayerParams(const LessThanLayerParams& from);

  inline LessThanLayerParams& operator=(const LessThanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LessThanLayerParams& default_instance();

  static inline const LessThanLayerParams* internal_default_instance() {
    return reinterpret_cast<const LessThanLayerParams*>(
               &_LessThanLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    27;

  void Swap(LessThanLayerParams* other);

  // implements Message ----------------------------------------------

  inline LessThanLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LessThanLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LessThanLayerParams& from);
  void MergeFrom(const LessThanLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LessThanLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 2;
  void clear_alpha();
  static const int kAlphaFieldNumber = 2;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LessThanLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LessEqualLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LessEqualLayerParams) */ {
 public:
  LessEqualLayerParams();
  virtual ~LessEqualLayerParams();

  LessEqualLayerParams(const LessEqualLayerParams& from);

  inline LessEqualLayerParams& operator=(const LessEqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LessEqualLayerParams& default_instance();

  static inline const LessEqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const LessEqualLayerParams*>(
               &_LessEqualLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    28;

  void Swap(LessEqualLayerParams* other);

  // implements Message ----------------------------------------------

  inline LessEqualLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LessEqualLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LessEqualLayerParams& from);
  void MergeFrom(const LessEqualLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LessEqualLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 2;
  void clear_alpha();
  static const int kAlphaFieldNumber = 2;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LessEqualLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class EqualLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EqualLayerParams) */ {
 public:
  EqualLayerParams();
  virtual ~EqualLayerParams();

  EqualLayerParams(const EqualLayerParams& from);

  inline EqualLayerParams& operator=(const EqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const EqualLayerParams& default_instance();

  static inline const EqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const EqualLayerParams*>(
               &_EqualLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    29;

  void Swap(EqualLayerParams* other);

  // implements Message ----------------------------------------------

  inline EqualLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  EqualLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const EqualLayerParams& from);
  void MergeFrom(const EqualLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EqualLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EqualLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NotEqualLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NotEqualLayerParams) */ {
 public:
  NotEqualLayerParams();
  virtual ~NotEqualLayerParams();

  NotEqualLayerParams(const NotEqualLayerParams& from);

  inline NotEqualLayerParams& operator=(const NotEqualLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const NotEqualLayerParams& default_instance();

  static inline const NotEqualLayerParams* internal_default_instance() {
    return reinterpret_cast<const NotEqualLayerParams*>(
               &_NotEqualLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    30;

  void Swap(NotEqualLayerParams* other);

  // implements Message ----------------------------------------------

  inline NotEqualLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  NotEqualLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NotEqualLayerParams& from);
  void MergeFrom(const NotEqualLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NotEqualLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NotEqualLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LogicalAndLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalAndLayerParams) */ {
 public:
  LogicalAndLayerParams();
  virtual ~LogicalAndLayerParams();

  LogicalAndLayerParams(const LogicalAndLayerParams& from);

  inline LogicalAndLayerParams& operator=(const LogicalAndLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LogicalAndLayerParams& default_instance();

  static inline const LogicalAndLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalAndLayerParams*>(
               &_LogicalAndLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    31;

  void Swap(LogicalAndLayerParams* other);

  // implements Message ----------------------------------------------

  inline LogicalAndLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LogicalAndLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LogicalAndLayerParams& from);
  void MergeFrom(const LogicalAndLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalAndLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalAndLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LogicalOrLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalOrLayerParams) */ {
 public:
  LogicalOrLayerParams();
  virtual ~LogicalOrLayerParams();

  LogicalOrLayerParams(const LogicalOrLayerParams& from);

  inline LogicalOrLayerParams& operator=(const LogicalOrLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LogicalOrLayerParams& default_instance();

  static inline const LogicalOrLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalOrLayerParams*>(
               &_LogicalOrLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    32;

  void Swap(LogicalOrLayerParams* other);

  // implements Message ----------------------------------------------

  inline LogicalOrLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LogicalOrLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LogicalOrLayerParams& from);
  void MergeFrom(const LogicalOrLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalOrLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalOrLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LogicalXorLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalXorLayerParams) */ {
 public:
  LogicalXorLayerParams();
  virtual ~LogicalXorLayerParams();

  LogicalXorLayerParams(const LogicalXorLayerParams& from);

  inline LogicalXorLayerParams& operator=(const LogicalXorLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LogicalXorLayerParams& default_instance();

  static inline const LogicalXorLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalXorLayerParams*>(
               &_LogicalXorLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    33;

  void Swap(LogicalXorLayerParams* other);

  // implements Message ----------------------------------------------

  inline LogicalXorLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LogicalXorLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LogicalXorLayerParams& from);
  void MergeFrom(const LogicalXorLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalXorLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalXorLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LogicalNotLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LogicalNotLayerParams) */ {
 public:
  LogicalNotLayerParams();
  virtual ~LogicalNotLayerParams();

  LogicalNotLayerParams(const LogicalNotLayerParams& from);

  inline LogicalNotLayerParams& operator=(const LogicalNotLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LogicalNotLayerParams& default_instance();

  static inline const LogicalNotLayerParams* internal_default_instance() {
    return reinterpret_cast<const LogicalNotLayerParams*>(
               &_LogicalNotLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    34;

  void Swap(LogicalNotLayerParams* other);

  // implements Message ----------------------------------------------

  inline LogicalNotLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LogicalNotLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LogicalNotLayerParams& from);
  void MergeFrom(const LogicalNotLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LogicalNotLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LogicalNotLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BorderAmounts_EdgeSizes : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BorderAmounts.EdgeSizes) */ {
 public:
  BorderAmounts_EdgeSizes();
  virtual ~BorderAmounts_EdgeSizes();

  BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from);

  inline BorderAmounts_EdgeSizes& operator=(const BorderAmounts_EdgeSizes& from) {
    CopyFrom(from);
    return *this;
  }

  static const BorderAmounts_EdgeSizes& default_instance();

  static inline const BorderAmounts_EdgeSizes* internal_default_instance() {
    return reinterpret_cast<const BorderAmounts_EdgeSizes*>(
               &_BorderAmounts_EdgeSizes_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    35;

  void Swap(BorderAmounts_EdgeSizes* other);

  // implements Message ----------------------------------------------

  inline BorderAmounts_EdgeSizes* New() const PROTOBUF_FINAL { return New(NULL); }

  BorderAmounts_EdgeSizes* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BorderAmounts_EdgeSizes& from);
  void MergeFrom(const BorderAmounts_EdgeSizes& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BorderAmounts_EdgeSizes* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // uint64 startEdgeSize = 1;
  void clear_startedgesize();
  static const int kStartEdgeSizeFieldNumber = 1;
  ::google::protobuf::uint64 startedgesize() const;
  void set_startedgesize(::google::protobuf::uint64 value);

  // uint64 endEdgeSize = 2;
  void clear_endedgesize();
  static const int kEndEdgeSizeFieldNumber = 2;
  ::google::protobuf::uint64 endedgesize() const;
  void set_endedgesize(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BorderAmounts.EdgeSizes)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::uint64 startedgesize_;
  ::google::protobuf::uint64 endedgesize_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BorderAmounts : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BorderAmounts) */ {
 public:
  BorderAmounts();
  virtual ~BorderAmounts();

  BorderAmounts(const BorderAmounts& from);

  inline BorderAmounts& operator=(const BorderAmounts& from) {
    CopyFrom(from);
    return *this;
  }

  static const BorderAmounts& default_instance();

  static inline const BorderAmounts* internal_default_instance() {
    return reinterpret_cast<const BorderAmounts*>(
               &_BorderAmounts_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    36;

  void Swap(BorderAmounts* other);

  // implements Message ----------------------------------------------

  inline BorderAmounts* New() const PROTOBUF_FINAL { return New(NULL); }

  BorderAmounts* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BorderAmounts& from);
  void MergeFrom(const BorderAmounts& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BorderAmounts* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef BorderAmounts_EdgeSizes EdgeSizes;

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  int borderamounts_size() const;
  void clear_borderamounts();
  static const int kBorderAmountsFieldNumber = 10;
  const ::CoreML::Specification::BorderAmounts_EdgeSizes& borderamounts(int index) const;
  ::CoreML::Specification::BorderAmounts_EdgeSizes* mutable_borderamounts(int index);
  ::CoreML::Specification::BorderAmounts_EdgeSizes* add_borderamounts();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
      mutable_borderamounts();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
      borderamounts() const;

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BorderAmounts)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes > borderamounts_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ValidPadding : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ValidPadding) */ {
 public:
  ValidPadding();
  virtual ~ValidPadding();

  ValidPadding(const ValidPadding& from);

  inline ValidPadding& operator=(const ValidPadding& from) {
    CopyFrom(from);
    return *this;
  }

  static const ValidPadding& default_instance();

  static inline const ValidPadding* internal_default_instance() {
    return reinterpret_cast<const ValidPadding*>(
               &_ValidPadding_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    37;

  void Swap(ValidPadding* other);

  // implements Message ----------------------------------------------

  inline ValidPadding* New() const PROTOBUF_FINAL { return New(NULL); }

  ValidPadding* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ValidPadding& from);
  void MergeFrom(const ValidPadding& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ValidPadding* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  bool has_paddingamounts() const;
  void clear_paddingamounts();
  static const int kPaddingAmountsFieldNumber = 1;
  const ::CoreML::Specification::BorderAmounts& paddingamounts() const;
  ::CoreML::Specification::BorderAmounts* mutable_paddingamounts();
  ::CoreML::Specification::BorderAmounts* release_paddingamounts();
  void set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ValidPadding)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::BorderAmounts* paddingamounts_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SamePadding : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SamePadding) */ {
 public:
  SamePadding();
  virtual ~SamePadding();

  SamePadding(const SamePadding& from);

  inline SamePadding& operator=(const SamePadding& from) {
    CopyFrom(from);
    return *this;
  }

  static const SamePadding& default_instance();

  static inline const SamePadding* internal_default_instance() {
    return reinterpret_cast<const SamePadding*>(
               &_SamePadding_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    38;

  void Swap(SamePadding* other);

  // implements Message ----------------------------------------------

  inline SamePadding* New() const PROTOBUF_FINAL { return New(NULL); }

  SamePadding* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SamePadding& from);
  void MergeFrom(const SamePadding& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SamePadding* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef SamePadding_SamePaddingMode SamePaddingMode;
  static const SamePaddingMode BOTTOM_RIGHT_HEAVY =
    SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY;
  static const SamePaddingMode TOP_LEFT_HEAVY =
    SamePadding_SamePaddingMode_TOP_LEFT_HEAVY;
  static inline bool SamePaddingMode_IsValid(int value) {
    return SamePadding_SamePaddingMode_IsValid(value);
  }
  static const SamePaddingMode SamePaddingMode_MIN =
    SamePadding_SamePaddingMode_SamePaddingMode_MIN;
  static const SamePaddingMode SamePaddingMode_MAX =
    SamePadding_SamePaddingMode_SamePaddingMode_MAX;
  static const int SamePaddingMode_ARRAYSIZE =
    SamePadding_SamePaddingMode_SamePaddingMode_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  void clear_asymmetrymode();
  static const int kAsymmetryModeFieldNumber = 1;
  ::CoreML::Specification::SamePadding_SamePaddingMode asymmetrymode() const;
  void set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SamePadding)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int asymmetrymode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SamplingMode : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SamplingMode) */ {
 public:
  SamplingMode();
  virtual ~SamplingMode();

  SamplingMode(const SamplingMode& from);

  inline SamplingMode& operator=(const SamplingMode& from) {
    CopyFrom(from);
    return *this;
  }

  static const SamplingMode& default_instance();

  static inline const SamplingMode* internal_default_instance() {
    return reinterpret_cast<const SamplingMode*>(
               &_SamplingMode_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    39;

  void Swap(SamplingMode* other);

  // implements Message ----------------------------------------------

  inline SamplingMode* New() const PROTOBUF_FINAL { return New(NULL); }

  SamplingMode* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SamplingMode& from);
  void MergeFrom(const SamplingMode& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SamplingMode* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef SamplingMode_Method Method;
  static const Method STRICT_ALIGN_ENDPOINTS_MODE =
    SamplingMode_Method_STRICT_ALIGN_ENDPOINTS_MODE;
  static const Method ALIGN_ENDPOINTS_MODE =
    SamplingMode_Method_ALIGN_ENDPOINTS_MODE;
  static const Method UPSAMPLE_MODE =
    SamplingMode_Method_UPSAMPLE_MODE;
  static const Method ROI_ALIGN_MODE =
    SamplingMode_Method_ROI_ALIGN_MODE;
  static inline bool Method_IsValid(int value) {
    return SamplingMode_Method_IsValid(value);
  }
  static const Method Method_MIN =
    SamplingMode_Method_Method_MIN;
  static const Method Method_MAX =
    SamplingMode_Method_Method_MAX;
  static const int Method_ARRAYSIZE =
    SamplingMode_Method_Method_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
  void clear_samplingmethod();
  static const int kSamplingMethodFieldNumber = 1;
  ::CoreML::Specification::SamplingMode_Method samplingmethod() const;
  void set_samplingmethod(::CoreML::Specification::SamplingMode_Method value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SamplingMode)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int samplingmethod_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BoxCoordinatesMode : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BoxCoordinatesMode) */ {
 public:
  BoxCoordinatesMode();
  virtual ~BoxCoordinatesMode();

  BoxCoordinatesMode(const BoxCoordinatesMode& from);

  inline BoxCoordinatesMode& operator=(const BoxCoordinatesMode& from) {
    CopyFrom(from);
    return *this;
  }

  static const BoxCoordinatesMode& default_instance();

  static inline const BoxCoordinatesMode* internal_default_instance() {
    return reinterpret_cast<const BoxCoordinatesMode*>(
               &_BoxCoordinatesMode_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    40;

  void Swap(BoxCoordinatesMode* other);

  // implements Message ----------------------------------------------

  inline BoxCoordinatesMode* New() const PROTOBUF_FINAL { return New(NULL); }

  BoxCoordinatesMode* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BoxCoordinatesMode& from);
  void MergeFrom(const BoxCoordinatesMode& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BoxCoordinatesMode* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef BoxCoordinatesMode_Coordinates Coordinates;
  static const Coordinates CORNERS_HEIGHT_FIRST =
    BoxCoordinatesMode_Coordinates_CORNERS_HEIGHT_FIRST;
  static const Coordinates CORNERS_WIDTH_FIRST =
    BoxCoordinatesMode_Coordinates_CORNERS_WIDTH_FIRST;
  static const Coordinates CENTER_SIZE_HEIGHT_FIRST =
    BoxCoordinatesMode_Coordinates_CENTER_SIZE_HEIGHT_FIRST;
  static const Coordinates CENTER_SIZE_WIDTH_FIRST =
    BoxCoordinatesMode_Coordinates_CENTER_SIZE_WIDTH_FIRST;
  static inline bool Coordinates_IsValid(int value) {
    return BoxCoordinatesMode_Coordinates_IsValid(value);
  }
  static const Coordinates Coordinates_MIN =
    BoxCoordinatesMode_Coordinates_Coordinates_MIN;
  static const Coordinates Coordinates_MAX =
    BoxCoordinatesMode_Coordinates_Coordinates_MAX;
  static const int Coordinates_ARRAYSIZE =
    BoxCoordinatesMode_Coordinates_Coordinates_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
  void clear_boxmode();
  static const int kBoxModeFieldNumber = 1;
  ::CoreML::Specification::BoxCoordinatesMode_Coordinates boxmode() const;
  void set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BoxCoordinatesMode)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int boxmode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class WeightParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WeightParams) */ {
 public:
  WeightParams();
  virtual ~WeightParams();

  WeightParams(const WeightParams& from);

  inline WeightParams& operator=(const WeightParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const WeightParams& default_instance();

  static inline const WeightParams* internal_default_instance() {
    return reinterpret_cast<const WeightParams*>(
               &_WeightParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    41;

  void Swap(WeightParams* other);

  // implements Message ----------------------------------------------

  inline WeightParams* New() const PROTOBUF_FINAL { return New(NULL); }

  WeightParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const WeightParams& from);
  void MergeFrom(const WeightParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WeightParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated float floatValue = 1;
  int floatvalue_size() const;
  void clear_floatvalue();
  static const int kFloatValueFieldNumber = 1;
  float floatvalue(int index) const;
  void set_floatvalue(int index, float value);
  void add_floatvalue(float value);
  const ::google::protobuf::RepeatedField< float >&
      floatvalue() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_floatvalue();

  // bytes float16Value = 2;
  void clear_float16value();
  static const int kFloat16ValueFieldNumber = 2;
  const ::std::string& float16value() const;
  void set_float16value(const ::std::string& value);
  #if LANG_CXX11
  void set_float16value(::std::string&& value);
  #endif
  void set_float16value(const char* value);
  void set_float16value(const void* value, size_t size);
  ::std::string* mutable_float16value();
  ::std::string* release_float16value();
  void set_allocated_float16value(::std::string* float16value);

  // bytes rawValue = 30;
  void clear_rawvalue();
  static const int kRawValueFieldNumber = 30;
  const ::std::string& rawvalue() const;
  void set_rawvalue(const ::std::string& value);
  #if LANG_CXX11
  void set_rawvalue(::std::string&& value);
  #endif
  void set_rawvalue(const char* value);
  void set_rawvalue(const void* value, size_t size);
  ::std::string* mutable_rawvalue();
  ::std::string* release_rawvalue();
  void set_allocated_rawvalue(::std::string* rawvalue);

  // .CoreML.Specification.QuantizationParams quantization = 40;
  bool has_quantization() const;
  void clear_quantization();
  static const int kQuantizationFieldNumber = 40;
  const ::CoreML::Specification::QuantizationParams& quantization() const;
  ::CoreML::Specification::QuantizationParams* mutable_quantization();
  ::CoreML::Specification::QuantizationParams* release_quantization();
  void set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization);

  // bool isUpdatable = 50;
  void clear_isupdatable();
  static const int kIsUpdatableFieldNumber = 50;
  bool isupdatable() const;
  void set_isupdatable(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WeightParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< float > floatvalue_;
  mutable int _floatvalue_cached_byte_size_;
  ::google::protobuf::internal::ArenaStringPtr float16value_;
  ::google::protobuf::internal::ArenaStringPtr rawvalue_;
  ::CoreML::Specification::QuantizationParams* quantization_;
  bool isupdatable_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class QuantizationParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.QuantizationParams) */ {
 public:
  QuantizationParams();
  virtual ~QuantizationParams();

  QuantizationParams(const QuantizationParams& from);

  inline QuantizationParams& operator=(const QuantizationParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const QuantizationParams& default_instance();

  enum QuantizationTypeCase {
    kLinearQuantization = 101,
    kLookupTableQuantization = 102,
    QUANTIZATIONTYPE_NOT_SET = 0,
  };

  static inline const QuantizationParams* internal_default_instance() {
    return reinterpret_cast<const QuantizationParams*>(
               &_QuantizationParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    42;

  void Swap(QuantizationParams* other);

  // implements Message ----------------------------------------------

  inline QuantizationParams* New() const PROTOBUF_FINAL { return New(NULL); }

  QuantizationParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const QuantizationParams& from);
  void MergeFrom(const QuantizationParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(QuantizationParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // uint64 numberOfBits = 1;
  void clear_numberofbits();
  static const int kNumberOfBitsFieldNumber = 1;
  ::google::protobuf::uint64 numberofbits() const;
  void set_numberofbits(::google::protobuf::uint64 value);

  // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
  bool has_linearquantization() const;
  void clear_linearquantization();
  static const int kLinearQuantizationFieldNumber = 101;
  const ::CoreML::Specification::LinearQuantizationParams& linearquantization() const;
  ::CoreML::Specification::LinearQuantizationParams* mutable_linearquantization();
  ::CoreML::Specification::LinearQuantizationParams* release_linearquantization();
  void set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization);

  // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
  bool has_lookuptablequantization() const;
  void clear_lookuptablequantization();
  static const int kLookupTableQuantizationFieldNumber = 102;
  const ::CoreML::Specification::LookUpTableQuantizationParams& lookuptablequantization() const;
  ::CoreML::Specification::LookUpTableQuantizationParams* mutable_lookuptablequantization();
  ::CoreML::Specification::LookUpTableQuantizationParams* release_lookuptablequantization();
  void set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization);

  QuantizationTypeCase QuantizationType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.QuantizationParams)
 private:
  void set_has_linearquantization();
  void set_has_lookuptablequantization();

  inline bool has_QuantizationType() const;
  void clear_QuantizationType();
  inline void clear_has_QuantizationType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::uint64 numberofbits_;
  union QuantizationTypeUnion {
    QuantizationTypeUnion() {}
    ::CoreML::Specification::LinearQuantizationParams* linearquantization_;
    ::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization_;
  } QuantizationType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LinearQuantizationParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LinearQuantizationParams) */ {
 public:
  LinearQuantizationParams();
  virtual ~LinearQuantizationParams();

  LinearQuantizationParams(const LinearQuantizationParams& from);

  inline LinearQuantizationParams& operator=(const LinearQuantizationParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LinearQuantizationParams& default_instance();

  static inline const LinearQuantizationParams* internal_default_instance() {
    return reinterpret_cast<const LinearQuantizationParams*>(
               &_LinearQuantizationParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    43;

  void Swap(LinearQuantizationParams* other);

  // implements Message ----------------------------------------------

  inline LinearQuantizationParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LinearQuantizationParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LinearQuantizationParams& from);
  void MergeFrom(const LinearQuantizationParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LinearQuantizationParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated float scale = 1;
  int scale_size() const;
  void clear_scale();
  static const int kScaleFieldNumber = 1;
  float scale(int index) const;
  void set_scale(int index, float value);
  void add_scale(float value);
  const ::google::protobuf::RepeatedField< float >&
      scale() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_scale();

  // repeated float bias = 2;
  int bias_size() const;
  void clear_bias();
  static const int kBiasFieldNumber = 2;
  float bias(int index) const;
  void set_bias(int index, float value);
  void add_bias(float value);
  const ::google::protobuf::RepeatedField< float >&
      bias() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_bias();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LinearQuantizationParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< float > scale_;
  mutable int _scale_cached_byte_size_;
  ::google::protobuf::RepeatedField< float > bias_;
  mutable int _bias_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LookUpTableQuantizationParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LookUpTableQuantizationParams) */ {
 public:
  LookUpTableQuantizationParams();
  virtual ~LookUpTableQuantizationParams();

  LookUpTableQuantizationParams(const LookUpTableQuantizationParams& from);

  inline LookUpTableQuantizationParams& operator=(const LookUpTableQuantizationParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LookUpTableQuantizationParams& default_instance();

  static inline const LookUpTableQuantizationParams* internal_default_instance() {
    return reinterpret_cast<const LookUpTableQuantizationParams*>(
               &_LookUpTableQuantizationParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    44;

  void Swap(LookUpTableQuantizationParams* other);

  // implements Message ----------------------------------------------

  inline LookUpTableQuantizationParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LookUpTableQuantizationParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LookUpTableQuantizationParams& from);
  void MergeFrom(const LookUpTableQuantizationParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LookUpTableQuantizationParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated float floatValue = 1;
  int floatvalue_size() const;
  void clear_floatvalue();
  static const int kFloatValueFieldNumber = 1;
  float floatvalue(int index) const;
  void set_floatvalue(int index, float value);
  void add_floatvalue(float value);
  const ::google::protobuf::RepeatedField< float >&
      floatvalue() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_floatvalue();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LookUpTableQuantizationParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< float > floatvalue_;
  mutable int _floatvalue_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ConvolutionLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConvolutionLayerParams) */ {
 public:
  ConvolutionLayerParams();
  virtual ~ConvolutionLayerParams();

  ConvolutionLayerParams(const ConvolutionLayerParams& from);

  inline ConvolutionLayerParams& operator=(const ConvolutionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ConvolutionLayerParams& default_instance();

  enum ConvolutionPaddingTypeCase {
    kValid = 50,
    kSame = 51,
    CONVOLUTIONPADDINGTYPE_NOT_SET = 0,
  };

  static inline const ConvolutionLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConvolutionLayerParams*>(
               &_ConvolutionLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    45;

  void Swap(ConvolutionLayerParams* other);

  // implements Message ----------------------------------------------

  inline ConvolutionLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ConvolutionLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ConvolutionLayerParams& from);
  void MergeFrom(const ConvolutionLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConvolutionLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 kernelSize = 20;
  int kernelsize_size() const;
  void clear_kernelsize();
  static const int kKernelSizeFieldNumber = 20;
  ::google::protobuf::uint64 kernelsize(int index) const;
  void set_kernelsize(int index, ::google::protobuf::uint64 value);
  void add_kernelsize(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      kernelsize() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_kernelsize();

  // repeated uint64 stride = 30;
  int stride_size() const;
  void clear_stride();
  static const int kStrideFieldNumber = 30;
  ::google::protobuf::uint64 stride(int index) const;
  void set_stride(int index, ::google::protobuf::uint64 value);
  void add_stride(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      stride() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_stride();

  // repeated uint64 dilationFactor = 40;
  int dilationfactor_size() const;
  void clear_dilationfactor();
  static const int kDilationFactorFieldNumber = 40;
  ::google::protobuf::uint64 dilationfactor(int index) const;
  void set_dilationfactor(int index, ::google::protobuf::uint64 value);
  void add_dilationfactor(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      dilationfactor() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_dilationfactor();

  // repeated uint64 outputShape = 100;
  int outputshape_size() const;
  void clear_outputshape();
  static const int kOutputShapeFieldNumber = 100;
  ::google::protobuf::uint64 outputshape(int index) const;
  void set_outputshape(int index, ::google::protobuf::uint64 value);
  void add_outputshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      outputshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_outputshape();

  // .CoreML.Specification.WeightParams weights = 90;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 90;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // .CoreML.Specification.WeightParams bias = 91;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 91;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // uint64 outputChannels = 1;
  void clear_outputchannels();
  static const int kOutputChannelsFieldNumber = 1;
  ::google::protobuf::uint64 outputchannels() const;
  void set_outputchannels(::google::protobuf::uint64 value);

  // uint64 kernelChannels = 2;
  void clear_kernelchannels();
  static const int kKernelChannelsFieldNumber = 2;
  ::google::protobuf::uint64 kernelchannels() const;
  void set_kernelchannels(::google::protobuf::uint64 value);

  // uint64 nGroups = 10;
  void clear_ngroups();
  static const int kNGroupsFieldNumber = 10;
  ::google::protobuf::uint64 ngroups() const;
  void set_ngroups(::google::protobuf::uint64 value);

  // bool isDeconvolution = 60;
  void clear_isdeconvolution();
  static const int kIsDeconvolutionFieldNumber = 60;
  bool isdeconvolution() const;
  void set_isdeconvolution(bool value);

  // bool hasBias = 70;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 70;
  bool hasbias() const;
  void set_hasbias(bool value);

  // .CoreML.Specification.ValidPadding valid = 50;
  bool has_valid() const;
  void clear_valid();
  static const int kValidFieldNumber = 50;
  const ::CoreML::Specification::ValidPadding& valid() const;
  ::CoreML::Specification::ValidPadding* mutable_valid();
  ::CoreML::Specification::ValidPadding* release_valid();
  void set_allocated_valid(::CoreML::Specification::ValidPadding* valid);

  // .CoreML.Specification.SamePadding same = 51;
  bool has_same() const;
  void clear_same();
  static const int kSameFieldNumber = 51;
  const ::CoreML::Specification::SamePadding& same() const;
  ::CoreML::Specification::SamePadding* mutable_same();
  ::CoreML::Specification::SamePadding* release_same();
  void set_allocated_same(::CoreML::Specification::SamePadding* same);

  ConvolutionPaddingTypeCase ConvolutionPaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConvolutionLayerParams)
 private:
  void set_has_valid();
  void set_has_same();

  inline bool has_ConvolutionPaddingType() const;
  void clear_ConvolutionPaddingType();
  inline void clear_has_ConvolutionPaddingType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > kernelsize_;
  mutable int _kernelsize_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > stride_;
  mutable int _stride_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > dilationfactor_;
  mutable int _dilationfactor_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > outputshape_;
  mutable int _outputshape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 outputchannels_;
  ::google::protobuf::uint64 kernelchannels_;
  ::google::protobuf::uint64 ngroups_;
  bool isdeconvolution_;
  bool hasbias_;
  union ConvolutionPaddingTypeUnion {
    ConvolutionPaddingTypeUnion() {}
    ::CoreML::Specification::ValidPadding* valid_;
    ::CoreML::Specification::SamePadding* same_;
  } ConvolutionPaddingType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class InnerProductLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.InnerProductLayerParams) */ {
 public:
  InnerProductLayerParams();
  virtual ~InnerProductLayerParams();

  InnerProductLayerParams(const InnerProductLayerParams& from);

  inline InnerProductLayerParams& operator=(const InnerProductLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const InnerProductLayerParams& default_instance();

  static inline const InnerProductLayerParams* internal_default_instance() {
    return reinterpret_cast<const InnerProductLayerParams*>(
               &_InnerProductLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    46;

  void Swap(InnerProductLayerParams* other);

  // implements Message ----------------------------------------------

  inline InnerProductLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  InnerProductLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const InnerProductLayerParams& from);
  void MergeFrom(const InnerProductLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(InnerProductLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // uint64 inputChannels = 1;
  void clear_inputchannels();
  static const int kInputChannelsFieldNumber = 1;
  ::google::protobuf::uint64 inputchannels() const;
  void set_inputchannels(::google::protobuf::uint64 value);

  // uint64 outputChannels = 2;
  void clear_outputchannels();
  static const int kOutputChannelsFieldNumber = 2;
  ::google::protobuf::uint64 outputchannels() const;
  void set_outputchannels(::google::protobuf::uint64 value);

  // bool hasBias = 10;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 10;
  bool hasbias() const;
  void set_hasbias(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.InnerProductLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 inputchannels_;
  ::google::protobuf::uint64 outputchannels_;
  bool hasbias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class EmbeddingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EmbeddingLayerParams) */ {
 public:
  EmbeddingLayerParams();
  virtual ~EmbeddingLayerParams();

  EmbeddingLayerParams(const EmbeddingLayerParams& from);

  inline EmbeddingLayerParams& operator=(const EmbeddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const EmbeddingLayerParams& default_instance();

  static inline const EmbeddingLayerParams* internal_default_instance() {
    return reinterpret_cast<const EmbeddingLayerParams*>(
               &_EmbeddingLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    47;

  void Swap(EmbeddingLayerParams* other);

  // implements Message ----------------------------------------------

  inline EmbeddingLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  EmbeddingLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const EmbeddingLayerParams& from);
  void MergeFrom(const EmbeddingLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EmbeddingLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // uint64 inputDim = 1;
  void clear_inputdim();
  static const int kInputDimFieldNumber = 1;
  ::google::protobuf::uint64 inputdim() const;
  void set_inputdim(::google::protobuf::uint64 value);

  // uint64 outputChannels = 2;
  void clear_outputchannels();
  static const int kOutputChannelsFieldNumber = 2;
  ::google::protobuf::uint64 outputchannels() const;
  void set_outputchannels(::google::protobuf::uint64 value);

  // bool hasBias = 10;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 10;
  bool hasbias() const;
  void set_hasbias(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EmbeddingLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 inputdim_;
  ::google::protobuf::uint64 outputchannels_;
  bool hasbias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class EmbeddingNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EmbeddingNDLayerParams) */ {
 public:
  EmbeddingNDLayerParams();
  virtual ~EmbeddingNDLayerParams();

  EmbeddingNDLayerParams(const EmbeddingNDLayerParams& from);

  inline EmbeddingNDLayerParams& operator=(const EmbeddingNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const EmbeddingNDLayerParams& default_instance();

  static inline const EmbeddingNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const EmbeddingNDLayerParams*>(
               &_EmbeddingNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    48;

  void Swap(EmbeddingNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline EmbeddingNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  EmbeddingNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const EmbeddingNDLayerParams& from);
  void MergeFrom(const EmbeddingNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EmbeddingNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // uint64 vocabSize = 1;
  void clear_vocabsize();
  static const int kVocabSizeFieldNumber = 1;
  ::google::protobuf::uint64 vocabsize() const;
  void set_vocabsize(::google::protobuf::uint64 value);

  // uint64 embeddingSize = 2;
  void clear_embeddingsize();
  static const int kEmbeddingSizeFieldNumber = 2;
  ::google::protobuf::uint64 embeddingsize() const;
  void set_embeddingsize(::google::protobuf::uint64 value);

  // bool hasBias = 3;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 3;
  bool hasbias() const;
  void set_hasbias(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EmbeddingNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 vocabsize_;
  ::google::protobuf::uint64 embeddingsize_;
  bool hasbias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BatchnormLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BatchnormLayerParams) */ {
 public:
  BatchnormLayerParams();
  virtual ~BatchnormLayerParams();

  BatchnormLayerParams(const BatchnormLayerParams& from);

  inline BatchnormLayerParams& operator=(const BatchnormLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BatchnormLayerParams& default_instance();

  static inline const BatchnormLayerParams* internal_default_instance() {
    return reinterpret_cast<const BatchnormLayerParams*>(
               &_BatchnormLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    49;

  void Swap(BatchnormLayerParams* other);

  // implements Message ----------------------------------------------

  inline BatchnormLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BatchnormLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BatchnormLayerParams& from);
  void MergeFrom(const BatchnormLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BatchnormLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams gamma = 15;
  bool has_gamma() const;
  void clear_gamma();
  static const int kGammaFieldNumber = 15;
  const ::CoreML::Specification::WeightParams& gamma() const;
  ::CoreML::Specification::WeightParams* mutable_gamma();
  ::CoreML::Specification::WeightParams* release_gamma();
  void set_allocated_gamma(::CoreML::Specification::WeightParams* gamma);

  // .CoreML.Specification.WeightParams beta = 16;
  bool has_beta() const;
  void clear_beta();
  static const int kBetaFieldNumber = 16;
  const ::CoreML::Specification::WeightParams& beta() const;
  ::CoreML::Specification::WeightParams* mutable_beta();
  ::CoreML::Specification::WeightParams* release_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);

  // .CoreML.Specification.WeightParams mean = 17;
  bool has_mean() const;
  void clear_mean();
  static const int kMeanFieldNumber = 17;
  const ::CoreML::Specification::WeightParams& mean() const;
  ::CoreML::Specification::WeightParams* mutable_mean();
  ::CoreML::Specification::WeightParams* release_mean();
  void set_allocated_mean(::CoreML::Specification::WeightParams* mean);

  // .CoreML.Specification.WeightParams variance = 18;
  bool has_variance() const;
  void clear_variance();
  static const int kVarianceFieldNumber = 18;
  const ::CoreML::Specification::WeightParams& variance() const;
  ::CoreML::Specification::WeightParams* mutable_variance();
  ::CoreML::Specification::WeightParams* release_variance();
  void set_allocated_variance(::CoreML::Specification::WeightParams* variance);

  // uint64 channels = 1;
  void clear_channels();
  static const int kChannelsFieldNumber = 1;
  ::google::protobuf::uint64 channels() const;
  void set_channels(::google::protobuf::uint64 value);

  // bool computeMeanVar = 5;
  void clear_computemeanvar();
  static const int kComputeMeanVarFieldNumber = 5;
  bool computemeanvar() const;
  void set_computemeanvar(bool value);

  // bool instanceNormalization = 6;
  void clear_instancenormalization();
  static const int kInstanceNormalizationFieldNumber = 6;
  bool instancenormalization() const;
  void set_instancenormalization(bool value);

  // float epsilon = 10;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 10;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BatchnormLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* gamma_;
  ::CoreML::Specification::WeightParams* beta_;
  ::CoreML::Specification::WeightParams* mean_;
  ::CoreML::Specification::WeightParams* variance_;
  ::google::protobuf::uint64 channels_;
  bool computemeanvar_;
  bool instancenormalization_;
  float epsilon_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PoolingLayerParams_ValidCompletePadding : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PoolingLayerParams.ValidCompletePadding) */ {
 public:
  PoolingLayerParams_ValidCompletePadding();
  virtual ~PoolingLayerParams_ValidCompletePadding();

  PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from);

  inline PoolingLayerParams_ValidCompletePadding& operator=(const PoolingLayerParams_ValidCompletePadding& from) {
    CopyFrom(from);
    return *this;
  }

  static const PoolingLayerParams_ValidCompletePadding& default_instance();

  static inline const PoolingLayerParams_ValidCompletePadding* internal_default_instance() {
    return reinterpret_cast<const PoolingLayerParams_ValidCompletePadding*>(
               &_PoolingLayerParams_ValidCompletePadding_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    50;

  void Swap(PoolingLayerParams_ValidCompletePadding* other);

  // implements Message ----------------------------------------------

  inline PoolingLayerParams_ValidCompletePadding* New() const PROTOBUF_FINAL { return New(NULL); }

  PoolingLayerParams_ValidCompletePadding* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PoolingLayerParams_ValidCompletePadding& from);
  void MergeFrom(const PoolingLayerParams_ValidCompletePadding& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PoolingLayerParams_ValidCompletePadding* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 paddingAmounts = 10;
  int paddingamounts_size() const;
  void clear_paddingamounts();
  static const int kPaddingAmountsFieldNumber = 10;
  ::google::protobuf::uint64 paddingamounts(int index) const;
  void set_paddingamounts(int index, ::google::protobuf::uint64 value);
  void add_paddingamounts(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      paddingamounts() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_paddingamounts();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > paddingamounts_;
  mutable int _paddingamounts_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PoolingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PoolingLayerParams) */ {
 public:
  PoolingLayerParams();
  virtual ~PoolingLayerParams();

  PoolingLayerParams(const PoolingLayerParams& from);

  inline PoolingLayerParams& operator=(const PoolingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PoolingLayerParams& default_instance();

  enum PoolingPaddingTypeCase {
    kValid = 30,
    kSame = 31,
    kIncludeLastPixel = 32,
    POOLINGPADDINGTYPE_NOT_SET = 0,
  };

  static inline const PoolingLayerParams* internal_default_instance() {
    return reinterpret_cast<const PoolingLayerParams*>(
               &_PoolingLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    51;

  void Swap(PoolingLayerParams* other);

  // implements Message ----------------------------------------------

  inline PoolingLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  PoolingLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PoolingLayerParams& from);
  void MergeFrom(const PoolingLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PoolingLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef PoolingLayerParams_ValidCompletePadding ValidCompletePadding;

  typedef PoolingLayerParams_PoolingType PoolingType;
  static const PoolingType MAX =
    PoolingLayerParams_PoolingType_MAX;
  static const PoolingType AVERAGE =
    PoolingLayerParams_PoolingType_AVERAGE;
  static const PoolingType L2 =
    PoolingLayerParams_PoolingType_L2;
  static inline bool PoolingType_IsValid(int value) {
    return PoolingLayerParams_PoolingType_IsValid(value);
  }
  static const PoolingType PoolingType_MIN =
    PoolingLayerParams_PoolingType_PoolingType_MIN;
  static const PoolingType PoolingType_MAX =
    PoolingLayerParams_PoolingType_PoolingType_MAX;
  static const int PoolingType_ARRAYSIZE =
    PoolingLayerParams_PoolingType_PoolingType_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // repeated uint64 kernelSize = 10;
  int kernelsize_size() const;
  void clear_kernelsize();
  static const int kKernelSizeFieldNumber = 10;
  ::google::protobuf::uint64 kernelsize(int index) const;
  void set_kernelsize(int index, ::google::protobuf::uint64 value);
  void add_kernelsize(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      kernelsize() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_kernelsize();

  // repeated uint64 stride = 20;
  int stride_size() const;
  void clear_stride();
  static const int kStrideFieldNumber = 20;
  ::google::protobuf::uint64 stride(int index) const;
  void set_stride(int index, ::google::protobuf::uint64 value);
  void add_stride(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      stride() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_stride();

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  void clear_type();
  static const int kTypeFieldNumber = 1;
  ::CoreML::Specification::PoolingLayerParams_PoolingType type() const;
  void set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value);

  // bool avgPoolExcludePadding = 50;
  void clear_avgpoolexcludepadding();
  static const int kAvgPoolExcludePaddingFieldNumber = 50;
  bool avgpoolexcludepadding() const;
  void set_avgpoolexcludepadding(bool value);

  // bool globalPooling = 60;
  void clear_globalpooling();
  static const int kGlobalPoolingFieldNumber = 60;
  bool globalpooling() const;
  void set_globalpooling(bool value);

  // .CoreML.Specification.ValidPadding valid = 30;
  bool has_valid() const;
  void clear_valid();
  static const int kValidFieldNumber = 30;
  const ::CoreML::Specification::ValidPadding& valid() const;
  ::CoreML::Specification::ValidPadding* mutable_valid();
  ::CoreML::Specification::ValidPadding* release_valid();
  void set_allocated_valid(::CoreML::Specification::ValidPadding* valid);

  // .CoreML.Specification.SamePadding same = 31;
  bool has_same() const;
  void clear_same();
  static const int kSameFieldNumber = 31;
  const ::CoreML::Specification::SamePadding& same() const;
  ::CoreML::Specification::SamePadding* mutable_same();
  ::CoreML::Specification::SamePadding* release_same();
  void set_allocated_same(::CoreML::Specification::SamePadding* same);

  // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  bool has_includelastpixel() const;
  void clear_includelastpixel();
  static const int kIncludeLastPixelFieldNumber = 32;
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& includelastpixel() const;
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* mutable_includelastpixel();
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* release_includelastpixel();
  void set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel);

  PoolingPaddingTypeCase PoolingPaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.PoolingLayerParams)
 private:
  void set_has_valid();
  void set_has_same();
  void set_has_includelastpixel();

  inline bool has_PoolingPaddingType() const;
  void clear_PoolingPaddingType();
  inline void clear_has_PoolingPaddingType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > kernelsize_;
  mutable int _kernelsize_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > stride_;
  mutable int _stride_cached_byte_size_;
  int type_;
  bool avgpoolexcludepadding_;
  bool globalpooling_;
  union PoolingPaddingTypeUnion {
    PoolingPaddingTypeUnion() {}
    ::CoreML::Specification::ValidPadding* valid_;
    ::CoreML::Specification::SamePadding* same_;
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel_;
  } PoolingPaddingType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PaddingLayerParams_PaddingConstant : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingConstant) */ {
 public:
  PaddingLayerParams_PaddingConstant();
  virtual ~PaddingLayerParams_PaddingConstant();

  PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from);

  inline PaddingLayerParams_PaddingConstant& operator=(const PaddingLayerParams_PaddingConstant& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams_PaddingConstant& default_instance();

  static inline const PaddingLayerParams_PaddingConstant* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams_PaddingConstant*>(
               &_PaddingLayerParams_PaddingConstant_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    52;

  void Swap(PaddingLayerParams_PaddingConstant* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams_PaddingConstant* New() const PROTOBUF_FINAL { return New(NULL); }

  PaddingLayerParams_PaddingConstant* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PaddingLayerParams_PaddingConstant& from);
  void MergeFrom(const PaddingLayerParams_PaddingConstant& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingConstant* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float value = 1;
  void clear_value();
  static const int kValueFieldNumber = 1;
  float value() const;
  void set_value(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingConstant)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float value_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PaddingLayerParams_PaddingReflection : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingReflection) */ {
 public:
  PaddingLayerParams_PaddingReflection();
  virtual ~PaddingLayerParams_PaddingReflection();

  PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from);

  inline PaddingLayerParams_PaddingReflection& operator=(const PaddingLayerParams_PaddingReflection& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams_PaddingReflection& default_instance();

  static inline const PaddingLayerParams_PaddingReflection* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams_PaddingReflection*>(
               &_PaddingLayerParams_PaddingReflection_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    53;

  void Swap(PaddingLayerParams_PaddingReflection* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams_PaddingReflection* New() const PROTOBUF_FINAL { return New(NULL); }

  PaddingLayerParams_PaddingReflection* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PaddingLayerParams_PaddingReflection& from);
  void MergeFrom(const PaddingLayerParams_PaddingReflection& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingReflection* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingReflection)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PaddingLayerParams_PaddingReplication : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingReplication) */ {
 public:
  PaddingLayerParams_PaddingReplication();
  virtual ~PaddingLayerParams_PaddingReplication();

  PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from);

  inline PaddingLayerParams_PaddingReplication& operator=(const PaddingLayerParams_PaddingReplication& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams_PaddingReplication& default_instance();

  static inline const PaddingLayerParams_PaddingReplication* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams_PaddingReplication*>(
               &_PaddingLayerParams_PaddingReplication_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    54;

  void Swap(PaddingLayerParams_PaddingReplication* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams_PaddingReplication* New() const PROTOBUF_FINAL { return New(NULL); }

  PaddingLayerParams_PaddingReplication* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PaddingLayerParams_PaddingReplication& from);
  void MergeFrom(const PaddingLayerParams_PaddingReplication& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingReplication* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingReplication)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PaddingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams) */ {
 public:
  PaddingLayerParams();
  virtual ~PaddingLayerParams();

  PaddingLayerParams(const PaddingLayerParams& from);

  inline PaddingLayerParams& operator=(const PaddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams& default_instance();

  enum PaddingTypeCase {
    kConstant = 1,
    kReflection = 2,
    kReplication = 3,
    PADDINGTYPE_NOT_SET = 0,
  };

  static inline const PaddingLayerParams* internal_default_instance() {
    return reinterpret_cast<const PaddingLayerParams*>(
               &_PaddingLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    55;

  void Swap(PaddingLayerParams* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  PaddingLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PaddingLayerParams& from);
  void MergeFrom(const PaddingLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef PaddingLayerParams_PaddingConstant PaddingConstant;
  typedef PaddingLayerParams_PaddingReflection PaddingReflection;
  typedef PaddingLayerParams_PaddingReplication PaddingReplication;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  bool has_paddingamounts() const;
  void clear_paddingamounts();
  static const int kPaddingAmountsFieldNumber = 10;
  const ::CoreML::Specification::BorderAmounts& paddingamounts() const;
  ::CoreML::Specification::BorderAmounts* mutable_paddingamounts();
  ::CoreML::Specification::BorderAmounts* release_paddingamounts();
  void set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts);

  // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  bool has_constant() const;
  void clear_constant();
  static const int kConstantFieldNumber = 1;
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& constant() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* mutable_constant();
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* release_constant();
  void set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant);

  // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  bool has_reflection() const;
  void clear_reflection();
  static const int kReflectionFieldNumber = 2;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& reflection() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* mutable_reflection();
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* release_reflection();
  void set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection);

  // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  bool has_replication() const;
  void clear_replication();
  static const int kReplicationFieldNumber = 3;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& replication() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* mutable_replication();
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* release_replication();
  void set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication);

  PaddingTypeCase PaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams)
 private:
  void set_has_constant();
  void set_has_reflection();
  void set_has_replication();

  inline bool has_PaddingType() const;
  void clear_PaddingType();
  inline void clear_has_PaddingType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::BorderAmounts* paddingamounts_;
  union PaddingTypeUnion {
    PaddingTypeUnion() {}
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant_;
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection_;
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication_;
  } PaddingType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ConcatLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConcatLayerParams) */ {
 public:
  ConcatLayerParams();
  virtual ~ConcatLayerParams();

  ConcatLayerParams(const ConcatLayerParams& from);

  inline ConcatLayerParams& operator=(const ConcatLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ConcatLayerParams& default_instance();

  static inline const ConcatLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConcatLayerParams*>(
               &_ConcatLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    56;

  void Swap(ConcatLayerParams* other);

  // implements Message ----------------------------------------------

  inline ConcatLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ConcatLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ConcatLayerParams& from);
  void MergeFrom(const ConcatLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConcatLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // bool sequenceConcat = 100;
  void clear_sequenceconcat();
  static const int kSequenceConcatFieldNumber = 100;
  bool sequenceconcat() const;
  void set_sequenceconcat(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConcatLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  bool sequenceconcat_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LRNLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LRNLayerParams) */ {
 public:
  LRNLayerParams();
  virtual ~LRNLayerParams();

  LRNLayerParams(const LRNLayerParams& from);

  inline LRNLayerParams& operator=(const LRNLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LRNLayerParams& default_instance();

  static inline const LRNLayerParams* internal_default_instance() {
    return reinterpret_cast<const LRNLayerParams*>(
               &_LRNLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    57;

  void Swap(LRNLayerParams* other);

  // implements Message ----------------------------------------------

  inline LRNLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LRNLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LRNLayerParams& from);
  void MergeFrom(const LRNLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LRNLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // uint64 localSize = 3;
  void clear_localsize();
  static const int kLocalSizeFieldNumber = 3;
  ::google::protobuf::uint64 localsize() const;
  void set_localsize(::google::protobuf::uint64 value);

  // float k = 4;
  void clear_k();
  static const int kKFieldNumber = 4;
  float k() const;
  void set_k(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LRNLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  float beta_;
  ::google::protobuf::uint64 localsize_;
  float k_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SoftmaxLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SoftmaxLayerParams) */ {
 public:
  SoftmaxLayerParams();
  virtual ~SoftmaxLayerParams();

  SoftmaxLayerParams(const SoftmaxLayerParams& from);

  inline SoftmaxLayerParams& operator=(const SoftmaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SoftmaxLayerParams& default_instance();

  static inline const SoftmaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const SoftmaxLayerParams*>(
               &_SoftmaxLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    58;

  void Swap(SoftmaxLayerParams* other);

  // implements Message ----------------------------------------------

  inline SoftmaxLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SoftmaxLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SoftmaxLayerParams& from);
  void MergeFrom(const SoftmaxLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SoftmaxLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SoftmaxLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SplitLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SplitLayerParams) */ {
 public:
  SplitLayerParams();
  virtual ~SplitLayerParams();

  SplitLayerParams(const SplitLayerParams& from);

  inline SplitLayerParams& operator=(const SplitLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SplitLayerParams& default_instance();

  static inline const SplitLayerParams* internal_default_instance() {
    return reinterpret_cast<const SplitLayerParams*>(
               &_SplitLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    59;

  void Swap(SplitLayerParams* other);

  // implements Message ----------------------------------------------

  inline SplitLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SplitLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SplitLayerParams& from);
  void MergeFrom(const SplitLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SplitLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // uint64 nOutputs = 1;
  void clear_noutputs();
  static const int kNOutputsFieldNumber = 1;
  ::google::protobuf::uint64 noutputs() const;
  void set_noutputs(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SplitLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::uint64 noutputs_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AddLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AddLayerParams) */ {
 public:
  AddLayerParams();
  virtual ~AddLayerParams();

  AddLayerParams(const AddLayerParams& from);

  inline AddLayerParams& operator=(const AddLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AddLayerParams& default_instance();

  static inline const AddLayerParams* internal_default_instance() {
    return reinterpret_cast<const AddLayerParams*>(
               &_AddLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    60;

  void Swap(AddLayerParams* other);

  // implements Message ----------------------------------------------

  inline AddLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AddLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AddLayerParams& from);
  void MergeFrom(const AddLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AddLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AddLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MultiplyLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MultiplyLayerParams) */ {
 public:
  MultiplyLayerParams();
  virtual ~MultiplyLayerParams();

  MultiplyLayerParams(const MultiplyLayerParams& from);

  inline MultiplyLayerParams& operator=(const MultiplyLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MultiplyLayerParams& default_instance();

  static inline const MultiplyLayerParams* internal_default_instance() {
    return reinterpret_cast<const MultiplyLayerParams*>(
               &_MultiplyLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    61;

  void Swap(MultiplyLayerParams* other);

  // implements Message ----------------------------------------------

  inline MultiplyLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MultiplyLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MultiplyLayerParams& from);
  void MergeFrom(const MultiplyLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MultiplyLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MultiplyLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float alpha_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class UnaryFunctionLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UnaryFunctionLayerParams) */ {
 public:
  UnaryFunctionLayerParams();
  virtual ~UnaryFunctionLayerParams();

  UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from);

  inline UnaryFunctionLayerParams& operator=(const UnaryFunctionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UnaryFunctionLayerParams& default_instance();

  static inline const UnaryFunctionLayerParams* internal_default_instance() {
    return reinterpret_cast<const UnaryFunctionLayerParams*>(
               &_UnaryFunctionLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    62;

  void Swap(UnaryFunctionLayerParams* other);

  // implements Message ----------------------------------------------

  inline UnaryFunctionLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  UnaryFunctionLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const UnaryFunctionLayerParams& from);
  void MergeFrom(const UnaryFunctionLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UnaryFunctionLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef UnaryFunctionLayerParams_Operation Operation;
  static const Operation SQRT =
    UnaryFunctionLayerParams_Operation_SQRT;
  static const Operation RSQRT =
    UnaryFunctionLayerParams_Operation_RSQRT;
  static const Operation INVERSE =
    UnaryFunctionLayerParams_Operation_INVERSE;
  static const Operation POWER =
    UnaryFunctionLayerParams_Operation_POWER;
  static const Operation EXP =
    UnaryFunctionLayerParams_Operation_EXP;
  static const Operation LOG =
    UnaryFunctionLayerParams_Operation_LOG;
  static const Operation ABS =
    UnaryFunctionLayerParams_Operation_ABS;
  static const Operation THRESHOLD =
    UnaryFunctionLayerParams_Operation_THRESHOLD;
  static inline bool Operation_IsValid(int value) {
    return UnaryFunctionLayerParams_Operation_IsValid(value);
  }
  static const Operation Operation_MIN =
    UnaryFunctionLayerParams_Operation_Operation_MIN;
  static const Operation Operation_MAX =
    UnaryFunctionLayerParams_Operation_Operation_MAX;
  static const int Operation_ARRAYSIZE =
    UnaryFunctionLayerParams_Operation_Operation_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  void clear_type();
  static const int kTypeFieldNumber = 1;
  ::CoreML::Specification::UnaryFunctionLayerParams_Operation type() const;
  void set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value);

  // float alpha = 2;
  void clear_alpha();
  static const int kAlphaFieldNumber = 2;
  float alpha() const;
  void set_alpha(float value);

  // float epsilon = 3;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 3;
  float epsilon() const;
  void set_epsilon(float value);

  // float shift = 4;
  void clear_shift();
  static const int kShiftFieldNumber = 4;
  float shift() const;
  void set_shift(float value);

  // float scale = 5;
  void clear_scale();
  static const int kScaleFieldNumber = 5;
  float scale() const;
  void set_scale(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UnaryFunctionLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int type_;
  float alpha_;
  float epsilon_;
  float shift_;
  float scale_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class UpsampleLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UpsampleLayerParams) */ {
 public:
  UpsampleLayerParams();
  virtual ~UpsampleLayerParams();

  UpsampleLayerParams(const UpsampleLayerParams& from);

  inline UpsampleLayerParams& operator=(const UpsampleLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UpsampleLayerParams& default_instance();

  static inline const UpsampleLayerParams* internal_default_instance() {
    return reinterpret_cast<const UpsampleLayerParams*>(
               &_UpsampleLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    63;

  void Swap(UpsampleLayerParams* other);

  // implements Message ----------------------------------------------

  inline UpsampleLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  UpsampleLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const UpsampleLayerParams& from);
  void MergeFrom(const UpsampleLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UpsampleLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef UpsampleLayerParams_InterpolationMode InterpolationMode;
  static const InterpolationMode NN =
    UpsampleLayerParams_InterpolationMode_NN;
  static const InterpolationMode BILINEAR =
    UpsampleLayerParams_InterpolationMode_BILINEAR;
  static inline bool InterpolationMode_IsValid(int value) {
    return UpsampleLayerParams_InterpolationMode_IsValid(value);
  }
  static const InterpolationMode InterpolationMode_MIN =
    UpsampleLayerParams_InterpolationMode_InterpolationMode_MIN;
  static const InterpolationMode InterpolationMode_MAX =
    UpsampleLayerParams_InterpolationMode_InterpolationMode_MAX;
  static const int InterpolationMode_ARRAYSIZE =
    UpsampleLayerParams_InterpolationMode_InterpolationMode_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // repeated uint64 scalingFactor = 1;
  int scalingfactor_size() const;
  void clear_scalingfactor();
  static const int kScalingFactorFieldNumber = 1;
  ::google::protobuf::uint64 scalingfactor(int index) const;
  void set_scalingfactor(int index, ::google::protobuf::uint64 value);
  void add_scalingfactor(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      scalingfactor() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_scalingfactor();

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  void clear_mode();
  static const int kModeFieldNumber = 5;
  ::CoreML::Specification::UpsampleLayerParams_InterpolationMode mode() const;
  void set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UpsampleLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > scalingfactor_;
  mutable int _scalingfactor_cached_byte_size_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ResizeBilinearLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ResizeBilinearLayerParams) */ {
 public:
  ResizeBilinearLayerParams();
  virtual ~ResizeBilinearLayerParams();

  ResizeBilinearLayerParams(const ResizeBilinearLayerParams& from);

  inline ResizeBilinearLayerParams& operator=(const ResizeBilinearLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ResizeBilinearLayerParams& default_instance();

  static inline const ResizeBilinearLayerParams* internal_default_instance() {
    return reinterpret_cast<const ResizeBilinearLayerParams*>(
               &_ResizeBilinearLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    64;

  void Swap(ResizeBilinearLayerParams* other);

  // implements Message ----------------------------------------------

  inline ResizeBilinearLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ResizeBilinearLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ResizeBilinearLayerParams& from);
  void MergeFrom(const ResizeBilinearLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ResizeBilinearLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 targetSize = 1;
  int targetsize_size() const;
  void clear_targetsize();
  static const int kTargetSizeFieldNumber = 1;
  ::google::protobuf::uint64 targetsize(int index) const;
  void set_targetsize(int index, ::google::protobuf::uint64 value);
  void add_targetsize(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      targetsize() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_targetsize();

  // .CoreML.Specification.SamplingMode mode = 2;
  bool has_mode() const;
  void clear_mode();
  static const int kModeFieldNumber = 2;
  const ::CoreML::Specification::SamplingMode& mode() const;
  ::CoreML::Specification::SamplingMode* mutable_mode();
  ::CoreML::Specification::SamplingMode* release_mode();
  void set_allocated_mode(::CoreML::Specification::SamplingMode* mode);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ResizeBilinearLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > targetsize_;
  mutable int _targetsize_cached_byte_size_;
  ::CoreML::Specification::SamplingMode* mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CropResizeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CropResizeLayerParams) */ {
 public:
  CropResizeLayerParams();
  virtual ~CropResizeLayerParams();

  CropResizeLayerParams(const CropResizeLayerParams& from);

  inline CropResizeLayerParams& operator=(const CropResizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CropResizeLayerParams& default_instance();

  static inline const CropResizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const CropResizeLayerParams*>(
               &_CropResizeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    65;

  void Swap(CropResizeLayerParams* other);

  // implements Message ----------------------------------------------

  inline CropResizeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CropResizeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CropResizeLayerParams& from);
  void MergeFrom(const CropResizeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CropResizeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 targetSize = 1;
  int targetsize_size() const;
  void clear_targetsize();
  static const int kTargetSizeFieldNumber = 1;
  ::google::protobuf::uint64 targetsize(int index) const;
  void set_targetsize(int index, ::google::protobuf::uint64 value);
  void add_targetsize(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      targetsize() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_targetsize();

  // .CoreML.Specification.SamplingMode mode = 3;
  bool has_mode() const;
  void clear_mode();
  static const int kModeFieldNumber = 3;
  const ::CoreML::Specification::SamplingMode& mode() const;
  ::CoreML::Specification::SamplingMode* mutable_mode();
  ::CoreML::Specification::SamplingMode* release_mode();
  void set_allocated_mode(::CoreML::Specification::SamplingMode* mode);

  // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
  bool has_boxindicesmode() const;
  void clear_boxindicesmode();
  static const int kBoxIndicesModeFieldNumber = 4;
  const ::CoreML::Specification::BoxCoordinatesMode& boxindicesmode() const;
  ::CoreML::Specification::BoxCoordinatesMode* mutable_boxindicesmode();
  ::CoreML::Specification::BoxCoordinatesMode* release_boxindicesmode();
  void set_allocated_boxindicesmode(::CoreML::Specification::BoxCoordinatesMode* boxindicesmode);

  // bool normalizedCoordinates = 2;
  void clear_normalizedcoordinates();
  static const int kNormalizedCoordinatesFieldNumber = 2;
  bool normalizedcoordinates() const;
  void set_normalizedcoordinates(bool value);

  // float spatialScale = 5;
  void clear_spatialscale();
  static const int kSpatialScaleFieldNumber = 5;
  float spatialscale() const;
  void set_spatialscale(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CropResizeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > targetsize_;
  mutable int _targetsize_cached_byte_size_;
  ::CoreML::Specification::SamplingMode* mode_;
  ::CoreML::Specification::BoxCoordinatesMode* boxindicesmode_;
  bool normalizedcoordinates_;
  float spatialscale_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BiasLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BiasLayerParams) */ {
 public:
  BiasLayerParams();
  virtual ~BiasLayerParams();

  BiasLayerParams(const BiasLayerParams& from);

  inline BiasLayerParams& operator=(const BiasLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BiasLayerParams& default_instance();

  static inline const BiasLayerParams* internal_default_instance() {
    return reinterpret_cast<const BiasLayerParams*>(
               &_BiasLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    66;

  void Swap(BiasLayerParams* other);

  // implements Message ----------------------------------------------

  inline BiasLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BiasLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BiasLayerParams& from);
  void MergeFrom(const BiasLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BiasLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shape = 1;
  int shape_size() const;
  void clear_shape();
  static const int kShapeFieldNumber = 1;
  ::google::protobuf::uint64 shape(int index) const;
  void set_shape(int index, ::google::protobuf::uint64 value);
  void add_shape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shape();

  // .CoreML.Specification.WeightParams bias = 2;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BiasLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shape_;
  mutable int _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* bias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ScaleLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScaleLayerParams) */ {
 public:
  ScaleLayerParams();
  virtual ~ScaleLayerParams();

  ScaleLayerParams(const ScaleLayerParams& from);

  inline ScaleLayerParams& operator=(const ScaleLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ScaleLayerParams& default_instance();

  static inline const ScaleLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScaleLayerParams*>(
               &_ScaleLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    67;

  void Swap(ScaleLayerParams* other);

  // implements Message ----------------------------------------------

  inline ScaleLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ScaleLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ScaleLayerParams& from);
  void MergeFrom(const ScaleLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScaleLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shapeScale = 1;
  int shapescale_size() const;
  void clear_shapescale();
  static const int kShapeScaleFieldNumber = 1;
  ::google::protobuf::uint64 shapescale(int index) const;
  void set_shapescale(int index, ::google::protobuf::uint64 value);
  void add_shapescale(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shapescale() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shapescale();

  // repeated uint64 shapeBias = 4;
  int shapebias_size() const;
  void clear_shapebias();
  static const int kShapeBiasFieldNumber = 4;
  ::google::protobuf::uint64 shapebias(int index) const;
  void set_shapebias(int index, ::google::protobuf::uint64 value);
  void add_shapebias(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shapebias() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shapebias();

  // .CoreML.Specification.WeightParams scale = 2;
  bool has_scale() const;
  void clear_scale();
  static const int kScaleFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& scale() const;
  ::CoreML::Specification::WeightParams* mutable_scale();
  ::CoreML::Specification::WeightParams* release_scale();
  void set_allocated_scale(::CoreML::Specification::WeightParams* scale);

  // .CoreML.Specification.WeightParams bias = 5;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 5;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // bool hasBias = 3;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 3;
  bool hasbias() const;
  void set_hasbias(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScaleLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shapescale_;
  mutable int _shapescale_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shapebias_;
  mutable int _shapebias_cached_byte_size_;
  ::CoreML::Specification::WeightParams* scale_;
  ::CoreML::Specification::WeightParams* bias_;
  bool hasbias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LoadConstantLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoadConstantLayerParams) */ {
 public:
  LoadConstantLayerParams();
  virtual ~LoadConstantLayerParams();

  LoadConstantLayerParams(const LoadConstantLayerParams& from);

  inline LoadConstantLayerParams& operator=(const LoadConstantLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LoadConstantLayerParams& default_instance();

  static inline const LoadConstantLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoadConstantLayerParams*>(
               &_LoadConstantLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    68;

  void Swap(LoadConstantLayerParams* other);

  // implements Message ----------------------------------------------

  inline LoadConstantLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LoadConstantLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LoadConstantLayerParams& from);
  void MergeFrom(const LoadConstantLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoadConstantLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shape = 1;
  int shape_size() const;
  void clear_shape();
  static const int kShapeFieldNumber = 1;
  ::google::protobuf::uint64 shape(int index) const;
  void set_shape(int index, ::google::protobuf::uint64 value);
  void add_shape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shape();

  // .CoreML.Specification.WeightParams data = 2;
  bool has_data() const;
  void clear_data();
  static const int kDataFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& data() const;
  ::CoreML::Specification::WeightParams* mutable_data();
  ::CoreML::Specification::WeightParams* release_data();
  void set_allocated_data(::CoreML::Specification::WeightParams* data);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoadConstantLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shape_;
  mutable int _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* data_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class L2NormalizeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.L2NormalizeLayerParams) */ {
 public:
  L2NormalizeLayerParams();
  virtual ~L2NormalizeLayerParams();

  L2NormalizeLayerParams(const L2NormalizeLayerParams& from);

  inline L2NormalizeLayerParams& operator=(const L2NormalizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const L2NormalizeLayerParams& default_instance();

  static inline const L2NormalizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const L2NormalizeLayerParams*>(
               &_L2NormalizeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    69;

  void Swap(L2NormalizeLayerParams* other);

  // implements Message ----------------------------------------------

  inline L2NormalizeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  L2NormalizeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const L2NormalizeLayerParams& from);
  void MergeFrom(const L2NormalizeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(L2NormalizeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float epsilon = 1;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 1;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.L2NormalizeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float epsilon_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FlattenLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FlattenLayerParams) */ {
 public:
  FlattenLayerParams();
  virtual ~FlattenLayerParams();

  FlattenLayerParams(const FlattenLayerParams& from);

  inline FlattenLayerParams& operator=(const FlattenLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FlattenLayerParams& default_instance();

  static inline const FlattenLayerParams* internal_default_instance() {
    return reinterpret_cast<const FlattenLayerParams*>(
               &_FlattenLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    70;

  void Swap(FlattenLayerParams* other);

  // implements Message ----------------------------------------------

  inline FlattenLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FlattenLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FlattenLayerParams& from);
  void MergeFrom(const FlattenLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FlattenLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef FlattenLayerParams_FlattenOrder FlattenOrder;
  static const FlattenOrder CHANNEL_FIRST =
    FlattenLayerParams_FlattenOrder_CHANNEL_FIRST;
  static const FlattenOrder CHANNEL_LAST =
    FlattenLayerParams_FlattenOrder_CHANNEL_LAST;
  static inline bool FlattenOrder_IsValid(int value) {
    return FlattenLayerParams_FlattenOrder_IsValid(value);
  }
  static const FlattenOrder FlattenOrder_MIN =
    FlattenLayerParams_FlattenOrder_FlattenOrder_MIN;
  static const FlattenOrder FlattenOrder_MAX =
    FlattenLayerParams_FlattenOrder_FlattenOrder_MAX;
  static const int FlattenOrder_ARRAYSIZE =
    FlattenLayerParams_FlattenOrder_FlattenOrder_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::FlattenLayerParams_FlattenOrder mode() const;
  void set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FlattenLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReshapeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeLayerParams) */ {
 public:
  ReshapeLayerParams();
  virtual ~ReshapeLayerParams();

  ReshapeLayerParams(const ReshapeLayerParams& from);

  inline ReshapeLayerParams& operator=(const ReshapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReshapeLayerParams& default_instance();

  static inline const ReshapeLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeLayerParams*>(
               &_ReshapeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    71;

  void Swap(ReshapeLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReshapeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReshapeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReshapeLayerParams& from);
  void MergeFrom(const ReshapeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef ReshapeLayerParams_ReshapeOrder ReshapeOrder;
  static const ReshapeOrder CHANNEL_FIRST =
    ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST;
  static const ReshapeOrder CHANNEL_LAST =
    ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST;
  static inline bool ReshapeOrder_IsValid(int value) {
    return ReshapeLayerParams_ReshapeOrder_IsValid(value);
  }
  static const ReshapeOrder ReshapeOrder_MIN =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MIN;
  static const ReshapeOrder ReshapeOrder_MAX =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX;
  static const int ReshapeOrder_ARRAYSIZE =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  void clear_targetshape();
  static const int kTargetShapeFieldNumber = 1;
  ::google::protobuf::int64 targetshape(int index) const;
  void set_targetshape(int index, ::google::protobuf::int64 value);
  void add_targetshape(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      targetshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_targetshape();

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  void clear_mode();
  static const int kModeFieldNumber = 2;
  ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder mode() const;
  void set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > targetshape_;
  mutable int _targetshape_cached_byte_size_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PermuteLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PermuteLayerParams) */ {
 public:
  PermuteLayerParams();
  virtual ~PermuteLayerParams();

  PermuteLayerParams(const PermuteLayerParams& from);

  inline PermuteLayerParams& operator=(const PermuteLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PermuteLayerParams& default_instance();

  static inline const PermuteLayerParams* internal_default_instance() {
    return reinterpret_cast<const PermuteLayerParams*>(
               &_PermuteLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    72;

  void Swap(PermuteLayerParams* other);

  // implements Message ----------------------------------------------

  inline PermuteLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  PermuteLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PermuteLayerParams& from);
  void MergeFrom(const PermuteLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PermuteLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 axis = 1;
  int axis_size() const;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::uint64 axis(int index) const;
  void set_axis(int index, ::google::protobuf::uint64 value);
  void add_axis(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      axis() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_axis();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PermuteLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > axis_;
  mutable int _axis_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReorganizeDataLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReorganizeDataLayerParams) */ {
 public:
  ReorganizeDataLayerParams();
  virtual ~ReorganizeDataLayerParams();

  ReorganizeDataLayerParams(const ReorganizeDataLayerParams& from);

  inline ReorganizeDataLayerParams& operator=(const ReorganizeDataLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReorganizeDataLayerParams& default_instance();

  static inline const ReorganizeDataLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReorganizeDataLayerParams*>(
               &_ReorganizeDataLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    73;

  void Swap(ReorganizeDataLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReorganizeDataLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReorganizeDataLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReorganizeDataLayerParams& from);
  void MergeFrom(const ReorganizeDataLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReorganizeDataLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef ReorganizeDataLayerParams_ReorganizationType ReorganizationType;
  static const ReorganizationType SPACE_TO_DEPTH =
    ReorganizeDataLayerParams_ReorganizationType_SPACE_TO_DEPTH;
  static const ReorganizationType DEPTH_TO_SPACE =
    ReorganizeDataLayerParams_ReorganizationType_DEPTH_TO_SPACE;
  static inline bool ReorganizationType_IsValid(int value) {
    return ReorganizeDataLayerParams_ReorganizationType_IsValid(value);
  }
  static const ReorganizationType ReorganizationType_MIN =
    ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MIN;
  static const ReorganizationType ReorganizationType_MAX =
    ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_MAX;
  static const int ReorganizationType_ARRAYSIZE =
    ReorganizeDataLayerParams_ReorganizationType_ReorganizationType_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // uint64 blockSize = 2;
  void clear_blocksize();
  static const int kBlockSizeFieldNumber = 2;
  ::google::protobuf::uint64 blocksize() const;
  void set_blocksize(::google::protobuf::uint64 value);

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType mode() const;
  void set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReorganizeDataLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::uint64 blocksize_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SliceLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceLayerParams) */ {
 public:
  SliceLayerParams();
  virtual ~SliceLayerParams();

  SliceLayerParams(const SliceLayerParams& from);

  inline SliceLayerParams& operator=(const SliceLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SliceLayerParams& default_instance();

  static inline const SliceLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceLayerParams*>(
               &_SliceLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    74;

  void Swap(SliceLayerParams* other);

  // implements Message ----------------------------------------------

  inline SliceLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SliceLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SliceLayerParams& from);
  void MergeFrom(const SliceLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef SliceLayerParams_SliceAxis SliceAxis;
  static const SliceAxis CHANNEL_AXIS =
    SliceLayerParams_SliceAxis_CHANNEL_AXIS;
  static const SliceAxis HEIGHT_AXIS =
    SliceLayerParams_SliceAxis_HEIGHT_AXIS;
  static const SliceAxis WIDTH_AXIS =
    SliceLayerParams_SliceAxis_WIDTH_AXIS;
  static inline bool SliceAxis_IsValid(int value) {
    return SliceLayerParams_SliceAxis_IsValid(value);
  }
  static const SliceAxis SliceAxis_MIN =
    SliceLayerParams_SliceAxis_SliceAxis_MIN;
  static const SliceAxis SliceAxis_MAX =
    SliceLayerParams_SliceAxis_SliceAxis_MAX;
  static const int SliceAxis_ARRAYSIZE =
    SliceLayerParams_SliceAxis_SliceAxis_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // int64 startIndex = 1;
  void clear_startindex();
  static const int kStartIndexFieldNumber = 1;
  ::google::protobuf::int64 startindex() const;
  void set_startindex(::google::protobuf::int64 value);

  // int64 endIndex = 2;
  void clear_endindex();
  static const int kEndIndexFieldNumber = 2;
  ::google::protobuf::int64 endindex() const;
  void set_endindex(::google::protobuf::int64 value);

  // uint64 stride = 3;
  void clear_stride();
  static const int kStrideFieldNumber = 3;
  ::google::protobuf::uint64 stride() const;
  void set_stride(::google::protobuf::uint64 value);

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  void clear_axis();
  static const int kAxisFieldNumber = 4;
  ::CoreML::Specification::SliceLayerParams_SliceAxis axis() const;
  void set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 startindex_;
  ::google::protobuf::int64 endindex_;
  ::google::protobuf::uint64 stride_;
  int axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLayerParams) */ {
 public:
  ReduceLayerParams();
  virtual ~ReduceLayerParams();

  ReduceLayerParams(const ReduceLayerParams& from);

  inline ReduceLayerParams& operator=(const ReduceLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceLayerParams& default_instance();

  static inline const ReduceLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceLayerParams*>(
               &_ReduceLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    75;

  void Swap(ReduceLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceLayerParams& from);
  void MergeFrom(const ReduceLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef ReduceLayerParams_ReduceOperation ReduceOperation;
  static const ReduceOperation SUM =
    ReduceLayerParams_ReduceOperation_SUM;
  static const ReduceOperation AVG =
    ReduceLayerParams_ReduceOperation_AVG;
  static const ReduceOperation PROD =
    ReduceLayerParams_ReduceOperation_PROD;
  static const ReduceOperation LOGSUM =
    ReduceLayerParams_ReduceOperation_LOGSUM;
  static const ReduceOperation SUMSQUARE =
    ReduceLayerParams_ReduceOperation_SUMSQUARE;
  static const ReduceOperation L1 =
    ReduceLayerParams_ReduceOperation_L1;
  static const ReduceOperation L2 =
    ReduceLayerParams_ReduceOperation_L2;
  static const ReduceOperation MAX =
    ReduceLayerParams_ReduceOperation_MAX;
  static const ReduceOperation MIN =
    ReduceLayerParams_ReduceOperation_MIN;
  static const ReduceOperation ARGMAX =
    ReduceLayerParams_ReduceOperation_ARGMAX;
  static inline bool ReduceOperation_IsValid(int value) {
    return ReduceLayerParams_ReduceOperation_IsValid(value);
  }
  static const ReduceOperation ReduceOperation_MIN =
    ReduceLayerParams_ReduceOperation_ReduceOperation_MIN;
  static const ReduceOperation ReduceOperation_MAX =
    ReduceLayerParams_ReduceOperation_ReduceOperation_MAX;
  static const int ReduceOperation_ARRAYSIZE =
    ReduceLayerParams_ReduceOperation_ReduceOperation_ARRAYSIZE;

  typedef ReduceLayerParams_ReduceAxis ReduceAxis;
  static const ReduceAxis CHW =
    ReduceLayerParams_ReduceAxis_CHW;
  static const ReduceAxis HW =
    ReduceLayerParams_ReduceAxis_HW;
  static const ReduceAxis C =
    ReduceLayerParams_ReduceAxis_C;
  static const ReduceAxis H =
    ReduceLayerParams_ReduceAxis_H;
  static const ReduceAxis W =
    ReduceLayerParams_ReduceAxis_W;
  static inline bool ReduceAxis_IsValid(int value) {
    return ReduceLayerParams_ReduceAxis_IsValid(value);
  }
  static const ReduceAxis ReduceAxis_MIN =
    ReduceLayerParams_ReduceAxis_ReduceAxis_MIN;
  static const ReduceAxis ReduceAxis_MAX =
    ReduceLayerParams_ReduceAxis_ReduceAxis_MAX;
  static const int ReduceAxis_ARRAYSIZE =
    ReduceLayerParams_ReduceAxis_ReduceAxis_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::ReduceLayerParams_ReduceOperation mode() const;
  void set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value);

  // float epsilon = 2;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 2;
  float epsilon() const;
  void set_epsilon(float value);

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  void clear_axis();
  static const int kAxisFieldNumber = 3;
  ::CoreML::Specification::ReduceLayerParams_ReduceAxis axis() const;
  void set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int mode_;
  float epsilon_;
  int axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CropLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CropLayerParams) */ {
 public:
  CropLayerParams();
  virtual ~CropLayerParams();

  CropLayerParams(const CropLayerParams& from);

  inline CropLayerParams& operator=(const CropLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CropLayerParams& default_instance();

  static inline const CropLayerParams* internal_default_instance() {
    return reinterpret_cast<const CropLayerParams*>(
               &_CropLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    76;

  void Swap(CropLayerParams* other);

  // implements Message ----------------------------------------------

  inline CropLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CropLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CropLayerParams& from);
  void MergeFrom(const CropLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CropLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 offset = 5;
  int offset_size() const;
  void clear_offset();
  static const int kOffsetFieldNumber = 5;
  ::google::protobuf::uint64 offset(int index) const;
  void set_offset(int index, ::google::protobuf::uint64 value);
  void add_offset(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      offset() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_offset();

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  bool has_cropamounts() const;
  void clear_cropamounts();
  static const int kCropAmountsFieldNumber = 1;
  const ::CoreML::Specification::BorderAmounts& cropamounts() const;
  ::CoreML::Specification::BorderAmounts* mutable_cropamounts();
  ::CoreML::Specification::BorderAmounts* release_cropamounts();
  void set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CropLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > offset_;
  mutable int _offset_cached_byte_size_;
  ::CoreML::Specification::BorderAmounts* cropamounts_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AverageLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AverageLayerParams) */ {
 public:
  AverageLayerParams();
  virtual ~AverageLayerParams();

  AverageLayerParams(const AverageLayerParams& from);

  inline AverageLayerParams& operator=(const AverageLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AverageLayerParams& default_instance();

  static inline const AverageLayerParams* internal_default_instance() {
    return reinterpret_cast<const AverageLayerParams*>(
               &_AverageLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    77;

  void Swap(AverageLayerParams* other);

  // implements Message ----------------------------------------------

  inline AverageLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AverageLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AverageLayerParams& from);
  void MergeFrom(const AverageLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AverageLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AverageLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MaxLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MaxLayerParams) */ {
 public:
  MaxLayerParams();
  virtual ~MaxLayerParams();

  MaxLayerParams(const MaxLayerParams& from);

  inline MaxLayerParams& operator=(const MaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MaxLayerParams& default_instance();

  static inline const MaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const MaxLayerParams*>(
               &_MaxLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    78;

  void Swap(MaxLayerParams* other);

  // implements Message ----------------------------------------------

  inline MaxLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MaxLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MaxLayerParams& from);
  void MergeFrom(const MaxLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MaxLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MaxLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MinLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MinLayerParams) */ {
 public:
  MinLayerParams();
  virtual ~MinLayerParams();

  MinLayerParams(const MinLayerParams& from);

  inline MinLayerParams& operator=(const MinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MinLayerParams& default_instance();

  static inline const MinLayerParams* internal_default_instance() {
    return reinterpret_cast<const MinLayerParams*>(
               &_MinLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    79;

  void Swap(MinLayerParams* other);

  // implements Message ----------------------------------------------

  inline MinLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MinLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MinLayerParams& from);
  void MergeFrom(const MinLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MinLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MinLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class DotProductLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.DotProductLayerParams) */ {
 public:
  DotProductLayerParams();
  virtual ~DotProductLayerParams();

  DotProductLayerParams(const DotProductLayerParams& from);

  inline DotProductLayerParams& operator=(const DotProductLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const DotProductLayerParams& default_instance();

  static inline const DotProductLayerParams* internal_default_instance() {
    return reinterpret_cast<const DotProductLayerParams*>(
               &_DotProductLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    80;

  void Swap(DotProductLayerParams* other);

  // implements Message ----------------------------------------------

  inline DotProductLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  DotProductLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const DotProductLayerParams& from);
  void MergeFrom(const DotProductLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(DotProductLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // bool cosineSimilarity = 1;
  void clear_cosinesimilarity();
  static const int kCosineSimilarityFieldNumber = 1;
  bool cosinesimilarity() const;
  void set_cosinesimilarity(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.DotProductLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  bool cosinesimilarity_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MeanVarianceNormalizeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MeanVarianceNormalizeLayerParams) */ {
 public:
  MeanVarianceNormalizeLayerParams();
  virtual ~MeanVarianceNormalizeLayerParams();

  MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from);

  inline MeanVarianceNormalizeLayerParams& operator=(const MeanVarianceNormalizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MeanVarianceNormalizeLayerParams& default_instance();

  static inline const MeanVarianceNormalizeLayerParams* internal_default_instance() {
    return reinterpret_cast<const MeanVarianceNormalizeLayerParams*>(
               &_MeanVarianceNormalizeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    81;

  void Swap(MeanVarianceNormalizeLayerParams* other);

  // implements Message ----------------------------------------------

  inline MeanVarianceNormalizeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MeanVarianceNormalizeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MeanVarianceNormalizeLayerParams& from);
  void MergeFrom(const MeanVarianceNormalizeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MeanVarianceNormalizeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // bool acrossChannels = 1;
  void clear_acrosschannels();
  static const int kAcrossChannelsFieldNumber = 1;
  bool acrosschannels() const;
  void set_acrosschannels(bool value);

  // bool normalizeVariance = 2;
  void clear_normalizevariance();
  static const int kNormalizeVarianceFieldNumber = 2;
  bool normalizevariance() const;
  void set_normalizevariance(bool value);

  // float epsilon = 3;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 3;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MeanVarianceNormalizeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  bool acrosschannels_;
  bool normalizevariance_;
  float epsilon_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SequenceRepeatLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SequenceRepeatLayerParams) */ {
 public:
  SequenceRepeatLayerParams();
  virtual ~SequenceRepeatLayerParams();

  SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from);

  inline SequenceRepeatLayerParams& operator=(const SequenceRepeatLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SequenceRepeatLayerParams& default_instance();

  static inline const SequenceRepeatLayerParams* internal_default_instance() {
    return reinterpret_cast<const SequenceRepeatLayerParams*>(
               &_SequenceRepeatLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    82;

  void Swap(SequenceRepeatLayerParams* other);

  // implements Message ----------------------------------------------

  inline SequenceRepeatLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SequenceRepeatLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SequenceRepeatLayerParams& from);
  void MergeFrom(const SequenceRepeatLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SequenceRepeatLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // uint64 nRepetitions = 1;
  void clear_nrepetitions();
  static const int kNRepetitionsFieldNumber = 1;
  ::google::protobuf::uint64 nrepetitions() const;
  void set_nrepetitions(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SequenceRepeatLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::uint64 nrepetitions_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SimpleRecurrentLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SimpleRecurrentLayerParams) */ {
 public:
  SimpleRecurrentLayerParams();
  virtual ~SimpleRecurrentLayerParams();

  SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from);

  inline SimpleRecurrentLayerParams& operator=(const SimpleRecurrentLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SimpleRecurrentLayerParams& default_instance();

  static inline const SimpleRecurrentLayerParams* internal_default_instance() {
    return reinterpret_cast<const SimpleRecurrentLayerParams*>(
               &_SimpleRecurrentLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    83;

  void Swap(SimpleRecurrentLayerParams* other);

  // implements Message ----------------------------------------------

  inline SimpleRecurrentLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SimpleRecurrentLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SimpleRecurrentLayerParams& from);
  void MergeFrom(const SimpleRecurrentLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SimpleRecurrentLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.ActivationParams activation = 10;
  bool has_activation() const;
  void clear_activation();
  static const int kActivationFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activation() const;
  ::CoreML::Specification::ActivationParams* mutable_activation();
  ::CoreML::Specification::ActivationParams* release_activation();
  void set_allocated_activation(::CoreML::Specification::ActivationParams* activation);

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  bool has_weightmatrix() const;
  void clear_weightmatrix();
  static const int kWeightMatrixFieldNumber = 30;
  const ::CoreML::Specification::WeightParams& weightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_weightmatrix();
  ::CoreML::Specification::WeightParams* release_weightmatrix();
  void set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix);

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  bool has_recursionmatrix() const;
  void clear_recursionmatrix();
  static const int kRecursionMatrixFieldNumber = 31;
  const ::CoreML::Specification::WeightParams& recursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_recursionmatrix();
  ::CoreML::Specification::WeightParams* release_recursionmatrix();
  void set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix);

  // .CoreML.Specification.WeightParams biasVector = 32;
  bool has_biasvector() const;
  void clear_biasvector();
  static const int kBiasVectorFieldNumber = 32;
  const ::CoreML::Specification::WeightParams& biasvector() const;
  ::CoreML::Specification::WeightParams* mutable_biasvector();
  ::CoreML::Specification::WeightParams* release_biasvector();
  void set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector);

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // bool sequenceOutput = 15;
  void clear_sequenceoutput();
  static const int kSequenceOutputFieldNumber = 15;
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);

  // bool hasBiasVector = 20;
  void clear_hasbiasvector();
  static const int kHasBiasVectorFieldNumber = 20;
  bool hasbiasvector() const;
  void set_hasbiasvector(bool value);

  // bool reverseInput = 100;
  void clear_reverseinput();
  static const int kReverseInputFieldNumber = 100;
  bool reverseinput() const;
  void set_reverseinput(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SimpleRecurrentLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::ActivationParams* activation_;
  ::CoreML::Specification::WeightParams* weightmatrix_;
  ::CoreML::Specification::WeightParams* recursionmatrix_;
  ::CoreML::Specification::WeightParams* biasvector_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  bool sequenceoutput_;
  bool hasbiasvector_;
  bool reverseinput_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GRULayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GRULayerParams) */ {
 public:
  GRULayerParams();
  virtual ~GRULayerParams();

  GRULayerParams(const GRULayerParams& from);

  inline GRULayerParams& operator=(const GRULayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GRULayerParams& default_instance();

  static inline const GRULayerParams* internal_default_instance() {
    return reinterpret_cast<const GRULayerParams*>(
               &_GRULayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    84;

  void Swap(GRULayerParams* other);

  // implements Message ----------------------------------------------

  inline GRULayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GRULayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GRULayerParams& from);
  void MergeFrom(const GRULayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GRULayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  int activations_size() const;
  void clear_activations();
  static const int kActivationsFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activations(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activations(int index);
  ::CoreML::Specification::ActivationParams* add_activations();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activations();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activations() const;

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  bool has_updategateweightmatrix() const;
  void clear_updategateweightmatrix();
  static const int kUpdateGateWeightMatrixFieldNumber = 30;
  const ::CoreML::Specification::WeightParams& updategateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_updategateweightmatrix();
  ::CoreML::Specification::WeightParams* release_updategateweightmatrix();
  void set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix);

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  bool has_resetgateweightmatrix() const;
  void clear_resetgateweightmatrix();
  static const int kResetGateWeightMatrixFieldNumber = 31;
  const ::CoreML::Specification::WeightParams& resetgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_resetgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_resetgateweightmatrix();
  void set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix);

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  bool has_outputgateweightmatrix() const;
  void clear_outputgateweightmatrix();
  static const int kOutputGateWeightMatrixFieldNumber = 32;
  const ::CoreML::Specification::WeightParams& outputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_outputgateweightmatrix();
  void set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix);

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  bool has_updategaterecursionmatrix() const;
  void clear_updategaterecursionmatrix();
  static const int kUpdateGateRecursionMatrixFieldNumber = 50;
  const ::CoreML::Specification::WeightParams& updategaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_updategaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_updategaterecursionmatrix();
  void set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix);

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  bool has_resetgaterecursionmatrix() const;
  void clear_resetgaterecursionmatrix();
  static const int kResetGateRecursionMatrixFieldNumber = 51;
  const ::CoreML::Specification::WeightParams& resetgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_resetgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_resetgaterecursionmatrix();
  void set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix);

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  bool has_outputgaterecursionmatrix() const;
  void clear_outputgaterecursionmatrix();
  static const int kOutputGateRecursionMatrixFieldNumber = 52;
  const ::CoreML::Specification::WeightParams& outputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_outputgaterecursionmatrix();
  void set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix);

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  bool has_updategatebiasvector() const;
  void clear_updategatebiasvector();
  static const int kUpdateGateBiasVectorFieldNumber = 70;
  const ::CoreML::Specification::WeightParams& updategatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_updategatebiasvector();
  ::CoreML::Specification::WeightParams* release_updategatebiasvector();
  void set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector);

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  bool has_resetgatebiasvector() const;
  void clear_resetgatebiasvector();
  static const int kResetGateBiasVectorFieldNumber = 71;
  const ::CoreML::Specification::WeightParams& resetgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_resetgatebiasvector();
  ::CoreML::Specification::WeightParams* release_resetgatebiasvector();
  void set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector);

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  bool has_outputgatebiasvector() const;
  void clear_outputgatebiasvector();
  static const int kOutputGateBiasVectorFieldNumber = 72;
  const ::CoreML::Specification::WeightParams& outputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_outputgatebiasvector();
  ::CoreML::Specification::WeightParams* release_outputgatebiasvector();
  void set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector);

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // bool sequenceOutput = 15;
  void clear_sequenceoutput();
  static const int kSequenceOutputFieldNumber = 15;
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);

  // bool hasBiasVectors = 20;
  void clear_hasbiasvectors();
  static const int kHasBiasVectorsFieldNumber = 20;
  bool hasbiasvectors() const;
  void set_hasbiasvectors(bool value);

  // bool reverseInput = 100;
  void clear_reverseinput();
  static const int kReverseInputFieldNumber = 100;
  bool reverseinput() const;
  void set_reverseinput(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GRULayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activations_;
  ::CoreML::Specification::WeightParams* updategateweightmatrix_;
  ::CoreML::Specification::WeightParams* resetgateweightmatrix_;
  ::CoreML::Specification::WeightParams* outputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* updategaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* resetgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* outputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* updategatebiasvector_;
  ::CoreML::Specification::WeightParams* resetgatebiasvector_;
  ::CoreML::Specification::WeightParams* outputgatebiasvector_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  bool sequenceoutput_;
  bool hasbiasvectors_;
  bool reverseinput_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LSTMParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LSTMParams) */ {
 public:
  LSTMParams();
  virtual ~LSTMParams();

  LSTMParams(const LSTMParams& from);

  inline LSTMParams& operator=(const LSTMParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LSTMParams& default_instance();

  static inline const LSTMParams* internal_default_instance() {
    return reinterpret_cast<const LSTMParams*>(
               &_LSTMParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    85;

  void Swap(LSTMParams* other);

  // implements Message ----------------------------------------------

  inline LSTMParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LSTMParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LSTMParams& from);
  void MergeFrom(const LSTMParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LSTMParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float cellClipThreshold = 60;
  void clear_cellclipthreshold();
  static const int kCellClipThresholdFieldNumber = 60;
  float cellclipthreshold() const;
  void set_cellclipthreshold(float value);

  // bool sequenceOutput = 10;
  void clear_sequenceoutput();
  static const int kSequenceOutputFieldNumber = 10;
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);

  // bool hasBiasVectors = 20;
  void clear_hasbiasvectors();
  static const int kHasBiasVectorsFieldNumber = 20;
  bool hasbiasvectors() const;
  void set_hasbiasvectors(bool value);

  // bool forgetBias = 30;
  void clear_forgetbias();
  static const int kForgetBiasFieldNumber = 30;
  bool forgetbias() const;
  void set_forgetbias(bool value);

  // bool hasPeepholeVectors = 40;
  void clear_haspeepholevectors();
  static const int kHasPeepholeVectorsFieldNumber = 40;
  bool haspeepholevectors() const;
  void set_haspeepholevectors(bool value);

  // bool coupledInputAndForgetGate = 50;
  void clear_coupledinputandforgetgate();
  static const int kCoupledInputAndForgetGateFieldNumber = 50;
  bool coupledinputandforgetgate() const;
  void set_coupledinputandforgetgate(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LSTMParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float cellclipthreshold_;
  bool sequenceoutput_;
  bool hasbiasvectors_;
  bool forgetbias_;
  bool haspeepholevectors_;
  bool coupledinputandforgetgate_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LSTMWeightParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LSTMWeightParams) */ {
 public:
  LSTMWeightParams();
  virtual ~LSTMWeightParams();

  LSTMWeightParams(const LSTMWeightParams& from);

  inline LSTMWeightParams& operator=(const LSTMWeightParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LSTMWeightParams& default_instance();

  static inline const LSTMWeightParams* internal_default_instance() {
    return reinterpret_cast<const LSTMWeightParams*>(
               &_LSTMWeightParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    86;

  void Swap(LSTMWeightParams* other);

  // implements Message ----------------------------------------------

  inline LSTMWeightParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LSTMWeightParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LSTMWeightParams& from);
  void MergeFrom(const LSTMWeightParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LSTMWeightParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  bool has_inputgateweightmatrix() const;
  void clear_inputgateweightmatrix();
  static const int kInputGateWeightMatrixFieldNumber = 1;
  const ::CoreML::Specification::WeightParams& inputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_inputgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_inputgateweightmatrix();
  void set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix);

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  bool has_forgetgateweightmatrix() const;
  void clear_forgetgateweightmatrix();
  static const int kForgetGateWeightMatrixFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& forgetgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_forgetgateweightmatrix();
  void set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix);

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  bool has_blockinputweightmatrix() const;
  void clear_blockinputweightmatrix();
  static const int kBlockInputWeightMatrixFieldNumber = 3;
  const ::CoreML::Specification::WeightParams& blockinputweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_blockinputweightmatrix();
  ::CoreML::Specification::WeightParams* release_blockinputweightmatrix();
  void set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix);

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  bool has_outputgateweightmatrix() const;
  void clear_outputgateweightmatrix();
  static const int kOutputGateWeightMatrixFieldNumber = 4;
  const ::CoreML::Specification::WeightParams& outputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_outputgateweightmatrix();
  void set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix);

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  bool has_inputgaterecursionmatrix() const;
  void clear_inputgaterecursionmatrix();
  static const int kInputGateRecursionMatrixFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& inputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_inputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_inputgaterecursionmatrix();
  void set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix);

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  bool has_forgetgaterecursionmatrix() const;
  void clear_forgetgaterecursionmatrix();
  static const int kForgetGateRecursionMatrixFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& forgetgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_forgetgaterecursionmatrix();
  void set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix);

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  bool has_blockinputrecursionmatrix() const;
  void clear_blockinputrecursionmatrix();
  static const int kBlockInputRecursionMatrixFieldNumber = 22;
  const ::CoreML::Specification::WeightParams& blockinputrecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_blockinputrecursionmatrix();
  ::CoreML::Specification::WeightParams* release_blockinputrecursionmatrix();
  void set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix);

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  bool has_outputgaterecursionmatrix() const;
  void clear_outputgaterecursionmatrix();
  static const int kOutputGateRecursionMatrixFieldNumber = 23;
  const ::CoreML::Specification::WeightParams& outputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_outputgaterecursionmatrix();
  void set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix);

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  bool has_inputgatebiasvector() const;
  void clear_inputgatebiasvector();
  static const int kInputGateBiasVectorFieldNumber = 40;
  const ::CoreML::Specification::WeightParams& inputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_inputgatebiasvector();
  ::CoreML::Specification::WeightParams* release_inputgatebiasvector();
  void set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector);

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  bool has_forgetgatebiasvector() const;
  void clear_forgetgatebiasvector();
  static const int kForgetGateBiasVectorFieldNumber = 41;
  const ::CoreML::Specification::WeightParams& forgetgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgatebiasvector();
  ::CoreML::Specification::WeightParams* release_forgetgatebiasvector();
  void set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector);

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  bool has_blockinputbiasvector() const;
  void clear_blockinputbiasvector();
  static const int kBlockInputBiasVectorFieldNumber = 42;
  const ::CoreML::Specification::WeightParams& blockinputbiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_blockinputbiasvector();
  ::CoreML::Specification::WeightParams* release_blockinputbiasvector();
  void set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector);

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  bool has_outputgatebiasvector() const;
  void clear_outputgatebiasvector();
  static const int kOutputGateBiasVectorFieldNumber = 43;
  const ::CoreML::Specification::WeightParams& outputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_outputgatebiasvector();
  ::CoreML::Specification::WeightParams* release_outputgatebiasvector();
  void set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector);

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  bool has_inputgatepeepholevector() const;
  void clear_inputgatepeepholevector();
  static const int kInputGatePeepholeVectorFieldNumber = 60;
  const ::CoreML::Specification::WeightParams& inputgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* mutable_inputgatepeepholevector();
  ::CoreML::Specification::WeightParams* release_inputgatepeepholevector();
  void set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector);

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  bool has_forgetgatepeepholevector() const;
  void clear_forgetgatepeepholevector();
  static const int kForgetGatePeepholeVectorFieldNumber = 61;
  const ::CoreML::Specification::WeightParams& forgetgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgatepeepholevector();
  ::CoreML::Specification::WeightParams* release_forgetgatepeepholevector();
  void set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector);

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  bool has_outputgatepeepholevector() const;
  void clear_outputgatepeepholevector();
  static const int kOutputGatePeepholeVectorFieldNumber = 62;
  const ::CoreML::Specification::WeightParams& outputgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* mutable_outputgatepeepholevector();
  ::CoreML::Specification::WeightParams* release_outputgatepeepholevector();
  void set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LSTMWeightParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* inputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* forgetgateweightmatrix_;
  ::CoreML::Specification::WeightParams* blockinputweightmatrix_;
  ::CoreML::Specification::WeightParams* outputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* inputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* forgetgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* blockinputrecursionmatrix_;
  ::CoreML::Specification::WeightParams* outputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* inputgatebiasvector_;
  ::CoreML::Specification::WeightParams* forgetgatebiasvector_;
  ::CoreML::Specification::WeightParams* blockinputbiasvector_;
  ::CoreML::Specification::WeightParams* outputgatebiasvector_;
  ::CoreML::Specification::WeightParams* inputgatepeepholevector_;
  ::CoreML::Specification::WeightParams* forgetgatepeepholevector_;
  ::CoreML::Specification::WeightParams* outputgatepeepholevector_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class UniDirectionalLSTMLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UniDirectionalLSTMLayerParams) */ {
 public:
  UniDirectionalLSTMLayerParams();
  virtual ~UniDirectionalLSTMLayerParams();

  UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from);

  inline UniDirectionalLSTMLayerParams& operator=(const UniDirectionalLSTMLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UniDirectionalLSTMLayerParams& default_instance();

  static inline const UniDirectionalLSTMLayerParams* internal_default_instance() {
    return reinterpret_cast<const UniDirectionalLSTMLayerParams*>(
               &_UniDirectionalLSTMLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    87;

  void Swap(UniDirectionalLSTMLayerParams* other);

  // implements Message ----------------------------------------------

  inline UniDirectionalLSTMLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  UniDirectionalLSTMLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const UniDirectionalLSTMLayerParams& from);
  void MergeFrom(const UniDirectionalLSTMLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UniDirectionalLSTMLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  int activations_size() const;
  void clear_activations();
  static const int kActivationsFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activations(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activations(int index);
  ::CoreML::Specification::ActivationParams* add_activations();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activations();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activations() const;

  // .CoreML.Specification.LSTMParams params = 15;
  bool has_params() const;
  void clear_params();
  static const int kParamsFieldNumber = 15;
  const ::CoreML::Specification::LSTMParams& params() const;
  ::CoreML::Specification::LSTMParams* mutable_params();
  ::CoreML::Specification::LSTMParams* release_params();
  void set_allocated_params(::CoreML::Specification::LSTMParams* params);

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  bool has_weightparams() const;
  void clear_weightparams();
  static const int kWeightParamsFieldNumber = 20;
  const ::CoreML::Specification::LSTMWeightParams& weightparams() const;
  ::CoreML::Specification::LSTMWeightParams* mutable_weightparams();
  ::CoreML::Specification::LSTMWeightParams* release_weightparams();
  void set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams);

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // bool reverseInput = 100;
  void clear_reverseinput();
  static const int kReverseInputFieldNumber = 100;
  bool reverseinput() const;
  void set_reverseinput(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UniDirectionalLSTMLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activations_;
  ::CoreML::Specification::LSTMParams* params_;
  ::CoreML::Specification::LSTMWeightParams* weightparams_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  bool reverseinput_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BiDirectionalLSTMLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BiDirectionalLSTMLayerParams) */ {
 public:
  BiDirectionalLSTMLayerParams();
  virtual ~BiDirectionalLSTMLayerParams();

  BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from);

  inline BiDirectionalLSTMLayerParams& operator=(const BiDirectionalLSTMLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BiDirectionalLSTMLayerParams& default_instance();

  static inline const BiDirectionalLSTMLayerParams* internal_default_instance() {
    return reinterpret_cast<const BiDirectionalLSTMLayerParams*>(
               &_BiDirectionalLSTMLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    88;

  void Swap(BiDirectionalLSTMLayerParams* other);

  // implements Message ----------------------------------------------

  inline BiDirectionalLSTMLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BiDirectionalLSTMLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BiDirectionalLSTMLayerParams& from);
  void MergeFrom(const BiDirectionalLSTMLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BiDirectionalLSTMLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  int activationsforwardlstm_size() const;
  void clear_activationsforwardlstm();
  static const int kActivationsForwardLSTMFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activationsforwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activationsforwardlstm(int index);
  ::CoreML::Specification::ActivationParams* add_activationsforwardlstm();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activationsforwardlstm();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activationsforwardlstm() const;

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  int activationsbackwardlstm_size() const;
  void clear_activationsbackwardlstm();
  static const int kActivationsBackwardLSTMFieldNumber = 11;
  const ::CoreML::Specification::ActivationParams& activationsbackwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activationsbackwardlstm(int index);
  ::CoreML::Specification::ActivationParams* add_activationsbackwardlstm();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activationsbackwardlstm();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activationsbackwardlstm() const;

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  int weightparams_size() const;
  void clear_weightparams();
  static const int kWeightParamsFieldNumber = 20;
  const ::CoreML::Specification::LSTMWeightParams& weightparams(int index) const;
  ::CoreML::Specification::LSTMWeightParams* mutable_weightparams(int index);
  ::CoreML::Specification::LSTMWeightParams* add_weightparams();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
      mutable_weightparams();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
      weightparams() const;

  // .CoreML.Specification.LSTMParams params = 15;
  bool has_params() const;
  void clear_params();
  static const int kParamsFieldNumber = 15;
  const ::CoreML::Specification::LSTMParams& params() const;
  ::CoreML::Specification::LSTMParams* mutable_params();
  ::CoreML::Specification::LSTMParams* release_params();
  void set_allocated_params(::CoreML::Specification::LSTMParams* params);

  // uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BiDirectionalLSTMLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activationsforwardlstm_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activationsbackwardlstm_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams > weightparams_;
  ::CoreML::Specification::LSTMParams* params_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CustomLayerParams_CustomLayerParamValue : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CustomLayerParams.CustomLayerParamValue) */ {
 public:
  CustomLayerParams_CustomLayerParamValue();
  virtual ~CustomLayerParams_CustomLayerParamValue();

  CustomLayerParams_CustomLayerParamValue(const CustomLayerParams_CustomLayerParamValue& from);

  inline CustomLayerParams_CustomLayerParamValue& operator=(const CustomLayerParams_CustomLayerParamValue& from) {
    CopyFrom(from);
    return *this;
  }

  static const CustomLayerParams_CustomLayerParamValue& default_instance();

  enum ValueCase {
    kDoubleValue = 10,
    kStringValue = 20,
    kIntValue = 30,
    kLongValue = 40,
    kBoolValue = 50,
    VALUE_NOT_SET = 0,
  };

  static inline const CustomLayerParams_CustomLayerParamValue* internal_default_instance() {
    return reinterpret_cast<const CustomLayerParams_CustomLayerParamValue*>(
               &_CustomLayerParams_CustomLayerParamValue_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    89;

  void Swap(CustomLayerParams_CustomLayerParamValue* other);

  // implements Message ----------------------------------------------

  inline CustomLayerParams_CustomLayerParamValue* New() const PROTOBUF_FINAL { return New(NULL); }

  CustomLayerParams_CustomLayerParamValue* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CustomLayerParams_CustomLayerParamValue& from);
  void MergeFrom(const CustomLayerParams_CustomLayerParamValue& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CustomLayerParams_CustomLayerParamValue* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // double doubleValue = 10;
  private:
  bool has_doublevalue() const;
  public:
  void clear_doublevalue();
  static const int kDoubleValueFieldNumber = 10;
  double doublevalue() const;
  void set_doublevalue(double value);

  // string stringValue = 20;
  private:
  bool has_stringvalue() const;
  public:
  void clear_stringvalue();
  static const int kStringValueFieldNumber = 20;
  const ::std::string& stringvalue() const;
  void set_stringvalue(const ::std::string& value);
  #if LANG_CXX11
  void set_stringvalue(::std::string&& value);
  #endif
  void set_stringvalue(const char* value);
  void set_stringvalue(const char* value, size_t size);
  ::std::string* mutable_stringvalue();
  ::std::string* release_stringvalue();
  void set_allocated_stringvalue(::std::string* stringvalue);

  // int32 intValue = 30;
  private:
  bool has_intvalue() const;
  public:
  void clear_intvalue();
  static const int kIntValueFieldNumber = 30;
  ::google::protobuf::int32 intvalue() const;
  void set_intvalue(::google::protobuf::int32 value);

  // int64 longValue = 40;
  private:
  bool has_longvalue() const;
  public:
  void clear_longvalue();
  static const int kLongValueFieldNumber = 40;
  ::google::protobuf::int64 longvalue() const;
  void set_longvalue(::google::protobuf::int64 value);

  // bool boolValue = 50;
  private:
  bool has_boolvalue() const;
  public:
  void clear_boolvalue();
  static const int kBoolValueFieldNumber = 50;
  bool boolvalue() const;
  void set_boolvalue(bool value);

  ValueCase value_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
 private:
  void set_has_doublevalue();
  void set_has_stringvalue();
  void set_has_intvalue();
  void set_has_longvalue();
  void set_has_boolvalue();

  inline bool has_value() const;
  void clear_value();
  inline void clear_has_value();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  union ValueUnion {
    ValueUnion() {}
    double doublevalue_;
    ::google::protobuf::internal::ArenaStringPtr stringvalue_;
    ::google::protobuf::int32 intvalue_;
    ::google::protobuf::int64 longvalue_;
    bool boolvalue_;
  } value_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------


// -------------------------------------------------------------------

class CustomLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CustomLayerParams) */ {
 public:
  CustomLayerParams();
  virtual ~CustomLayerParams();

  CustomLayerParams(const CustomLayerParams& from);

  inline CustomLayerParams& operator=(const CustomLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CustomLayerParams& default_instance();

  static inline const CustomLayerParams* internal_default_instance() {
    return reinterpret_cast<const CustomLayerParams*>(
               &_CustomLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    91;

  void Swap(CustomLayerParams* other);

  // implements Message ----------------------------------------------

  inline CustomLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CustomLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CustomLayerParams& from);
  void MergeFrom(const CustomLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CustomLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef CustomLayerParams_CustomLayerParamValue CustomLayerParamValue;

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.WeightParams weights = 20;
  int weights_size() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& weights(int index) const;
  ::CoreML::Specification::WeightParams* mutable_weights(int index);
  ::CoreML::Specification::WeightParams* add_weights();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
      mutable_weights();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
      weights() const;

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  int parameters_size() const;
  void clear_parameters();
  static const int kParametersFieldNumber = 30;
  const ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
      parameters() const;
  ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
      mutable_parameters();

  // string className = 10;
  void clear_classname();
  static const int kClassNameFieldNumber = 10;
  const ::std::string& classname() const;
  void set_classname(const ::std::string& value);
  #if LANG_CXX11
  void set_classname(::std::string&& value);
  #endif
  void set_classname(const char* value);
  void set_classname(const char* value, size_t size);
  ::std::string* mutable_classname();
  ::std::string* release_classname();
  void set_allocated_classname(::std::string* classname);

  // string description = 40;
  void clear_description();
  static const int kDescriptionFieldNumber = 40;
  const ::std::string& description() const;
  void set_description(const ::std::string& value);
  #if LANG_CXX11
  void set_description(::std::string&& value);
  #endif
  void set_description(const char* value);
  void set_description(const char* value, size_t size);
  ::std::string* mutable_description();
  ::std::string* release_description();
  void set_allocated_description(::std::string* description);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CustomLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams > weights_;
  public:
  typedef ::google::protobuf::internal::MapEntryLite<
      ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 >
      CustomLayerParams_ParametersEntry;
  ::google::protobuf::internal::MapFieldLite<
      CustomLayerParams_ParametersEntry,
      ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > parameters_;
  private:
  ::google::protobuf::internal::ArenaStringPtr classname_;
  ::google::protobuf::internal::ArenaStringPtr description_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class TransposeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TransposeLayerParams) */ {
 public:
  TransposeLayerParams();
  virtual ~TransposeLayerParams();

  TransposeLayerParams(const TransposeLayerParams& from);

  inline TransposeLayerParams& operator=(const TransposeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const TransposeLayerParams& default_instance();

  static inline const TransposeLayerParams* internal_default_instance() {
    return reinterpret_cast<const TransposeLayerParams*>(
               &_TransposeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    92;

  void Swap(TransposeLayerParams* other);

  // implements Message ----------------------------------------------

  inline TransposeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  TransposeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const TransposeLayerParams& from);
  void MergeFrom(const TransposeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TransposeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::uint64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::uint64 value);
  void add_axes(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_axes();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TransposeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > axes_;
  mutable int _axes_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BatchedMatMulLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BatchedMatMulLayerParams) */ {
 public:
  BatchedMatMulLayerParams();
  virtual ~BatchedMatMulLayerParams();

  BatchedMatMulLayerParams(const BatchedMatMulLayerParams& from);

  inline BatchedMatMulLayerParams& operator=(const BatchedMatMulLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BatchedMatMulLayerParams& default_instance();

  static inline const BatchedMatMulLayerParams* internal_default_instance() {
    return reinterpret_cast<const BatchedMatMulLayerParams*>(
               &_BatchedMatMulLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    93;

  void Swap(BatchedMatMulLayerParams* other);

  // implements Message ----------------------------------------------

  inline BatchedMatMulLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BatchedMatMulLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BatchedMatMulLayerParams& from);
  void MergeFrom(const BatchedMatMulLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BatchedMatMulLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.WeightParams weights = 8;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 8;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // .CoreML.Specification.WeightParams bias = 9;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 9;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // uint64 weightMatrixFirstDimension = 5;
  void clear_weightmatrixfirstdimension();
  static const int kWeightMatrixFirstDimensionFieldNumber = 5;
  ::google::protobuf::uint64 weightmatrixfirstdimension() const;
  void set_weightmatrixfirstdimension(::google::protobuf::uint64 value);

  // uint64 weightMatrixSecondDimension = 6;
  void clear_weightmatrixseconddimension();
  static const int kWeightMatrixSecondDimensionFieldNumber = 6;
  ::google::protobuf::uint64 weightmatrixseconddimension() const;
  void set_weightmatrixseconddimension(::google::protobuf::uint64 value);

  // bool transposeA = 1;
  void clear_transposea();
  static const int kTransposeAFieldNumber = 1;
  bool transposea() const;
  void set_transposea(bool value);

  // bool transposeB = 2;
  void clear_transposeb();
  static const int kTransposeBFieldNumber = 2;
  bool transposeb() const;
  void set_transposeb(bool value);

  // bool hasBias = 7;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 7;
  bool hasbias() const;
  void set_hasbias(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BatchedMatMulLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 weightmatrixfirstdimension_;
  ::google::protobuf::uint64 weightmatrixseconddimension_;
  bool transposea_;
  bool transposeb_;
  bool hasbias_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ConcatNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConcatNDLayerParams) */ {
 public:
  ConcatNDLayerParams();
  virtual ~ConcatNDLayerParams();

  ConcatNDLayerParams(const ConcatNDLayerParams& from);

  inline ConcatNDLayerParams& operator=(const ConcatNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ConcatNDLayerParams& default_instance();

  static inline const ConcatNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConcatNDLayerParams*>(
               &_ConcatNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    94;

  void Swap(ConcatNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline ConcatNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ConcatNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ConcatNDLayerParams& from);
  void MergeFrom(const ConcatNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConcatNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConcatNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SoftmaxNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SoftmaxNDLayerParams) */ {
 public:
  SoftmaxNDLayerParams();
  virtual ~SoftmaxNDLayerParams();

  SoftmaxNDLayerParams(const SoftmaxNDLayerParams& from);

  inline SoftmaxNDLayerParams& operator=(const SoftmaxNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SoftmaxNDLayerParams& default_instance();

  static inline const SoftmaxNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const SoftmaxNDLayerParams*>(
               &_SoftmaxNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    95;

  void Swap(SoftmaxNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline SoftmaxNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SoftmaxNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SoftmaxNDLayerParams& from);
  void MergeFrom(const SoftmaxNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SoftmaxNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SoftmaxNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReverseLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReverseLayerParams) */ {
 public:
  ReverseLayerParams();
  virtual ~ReverseLayerParams();

  ReverseLayerParams(const ReverseLayerParams& from);

  inline ReverseLayerParams& operator=(const ReverseLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReverseLayerParams& default_instance();

  static inline const ReverseLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReverseLayerParams*>(
               &_ReverseLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    96;

  void Swap(ReverseLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReverseLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReverseLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReverseLayerParams& from);
  void MergeFrom(const ReverseLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReverseLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated bool reverseDim = 1;
  int reversedim_size() const;
  void clear_reversedim();
  static const int kReverseDimFieldNumber = 1;
  bool reversedim(int index) const;
  void set_reversedim(int index, bool value);
  void add_reversedim(bool value);
  const ::google::protobuf::RepeatedField< bool >&
      reversedim() const;
  ::google::protobuf::RepeatedField< bool >*
      mutable_reversedim();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReverseLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< bool > reversedim_;
  mutable int _reversedim_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReverseSeqLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReverseSeqLayerParams) */ {
 public:
  ReverseSeqLayerParams();
  virtual ~ReverseSeqLayerParams();

  ReverseSeqLayerParams(const ReverseSeqLayerParams& from);

  inline ReverseSeqLayerParams& operator=(const ReverseSeqLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReverseSeqLayerParams& default_instance();

  static inline const ReverseSeqLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReverseSeqLayerParams*>(
               &_ReverseSeqLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    97;

  void Swap(ReverseSeqLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReverseSeqLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReverseSeqLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReverseSeqLayerParams& from);
  void MergeFrom(const ReverseSeqLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReverseSeqLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 batchAxis = 1;
  void clear_batchaxis();
  static const int kBatchAxisFieldNumber = 1;
  ::google::protobuf::int64 batchaxis() const;
  void set_batchaxis(::google::protobuf::int64 value);

  // int64 sequenceAxis = 2;
  void clear_sequenceaxis();
  static const int kSequenceAxisFieldNumber = 2;
  ::google::protobuf::int64 sequenceaxis() const;
  void set_sequenceaxis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReverseSeqLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 batchaxis_;
  ::google::protobuf::int64 sequenceaxis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LoadConstantNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoadConstantNDLayerParams) */ {
 public:
  LoadConstantNDLayerParams();
  virtual ~LoadConstantNDLayerParams();

  LoadConstantNDLayerParams(const LoadConstantNDLayerParams& from);

  inline LoadConstantNDLayerParams& operator=(const LoadConstantNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LoadConstantNDLayerParams& default_instance();

  static inline const LoadConstantNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const LoadConstantNDLayerParams*>(
               &_LoadConstantNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    98;

  void Swap(LoadConstantNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline LoadConstantNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LoadConstantNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LoadConstantNDLayerParams& from);
  void MergeFrom(const LoadConstantNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoadConstantNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shape = 1;
  int shape_size() const;
  void clear_shape();
  static const int kShapeFieldNumber = 1;
  ::google::protobuf::uint64 shape(int index) const;
  void set_shape(int index, ::google::protobuf::uint64 value);
  void add_shape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shape();

  // .CoreML.Specification.WeightParams data = 2;
  bool has_data() const;
  void clear_data();
  static const int kDataFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& data() const;
  ::CoreML::Specification::WeightParams* mutable_data();
  ::CoreML::Specification::WeightParams* release_data();
  void set_allocated_data(::CoreML::Specification::WeightParams* data);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoadConstantNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shape_;
  mutable int _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* data_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FillLikeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FillLikeLayerParams) */ {
 public:
  FillLikeLayerParams();
  virtual ~FillLikeLayerParams();

  FillLikeLayerParams(const FillLikeLayerParams& from);

  inline FillLikeLayerParams& operator=(const FillLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FillLikeLayerParams& default_instance();

  static inline const FillLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const FillLikeLayerParams*>(
               &_FillLikeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    99;

  void Swap(FillLikeLayerParams* other);

  // implements Message ----------------------------------------------

  inline FillLikeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FillLikeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FillLikeLayerParams& from);
  void MergeFrom(const FillLikeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FillLikeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float value = 1;
  void clear_value();
  static const int kValueFieldNumber = 1;
  float value() const;
  void set_value(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FillLikeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float value_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FillStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FillStaticLayerParams) */ {
 public:
  FillStaticLayerParams();
  virtual ~FillStaticLayerParams();

  FillStaticLayerParams(const FillStaticLayerParams& from);

  inline FillStaticLayerParams& operator=(const FillStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FillStaticLayerParams& default_instance();

  static inline const FillStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const FillStaticLayerParams*>(
               &_FillStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    100;

  void Swap(FillStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline FillStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FillStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FillStaticLayerParams& from);
  void MergeFrom(const FillStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FillStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 targetShape = 2;
  int targetshape_size() const;
  void clear_targetshape();
  static const int kTargetShapeFieldNumber = 2;
  ::google::protobuf::uint64 targetshape(int index) const;
  void set_targetshape(int index, ::google::protobuf::uint64 value);
  void add_targetshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      targetshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_targetshape();

  // float value = 1;
  void clear_value();
  static const int kValueFieldNumber = 1;
  float value() const;
  void set_value(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FillStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > targetshape_;
  mutable int _targetshape_cached_byte_size_;
  float value_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FillDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FillDynamicLayerParams) */ {
 public:
  FillDynamicLayerParams();
  virtual ~FillDynamicLayerParams();

  FillDynamicLayerParams(const FillDynamicLayerParams& from);

  inline FillDynamicLayerParams& operator=(const FillDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FillDynamicLayerParams& default_instance();

  static inline const FillDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const FillDynamicLayerParams*>(
               &_FillDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    101;

  void Swap(FillDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline FillDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FillDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FillDynamicLayerParams& from);
  void MergeFrom(const FillDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FillDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float value = 1;
  void clear_value();
  static const int kValueFieldNumber = 1;
  float value() const;
  void set_value(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FillDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float value_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class WhereBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WhereBroadcastableLayerParams) */ {
 public:
  WhereBroadcastableLayerParams();
  virtual ~WhereBroadcastableLayerParams();

  WhereBroadcastableLayerParams(const WhereBroadcastableLayerParams& from);

  inline WhereBroadcastableLayerParams& operator=(const WhereBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const WhereBroadcastableLayerParams& default_instance();

  static inline const WhereBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const WhereBroadcastableLayerParams*>(
               &_WhereBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    102;

  void Swap(WhereBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline WhereBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  WhereBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const WhereBroadcastableLayerParams& from);
  void MergeFrom(const WhereBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WhereBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WhereBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SinLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SinLayerParams) */ {
 public:
  SinLayerParams();
  virtual ~SinLayerParams();

  SinLayerParams(const SinLayerParams& from);

  inline SinLayerParams& operator=(const SinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SinLayerParams& default_instance();

  static inline const SinLayerParams* internal_default_instance() {
    return reinterpret_cast<const SinLayerParams*>(
               &_SinLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    103;

  void Swap(SinLayerParams* other);

  // implements Message ----------------------------------------------

  inline SinLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SinLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SinLayerParams& from);
  void MergeFrom(const SinLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SinLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SinLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CosLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CosLayerParams) */ {
 public:
  CosLayerParams();
  virtual ~CosLayerParams();

  CosLayerParams(const CosLayerParams& from);

  inline CosLayerParams& operator=(const CosLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CosLayerParams& default_instance();

  static inline const CosLayerParams* internal_default_instance() {
    return reinterpret_cast<const CosLayerParams*>(
               &_CosLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    104;

  void Swap(CosLayerParams* other);

  // implements Message ----------------------------------------------

  inline CosLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CosLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CosLayerParams& from);
  void MergeFrom(const CosLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CosLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CosLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class TanLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TanLayerParams) */ {
 public:
  TanLayerParams();
  virtual ~TanLayerParams();

  TanLayerParams(const TanLayerParams& from);

  inline TanLayerParams& operator=(const TanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const TanLayerParams& default_instance();

  static inline const TanLayerParams* internal_default_instance() {
    return reinterpret_cast<const TanLayerParams*>(
               &_TanLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    105;

  void Swap(TanLayerParams* other);

  // implements Message ----------------------------------------------

  inline TanLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  TanLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const TanLayerParams& from);
  void MergeFrom(const TanLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TanLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TanLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AsinLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AsinLayerParams) */ {
 public:
  AsinLayerParams();
  virtual ~AsinLayerParams();

  AsinLayerParams(const AsinLayerParams& from);

  inline AsinLayerParams& operator=(const AsinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AsinLayerParams& default_instance();

  static inline const AsinLayerParams* internal_default_instance() {
    return reinterpret_cast<const AsinLayerParams*>(
               &_AsinLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    106;

  void Swap(AsinLayerParams* other);

  // implements Message ----------------------------------------------

  inline AsinLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AsinLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AsinLayerParams& from);
  void MergeFrom(const AsinLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AsinLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AsinLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AcosLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AcosLayerParams) */ {
 public:
  AcosLayerParams();
  virtual ~AcosLayerParams();

  AcosLayerParams(const AcosLayerParams& from);

  inline AcosLayerParams& operator=(const AcosLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AcosLayerParams& default_instance();

  static inline const AcosLayerParams* internal_default_instance() {
    return reinterpret_cast<const AcosLayerParams*>(
               &_AcosLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    107;

  void Swap(AcosLayerParams* other);

  // implements Message ----------------------------------------------

  inline AcosLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AcosLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AcosLayerParams& from);
  void MergeFrom(const AcosLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AcosLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AcosLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AtanLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AtanLayerParams) */ {
 public:
  AtanLayerParams();
  virtual ~AtanLayerParams();

  AtanLayerParams(const AtanLayerParams& from);

  inline AtanLayerParams& operator=(const AtanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AtanLayerParams& default_instance();

  static inline const AtanLayerParams* internal_default_instance() {
    return reinterpret_cast<const AtanLayerParams*>(
               &_AtanLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    108;

  void Swap(AtanLayerParams* other);

  // implements Message ----------------------------------------------

  inline AtanLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AtanLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AtanLayerParams& from);
  void MergeFrom(const AtanLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AtanLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AtanLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SinhLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SinhLayerParams) */ {
 public:
  SinhLayerParams();
  virtual ~SinhLayerParams();

  SinhLayerParams(const SinhLayerParams& from);

  inline SinhLayerParams& operator=(const SinhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SinhLayerParams& default_instance();

  static inline const SinhLayerParams* internal_default_instance() {
    return reinterpret_cast<const SinhLayerParams*>(
               &_SinhLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    109;

  void Swap(SinhLayerParams* other);

  // implements Message ----------------------------------------------

  inline SinhLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SinhLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SinhLayerParams& from);
  void MergeFrom(const SinhLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SinhLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SinhLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CoshLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CoshLayerParams) */ {
 public:
  CoshLayerParams();
  virtual ~CoshLayerParams();

  CoshLayerParams(const CoshLayerParams& from);

  inline CoshLayerParams& operator=(const CoshLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CoshLayerParams& default_instance();

  static inline const CoshLayerParams* internal_default_instance() {
    return reinterpret_cast<const CoshLayerParams*>(
               &_CoshLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    110;

  void Swap(CoshLayerParams* other);

  // implements Message ----------------------------------------------

  inline CoshLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CoshLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CoshLayerParams& from);
  void MergeFrom(const CoshLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CoshLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CoshLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class TanhLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TanhLayerParams) */ {
 public:
  TanhLayerParams();
  virtual ~TanhLayerParams();

  TanhLayerParams(const TanhLayerParams& from);

  inline TanhLayerParams& operator=(const TanhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const TanhLayerParams& default_instance();

  static inline const TanhLayerParams* internal_default_instance() {
    return reinterpret_cast<const TanhLayerParams*>(
               &_TanhLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    111;

  void Swap(TanhLayerParams* other);

  // implements Message ----------------------------------------------

  inline TanhLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  TanhLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const TanhLayerParams& from);
  void MergeFrom(const TanhLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TanhLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TanhLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AsinhLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AsinhLayerParams) */ {
 public:
  AsinhLayerParams();
  virtual ~AsinhLayerParams();

  AsinhLayerParams(const AsinhLayerParams& from);

  inline AsinhLayerParams& operator=(const AsinhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AsinhLayerParams& default_instance();

  static inline const AsinhLayerParams* internal_default_instance() {
    return reinterpret_cast<const AsinhLayerParams*>(
               &_AsinhLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    112;

  void Swap(AsinhLayerParams* other);

  // implements Message ----------------------------------------------

  inline AsinhLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AsinhLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AsinhLayerParams& from);
  void MergeFrom(const AsinhLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AsinhLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AsinhLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AcoshLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AcoshLayerParams) */ {
 public:
  AcoshLayerParams();
  virtual ~AcoshLayerParams();

  AcoshLayerParams(const AcoshLayerParams& from);

  inline AcoshLayerParams& operator=(const AcoshLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AcoshLayerParams& default_instance();

  static inline const AcoshLayerParams* internal_default_instance() {
    return reinterpret_cast<const AcoshLayerParams*>(
               &_AcoshLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    113;

  void Swap(AcoshLayerParams* other);

  // implements Message ----------------------------------------------

  inline AcoshLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AcoshLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AcoshLayerParams& from);
  void MergeFrom(const AcoshLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AcoshLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AcoshLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AtanhLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AtanhLayerParams) */ {
 public:
  AtanhLayerParams();
  virtual ~AtanhLayerParams();

  AtanhLayerParams(const AtanhLayerParams& from);

  inline AtanhLayerParams& operator=(const AtanhLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AtanhLayerParams& default_instance();

  static inline const AtanhLayerParams* internal_default_instance() {
    return reinterpret_cast<const AtanhLayerParams*>(
               &_AtanhLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    114;

  void Swap(AtanhLayerParams* other);

  // implements Message ----------------------------------------------

  inline AtanhLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AtanhLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AtanhLayerParams& from);
  void MergeFrom(const AtanhLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AtanhLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AtanhLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class PowBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PowBroadcastableLayerParams) */ {
 public:
  PowBroadcastableLayerParams();
  virtual ~PowBroadcastableLayerParams();

  PowBroadcastableLayerParams(const PowBroadcastableLayerParams& from);

  inline PowBroadcastableLayerParams& operator=(const PowBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PowBroadcastableLayerParams& default_instance();

  static inline const PowBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const PowBroadcastableLayerParams*>(
               &_PowBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    115;

  void Swap(PowBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline PowBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  PowBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const PowBroadcastableLayerParams& from);
  void MergeFrom(const PowBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PowBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PowBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class Exp2LayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Exp2LayerParams) */ {
 public:
  Exp2LayerParams();
  virtual ~Exp2LayerParams();

  Exp2LayerParams(const Exp2LayerParams& from);

  inline Exp2LayerParams& operator=(const Exp2LayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const Exp2LayerParams& default_instance();

  static inline const Exp2LayerParams* internal_default_instance() {
    return reinterpret_cast<const Exp2LayerParams*>(
               &_Exp2LayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    116;

  void Swap(Exp2LayerParams* other);

  // implements Message ----------------------------------------------

  inline Exp2LayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  Exp2LayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const Exp2LayerParams& from);
  void MergeFrom(const Exp2LayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Exp2LayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.Exp2LayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class WhereNonZeroLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WhereNonZeroLayerParams) */ {
 public:
  WhereNonZeroLayerParams();
  virtual ~WhereNonZeroLayerParams();

  WhereNonZeroLayerParams(const WhereNonZeroLayerParams& from);

  inline WhereNonZeroLayerParams& operator=(const WhereNonZeroLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const WhereNonZeroLayerParams& default_instance();

  static inline const WhereNonZeroLayerParams* internal_default_instance() {
    return reinterpret_cast<const WhereNonZeroLayerParams*>(
               &_WhereNonZeroLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    117;

  void Swap(WhereNonZeroLayerParams* other);

  // implements Message ----------------------------------------------

  inline WhereNonZeroLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  WhereNonZeroLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const WhereNonZeroLayerParams& from);
  void MergeFrom(const WhereNonZeroLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WhereNonZeroLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WhereNonZeroLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MatrixBandPartLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MatrixBandPartLayerParams) */ {
 public:
  MatrixBandPartLayerParams();
  virtual ~MatrixBandPartLayerParams();

  MatrixBandPartLayerParams(const MatrixBandPartLayerParams& from);

  inline MatrixBandPartLayerParams& operator=(const MatrixBandPartLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MatrixBandPartLayerParams& default_instance();

  static inline const MatrixBandPartLayerParams* internal_default_instance() {
    return reinterpret_cast<const MatrixBandPartLayerParams*>(
               &_MatrixBandPartLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    118;

  void Swap(MatrixBandPartLayerParams* other);

  // implements Message ----------------------------------------------

  inline MatrixBandPartLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MatrixBandPartLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MatrixBandPartLayerParams& from);
  void MergeFrom(const MatrixBandPartLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MatrixBandPartLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 numLower = 1;
  void clear_numlower();
  static const int kNumLowerFieldNumber = 1;
  ::google::protobuf::int64 numlower() const;
  void set_numlower(::google::protobuf::int64 value);

  // int64 numUpper = 2;
  void clear_numupper();
  static const int kNumUpperFieldNumber = 2;
  ::google::protobuf::int64 numupper() const;
  void set_numupper(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MatrixBandPartLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 numlower_;
  ::google::protobuf::int64 numupper_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class UpperTriangularLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UpperTriangularLayerParams) */ {
 public:
  UpperTriangularLayerParams();
  virtual ~UpperTriangularLayerParams();

  UpperTriangularLayerParams(const UpperTriangularLayerParams& from);

  inline UpperTriangularLayerParams& operator=(const UpperTriangularLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UpperTriangularLayerParams& default_instance();

  static inline const UpperTriangularLayerParams* internal_default_instance() {
    return reinterpret_cast<const UpperTriangularLayerParams*>(
               &_UpperTriangularLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    119;

  void Swap(UpperTriangularLayerParams* other);

  // implements Message ----------------------------------------------

  inline UpperTriangularLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  UpperTriangularLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const UpperTriangularLayerParams& from);
  void MergeFrom(const UpperTriangularLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UpperTriangularLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 k = 1;
  void clear_k();
  static const int kKFieldNumber = 1;
  ::google::protobuf::int64 k() const;
  void set_k(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UpperTriangularLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 k_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LowerTriangularLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LowerTriangularLayerParams) */ {
 public:
  LowerTriangularLayerParams();
  virtual ~LowerTriangularLayerParams();

  LowerTriangularLayerParams(const LowerTriangularLayerParams& from);

  inline LowerTriangularLayerParams& operator=(const LowerTriangularLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LowerTriangularLayerParams& default_instance();

  static inline const LowerTriangularLayerParams* internal_default_instance() {
    return reinterpret_cast<const LowerTriangularLayerParams*>(
               &_LowerTriangularLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    120;

  void Swap(LowerTriangularLayerParams* other);

  // implements Message ----------------------------------------------

  inline LowerTriangularLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LowerTriangularLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LowerTriangularLayerParams& from);
  void MergeFrom(const LowerTriangularLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LowerTriangularLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 k = 1;
  void clear_k();
  static const int kKFieldNumber = 1;
  ::google::protobuf::int64 k() const;
  void set_k(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LowerTriangularLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 k_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BroadcastToLikeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BroadcastToLikeLayerParams) */ {
 public:
  BroadcastToLikeLayerParams();
  virtual ~BroadcastToLikeLayerParams();

  BroadcastToLikeLayerParams(const BroadcastToLikeLayerParams& from);

  inline BroadcastToLikeLayerParams& operator=(const BroadcastToLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BroadcastToLikeLayerParams& default_instance();

  static inline const BroadcastToLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const BroadcastToLikeLayerParams*>(
               &_BroadcastToLikeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    121;

  void Swap(BroadcastToLikeLayerParams* other);

  // implements Message ----------------------------------------------

  inline BroadcastToLikeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BroadcastToLikeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BroadcastToLikeLayerParams& from);
  void MergeFrom(const BroadcastToLikeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BroadcastToLikeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BroadcastToLikeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BroadcastToStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BroadcastToStaticLayerParams) */ {
 public:
  BroadcastToStaticLayerParams();
  virtual ~BroadcastToStaticLayerParams();

  BroadcastToStaticLayerParams(const BroadcastToStaticLayerParams& from);

  inline BroadcastToStaticLayerParams& operator=(const BroadcastToStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BroadcastToStaticLayerParams& default_instance();

  static inline const BroadcastToStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const BroadcastToStaticLayerParams*>(
               &_BroadcastToStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    122;

  void Swap(BroadcastToStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline BroadcastToStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BroadcastToStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BroadcastToStaticLayerParams& from);
  void MergeFrom(const BroadcastToStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BroadcastToStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 targetShape = 1;
  int targetshape_size() const;
  void clear_targetshape();
  static const int kTargetShapeFieldNumber = 1;
  ::google::protobuf::uint64 targetshape(int index) const;
  void set_targetshape(int index, ::google::protobuf::uint64 value);
  void add_targetshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      targetshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_targetshape();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BroadcastToStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > targetshape_;
  mutable int _targetshape_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BroadcastToDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BroadcastToDynamicLayerParams) */ {
 public:
  BroadcastToDynamicLayerParams();
  virtual ~BroadcastToDynamicLayerParams();

  BroadcastToDynamicLayerParams(const BroadcastToDynamicLayerParams& from);

  inline BroadcastToDynamicLayerParams& operator=(const BroadcastToDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BroadcastToDynamicLayerParams& default_instance();

  static inline const BroadcastToDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const BroadcastToDynamicLayerParams*>(
               &_BroadcastToDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    123;

  void Swap(BroadcastToDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline BroadcastToDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  BroadcastToDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const BroadcastToDynamicLayerParams& from);
  void MergeFrom(const BroadcastToDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BroadcastToDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BroadcastToDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AddBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AddBroadcastableLayerParams) */ {
 public:
  AddBroadcastableLayerParams();
  virtual ~AddBroadcastableLayerParams();

  AddBroadcastableLayerParams(const AddBroadcastableLayerParams& from);

  inline AddBroadcastableLayerParams& operator=(const AddBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AddBroadcastableLayerParams& default_instance();

  static inline const AddBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const AddBroadcastableLayerParams*>(
               &_AddBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    124;

  void Swap(AddBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline AddBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  AddBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AddBroadcastableLayerParams& from);
  void MergeFrom(const AddBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AddBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AddBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MaxBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MaxBroadcastableLayerParams) */ {
 public:
  MaxBroadcastableLayerParams();
  virtual ~MaxBroadcastableLayerParams();

  MaxBroadcastableLayerParams(const MaxBroadcastableLayerParams& from);

  inline MaxBroadcastableLayerParams& operator=(const MaxBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MaxBroadcastableLayerParams& default_instance();

  static inline const MaxBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const MaxBroadcastableLayerParams*>(
               &_MaxBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    125;

  void Swap(MaxBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline MaxBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MaxBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MaxBroadcastableLayerParams& from);
  void MergeFrom(const MaxBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MaxBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MaxBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MinBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MinBroadcastableLayerParams) */ {
 public:
  MinBroadcastableLayerParams();
  virtual ~MinBroadcastableLayerParams();

  MinBroadcastableLayerParams(const MinBroadcastableLayerParams& from);

  inline MinBroadcastableLayerParams& operator=(const MinBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MinBroadcastableLayerParams& default_instance();

  static inline const MinBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const MinBroadcastableLayerParams*>(
               &_MinBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    126;

  void Swap(MinBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline MinBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MinBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MinBroadcastableLayerParams& from);
  void MergeFrom(const MinBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MinBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MinBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ModBroadcastableLayerParams) */ {
 public:
  ModBroadcastableLayerParams();
  virtual ~ModBroadcastableLayerParams();

  ModBroadcastableLayerParams(const ModBroadcastableLayerParams& from);

  inline ModBroadcastableLayerParams& operator=(const ModBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ModBroadcastableLayerParams& default_instance();

  static inline const ModBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const ModBroadcastableLayerParams*>(
               &_ModBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    127;

  void Swap(ModBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline ModBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ModBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ModBroadcastableLayerParams& from);
  void MergeFrom(const ModBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ModBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ModBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FloorDivBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FloorDivBroadcastableLayerParams) */ {
 public:
  FloorDivBroadcastableLayerParams();
  virtual ~FloorDivBroadcastableLayerParams();

  FloorDivBroadcastableLayerParams(const FloorDivBroadcastableLayerParams& from);

  inline FloorDivBroadcastableLayerParams& operator=(const FloorDivBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FloorDivBroadcastableLayerParams& default_instance();

  static inline const FloorDivBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const FloorDivBroadcastableLayerParams*>(
               &_FloorDivBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    128;

  void Swap(FloorDivBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline FloorDivBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FloorDivBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FloorDivBroadcastableLayerParams& from);
  void MergeFrom(const FloorDivBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FloorDivBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FloorDivBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SubtractBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SubtractBroadcastableLayerParams) */ {
 public:
  SubtractBroadcastableLayerParams();
  virtual ~SubtractBroadcastableLayerParams();

  SubtractBroadcastableLayerParams(const SubtractBroadcastableLayerParams& from);

  inline SubtractBroadcastableLayerParams& operator=(const SubtractBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SubtractBroadcastableLayerParams& default_instance();

  static inline const SubtractBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const SubtractBroadcastableLayerParams*>(
               &_SubtractBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    129;

  void Swap(SubtractBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline SubtractBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SubtractBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SubtractBroadcastableLayerParams& from);
  void MergeFrom(const SubtractBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SubtractBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SubtractBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MultiplyBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MultiplyBroadcastableLayerParams) */ {
 public:
  MultiplyBroadcastableLayerParams();
  virtual ~MultiplyBroadcastableLayerParams();

  MultiplyBroadcastableLayerParams(const MultiplyBroadcastableLayerParams& from);

  inline MultiplyBroadcastableLayerParams& operator=(const MultiplyBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MultiplyBroadcastableLayerParams& default_instance();

  static inline const MultiplyBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const MultiplyBroadcastableLayerParams*>(
               &_MultiplyBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    130;

  void Swap(MultiplyBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline MultiplyBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  MultiplyBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MultiplyBroadcastableLayerParams& from);
  void MergeFrom(const MultiplyBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MultiplyBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MultiplyBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class DivideBroadcastableLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.DivideBroadcastableLayerParams) */ {
 public:
  DivideBroadcastableLayerParams();
  virtual ~DivideBroadcastableLayerParams();

  DivideBroadcastableLayerParams(const DivideBroadcastableLayerParams& from);

  inline DivideBroadcastableLayerParams& operator=(const DivideBroadcastableLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const DivideBroadcastableLayerParams& default_instance();

  static inline const DivideBroadcastableLayerParams* internal_default_instance() {
    return reinterpret_cast<const DivideBroadcastableLayerParams*>(
               &_DivideBroadcastableLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    131;

  void Swap(DivideBroadcastableLayerParams* other);

  // implements Message ----------------------------------------------

  inline DivideBroadcastableLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  DivideBroadcastableLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const DivideBroadcastableLayerParams& from);
  void MergeFrom(const DivideBroadcastableLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(DivideBroadcastableLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.DivideBroadcastableLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GatherLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GatherLayerParams) */ {
 public:
  GatherLayerParams();
  virtual ~GatherLayerParams();

  GatherLayerParams(const GatherLayerParams& from);

  inline GatherLayerParams& operator=(const GatherLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GatherLayerParams& default_instance();

  static inline const GatherLayerParams* internal_default_instance() {
    return reinterpret_cast<const GatherLayerParams*>(
               &_GatherLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    132;

  void Swap(GatherLayerParams* other);

  // implements Message ----------------------------------------------

  inline GatherLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GatherLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GatherLayerParams& from);
  void MergeFrom(const GatherLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GatherLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GatherLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ScatterLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScatterLayerParams) */ {
 public:
  ScatterLayerParams();
  virtual ~ScatterLayerParams();

  ScatterLayerParams(const ScatterLayerParams& from);

  inline ScatterLayerParams& operator=(const ScatterLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ScatterLayerParams& default_instance();

  static inline const ScatterLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScatterLayerParams*>(
               &_ScatterLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    133;

  void Swap(ScatterLayerParams* other);

  // implements Message ----------------------------------------------

  inline ScatterLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ScatterLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ScatterLayerParams& from);
  void MergeFrom(const ScatterLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScatterLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // .CoreML.Specification.ScatterMode mode = 2;
  void clear_mode();
  static const int kModeFieldNumber = 2;
  ::CoreML::Specification::ScatterMode mode() const;
  void set_mode(::CoreML::Specification::ScatterMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScatterLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GatherNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GatherNDLayerParams) */ {
 public:
  GatherNDLayerParams();
  virtual ~GatherNDLayerParams();

  GatherNDLayerParams(const GatherNDLayerParams& from);

  inline GatherNDLayerParams& operator=(const GatherNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GatherNDLayerParams& default_instance();

  static inline const GatherNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const GatherNDLayerParams*>(
               &_GatherNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    134;

  void Swap(GatherNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline GatherNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GatherNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GatherNDLayerParams& from);
  void MergeFrom(const GatherNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GatherNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GatherNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ScatterNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScatterNDLayerParams) */ {
 public:
  ScatterNDLayerParams();
  virtual ~ScatterNDLayerParams();

  ScatterNDLayerParams(const ScatterNDLayerParams& from);

  inline ScatterNDLayerParams& operator=(const ScatterNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ScatterNDLayerParams& default_instance();

  static inline const ScatterNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScatterNDLayerParams*>(
               &_ScatterNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    135;

  void Swap(ScatterNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline ScatterNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ScatterNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ScatterNDLayerParams& from);
  void MergeFrom(const ScatterNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScatterNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.ScatterMode mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::ScatterMode mode() const;
  void set_mode(::CoreML::Specification::ScatterMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScatterNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GatherAlongAxisLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GatherAlongAxisLayerParams) */ {
 public:
  GatherAlongAxisLayerParams();
  virtual ~GatherAlongAxisLayerParams();

  GatherAlongAxisLayerParams(const GatherAlongAxisLayerParams& from);

  inline GatherAlongAxisLayerParams& operator=(const GatherAlongAxisLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GatherAlongAxisLayerParams& default_instance();

  static inline const GatherAlongAxisLayerParams* internal_default_instance() {
    return reinterpret_cast<const GatherAlongAxisLayerParams*>(
               &_GatherAlongAxisLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    136;

  void Swap(GatherAlongAxisLayerParams* other);

  // implements Message ----------------------------------------------

  inline GatherAlongAxisLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GatherAlongAxisLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GatherAlongAxisLayerParams& from);
  void MergeFrom(const GatherAlongAxisLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GatherAlongAxisLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GatherAlongAxisLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ScatterAlongAxisLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScatterAlongAxisLayerParams) */ {
 public:
  ScatterAlongAxisLayerParams();
  virtual ~ScatterAlongAxisLayerParams();

  ScatterAlongAxisLayerParams(const ScatterAlongAxisLayerParams& from);

  inline ScatterAlongAxisLayerParams& operator=(const ScatterAlongAxisLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ScatterAlongAxisLayerParams& default_instance();

  static inline const ScatterAlongAxisLayerParams* internal_default_instance() {
    return reinterpret_cast<const ScatterAlongAxisLayerParams*>(
               &_ScatterAlongAxisLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    137;

  void Swap(ScatterAlongAxisLayerParams* other);

  // implements Message ----------------------------------------------

  inline ScatterAlongAxisLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ScatterAlongAxisLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ScatterAlongAxisLayerParams& from);
  void MergeFrom(const ScatterAlongAxisLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScatterAlongAxisLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // .CoreML.Specification.ScatterMode mode = 2;
  void clear_mode();
  static const int kModeFieldNumber = 2;
  ::CoreML::Specification::ScatterMode mode() const;
  void set_mode(::CoreML::Specification::ScatterMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScatterAlongAxisLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StackLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.StackLayerParams) */ {
 public:
  StackLayerParams();
  virtual ~StackLayerParams();

  StackLayerParams(const StackLayerParams& from);

  inline StackLayerParams& operator=(const StackLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const StackLayerParams& default_instance();

  static inline const StackLayerParams* internal_default_instance() {
    return reinterpret_cast<const StackLayerParams*>(
               &_StackLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    138;

  void Swap(StackLayerParams* other);

  // implements Message ----------------------------------------------

  inline StackLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  StackLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const StackLayerParams& from);
  void MergeFrom(const StackLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(StackLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.StackLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RankPreservingReshapeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RankPreservingReshapeLayerParams) */ {
 public:
  RankPreservingReshapeLayerParams();
  virtual ~RankPreservingReshapeLayerParams();

  RankPreservingReshapeLayerParams(const RankPreservingReshapeLayerParams& from);

  inline RankPreservingReshapeLayerParams& operator=(const RankPreservingReshapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RankPreservingReshapeLayerParams& default_instance();

  static inline const RankPreservingReshapeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RankPreservingReshapeLayerParams*>(
               &_RankPreservingReshapeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    139;

  void Swap(RankPreservingReshapeLayerParams* other);

  // implements Message ----------------------------------------------

  inline RankPreservingReshapeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RankPreservingReshapeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RankPreservingReshapeLayerParams& from);
  void MergeFrom(const RankPreservingReshapeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RankPreservingReshapeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  void clear_targetshape();
  static const int kTargetShapeFieldNumber = 1;
  ::google::protobuf::int64 targetshape(int index) const;
  void set_targetshape(int index, ::google::protobuf::int64 value);
  void add_targetshape(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      targetshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_targetshape();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RankPreservingReshapeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > targetshape_;
  mutable int _targetshape_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ConstantPaddingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConstantPaddingLayerParams) */ {
 public:
  ConstantPaddingLayerParams();
  virtual ~ConstantPaddingLayerParams();

  ConstantPaddingLayerParams(const ConstantPaddingLayerParams& from);

  inline ConstantPaddingLayerParams& operator=(const ConstantPaddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ConstantPaddingLayerParams& default_instance();

  static inline const ConstantPaddingLayerParams* internal_default_instance() {
    return reinterpret_cast<const ConstantPaddingLayerParams*>(
               &_ConstantPaddingLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    140;

  void Swap(ConstantPaddingLayerParams* other);

  // implements Message ----------------------------------------------

  inline ConstantPaddingLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ConstantPaddingLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ConstantPaddingLayerParams& from);
  void MergeFrom(const ConstantPaddingLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConstantPaddingLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 padAmounts = 2;
  int padamounts_size() const;
  void clear_padamounts();
  static const int kPadAmountsFieldNumber = 2;
  ::google::protobuf::uint64 padamounts(int index) const;
  void set_padamounts(int index, ::google::protobuf::uint64 value);
  void add_padamounts(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      padamounts() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_padamounts();

  // float value = 1;
  void clear_value();
  static const int kValueFieldNumber = 1;
  float value() const;
  void set_value(float value);

  // bool padToGivenOutputSizeMode = 3;
  void clear_padtogivenoutputsizemode();
  static const int kPadToGivenOutputSizeModeFieldNumber = 3;
  bool padtogivenoutputsizemode() const;
  void set_padtogivenoutputsizemode(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConstantPaddingLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > padamounts_;
  mutable int _padamounts_cached_byte_size_;
  float value_;
  bool padtogivenoutputsizemode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomNormalLikeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomNormalLikeLayerParams) */ {
 public:
  RandomNormalLikeLayerParams();
  virtual ~RandomNormalLikeLayerParams();

  RandomNormalLikeLayerParams(const RandomNormalLikeLayerParams& from);

  inline RandomNormalLikeLayerParams& operator=(const RandomNormalLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomNormalLikeLayerParams& default_instance();

  static inline const RandomNormalLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomNormalLikeLayerParams*>(
               &_RandomNormalLikeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    141;

  void Swap(RandomNormalLikeLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomNormalLikeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomNormalLikeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomNormalLikeLayerParams& from);
  void MergeFrom(const RandomNormalLikeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomNormalLikeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float mean = 2;
  void clear_mean();
  static const int kMeanFieldNumber = 2;
  float mean() const;
  void set_mean(float value);

  // float stdDev = 3;
  void clear_stddev();
  static const int kStdDevFieldNumber = 3;
  float stddev() const;
  void set_stddev(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomNormalLikeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  float mean_;
  float stddev_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomNormalStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomNormalStaticLayerParams) */ {
 public:
  RandomNormalStaticLayerParams();
  virtual ~RandomNormalStaticLayerParams();

  RandomNormalStaticLayerParams(const RandomNormalStaticLayerParams& from);

  inline RandomNormalStaticLayerParams& operator=(const RandomNormalStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomNormalStaticLayerParams& default_instance();

  static inline const RandomNormalStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomNormalStaticLayerParams*>(
               &_RandomNormalStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    142;

  void Swap(RandomNormalStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomNormalStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomNormalStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomNormalStaticLayerParams& from);
  void MergeFrom(const RandomNormalStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomNormalStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 outputShape = 4;
  int outputshape_size() const;
  void clear_outputshape();
  static const int kOutputShapeFieldNumber = 4;
  ::google::protobuf::uint64 outputshape(int index) const;
  void set_outputshape(int index, ::google::protobuf::uint64 value);
  void add_outputshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      outputshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_outputshape();

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float mean = 2;
  void clear_mean();
  static const int kMeanFieldNumber = 2;
  float mean() const;
  void set_mean(float value);

  // float stdDev = 3;
  void clear_stddev();
  static const int kStdDevFieldNumber = 3;
  float stddev() const;
  void set_stddev(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomNormalStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > outputshape_;
  mutable int _outputshape_cached_byte_size_;
  ::google::protobuf::int64 seed_;
  float mean_;
  float stddev_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomNormalDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomNormalDynamicLayerParams) */ {
 public:
  RandomNormalDynamicLayerParams();
  virtual ~RandomNormalDynamicLayerParams();

  RandomNormalDynamicLayerParams(const RandomNormalDynamicLayerParams& from);

  inline RandomNormalDynamicLayerParams& operator=(const RandomNormalDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomNormalDynamicLayerParams& default_instance();

  static inline const RandomNormalDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomNormalDynamicLayerParams*>(
               &_RandomNormalDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    143;

  void Swap(RandomNormalDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomNormalDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomNormalDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomNormalDynamicLayerParams& from);
  void MergeFrom(const RandomNormalDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomNormalDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float mean = 2;
  void clear_mean();
  static const int kMeanFieldNumber = 2;
  float mean() const;
  void set_mean(float value);

  // float stdDev = 3;
  void clear_stddev();
  static const int kStdDevFieldNumber = 3;
  float stddev() const;
  void set_stddev(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomNormalDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  float mean_;
  float stddev_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomUniformLikeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomUniformLikeLayerParams) */ {
 public:
  RandomUniformLikeLayerParams();
  virtual ~RandomUniformLikeLayerParams();

  RandomUniformLikeLayerParams(const RandomUniformLikeLayerParams& from);

  inline RandomUniformLikeLayerParams& operator=(const RandomUniformLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomUniformLikeLayerParams& default_instance();

  static inline const RandomUniformLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomUniformLikeLayerParams*>(
               &_RandomUniformLikeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    144;

  void Swap(RandomUniformLikeLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomUniformLikeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomUniformLikeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomUniformLikeLayerParams& from);
  void MergeFrom(const RandomUniformLikeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomUniformLikeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float minVal = 2;
  void clear_minval();
  static const int kMinValFieldNumber = 2;
  float minval() const;
  void set_minval(float value);

  // float maxVal = 3;
  void clear_maxval();
  static const int kMaxValFieldNumber = 3;
  float maxval() const;
  void set_maxval(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomUniformLikeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  float minval_;
  float maxval_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomUniformStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomUniformStaticLayerParams) */ {
 public:
  RandomUniformStaticLayerParams();
  virtual ~RandomUniformStaticLayerParams();

  RandomUniformStaticLayerParams(const RandomUniformStaticLayerParams& from);

  inline RandomUniformStaticLayerParams& operator=(const RandomUniformStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomUniformStaticLayerParams& default_instance();

  static inline const RandomUniformStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomUniformStaticLayerParams*>(
               &_RandomUniformStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    145;

  void Swap(RandomUniformStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomUniformStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomUniformStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomUniformStaticLayerParams& from);
  void MergeFrom(const RandomUniformStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomUniformStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 outputShape = 4;
  int outputshape_size() const;
  void clear_outputshape();
  static const int kOutputShapeFieldNumber = 4;
  ::google::protobuf::uint64 outputshape(int index) const;
  void set_outputshape(int index, ::google::protobuf::uint64 value);
  void add_outputshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      outputshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_outputshape();

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float minVal = 2;
  void clear_minval();
  static const int kMinValFieldNumber = 2;
  float minval() const;
  void set_minval(float value);

  // float maxVal = 3;
  void clear_maxval();
  static const int kMaxValFieldNumber = 3;
  float maxval() const;
  void set_maxval(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomUniformStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > outputshape_;
  mutable int _outputshape_cached_byte_size_;
  ::google::protobuf::int64 seed_;
  float minval_;
  float maxval_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomUniformDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomUniformDynamicLayerParams) */ {
 public:
  RandomUniformDynamicLayerParams();
  virtual ~RandomUniformDynamicLayerParams();

  RandomUniformDynamicLayerParams(const RandomUniformDynamicLayerParams& from);

  inline RandomUniformDynamicLayerParams& operator=(const RandomUniformDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomUniformDynamicLayerParams& default_instance();

  static inline const RandomUniformDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomUniformDynamicLayerParams*>(
               &_RandomUniformDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    146;

  void Swap(RandomUniformDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomUniformDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomUniformDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomUniformDynamicLayerParams& from);
  void MergeFrom(const RandomUniformDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomUniformDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float minVal = 2;
  void clear_minval();
  static const int kMinValFieldNumber = 2;
  float minval() const;
  void set_minval(float value);

  // float maxVal = 3;
  void clear_maxval();
  static const int kMaxValFieldNumber = 3;
  float maxval() const;
  void set_maxval(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomUniformDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  float minval_;
  float maxval_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomBernoulliLikeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomBernoulliLikeLayerParams) */ {
 public:
  RandomBernoulliLikeLayerParams();
  virtual ~RandomBernoulliLikeLayerParams();

  RandomBernoulliLikeLayerParams(const RandomBernoulliLikeLayerParams& from);

  inline RandomBernoulliLikeLayerParams& operator=(const RandomBernoulliLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomBernoulliLikeLayerParams& default_instance();

  static inline const RandomBernoulliLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomBernoulliLikeLayerParams*>(
               &_RandomBernoulliLikeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    147;

  void Swap(RandomBernoulliLikeLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomBernoulliLikeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomBernoulliLikeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomBernoulliLikeLayerParams& from);
  void MergeFrom(const RandomBernoulliLikeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomBernoulliLikeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float prob = 2;
  void clear_prob();
  static const int kProbFieldNumber = 2;
  float prob() const;
  void set_prob(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomBernoulliLikeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  float prob_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomBernoulliStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomBernoulliStaticLayerParams) */ {
 public:
  RandomBernoulliStaticLayerParams();
  virtual ~RandomBernoulliStaticLayerParams();

  RandomBernoulliStaticLayerParams(const RandomBernoulliStaticLayerParams& from);

  inline RandomBernoulliStaticLayerParams& operator=(const RandomBernoulliStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomBernoulliStaticLayerParams& default_instance();

  static inline const RandomBernoulliStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomBernoulliStaticLayerParams*>(
               &_RandomBernoulliStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    148;

  void Swap(RandomBernoulliStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomBernoulliStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomBernoulliStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomBernoulliStaticLayerParams& from);
  void MergeFrom(const RandomBernoulliStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomBernoulliStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 outputShape = 3;
  int outputshape_size() const;
  void clear_outputshape();
  static const int kOutputShapeFieldNumber = 3;
  ::google::protobuf::uint64 outputshape(int index) const;
  void set_outputshape(int index, ::google::protobuf::uint64 value);
  void add_outputshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      outputshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_outputshape();

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float prob = 2;
  void clear_prob();
  static const int kProbFieldNumber = 2;
  float prob() const;
  void set_prob(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomBernoulliStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > outputshape_;
  mutable int _outputshape_cached_byte_size_;
  ::google::protobuf::int64 seed_;
  float prob_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RandomBernoulliDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RandomBernoulliDynamicLayerParams) */ {
 public:
  RandomBernoulliDynamicLayerParams();
  virtual ~RandomBernoulliDynamicLayerParams();

  RandomBernoulliDynamicLayerParams(const RandomBernoulliDynamicLayerParams& from);

  inline RandomBernoulliDynamicLayerParams& operator=(const RandomBernoulliDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RandomBernoulliDynamicLayerParams& default_instance();

  static inline const RandomBernoulliDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RandomBernoulliDynamicLayerParams*>(
               &_RandomBernoulliDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    149;

  void Swap(RandomBernoulliDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline RandomBernoulliDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RandomBernoulliDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RandomBernoulliDynamicLayerParams& from);
  void MergeFrom(const RandomBernoulliDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RandomBernoulliDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // float prob = 2;
  void clear_prob();
  static const int kProbFieldNumber = 2;
  float prob() const;
  void set_prob(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RandomBernoulliDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  float prob_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CategoricalDistributionLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CategoricalDistributionLayerParams) */ {
 public:
  CategoricalDistributionLayerParams();
  virtual ~CategoricalDistributionLayerParams();

  CategoricalDistributionLayerParams(const CategoricalDistributionLayerParams& from);

  inline CategoricalDistributionLayerParams& operator=(const CategoricalDistributionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CategoricalDistributionLayerParams& default_instance();

  static inline const CategoricalDistributionLayerParams* internal_default_instance() {
    return reinterpret_cast<const CategoricalDistributionLayerParams*>(
               &_CategoricalDistributionLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    150;

  void Swap(CategoricalDistributionLayerParams* other);

  // implements Message ----------------------------------------------

  inline CategoricalDistributionLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CategoricalDistributionLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CategoricalDistributionLayerParams& from);
  void MergeFrom(const CategoricalDistributionLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CategoricalDistributionLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 seed = 1;
  void clear_seed();
  static const int kSeedFieldNumber = 1;
  ::google::protobuf::int64 seed() const;
  void set_seed(::google::protobuf::int64 value);

  // int64 numSamples = 2;
  void clear_numsamples();
  static const int kNumSamplesFieldNumber = 2;
  ::google::protobuf::int64 numsamples() const;
  void set_numsamples(::google::protobuf::int64 value);

  // bool isLogits = 3;
  void clear_islogits();
  static const int kIsLogitsFieldNumber = 3;
  bool islogits() const;
  void set_islogits(bool value);

  // float eps = 4;
  void clear_eps();
  static const int kEpsFieldNumber = 4;
  float eps() const;
  void set_eps(float value);

  // float temperature = 5;
  void clear_temperature();
  static const int kTemperatureFieldNumber = 5;
  float temperature() const;
  void set_temperature(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CategoricalDistributionLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 seed_;
  ::google::protobuf::int64 numsamples_;
  bool islogits_;
  float eps_;
  float temperature_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceL1LayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceL1LayerParams) */ {
 public:
  ReduceL1LayerParams();
  virtual ~ReduceL1LayerParams();

  ReduceL1LayerParams(const ReduceL1LayerParams& from);

  inline ReduceL1LayerParams& operator=(const ReduceL1LayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceL1LayerParams& default_instance();

  static inline const ReduceL1LayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceL1LayerParams*>(
               &_ReduceL1LayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    151;

  void Swap(ReduceL1LayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceL1LayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceL1LayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceL1LayerParams& from);
  void MergeFrom(const ReduceL1LayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceL1LayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceL1LayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceL2LayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceL2LayerParams) */ {
 public:
  ReduceL2LayerParams();
  virtual ~ReduceL2LayerParams();

  ReduceL2LayerParams(const ReduceL2LayerParams& from);

  inline ReduceL2LayerParams& operator=(const ReduceL2LayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceL2LayerParams& default_instance();

  static inline const ReduceL2LayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceL2LayerParams*>(
               &_ReduceL2LayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    152;

  void Swap(ReduceL2LayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceL2LayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceL2LayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceL2LayerParams& from);
  void MergeFrom(const ReduceL2LayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceL2LayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceL2LayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceMaxLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceMaxLayerParams) */ {
 public:
  ReduceMaxLayerParams();
  virtual ~ReduceMaxLayerParams();

  ReduceMaxLayerParams(const ReduceMaxLayerParams& from);

  inline ReduceMaxLayerParams& operator=(const ReduceMaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceMaxLayerParams& default_instance();

  static inline const ReduceMaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceMaxLayerParams*>(
               &_ReduceMaxLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    153;

  void Swap(ReduceMaxLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceMaxLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceMaxLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceMaxLayerParams& from);
  void MergeFrom(const ReduceMaxLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceMaxLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceMaxLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceMinLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceMinLayerParams) */ {
 public:
  ReduceMinLayerParams();
  virtual ~ReduceMinLayerParams();

  ReduceMinLayerParams(const ReduceMinLayerParams& from);

  inline ReduceMinLayerParams& operator=(const ReduceMinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceMinLayerParams& default_instance();

  static inline const ReduceMinLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceMinLayerParams*>(
               &_ReduceMinLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    154;

  void Swap(ReduceMinLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceMinLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceMinLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceMinLayerParams& from);
  void MergeFrom(const ReduceMinLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceMinLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceMinLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceSumLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceSumLayerParams) */ {
 public:
  ReduceSumLayerParams();
  virtual ~ReduceSumLayerParams();

  ReduceSumLayerParams(const ReduceSumLayerParams& from);

  inline ReduceSumLayerParams& operator=(const ReduceSumLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceSumLayerParams& default_instance();

  static inline const ReduceSumLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceSumLayerParams*>(
               &_ReduceSumLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    155;

  void Swap(ReduceSumLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceSumLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceSumLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceSumLayerParams& from);
  void MergeFrom(const ReduceSumLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceSumLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceSumLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceProdLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceProdLayerParams) */ {
 public:
  ReduceProdLayerParams();
  virtual ~ReduceProdLayerParams();

  ReduceProdLayerParams(const ReduceProdLayerParams& from);

  inline ReduceProdLayerParams& operator=(const ReduceProdLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceProdLayerParams& default_instance();

  static inline const ReduceProdLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceProdLayerParams*>(
               &_ReduceProdLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    156;

  void Swap(ReduceProdLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceProdLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceProdLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceProdLayerParams& from);
  void MergeFrom(const ReduceProdLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceProdLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceProdLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceMeanLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceMeanLayerParams) */ {
 public:
  ReduceMeanLayerParams();
  virtual ~ReduceMeanLayerParams();

  ReduceMeanLayerParams(const ReduceMeanLayerParams& from);

  inline ReduceMeanLayerParams& operator=(const ReduceMeanLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceMeanLayerParams& default_instance();

  static inline const ReduceMeanLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceMeanLayerParams*>(
               &_ReduceMeanLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    157;

  void Swap(ReduceMeanLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceMeanLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceMeanLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceMeanLayerParams& from);
  void MergeFrom(const ReduceMeanLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceMeanLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceMeanLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceLogSumLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLogSumLayerParams) */ {
 public:
  ReduceLogSumLayerParams();
  virtual ~ReduceLogSumLayerParams();

  ReduceLogSumLayerParams(const ReduceLogSumLayerParams& from);

  inline ReduceLogSumLayerParams& operator=(const ReduceLogSumLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceLogSumLayerParams& default_instance();

  static inline const ReduceLogSumLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceLogSumLayerParams*>(
               &_ReduceLogSumLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    158;

  void Swap(ReduceLogSumLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceLogSumLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceLogSumLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceLogSumLayerParams& from);
  void MergeFrom(const ReduceLogSumLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLogSumLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLogSumLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceSumSquareLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceSumSquareLayerParams) */ {
 public:
  ReduceSumSquareLayerParams();
  virtual ~ReduceSumSquareLayerParams();

  ReduceSumSquareLayerParams(const ReduceSumSquareLayerParams& from);

  inline ReduceSumSquareLayerParams& operator=(const ReduceSumSquareLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceSumSquareLayerParams& default_instance();

  static inline const ReduceSumSquareLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceSumSquareLayerParams*>(
               &_ReduceSumSquareLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    159;

  void Swap(ReduceSumSquareLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceSumSquareLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceSumSquareLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceSumSquareLayerParams& from);
  void MergeFrom(const ReduceSumSquareLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceSumSquareLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceSumSquareLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReduceLogSumExpLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLogSumExpLayerParams) */ {
 public:
  ReduceLogSumExpLayerParams();
  virtual ~ReduceLogSumExpLayerParams();

  ReduceLogSumExpLayerParams(const ReduceLogSumExpLayerParams& from);

  inline ReduceLogSumExpLayerParams& operator=(const ReduceLogSumExpLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceLogSumExpLayerParams& default_instance();

  static inline const ReduceLogSumExpLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReduceLogSumExpLayerParams*>(
               &_ReduceLogSumExpLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    160;

  void Swap(ReduceLogSumExpLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceLogSumExpLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReduceLogSumExpLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReduceLogSumExpLayerParams& from);
  void MergeFrom(const ReduceLogSumExpLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLogSumExpLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool keepDims = 2;
  void clear_keepdims();
  static const int kKeepDimsFieldNumber = 2;
  bool keepdims() const;
  void set_keepdims(bool value);

  // bool reduceAll = 3;
  void clear_reduceall();
  static const int kReduceAllFieldNumber = 3;
  bool reduceall() const;
  void set_reduceall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLogSumExpLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool keepdims_;
  bool reduceall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ExpandDimsLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ExpandDimsLayerParams) */ {
 public:
  ExpandDimsLayerParams();
  virtual ~ExpandDimsLayerParams();

  ExpandDimsLayerParams(const ExpandDimsLayerParams& from);

  inline ExpandDimsLayerParams& operator=(const ExpandDimsLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ExpandDimsLayerParams& default_instance();

  static inline const ExpandDimsLayerParams* internal_default_instance() {
    return reinterpret_cast<const ExpandDimsLayerParams*>(
               &_ExpandDimsLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    161;

  void Swap(ExpandDimsLayerParams* other);

  // implements Message ----------------------------------------------

  inline ExpandDimsLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ExpandDimsLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ExpandDimsLayerParams& from);
  void MergeFrom(const ExpandDimsLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ExpandDimsLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ExpandDimsLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FlattenTo2DLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FlattenTo2DLayerParams) */ {
 public:
  FlattenTo2DLayerParams();
  virtual ~FlattenTo2DLayerParams();

  FlattenTo2DLayerParams(const FlattenTo2DLayerParams& from);

  inline FlattenTo2DLayerParams& operator=(const FlattenTo2DLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FlattenTo2DLayerParams& default_instance();

  static inline const FlattenTo2DLayerParams* internal_default_instance() {
    return reinterpret_cast<const FlattenTo2DLayerParams*>(
               &_FlattenTo2DLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    162;

  void Swap(FlattenTo2DLayerParams* other);

  // implements Message ----------------------------------------------

  inline FlattenTo2DLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FlattenTo2DLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FlattenTo2DLayerParams& from);
  void MergeFrom(const FlattenTo2DLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FlattenTo2DLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FlattenTo2DLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReshapeStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeStaticLayerParams) */ {
 public:
  ReshapeStaticLayerParams();
  virtual ~ReshapeStaticLayerParams();

  ReshapeStaticLayerParams(const ReshapeStaticLayerParams& from);

  inline ReshapeStaticLayerParams& operator=(const ReshapeStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReshapeStaticLayerParams& default_instance();

  static inline const ReshapeStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeStaticLayerParams*>(
               &_ReshapeStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    163;

  void Swap(ReshapeStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReshapeStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReshapeStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReshapeStaticLayerParams& from);
  void MergeFrom(const ReshapeStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  void clear_targetshape();
  static const int kTargetShapeFieldNumber = 1;
  ::google::protobuf::int64 targetshape(int index) const;
  void set_targetshape(int index, ::google::protobuf::int64 value);
  void add_targetshape(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      targetshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_targetshape();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > targetshape_;
  mutable int _targetshape_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReshapeLikeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeLikeLayerParams) */ {
 public:
  ReshapeLikeLayerParams();
  virtual ~ReshapeLikeLayerParams();

  ReshapeLikeLayerParams(const ReshapeLikeLayerParams& from);

  inline ReshapeLikeLayerParams& operator=(const ReshapeLikeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReshapeLikeLayerParams& default_instance();

  static inline const ReshapeLikeLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeLikeLayerParams*>(
               &_ReshapeLikeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    164;

  void Swap(ReshapeLikeLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReshapeLikeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReshapeLikeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReshapeLikeLayerParams& from);
  void MergeFrom(const ReshapeLikeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeLikeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeLikeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReshapeDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeDynamicLayerParams) */ {
 public:
  ReshapeDynamicLayerParams();
  virtual ~ReshapeDynamicLayerParams();

  ReshapeDynamicLayerParams(const ReshapeDynamicLayerParams& from);

  inline ReshapeDynamicLayerParams& operator=(const ReshapeDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReshapeDynamicLayerParams& default_instance();

  static inline const ReshapeDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const ReshapeDynamicLayerParams*>(
               &_ReshapeDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    165;

  void Swap(ReshapeDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReshapeDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ReshapeDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ReshapeDynamicLayerParams& from);
  void MergeFrom(const ReshapeDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SqueezeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SqueezeLayerParams) */ {
 public:
  SqueezeLayerParams();
  virtual ~SqueezeLayerParams();

  SqueezeLayerParams(const SqueezeLayerParams& from);

  inline SqueezeLayerParams& operator=(const SqueezeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SqueezeLayerParams& default_instance();

  static inline const SqueezeLayerParams* internal_default_instance() {
    return reinterpret_cast<const SqueezeLayerParams*>(
               &_SqueezeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    166;

  void Swap(SqueezeLayerParams* other);

  // implements Message ----------------------------------------------

  inline SqueezeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SqueezeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SqueezeLayerParams& from);
  void MergeFrom(const SqueezeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SqueezeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 axes = 1;
  int axes_size() const;
  void clear_axes();
  static const int kAxesFieldNumber = 1;
  ::google::protobuf::int64 axes(int index) const;
  void set_axes(int index, ::google::protobuf::int64 value);
  void add_axes(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      axes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_axes();

  // bool squeezeAll = 2;
  void clear_squeezeall();
  static const int kSqueezeAllFieldNumber = 2;
  bool squeezeall() const;
  void set_squeezeall(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SqueezeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > axes_;
  mutable int _axes_cached_byte_size_;
  bool squeezeall_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class TopKLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TopKLayerParams) */ {
 public:
  TopKLayerParams();
  virtual ~TopKLayerParams();

  TopKLayerParams(const TopKLayerParams& from);

  inline TopKLayerParams& operator=(const TopKLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const TopKLayerParams& default_instance();

  static inline const TopKLayerParams* internal_default_instance() {
    return reinterpret_cast<const TopKLayerParams*>(
               &_TopKLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    167;

  void Swap(TopKLayerParams* other);

  // implements Message ----------------------------------------------

  inline TopKLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  TopKLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const TopKLayerParams& from);
  void MergeFrom(const TopKLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TopKLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // uint64 K = 2;
  void clear_k();
  static const int kKFieldNumber = 2;
  ::google::protobuf::uint64 k() const;
  void set_k(::google::protobuf::uint64 value);

  // bool useBottomK = 3;
  void clear_usebottomk();
  static const int kUseBottomKFieldNumber = 3;
  bool usebottomk() const;
  void set_usebottomk(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TopKLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  ::google::protobuf::uint64 k_;
  bool usebottomk_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ArgMaxLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ArgMaxLayerParams) */ {
 public:
  ArgMaxLayerParams();
  virtual ~ArgMaxLayerParams();

  ArgMaxLayerParams(const ArgMaxLayerParams& from);

  inline ArgMaxLayerParams& operator=(const ArgMaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ArgMaxLayerParams& default_instance();

  static inline const ArgMaxLayerParams* internal_default_instance() {
    return reinterpret_cast<const ArgMaxLayerParams*>(
               &_ArgMaxLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    168;

  void Swap(ArgMaxLayerParams* other);

  // implements Message ----------------------------------------------

  inline ArgMaxLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ArgMaxLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ArgMaxLayerParams& from);
  void MergeFrom(const ArgMaxLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ArgMaxLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // bool removeDim = 2;
  void clear_removedim();
  static const int kRemoveDimFieldNumber = 2;
  bool removedim() const;
  void set_removedim(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ArgMaxLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  bool removedim_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ArgMinLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ArgMinLayerParams) */ {
 public:
  ArgMinLayerParams();
  virtual ~ArgMinLayerParams();

  ArgMinLayerParams(const ArgMinLayerParams& from);

  inline ArgMinLayerParams& operator=(const ArgMinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ArgMinLayerParams& default_instance();

  static inline const ArgMinLayerParams* internal_default_instance() {
    return reinterpret_cast<const ArgMinLayerParams*>(
               &_ArgMinLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    169;

  void Swap(ArgMinLayerParams* other);

  // implements Message ----------------------------------------------

  inline ArgMinLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ArgMinLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ArgMinLayerParams& from);
  void MergeFrom(const ArgMinLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ArgMinLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // bool removeDim = 2;
  void clear_removedim();
  static const int kRemoveDimFieldNumber = 2;
  bool removedim() const;
  void set_removedim(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ArgMinLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  bool removedim_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SplitNDLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SplitNDLayerParams) */ {
 public:
  SplitNDLayerParams();
  virtual ~SplitNDLayerParams();

  SplitNDLayerParams(const SplitNDLayerParams& from);

  inline SplitNDLayerParams& operator=(const SplitNDLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SplitNDLayerParams& default_instance();

  static inline const SplitNDLayerParams* internal_default_instance() {
    return reinterpret_cast<const SplitNDLayerParams*>(
               &_SplitNDLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    170;

  void Swap(SplitNDLayerParams* other);

  // implements Message ----------------------------------------------

  inline SplitNDLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SplitNDLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SplitNDLayerParams& from);
  void MergeFrom(const SplitNDLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SplitNDLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 splitSizes = 3;
  int splitsizes_size() const;
  void clear_splitsizes();
  static const int kSplitSizesFieldNumber = 3;
  ::google::protobuf::uint64 splitsizes(int index) const;
  void set_splitsizes(int index, ::google::protobuf::uint64 value);
  void add_splitsizes(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      splitsizes() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_splitsizes();

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // uint64 numSplits = 2;
  void clear_numsplits();
  static const int kNumSplitsFieldNumber = 2;
  ::google::protobuf::uint64 numsplits() const;
  void set_numsplits(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SplitNDLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > splitsizes_;
  mutable int _splitsizes_cached_byte_size_;
  ::google::protobuf::int64 axis_;
  ::google::protobuf::uint64 numsplits_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CeilLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CeilLayerParams) */ {
 public:
  CeilLayerParams();
  virtual ~CeilLayerParams();

  CeilLayerParams(const CeilLayerParams& from);

  inline CeilLayerParams& operator=(const CeilLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CeilLayerParams& default_instance();

  static inline const CeilLayerParams* internal_default_instance() {
    return reinterpret_cast<const CeilLayerParams*>(
               &_CeilLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    171;

  void Swap(CeilLayerParams* other);

  // implements Message ----------------------------------------------

  inline CeilLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  CeilLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CeilLayerParams& from);
  void MergeFrom(const CeilLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CeilLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CeilLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RoundLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RoundLayerParams) */ {
 public:
  RoundLayerParams();
  virtual ~RoundLayerParams();

  RoundLayerParams(const RoundLayerParams& from);

  inline RoundLayerParams& operator=(const RoundLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RoundLayerParams& default_instance();

  static inline const RoundLayerParams* internal_default_instance() {
    return reinterpret_cast<const RoundLayerParams*>(
               &_RoundLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    172;

  void Swap(RoundLayerParams* other);

  // implements Message ----------------------------------------------

  inline RoundLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RoundLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RoundLayerParams& from);
  void MergeFrom(const RoundLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RoundLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RoundLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FloorLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FloorLayerParams) */ {
 public:
  FloorLayerParams();
  virtual ~FloorLayerParams();

  FloorLayerParams(const FloorLayerParams& from);

  inline FloorLayerParams& operator=(const FloorLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FloorLayerParams& default_instance();

  static inline const FloorLayerParams* internal_default_instance() {
    return reinterpret_cast<const FloorLayerParams*>(
               &_FloorLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    173;

  void Swap(FloorLayerParams* other);

  // implements Message ----------------------------------------------

  inline FloorLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  FloorLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const FloorLayerParams& from);
  void MergeFrom(const FloorLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FloorLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FloorLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SignLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SignLayerParams) */ {
 public:
  SignLayerParams();
  virtual ~SignLayerParams();

  SignLayerParams(const SignLayerParams& from);

  inline SignLayerParams& operator=(const SignLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SignLayerParams& default_instance();

  static inline const SignLayerParams* internal_default_instance() {
    return reinterpret_cast<const SignLayerParams*>(
               &_SignLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    174;

  void Swap(SignLayerParams* other);

  // implements Message ----------------------------------------------

  inline SignLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SignLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SignLayerParams& from);
  void MergeFrom(const SignLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SignLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SignLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ClipLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ClipLayerParams) */ {
 public:
  ClipLayerParams();
  virtual ~ClipLayerParams();

  ClipLayerParams(const ClipLayerParams& from);

  inline ClipLayerParams& operator=(const ClipLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ClipLayerParams& default_instance();

  static inline const ClipLayerParams* internal_default_instance() {
    return reinterpret_cast<const ClipLayerParams*>(
               &_ClipLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    175;

  void Swap(ClipLayerParams* other);

  // implements Message ----------------------------------------------

  inline ClipLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ClipLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ClipLayerParams& from);
  void MergeFrom(const ClipLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ClipLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float minVal = 1;
  void clear_minval();
  static const int kMinValFieldNumber = 1;
  float minval() const;
  void set_minval(float value);

  // float maxVal = 2;
  void clear_maxval();
  static const int kMaxValFieldNumber = 2;
  float maxval() const;
  void set_maxval(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ClipLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float minval_;
  float maxval_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SliceStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceStaticLayerParams) */ {
 public:
  SliceStaticLayerParams();
  virtual ~SliceStaticLayerParams();

  SliceStaticLayerParams(const SliceStaticLayerParams& from);

  inline SliceStaticLayerParams& operator=(const SliceStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SliceStaticLayerParams& default_instance();

  static inline const SliceStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceStaticLayerParams*>(
               &_SliceStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    176;

  void Swap(SliceStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline SliceStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SliceStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SliceStaticLayerParams& from);
  void MergeFrom(const SliceStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 beginIds = 1;
  int beginids_size() const;
  void clear_beginids();
  static const int kBeginIdsFieldNumber = 1;
  ::google::protobuf::int64 beginids(int index) const;
  void set_beginids(int index, ::google::protobuf::int64 value);
  void add_beginids(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      beginids() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_beginids();

  // repeated bool beginMasks = 2;
  int beginmasks_size() const;
  void clear_beginmasks();
  static const int kBeginMasksFieldNumber = 2;
  bool beginmasks(int index) const;
  void set_beginmasks(int index, bool value);
  void add_beginmasks(bool value);
  const ::google::protobuf::RepeatedField< bool >&
      beginmasks() const;
  ::google::protobuf::RepeatedField< bool >*
      mutable_beginmasks();

  // repeated int64 endIds = 3;
  int endids_size() const;
  void clear_endids();
  static const int kEndIdsFieldNumber = 3;
  ::google::protobuf::int64 endids(int index) const;
  void set_endids(int index, ::google::protobuf::int64 value);
  void add_endids(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      endids() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_endids();

  // repeated bool endMasks = 4;
  int endmasks_size() const;
  void clear_endmasks();
  static const int kEndMasksFieldNumber = 4;
  bool endmasks(int index) const;
  void set_endmasks(int index, bool value);
  void add_endmasks(bool value);
  const ::google::protobuf::RepeatedField< bool >&
      endmasks() const;
  ::google::protobuf::RepeatedField< bool >*
      mutable_endmasks();

  // repeated int64 strides = 5;
  int strides_size() const;
  void clear_strides();
  static const int kStridesFieldNumber = 5;
  ::google::protobuf::int64 strides(int index) const;
  void set_strides(int index, ::google::protobuf::int64 value);
  void add_strides(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      strides() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_strides();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > beginids_;
  mutable int _beginids_cached_byte_size_;
  ::google::protobuf::RepeatedField< bool > beginmasks_;
  mutable int _beginmasks_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > endids_;
  mutable int _endids_cached_byte_size_;
  ::google::protobuf::RepeatedField< bool > endmasks_;
  mutable int _endmasks_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > strides_;
  mutable int _strides_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SliceDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SliceDynamicLayerParams) */ {
 public:
  SliceDynamicLayerParams();
  virtual ~SliceDynamicLayerParams();

  SliceDynamicLayerParams(const SliceDynamicLayerParams& from);

  inline SliceDynamicLayerParams& operator=(const SliceDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SliceDynamicLayerParams& default_instance();

  static inline const SliceDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const SliceDynamicLayerParams*>(
               &_SliceDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    177;

  void Swap(SliceDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline SliceDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SliceDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SliceDynamicLayerParams& from);
  void MergeFrom(const SliceDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SliceDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated bool beginMasks = 2;
  int beginmasks_size() const;
  void clear_beginmasks();
  static const int kBeginMasksFieldNumber = 2;
  bool beginmasks(int index) const;
  void set_beginmasks(int index, bool value);
  void add_beginmasks(bool value);
  const ::google::protobuf::RepeatedField< bool >&
      beginmasks() const;
  ::google::protobuf::RepeatedField< bool >*
      mutable_beginmasks();

  // repeated int64 endIds = 3;
  int endids_size() const;
  void clear_endids();
  static const int kEndIdsFieldNumber = 3;
  ::google::protobuf::int64 endids(int index) const;
  void set_endids(int index, ::google::protobuf::int64 value);
  void add_endids(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      endids() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_endids();

  // repeated bool endMasks = 4;
  int endmasks_size() const;
  void clear_endmasks();
  static const int kEndMasksFieldNumber = 4;
  bool endmasks(int index) const;
  void set_endmasks(int index, bool value);
  void add_endmasks(bool value);
  const ::google::protobuf::RepeatedField< bool >&
      endmasks() const;
  ::google::protobuf::RepeatedField< bool >*
      mutable_endmasks();

  // repeated int64 strides = 5;
  int strides_size() const;
  void clear_strides();
  static const int kStridesFieldNumber = 5;
  ::google::protobuf::int64 strides(int index) const;
  void set_strides(int index, ::google::protobuf::int64 value);
  void add_strides(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      strides() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_strides();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SliceDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< bool > beginmasks_;
  mutable int _beginmasks_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > endids_;
  mutable int _endids_cached_byte_size_;
  ::google::protobuf::RepeatedField< bool > endmasks_;
  mutable int _endmasks_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > strides_;
  mutable int _strides_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class TileLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.TileLayerParams) */ {
 public:
  TileLayerParams();
  virtual ~TileLayerParams();

  TileLayerParams(const TileLayerParams& from);

  inline TileLayerParams& operator=(const TileLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const TileLayerParams& default_instance();

  static inline const TileLayerParams* internal_default_instance() {
    return reinterpret_cast<const TileLayerParams*>(
               &_TileLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    178;

  void Swap(TileLayerParams* other);

  // implements Message ----------------------------------------------

  inline TileLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  TileLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const TileLayerParams& from);
  void MergeFrom(const TileLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(TileLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 reps = 1;
  int reps_size() const;
  void clear_reps();
  static const int kRepsFieldNumber = 1;
  ::google::protobuf::uint64 reps(int index) const;
  void set_reps(int index, ::google::protobuf::uint64 value);
  void add_reps(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      reps() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_reps();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.TileLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > reps_;
  mutable int _reps_cached_byte_size_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GetShapeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GetShapeLayerParams) */ {
 public:
  GetShapeLayerParams();
  virtual ~GetShapeLayerParams();

  GetShapeLayerParams(const GetShapeLayerParams& from);

  inline GetShapeLayerParams& operator=(const GetShapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GetShapeLayerParams& default_instance();

  static inline const GetShapeLayerParams* internal_default_instance() {
    return reinterpret_cast<const GetShapeLayerParams*>(
               &_GetShapeLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    179;

  void Swap(GetShapeLayerParams* other);

  // implements Message ----------------------------------------------

  inline GetShapeLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GetShapeLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GetShapeLayerParams& from);
  void MergeFrom(const GetShapeLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GetShapeLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GetShapeLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ErfLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ErfLayerParams) */ {
 public:
  ErfLayerParams();
  virtual ~ErfLayerParams();

  ErfLayerParams(const ErfLayerParams& from);

  inline ErfLayerParams& operator=(const ErfLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ErfLayerParams& default_instance();

  static inline const ErfLayerParams* internal_default_instance() {
    return reinterpret_cast<const ErfLayerParams*>(
               &_ErfLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    180;

  void Swap(ErfLayerParams* other);

  // implements Message ----------------------------------------------

  inline ErfLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  ErfLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const ErfLayerParams& from);
  void MergeFrom(const ErfLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ErfLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ErfLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GeluLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GeluLayerParams) */ {
 public:
  GeluLayerParams();
  virtual ~GeluLayerParams();

  GeluLayerParams(const GeluLayerParams& from);

  inline GeluLayerParams& operator=(const GeluLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GeluLayerParams& default_instance();

  static inline const GeluLayerParams* internal_default_instance() {
    return reinterpret_cast<const GeluLayerParams*>(
               &_GeluLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    181;

  void Swap(GeluLayerParams* other);

  // implements Message ----------------------------------------------

  inline GeluLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  GeluLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const GeluLayerParams& from);
  void MergeFrom(const GeluLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GeluLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  typedef GeluLayerParams_GeluMode GeluMode;
  static const GeluMode EXACT =
    GeluLayerParams_GeluMode_EXACT;
  static const GeluMode TANH_APPROXIMATION =
    GeluLayerParams_GeluMode_TANH_APPROXIMATION;
  static const GeluMode SIGMOID_APPROXIMATION =
    GeluLayerParams_GeluMode_SIGMOID_APPROXIMATION;
  static inline bool GeluMode_IsValid(int value) {
    return GeluLayerParams_GeluMode_IsValid(value);
  }
  static const GeluMode GeluMode_MIN =
    GeluLayerParams_GeluMode_GeluMode_MIN;
  static const GeluMode GeluMode_MAX =
    GeluLayerParams_GeluMode_GeluMode_MAX;
  static const int GeluMode_ARRAYSIZE =
    GeluLayerParams_GeluMode_GeluMode_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::GeluLayerParams_GeluMode mode() const;
  void set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GeluLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  int mode_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RangeStaticLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RangeStaticLayerParams) */ {
 public:
  RangeStaticLayerParams();
  virtual ~RangeStaticLayerParams();

  RangeStaticLayerParams(const RangeStaticLayerParams& from);

  inline RangeStaticLayerParams& operator=(const RangeStaticLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RangeStaticLayerParams& default_instance();

  static inline const RangeStaticLayerParams* internal_default_instance() {
    return reinterpret_cast<const RangeStaticLayerParams*>(
               &_RangeStaticLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    182;

  void Swap(RangeStaticLayerParams* other);

  // implements Message ----------------------------------------------

  inline RangeStaticLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RangeStaticLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RangeStaticLayerParams& from);
  void MergeFrom(const RangeStaticLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RangeStaticLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float endValue = 1;
  void clear_endvalue();
  static const int kEndValueFieldNumber = 1;
  float endvalue() const;
  void set_endvalue(float value);

  // float startValue = 2;
  void clear_startvalue();
  static const int kStartValueFieldNumber = 2;
  float startvalue() const;
  void set_startvalue(float value);

  // float stepSizeValue = 3;
  void clear_stepsizevalue();
  static const int kStepSizeValueFieldNumber = 3;
  float stepsizevalue() const;
  void set_stepsizevalue(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RangeStaticLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float endvalue_;
  float startvalue_;
  float stepsizevalue_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RangeDynamicLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.RangeDynamicLayerParams) */ {
 public:
  RangeDynamicLayerParams();
  virtual ~RangeDynamicLayerParams();

  RangeDynamicLayerParams(const RangeDynamicLayerParams& from);

  inline RangeDynamicLayerParams& operator=(const RangeDynamicLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const RangeDynamicLayerParams& default_instance();

  static inline const RangeDynamicLayerParams* internal_default_instance() {
    return reinterpret_cast<const RangeDynamicLayerParams*>(
               &_RangeDynamicLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    183;

  void Swap(RangeDynamicLayerParams* other);

  // implements Message ----------------------------------------------

  inline RangeDynamicLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  RangeDynamicLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const RangeDynamicLayerParams& from);
  void MergeFrom(const RangeDynamicLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RangeDynamicLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float startValue = 2;
  void clear_startvalue();
  static const int kStartValueFieldNumber = 2;
  float startvalue() const;
  void set_startvalue(float value);

  // float stepSizeValue = 3;
  void clear_stepsizevalue();
  static const int kStepSizeValueFieldNumber = 3;
  float stepsizevalue() const;
  void set_stepsizevalue(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.RangeDynamicLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float startvalue_;
  float stepsizevalue_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SlidingWindowsLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SlidingWindowsLayerParams) */ {
 public:
  SlidingWindowsLayerParams();
  virtual ~SlidingWindowsLayerParams();

  SlidingWindowsLayerParams(const SlidingWindowsLayerParams& from);

  inline SlidingWindowsLayerParams& operator=(const SlidingWindowsLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SlidingWindowsLayerParams& default_instance();

  static inline const SlidingWindowsLayerParams* internal_default_instance() {
    return reinterpret_cast<const SlidingWindowsLayerParams*>(
               &_SlidingWindowsLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    184;

  void Swap(SlidingWindowsLayerParams* other);

  // implements Message ----------------------------------------------

  inline SlidingWindowsLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  SlidingWindowsLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SlidingWindowsLayerParams& from);
  void MergeFrom(const SlidingWindowsLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SlidingWindowsLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 axis = 1;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::int64 axis() const;
  void set_axis(::google::protobuf::int64 value);

  // uint64 windowSize = 2;
  void clear_windowsize();
  static const int kWindowSizeFieldNumber = 2;
  ::google::protobuf::uint64 windowsize() const;
  void set_windowsize(::google::protobuf::uint64 value);

  // uint64 step = 3;
  void clear_step();
  static const int kStepFieldNumber = 3;
  ::google::protobuf::uint64 step() const;
  void set_step(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SlidingWindowsLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::int64 axis_;
  ::google::protobuf::uint64 windowsize_;
  ::google::protobuf::uint64 step_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LayerNormalizationLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LayerNormalizationLayerParams) */ {
 public:
  LayerNormalizationLayerParams();
  virtual ~LayerNormalizationLayerParams();

  LayerNormalizationLayerParams(const LayerNormalizationLayerParams& from);

  inline LayerNormalizationLayerParams& operator=(const LayerNormalizationLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LayerNormalizationLayerParams& default_instance();

  static inline const LayerNormalizationLayerParams* internal_default_instance() {
    return reinterpret_cast<const LayerNormalizationLayerParams*>(
               &_LayerNormalizationLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    185;

  void Swap(LayerNormalizationLayerParams* other);

  // implements Message ----------------------------------------------

  inline LayerNormalizationLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  LayerNormalizationLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LayerNormalizationLayerParams& from);
  void MergeFrom(const LayerNormalizationLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LayerNormalizationLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 normalizedShape = 1;
  int normalizedshape_size() const;
  void clear_normalizedshape();
  static const int kNormalizedShapeFieldNumber = 1;
  ::google::protobuf::int64 normalizedshape(int index) const;
  void set_normalizedshape(int index, ::google::protobuf::int64 value);
  void add_normalizedshape(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      normalizedshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_normalizedshape();

  // .CoreML.Specification.WeightParams gamma = 3;
  bool has_gamma() const;
  void clear_gamma();
  static const int kGammaFieldNumber = 3;
  const ::CoreML::Specification::WeightParams& gamma() const;
  ::CoreML::Specification::WeightParams* mutable_gamma();
  ::CoreML::Specification::WeightParams* release_gamma();
  void set_allocated_gamma(::CoreML::Specification::WeightParams* gamma);

  // .CoreML.Specification.WeightParams beta = 4;
  bool has_beta() const;
  void clear_beta();
  static const int kBetaFieldNumber = 4;
  const ::CoreML::Specification::WeightParams& beta() const;
  ::CoreML::Specification::WeightParams* mutable_beta();
  ::CoreML::Specification::WeightParams* release_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);

  // float eps = 2;
  void clear_eps();
  static const int kEpsFieldNumber = 2;
  float eps() const;
  void set_eps(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LayerNormalizationLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > normalizedshape_;
  mutable int _normalizedshape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* gamma_;
  ::CoreML::Specification::WeightParams* beta_;
  float eps_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NonMaximumSuppressionLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NonMaximumSuppressionLayerParams) */ {
 public:
  NonMaximumSuppressionLayerParams();
  virtual ~NonMaximumSuppressionLayerParams();

  NonMaximumSuppressionLayerParams(const NonMaximumSuppressionLayerParams& from);

  inline NonMaximumSuppressionLayerParams& operator=(const NonMaximumSuppressionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const NonMaximumSuppressionLayerParams& default_instance();

  static inline const NonMaximumSuppressionLayerParams* internal_default_instance() {
    return reinterpret_cast<const NonMaximumSuppressionLayerParams*>(
               &_NonMaximumSuppressionLayerParams_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    186;

  void Swap(NonMaximumSuppressionLayerParams* other);

  // implements Message ----------------------------------------------

  inline NonMaximumSuppressionLayerParams* New() const PROTOBUF_FINAL { return New(NULL); }

  NonMaximumSuppressionLayerParams* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NonMaximumSuppressionLayerParams& from);
  void MergeFrom(const NonMaximumSuppressionLayerParams& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NonMaximumSuppressionLayerParams* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float iouThreshold = 1;
  void clear_iouthreshold();
  static const int kIouThresholdFieldNumber = 1;
  float iouthreshold() const;
  void set_iouthreshold(float value);

  // float scoreThreshold = 2;
  void clear_scorethreshold();
  static const int kScoreThresholdFieldNumber = 2;
  float scorethreshold() const;
  void set_scorethreshold(float value);

  // uint64 maxBoxes = 3;
  void clear_maxboxes();
  static const int kMaxBoxesFieldNumber = 3;
  ::google::protobuf::uint64 maxboxes() const;
  void set_maxboxes(::google::protobuf::uint64 value);

  // bool perClassSuppression = 4;
  void clear_perclasssuppression();
  static const int kPerClassSuppressionFieldNumber = 4;
  bool perclasssuppression() const;
  void set_perclasssuppression(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NonMaximumSuppressionLayerParams)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  float iouthreshold_;
  float scorethreshold_;
  ::google::protobuf::uint64 maxboxes_;
  bool perclasssuppression_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NeuralNetworkClassifier : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkClassifier) */ {
 public:
  NeuralNetworkClassifier();
  virtual ~NeuralNetworkClassifier();

  NeuralNetworkClassifier(const NeuralNetworkClassifier& from);

  inline NeuralNetworkClassifier& operator=(const NeuralNetworkClassifier& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkClassifier& default_instance();

  enum ClassLabelsCase {
    kStringClassLabels = 100,
    kInt64ClassLabels = 101,
    CLASSLABELS_NOT_SET = 0,
  };

  static inline const NeuralNetworkClassifier* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkClassifier*>(
               &_NeuralNetworkClassifier_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    187;

  void Swap(NeuralNetworkClassifier* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkClassifier* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetworkClassifier* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetworkClassifier& from);
  void MergeFrom(const NeuralNetworkClassifier& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkClassifier* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  void clear_layers();
  static const int kLayersFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  void clear_preprocessing();
  static const int kPreprocessingFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // string labelProbabilityLayerName = 200;
  void clear_labelprobabilitylayername();
  static const int kLabelProbabilityLayerNameFieldNumber = 200;
  const ::std::string& labelprobabilitylayername() const;
  void set_labelprobabilitylayername(const ::std::string& value);
  #if LANG_CXX11
  void set_labelprobabilitylayername(::std::string&& value);
  #endif
  void set_labelprobabilitylayername(const char* value);
  void set_labelprobabilitylayername(const char* value, size_t size);
  ::std::string* mutable_labelprobabilitylayername();
  ::std::string* release_labelprobabilitylayername();
  void set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername);

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  bool has_updateparams() const;
  void clear_updateparams();
  static const int kUpdateParamsFieldNumber = 10;
  const ::CoreML::Specification::NetworkUpdateParameters& updateparams() const;
  ::CoreML::Specification::NetworkUpdateParameters* mutable_updateparams();
  ::CoreML::Specification::NetworkUpdateParameters* release_updateparams();
  void set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams);

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  void clear_arrayinputshapemapping();
  static const int kArrayInputShapeMappingFieldNumber = 5;
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping arrayinputshapemapping() const;
  void set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  void clear_imageinputshapemapping();
  static const int kImageInputShapeMappingFieldNumber = 6;
  ::CoreML::Specification::NeuralNetworkImageShapeMapping imageinputshapemapping() const;
  void set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);

  // .CoreML.Specification.StringVector stringClassLabels = 100;
  bool has_stringclasslabels() const;
  void clear_stringclasslabels();
  static const int kStringClassLabelsFieldNumber = 100;
  const ::CoreML::Specification::StringVector& stringclasslabels() const;
  ::CoreML::Specification::StringVector* mutable_stringclasslabels();
  ::CoreML::Specification::StringVector* release_stringclasslabels();
  void set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels);

  // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  bool has_int64classlabels() const;
  void clear_int64classlabels();
  static const int kInt64ClassLabelsFieldNumber = 101;
  const ::CoreML::Specification::Int64Vector& int64classlabels() const;
  ::CoreML::Specification::Int64Vector* mutable_int64classlabels();
  ::CoreML::Specification::Int64Vector* release_int64classlabels();
  void set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels);

  ClassLabelsCase ClassLabels_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkClassifier)
 private:
  void set_has_stringclasslabels();
  void set_has_int64classlabels();

  inline bool has_ClassLabels() const;
  void clear_ClassLabels();
  inline void clear_has_ClassLabels();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::google::protobuf::internal::ArenaStringPtr labelprobabilitylayername_;
  ::CoreML::Specification::NetworkUpdateParameters* updateparams_;
  int arrayinputshapemapping_;
  int imageinputshapemapping_;
  union ClassLabelsUnion {
    ClassLabelsUnion() {}
    ::CoreML::Specification::StringVector* stringclasslabels_;
    ::CoreML::Specification::Int64Vector* int64classlabels_;
  } ClassLabels_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NeuralNetworkRegressor : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkRegressor) */ {
 public:
  NeuralNetworkRegressor();
  virtual ~NeuralNetworkRegressor();

  NeuralNetworkRegressor(const NeuralNetworkRegressor& from);

  inline NeuralNetworkRegressor& operator=(const NeuralNetworkRegressor& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkRegressor& default_instance();

  static inline const NeuralNetworkRegressor* internal_default_instance() {
    return reinterpret_cast<const NeuralNetworkRegressor*>(
               &_NeuralNetworkRegressor_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    188;

  void Swap(NeuralNetworkRegressor* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkRegressor* New() const PROTOBUF_FINAL { return New(NULL); }

  NeuralNetworkRegressor* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NeuralNetworkRegressor& from);
  void MergeFrom(const NeuralNetworkRegressor& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkRegressor* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  void clear_layers();
  static const int kLayersFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  void clear_preprocessing();
  static const int kPreprocessingFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  bool has_updateparams() const;
  void clear_updateparams();
  static const int kUpdateParamsFieldNumber = 10;
  const ::CoreML::Specification::NetworkUpdateParameters& updateparams() const;
  ::CoreML::Specification::NetworkUpdateParameters* mutable_updateparams();
  ::CoreML::Specification::NetworkUpdateParameters* release_updateparams();
  void set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams);

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  void clear_arrayinputshapemapping();
  static const int kArrayInputShapeMappingFieldNumber = 5;
  ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping arrayinputshapemapping() const;
  void set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value);

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  void clear_imageinputshapemapping();
  static const int kImageInputShapeMappingFieldNumber = 6;
  ::CoreML::Specification::NeuralNetworkImageShapeMapping imageinputshapemapping() const;
  void set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkRegressor)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::CoreML::Specification::NetworkUpdateParameters* updateparams_;
  int arrayinputshapemapping_;
  int imageinputshapemapping_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class NetworkUpdateParameters : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NetworkUpdateParameters) */ {
 public:
  NetworkUpdateParameters();
  virtual ~NetworkUpdateParameters();

  NetworkUpdateParameters(const NetworkUpdateParameters& from);

  inline NetworkUpdateParameters& operator=(const NetworkUpdateParameters& from) {
    CopyFrom(from);
    return *this;
  }

  static const NetworkUpdateParameters& default_instance();

  static inline const NetworkUpdateParameters* internal_default_instance() {
    return reinterpret_cast<const NetworkUpdateParameters*>(
               &_NetworkUpdateParameters_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    189;

  void Swap(NetworkUpdateParameters* other);

  // implements Message ----------------------------------------------

  inline NetworkUpdateParameters* New() const PROTOBUF_FINAL { return New(NULL); }

  NetworkUpdateParameters* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const NetworkUpdateParameters& from);
  void MergeFrom(const NetworkUpdateParameters& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NetworkUpdateParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.LossLayer lossLayers = 1;
  int losslayers_size() const;
  void clear_losslayers();
  static const int kLossLayersFieldNumber = 1;
  const ::CoreML::Specification::LossLayer& losslayers(int index) const;
  ::CoreML::Specification::LossLayer* mutable_losslayers(int index);
  ::CoreML::Specification::LossLayer* add_losslayers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >*
      mutable_losslayers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >&
      losslayers() const;

  // .CoreML.Specification.Optimizer optimizer = 2;
  bool has_optimizer() const;
  void clear_optimizer();
  static const int kOptimizerFieldNumber = 2;
  const ::CoreML::Specification::Optimizer& optimizer() const;
  ::CoreML::Specification::Optimizer* mutable_optimizer();
  ::CoreML::Specification::Optimizer* release_optimizer();
  void set_allocated_optimizer(::CoreML::Specification::Optimizer* optimizer);

  // .CoreML.Specification.Int64Parameter epochs = 3;
  bool has_epochs() const;
  void clear_epochs();
  static const int kEpochsFieldNumber = 3;
  const ::CoreML::Specification::Int64Parameter& epochs() const;
  ::CoreML::Specification::Int64Parameter* mutable_epochs();
  ::CoreML::Specification::Int64Parameter* release_epochs();
  void set_allocated_epochs(::CoreML::Specification::Int64Parameter* epochs);

  // .CoreML.Specification.BoolParameter shuffle = 10;
  bool has_shuffle() const;
  void clear_shuffle();
  static const int kShuffleFieldNumber = 10;
  const ::CoreML::Specification::BoolParameter& shuffle() const;
  ::CoreML::Specification::BoolParameter* mutable_shuffle();
  ::CoreML::Specification::BoolParameter* release_shuffle();
  void set_allocated_shuffle(::CoreML::Specification::BoolParameter* shuffle);

  // .CoreML.Specification.Int64Parameter seed = 20;
  bool has_seed() const;
  void clear_seed();
  static const int kSeedFieldNumber = 20;
  const ::CoreML::Specification::Int64Parameter& seed() const;
  ::CoreML::Specification::Int64Parameter* mutable_seed();
  ::CoreML::Specification::Int64Parameter* release_seed();
  void set_allocated_seed(::CoreML::Specification::Int64Parameter* seed);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NetworkUpdateParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer > losslayers_;
  ::CoreML::Specification::Optimizer* optimizer_;
  ::CoreML::Specification::Int64Parameter* epochs_;
  ::CoreML::Specification::BoolParameter* shuffle_;
  ::CoreML::Specification::Int64Parameter* seed_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LossLayer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LossLayer) */ {
 public:
  LossLayer();
  virtual ~LossLayer();

  LossLayer(const LossLayer& from);

  inline LossLayer& operator=(const LossLayer& from) {
    CopyFrom(from);
    return *this;
  }

  static const LossLayer& default_instance();

  enum LossLayerTypeCase {
    kCategoricalCrossEntropyLossLayer = 10,
    kMeanSquaredErrorLossLayer = 11,
    LOSSLAYERTYPE_NOT_SET = 0,
  };

  static inline const LossLayer* internal_default_instance() {
    return reinterpret_cast<const LossLayer*>(
               &_LossLayer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    190;

  void Swap(LossLayer* other);

  // implements Message ----------------------------------------------

  inline LossLayer* New() const PROTOBUF_FINAL { return New(NULL); }

  LossLayer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const LossLayer& from);
  void MergeFrom(const LossLayer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LossLayer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
  bool has_categoricalcrossentropylosslayer() const;
  void clear_categoricalcrossentropylosslayer();
  static const int kCategoricalCrossEntropyLossLayerFieldNumber = 10;
  const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& categoricalcrossentropylosslayer() const;
  ::CoreML::Specification::CategoricalCrossEntropyLossLayer* mutable_categoricalcrossentropylosslayer();
  ::CoreML::Specification::CategoricalCrossEntropyLossLayer* release_categoricalcrossentropylosslayer();
  void set_allocated_categoricalcrossentropylosslayer(::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer);

  // .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
  bool has_meansquarederrorlosslayer() const;
  void clear_meansquarederrorlosslayer();
  static const int kMeanSquaredErrorLossLayerFieldNumber = 11;
  const ::CoreML::Specification::MeanSquaredErrorLossLayer& meansquarederrorlosslayer() const;
  ::CoreML::Specification::MeanSquaredErrorLossLayer* mutable_meansquarederrorlosslayer();
  ::CoreML::Specification::MeanSquaredErrorLossLayer* release_meansquarederrorlosslayer();
  void set_allocated_meansquarederrorlosslayer(::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer);

  LossLayerTypeCase LossLayerType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.LossLayer)
 private:
  void set_has_categoricalcrossentropylosslayer();
  void set_has_meansquarederrorlosslayer();

  inline bool has_LossLayerType() const;
  void clear_LossLayerType();
  inline void clear_has_LossLayerType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  union LossLayerTypeUnion {
    LossLayerTypeUnion() {}
    ::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer_;
    ::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer_;
  } LossLayerType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CategoricalCrossEntropyLossLayer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CategoricalCrossEntropyLossLayer) */ {
 public:
  CategoricalCrossEntropyLossLayer();
  virtual ~CategoricalCrossEntropyLossLayer();

  CategoricalCrossEntropyLossLayer(const CategoricalCrossEntropyLossLayer& from);

  inline CategoricalCrossEntropyLossLayer& operator=(const CategoricalCrossEntropyLossLayer& from) {
    CopyFrom(from);
    return *this;
  }

  static const CategoricalCrossEntropyLossLayer& default_instance();

  static inline const CategoricalCrossEntropyLossLayer* internal_default_instance() {
    return reinterpret_cast<const CategoricalCrossEntropyLossLayer*>(
               &_CategoricalCrossEntropyLossLayer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    191;

  void Swap(CategoricalCrossEntropyLossLayer* other);

  // implements Message ----------------------------------------------

  inline CategoricalCrossEntropyLossLayer* New() const PROTOBUF_FINAL { return New(NULL); }

  CategoricalCrossEntropyLossLayer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const CategoricalCrossEntropyLossLayer& from);
  void MergeFrom(const CategoricalCrossEntropyLossLayer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CategoricalCrossEntropyLossLayer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string input = 1;
  void clear_input();
  static const int kInputFieldNumber = 1;
  const ::std::string& input() const;
  void set_input(const ::std::string& value);
  #if LANG_CXX11
  void set_input(::std::string&& value);
  #endif
  void set_input(const char* value);
  void set_input(const char* value, size_t size);
  ::std::string* mutable_input();
  ::std::string* release_input();
  void set_allocated_input(::std::string* input);

  // string target = 2;
  void clear_target();
  static const int kTargetFieldNumber = 2;
  const ::std::string& target() const;
  void set_target(const ::std::string& value);
  #if LANG_CXX11
  void set_target(::std::string&& value);
  #endif
  void set_target(const char* value);
  void set_target(const char* value, size_t size);
  ::std::string* mutable_target();
  ::std::string* release_target();
  void set_allocated_target(::std::string* target);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CategoricalCrossEntropyLossLayer)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr input_;
  ::google::protobuf::internal::ArenaStringPtr target_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MeanSquaredErrorLossLayer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MeanSquaredErrorLossLayer) */ {
 public:
  MeanSquaredErrorLossLayer();
  virtual ~MeanSquaredErrorLossLayer();

  MeanSquaredErrorLossLayer(const MeanSquaredErrorLossLayer& from);

  inline MeanSquaredErrorLossLayer& operator=(const MeanSquaredErrorLossLayer& from) {
    CopyFrom(from);
    return *this;
  }

  static const MeanSquaredErrorLossLayer& default_instance();

  static inline const MeanSquaredErrorLossLayer* internal_default_instance() {
    return reinterpret_cast<const MeanSquaredErrorLossLayer*>(
               &_MeanSquaredErrorLossLayer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    192;

  void Swap(MeanSquaredErrorLossLayer* other);

  // implements Message ----------------------------------------------

  inline MeanSquaredErrorLossLayer* New() const PROTOBUF_FINAL { return New(NULL); }

  MeanSquaredErrorLossLayer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const MeanSquaredErrorLossLayer& from);
  void MergeFrom(const MeanSquaredErrorLossLayer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MeanSquaredErrorLossLayer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string input = 1;
  void clear_input();
  static const int kInputFieldNumber = 1;
  const ::std::string& input() const;
  void set_input(const ::std::string& value);
  #if LANG_CXX11
  void set_input(::std::string&& value);
  #endif
  void set_input(const char* value);
  void set_input(const char* value, size_t size);
  ::std::string* mutable_input();
  ::std::string* release_input();
  void set_allocated_input(::std::string* input);

  // string target = 2;
  void clear_target();
  static const int kTargetFieldNumber = 2;
  const ::std::string& target() const;
  void set_target(const ::std::string& value);
  #if LANG_CXX11
  void set_target(::std::string&& value);
  #endif
  void set_target(const char* value);
  void set_target(const char* value, size_t size);
  ::std::string* mutable_target();
  ::std::string* release_target();
  void set_allocated_target(::std::string* target);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MeanSquaredErrorLossLayer)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr input_;
  ::google::protobuf::internal::ArenaStringPtr target_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class Optimizer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.Optimizer) */ {
 public:
  Optimizer();
  virtual ~Optimizer();

  Optimizer(const Optimizer& from);

  inline Optimizer& operator=(const Optimizer& from) {
    CopyFrom(from);
    return *this;
  }

  static const Optimizer& default_instance();

  enum OptimizerTypeCase {
    kSgdOptimizer = 10,
    kAdamOptimizer = 11,
    OPTIMIZERTYPE_NOT_SET = 0,
  };

  static inline const Optimizer* internal_default_instance() {
    return reinterpret_cast<const Optimizer*>(
               &_Optimizer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    193;

  void Swap(Optimizer* other);

  // implements Message ----------------------------------------------

  inline Optimizer* New() const PROTOBUF_FINAL { return New(NULL); }

  Optimizer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const Optimizer& from);
  void MergeFrom(const Optimizer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(Optimizer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
  bool has_sgdoptimizer() const;
  void clear_sgdoptimizer();
  static const int kSgdOptimizerFieldNumber = 10;
  const ::CoreML::Specification::SGDOptimizer& sgdoptimizer() const;
  ::CoreML::Specification::SGDOptimizer* mutable_sgdoptimizer();
  ::CoreML::Specification::SGDOptimizer* release_sgdoptimizer();
  void set_allocated_sgdoptimizer(::CoreML::Specification::SGDOptimizer* sgdoptimizer);

  // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
  bool has_adamoptimizer() const;
  void clear_adamoptimizer();
  static const int kAdamOptimizerFieldNumber = 11;
  const ::CoreML::Specification::AdamOptimizer& adamoptimizer() const;
  ::CoreML::Specification::AdamOptimizer* mutable_adamoptimizer();
  ::CoreML::Specification::AdamOptimizer* release_adamoptimizer();
  void set_allocated_adamoptimizer(::CoreML::Specification::AdamOptimizer* adamoptimizer);

  OptimizerTypeCase OptimizerType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.Optimizer)
 private:
  void set_has_sgdoptimizer();
  void set_has_adamoptimizer();

  inline bool has_OptimizerType() const;
  void clear_OptimizerType();
  inline void clear_has_OptimizerType();

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  union OptimizerTypeUnion {
    OptimizerTypeUnion() {}
    ::CoreML::Specification::SGDOptimizer* sgdoptimizer_;
    ::CoreML::Specification::AdamOptimizer* adamoptimizer_;
  } OptimizerType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SGDOptimizer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SGDOptimizer) */ {
 public:
  SGDOptimizer();
  virtual ~SGDOptimizer();

  SGDOptimizer(const SGDOptimizer& from);

  inline SGDOptimizer& operator=(const SGDOptimizer& from) {
    CopyFrom(from);
    return *this;
  }

  static const SGDOptimizer& default_instance();

  static inline const SGDOptimizer* internal_default_instance() {
    return reinterpret_cast<const SGDOptimizer*>(
               &_SGDOptimizer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    194;

  void Swap(SGDOptimizer* other);

  // implements Message ----------------------------------------------

  inline SGDOptimizer* New() const PROTOBUF_FINAL { return New(NULL); }

  SGDOptimizer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const SGDOptimizer& from);
  void MergeFrom(const SGDOptimizer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SGDOptimizer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  bool has_learningrate() const;
  void clear_learningrate();
  static const int kLearningRateFieldNumber = 1;
  const ::CoreML::Specification::DoubleParameter& learningrate() const;
  ::CoreML::Specification::DoubleParameter* mutable_learningrate();
  ::CoreML::Specification::DoubleParameter* release_learningrate();
  void set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate);

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  bool has_minibatchsize() const;
  void clear_minibatchsize();
  static const int kMiniBatchSizeFieldNumber = 2;
  const ::CoreML::Specification::Int64Parameter& minibatchsize() const;
  ::CoreML::Specification::Int64Parameter* mutable_minibatchsize();
  ::CoreML::Specification::Int64Parameter* release_minibatchsize();
  void set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize);

  // .CoreML.Specification.DoubleParameter momentum = 3;
  bool has_momentum() const;
  void clear_momentum();
  static const int kMomentumFieldNumber = 3;
  const ::CoreML::Specification::DoubleParameter& momentum() const;
  ::CoreML::Specification::DoubleParameter* mutable_momentum();
  ::CoreML::Specification::DoubleParameter* release_momentum();
  void set_allocated_momentum(::CoreML::Specification::DoubleParameter* momentum);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SGDOptimizer)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::DoubleParameter* learningrate_;
  ::CoreML::Specification::Int64Parameter* minibatchsize_;
  ::CoreML::Specification::DoubleParameter* momentum_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AdamOptimizer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AdamOptimizer) */ {
 public:
  AdamOptimizer();
  virtual ~AdamOptimizer();

  AdamOptimizer(const AdamOptimizer& from);

  inline AdamOptimizer& operator=(const AdamOptimizer& from) {
    CopyFrom(from);
    return *this;
  }

  static const AdamOptimizer& default_instance();

  static inline const AdamOptimizer* internal_default_instance() {
    return reinterpret_cast<const AdamOptimizer*>(
               &_AdamOptimizer_default_instance_);
  }
  static PROTOBUF_CONSTEXPR int const kIndexInFileMessages =
    195;

  void Swap(AdamOptimizer* other);

  // implements Message ----------------------------------------------

  inline AdamOptimizer* New() const PROTOBUF_FINAL { return New(NULL); }

  AdamOptimizer* New(::google::protobuf::Arena* arena) const PROTOBUF_FINAL;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from)
    PROTOBUF_FINAL;
  void CopyFrom(const AdamOptimizer& from);
  void MergeFrom(const AdamOptimizer& from);
  void Clear() PROTOBUF_FINAL;
  bool IsInitialized() const PROTOBUF_FINAL;

  size_t ByteSizeLong() const PROTOBUF_FINAL;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) PROTOBUF_FINAL;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const PROTOBUF_FINAL;
  void DiscardUnknownFields();
  int GetCachedSize() const PROTOBUF_FINAL { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AdamOptimizer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::std::string GetTypeName() const PROTOBUF_FINAL;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  bool has_learningrate() const;
  void clear_learningrate();
  static const int kLearningRateFieldNumber = 1;
  const ::CoreML::Specification::DoubleParameter& learningrate() const;
  ::CoreML::Specification::DoubleParameter* mutable_learningrate();
  ::CoreML::Specification::DoubleParameter* release_learningrate();
  void set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate);

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  bool has_minibatchsize() const;
  void clear_minibatchsize();
  static const int kMiniBatchSizeFieldNumber = 2;
  const ::CoreML::Specification::Int64Parameter& minibatchsize() const;
  ::CoreML::Specification::Int64Parameter* mutable_minibatchsize();
  ::CoreML::Specification::Int64Parameter* release_minibatchsize();
  void set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize);

  // .CoreML.Specification.DoubleParameter beta1 = 3;
  bool has_beta1() const;
  void clear_beta1();
  static const int kBeta1FieldNumber = 3;
  const ::CoreML::Specification::DoubleParameter& beta1() const;
  ::CoreML::Specification::DoubleParameter* mutable_beta1();
  ::CoreML::Specification::DoubleParameter* release_beta1();
  void set_allocated_beta1(::CoreML::Specification::DoubleParameter* beta1);

  // .CoreML.Specification.DoubleParameter beta2 = 4;
  bool has_beta2() const;
  void clear_beta2();
  static const int kBeta2FieldNumber = 4;
  const ::CoreML::Specification::DoubleParameter& beta2() const;
  ::CoreML::Specification::DoubleParameter* mutable_beta2();
  ::CoreML::Specification::DoubleParameter* release_beta2();
  void set_allocated_beta2(::CoreML::Specification::DoubleParameter* beta2);

  // .CoreML.Specification.DoubleParameter eps = 5;
  bool has_eps() const;
  void clear_eps();
  static const int kEpsFieldNumber = 5;
  const ::CoreML::Specification::DoubleParameter& eps() const;
  ::CoreML::Specification::DoubleParameter* mutable_eps();
  ::CoreML::Specification::DoubleParameter* release_eps();
  void set_allocated_eps(::CoreML::Specification::DoubleParameter* eps);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AdamOptimizer)
 private:

  ::google::protobuf::internal::InternalMetadataWithArenaLite _internal_metadata_;
  ::CoreML::Specification::DoubleParameter* learningrate_;
  ::CoreML::Specification::Int64Parameter* minibatchsize_;
  ::CoreML::Specification::DoubleParameter* beta1_;
  ::CoreML::Specification::DoubleParameter* beta2_;
  ::CoreML::Specification::DoubleParameter* eps_;
  mutable int _cached_size_;
  friend struct protobuf_NeuralNetwork_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#if !PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetwork::layers_size() const {
  return layers_.size();
}
inline void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetwork::preprocessing_size() const {
  return preprocessing_.size();
}
inline void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
inline void NeuralNetwork::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetwork::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
inline void NeuralNetwork::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
inline void NeuralNetwork::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetwork::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
inline void NeuralNetwork::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
inline bool NeuralNetwork::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
inline void NeuralNetwork::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetwork::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.updateParams)
  return updateparams_;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetwork.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
inline void NeuralNetwork::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetwork.updateParams)
}

// -------------------------------------------------------------------

// NeuralNetworkImageScaler

// float channelScale = 10;
inline void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
inline float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return channelscale_;
}
inline void NeuralNetworkImageScaler::set_channelscale(float value) {
  
  channelscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// float blueBias = 20;
inline void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
inline float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return bluebias_;
}
inline void NeuralNetworkImageScaler::set_bluebias(float value) {
  
  bluebias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// float greenBias = 21;
inline void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
inline float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return greenbias_;
}
inline void NeuralNetworkImageScaler::set_greenbias(float value) {
  
  greenbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// float redBias = 22;
inline void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
inline float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return redbias_;
}
inline void NeuralNetworkImageScaler::set_redbias(float value) {
  
  redbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// float grayBias = 30;
inline void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
inline float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return graybias_;
}
inline void NeuralNetworkImageScaler::set_graybias(float value) {
  
  graybias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

// -------------------------------------------------------------------

// NeuralNetworkMeanImage

// repeated float meanImage = 1;
inline int NeuralNetworkMeanImage::meanimage_size() const {
  return meanimage_.size();
}
inline void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
inline float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_.Get(index);
}
inline void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
inline void NeuralNetworkMeanImage::add_meanimage(float value) {
  meanimage_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
inline const ::google::protobuf::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_;
}
inline ::google::protobuf::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return &meanimage_;
}

// -------------------------------------------------------------------

// NeuralNetworkPreprocessing

// string featureName = 1;
inline void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.GetNoArena();
}
inline void NeuralNetworkPreprocessing::set_featurename(const ::std::string& value) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#if LANG_CXX11
inline void NeuralNetworkPreprocessing::set_featurename(::std::string&& value) {
  
  featurename_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#endif
inline void NeuralNetworkPreprocessing::set_featurename(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
inline void NeuralNetworkPreprocessing::set_featurename(const char* value, size_t size) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
inline ::std::string* NeuralNetworkPreprocessing::mutable_featurename() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  
  return featurename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkPreprocessing::set_allocated_featurename(::std::string* featurename) {
  if (featurename != NULL) {
    
  } else {
    
  }
  featurename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), featurename);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
inline bool NeuralNetworkPreprocessing::has_scaler() const {
  return preprocessor_case() == kScaler;
}
inline void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
inline void NeuralNetworkPreprocessing::clear_scaler() {
  if (has_scaler()) {
    delete preprocessor_.scaler_;
    clear_has_preprocessor();
  }
}
inline  const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return has_scaler()
      ? *preprocessor_.scaler_
      : ::CoreML::Specification::NeuralNetworkImageScaler::default_instance();
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  if (!has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = new ::CoreML::Specification::NeuralNetworkImageScaler;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return preprocessor_.scaler_;
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkPreprocessing::set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}

// .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
inline bool NeuralNetworkPreprocessing::has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
inline void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
inline void NeuralNetworkPreprocessing::clear_meanimage() {
  if (has_meanimage()) {
    delete preprocessor_.meanimage_;
    clear_has_preprocessor();
  }
}
inline  const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return has_meanimage()
      ? *preprocessor_.meanimage_
      : ::CoreML::Specification::NeuralNetworkMeanImage::default_instance();
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  if (!has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = new ::CoreML::Specification::NeuralNetworkMeanImage;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return preprocessor_.meanimage_;
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkPreprocessing::set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}

inline bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
inline void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
inline NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// ActivationReLU

// -------------------------------------------------------------------

// ActivationLeakyReLU

// float alpha = 1;
inline void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return alpha_;
}
inline void ActivationLeakyReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

// -------------------------------------------------------------------

// ActivationTanh

// -------------------------------------------------------------------

// ActivationScaledTanh

// float alpha = 1;
inline void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return alpha_;
}
inline void ActivationScaledTanh::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// float beta = 2;
inline void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
inline float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return beta_;
}
inline void ActivationScaledTanh::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

// -------------------------------------------------------------------

// ActivationSigmoid

// -------------------------------------------------------------------

// ActivationLinear

// float alpha = 1;
inline void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return alpha_;
}
inline void ActivationLinear::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// float beta = 2;
inline void ActivationLinear::clear_beta() {
  beta_ = 0;
}
inline float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return beta_;
}
inline void ActivationLinear::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

// -------------------------------------------------------------------

// ActivationSigmoidHard

// float alpha = 1;
inline void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return alpha_;
}
inline void ActivationSigmoidHard::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// float beta = 2;
inline void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
inline float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return beta_;
}
inline void ActivationSigmoidHard::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

// -------------------------------------------------------------------

// ActivationPReLU

// .CoreML.Specification.WeightParams alpha = 1;
inline bool ActivationPReLU::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
inline void ActivationPReLU::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_;
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
inline void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

// -------------------------------------------------------------------

// ActivationELU

// float alpha = 1;
inline void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return alpha_;
}
inline void ActivationELU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

// -------------------------------------------------------------------

// ActivationThresholdedReLU

// float alpha = 1;
inline void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return alpha_;
}
inline void ActivationThresholdedReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

// -------------------------------------------------------------------

// ActivationSoftsign

// -------------------------------------------------------------------

// ActivationSoftplus

// -------------------------------------------------------------------

// ActivationParametricSoftplus

// .CoreML.Specification.WeightParams alpha = 1;
inline bool ActivationParametricSoftplus::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
inline void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
inline void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// .CoreML.Specification.WeightParams beta = 2;
inline bool ActivationParametricSoftplus::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
inline void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
inline void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

// -------------------------------------------------------------------

// ActivationParams

// .CoreML.Specification.ActivationLinear linear = 5;
inline bool ActivationParams::has_linear() const {
  return NonlinearityType_case() == kLinear;
}
inline void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
inline void ActivationParams::clear_linear() {
  if (has_linear()) {
    delete NonlinearityType_.linear_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return has_linear()
      ? *NonlinearityType_.linear_
      : ::CoreML::Specification::ActivationLinear::default_instance();
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  if (!has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = new ::CoreML::Specification::ActivationLinear;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return NonlinearityType_.linear_;
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.linear)
}

// .CoreML.Specification.ActivationReLU ReLU = 10;
inline bool ActivationParams::has_relu() const {
  return NonlinearityType_case() == kReLU;
}
inline void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
inline void ActivationParams::clear_relu() {
  if (has_relu()) {
    delete NonlinearityType_.relu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return has_relu()
      ? *NonlinearityType_.relu_
      : ::CoreML::Specification::ActivationReLU::default_instance();
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  if (!has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = new ::CoreML::Specification::ActivationReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return NonlinearityType_.relu_;
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}

// .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
inline bool ActivationParams::has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
inline void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
inline void ActivationParams::clear_leakyrelu() {
  if (has_leakyrelu()) {
    delete NonlinearityType_.leakyrelu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : ::CoreML::Specification::ActivationLeakyReLU::default_instance();
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  if (!has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = new ::CoreML::Specification::ActivationLeakyReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return NonlinearityType_.leakyrelu_;
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}

// .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
inline bool ActivationParams::has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
inline void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
inline void ActivationParams::clear_thresholdedrelu() {
  if (has_thresholdedrelu()) {
    delete NonlinearityType_.thresholdedrelu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : ::CoreML::Specification::ActivationThresholdedReLU::default_instance();
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  if (!has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = new ::CoreML::Specification::ActivationThresholdedReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return NonlinearityType_.thresholdedrelu_;
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}

// .CoreML.Specification.ActivationPReLU PReLU = 25;
inline bool ActivationParams::has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
inline void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
inline void ActivationParams::clear_prelu() {
  if (has_prelu()) {
    delete NonlinearityType_.prelu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return has_prelu()
      ? *NonlinearityType_.prelu_
      : ::CoreML::Specification::ActivationPReLU::default_instance();
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  if (!has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = new ::CoreML::Specification::ActivationPReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return NonlinearityType_.prelu_;
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}

// .CoreML.Specification.ActivationTanh tanh = 30;
inline bool ActivationParams::has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
inline void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
inline void ActivationParams::clear_tanh() {
  if (has_tanh()) {
    delete NonlinearityType_.tanh_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return has_tanh()
      ? *NonlinearityType_.tanh_
      : ::CoreML::Specification::ActivationTanh::default_instance();
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  if (!has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = new ::CoreML::Specification::ActivationTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return NonlinearityType_.tanh_;
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.tanh)
}

// .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
inline bool ActivationParams::has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
inline void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
inline void ActivationParams::clear_scaledtanh() {
  if (has_scaledtanh()) {
    delete NonlinearityType_.scaledtanh_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : ::CoreML::Specification::ActivationScaledTanh::default_instance();
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  if (!has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = new ::CoreML::Specification::ActivationScaledTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return NonlinearityType_.scaledtanh_;
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}

// .CoreML.Specification.ActivationSigmoid sigmoid = 40;
inline bool ActivationParams::has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
inline void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
inline void ActivationParams::clear_sigmoid() {
  if (has_sigmoid()) {
    delete NonlinearityType_.sigmoid_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : ::CoreML::Specification::ActivationSigmoid::default_instance();
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  if (!has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = new ::CoreML::Specification::ActivationSigmoid;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return NonlinearityType_.sigmoid_;
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}

// .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
inline bool ActivationParams::has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
inline void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
inline void ActivationParams::clear_sigmoidhard() {
  if (has_sigmoidhard()) {
    delete NonlinearityType_.sigmoidhard_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : ::CoreML::Specification::ActivationSigmoidHard::default_instance();
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  if (!has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = new ::CoreML::Specification::ActivationSigmoidHard;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return NonlinearityType_.sigmoidhard_;
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}

// .CoreML.Specification.ActivationELU ELU = 50;
inline bool ActivationParams::has_elu() const {
  return NonlinearityType_case() == kELU;
}
inline void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
inline void ActivationParams::clear_elu() {
  if (has_elu()) {
    delete NonlinearityType_.elu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return has_elu()
      ? *NonlinearityType_.elu_
      : ::CoreML::Specification::ActivationELU::default_instance();
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  if (!has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = new ::CoreML::Specification::ActivationELU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return NonlinearityType_.elu_;
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ELU)
}

// .CoreML.Specification.ActivationSoftsign softsign = 60;
inline bool ActivationParams::has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
inline void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
inline void ActivationParams::clear_softsign() {
  if (has_softsign()) {
    delete NonlinearityType_.softsign_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return has_softsign()
      ? *NonlinearityType_.softsign_
      : ::CoreML::Specification::ActivationSoftsign::default_instance();
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  if (!has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = new ::CoreML::Specification::ActivationSoftsign;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return NonlinearityType_.softsign_;
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softsign)
}

// .CoreML.Specification.ActivationSoftplus softplus = 70;
inline bool ActivationParams::has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
inline void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
inline void ActivationParams::clear_softplus() {
  if (has_softplus()) {
    delete NonlinearityType_.softplus_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return has_softplus()
      ? *NonlinearityType_.softplus_
      : ::CoreML::Specification::ActivationSoftplus::default_instance();
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  if (!has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = new ::CoreML::Specification::ActivationSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return NonlinearityType_.softplus_;
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softplus)
}

// .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
inline bool ActivationParams::has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
inline void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
inline void ActivationParams::clear_parametricsoftplus() {
  if (has_parametricsoftplus()) {
    delete NonlinearityType_.parametricsoftplus_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : ::CoreML::Specification::ActivationParametricSoftplus::default_instance();
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  if (!has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = new ::CoreML::Specification::ActivationParametricSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return NonlinearityType_.parametricsoftplus_;
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}

inline bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
inline void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
inline ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// Tensor

// uint32 rank = 1;
inline void Tensor::clear_rank() {
  rank_ = 0u;
}
inline ::google::protobuf::uint32 Tensor::rank() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.rank)
  return rank_;
}
inline void Tensor::set_rank(::google::protobuf::uint32 value) {
  
  rank_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.rank)
}

// repeated int64 dimValue = 2;
inline int Tensor::dimvalue_size() const {
  return dimvalue_.size();
}
inline void Tensor::clear_dimvalue() {
  dimvalue_.Clear();
}
inline ::google::protobuf::int64 Tensor::dimvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.dimValue)
  return dimvalue_.Get(index);
}
inline void Tensor::set_dimvalue(int index, ::google::protobuf::int64 value) {
  dimvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.dimValue)
}
inline void Tensor::add_dimvalue(::google::protobuf::int64 value) {
  dimvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.Tensor.dimValue)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
Tensor::dimvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.Tensor.dimValue)
  return dimvalue_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
Tensor::mutable_dimvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.Tensor.dimValue)
  return &dimvalue_;
}

// -------------------------------------------------------------------

// NeuralNetworkLayer

// string name = 1;
inline void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.GetNoArena();
}
inline void NeuralNetworkLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
#if LANG_CXX11
inline void NeuralNetworkLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkLayer.name)
}
#endif
inline void NeuralNetworkLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.name)
}
inline void NeuralNetworkLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.name)
}
inline ::std::string* NeuralNetworkLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
inline int NeuralNetworkLayer::input_size() const {
  return input_.size();
}
inline void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
inline const ::std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Get(index);
}
inline ::std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
inline void NeuralNetworkLayer::set_input(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(value);
}
#if LANG_CXX11
inline void NeuralNetworkLayer::set_input(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(std::move(value));
}
#endif
inline void NeuralNetworkLayer::set_input(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
inline ::std::string* NeuralNetworkLayer::add_input() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Add();
}
inline void NeuralNetworkLayer::add_input(const ::std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#if LANG_CXX11
inline void NeuralNetworkLayer::add_input(::std::string&& value) {
  input_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#endif
inline void NeuralNetworkLayer::add_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
inline int NeuralNetworkLayer::output_size() const {
  return output_.size();
}
inline void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
inline const ::std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Get(index);
}
inline ::std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
inline void NeuralNetworkLayer::set_output(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(value);
}
#if LANG_CXX11
inline void NeuralNetworkLayer::set_output(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(std::move(value));
}
#endif
inline void NeuralNetworkLayer::set_output(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
inline ::std::string* NeuralNetworkLayer::add_output() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Add();
}
inline void NeuralNetworkLayer::add_output(const ::std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#if LANG_CXX11
inline void NeuralNetworkLayer::add_output(::std::string&& value) {
  output_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#endif
inline void NeuralNetworkLayer::add_output(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// repeated .CoreML.Specification.Tensor inputTensor = 4;
inline int NeuralNetworkLayer::inputtensor_size() const {
  return inputtensor_.size();
}
inline void NeuralNetworkLayer::clear_inputtensor() {
  inputtensor_.Clear();
}
inline const ::CoreML::Specification::Tensor& NeuralNetworkLayer::inputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Get(index);
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_inputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Mutable(index);
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::add_inputtensor() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_inputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return &inputtensor_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::inputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_;
}

// repeated .CoreML.Specification.Tensor outputTensor = 5;
inline int NeuralNetworkLayer::outputtensor_size() const {
  return outputtensor_.size();
}
inline void NeuralNetworkLayer::clear_outputtensor() {
  outputtensor_.Clear();
}
inline const ::CoreML::Specification::Tensor& NeuralNetworkLayer::outputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Get(index);
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_outputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Mutable(index);
}
inline ::CoreML::Specification::Tensor* NeuralNetworkLayer::add_outputtensor() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_outputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return &outputtensor_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::outputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_;
}

// bool isUpdatable = 10;
inline void NeuralNetworkLayer::clear_isupdatable() {
  isupdatable_ = false;
}
inline bool NeuralNetworkLayer::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
  return isupdatable_;
}
inline void NeuralNetworkLayer::set_isupdatable(bool value) {
  
  isupdatable_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
}

// .CoreML.Specification.ConvolutionLayerParams convolution = 100;
inline bool NeuralNetworkLayer::has_convolution() const {
  return layer_case() == kConvolution;
}
inline void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
inline void NeuralNetworkLayer::clear_convolution() {
  if (has_convolution()) {
    delete layer_.convolution_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return has_convolution()
      ? *layer_.convolution_
      : ::CoreML::Specification::ConvolutionLayerParams::default_instance();
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  if (!has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = new ::CoreML::Specification::ConvolutionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return layer_.convolution_;
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}

// .CoreML.Specification.PoolingLayerParams pooling = 120;
inline bool NeuralNetworkLayer::has_pooling() const {
  return layer_case() == kPooling;
}
inline void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
inline void NeuralNetworkLayer::clear_pooling() {
  if (has_pooling()) {
    delete layer_.pooling_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return has_pooling()
      ? *layer_.pooling_
      : ::CoreML::Specification::PoolingLayerParams::default_instance();
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  if (!has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = new ::CoreML::Specification::PoolingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return layer_.pooling_;
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}

// .CoreML.Specification.ActivationParams activation = 130;
inline bool NeuralNetworkLayer::has_activation() const {
  return layer_case() == kActivation;
}
inline void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
inline void NeuralNetworkLayer::clear_activation() {
  if (has_activation()) {
    delete layer_.activation_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return has_activation()
      ? *layer_.activation_
      : ::CoreML::Specification::ActivationParams::default_instance();
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  if (!has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return layer_.activation_;
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}

// .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
inline bool NeuralNetworkLayer::has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
inline void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
inline void NeuralNetworkLayer::clear_innerproduct() {
  if (has_innerproduct()) {
    delete layer_.innerproduct_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return has_innerproduct()
      ? *layer_.innerproduct_
      : ::CoreML::Specification::InnerProductLayerParams::default_instance();
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  if (!has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = new ::CoreML::Specification::InnerProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return layer_.innerproduct_;
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}

// .CoreML.Specification.EmbeddingLayerParams embedding = 150;
inline bool NeuralNetworkLayer::has_embedding() const {
  return layer_case() == kEmbedding;
}
inline void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
inline void NeuralNetworkLayer::clear_embedding() {
  if (has_embedding()) {
    delete layer_.embedding_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return has_embedding()
      ? *layer_.embedding_
      : ::CoreML::Specification::EmbeddingLayerParams::default_instance();
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  if (!has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = new ::CoreML::Specification::EmbeddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return layer_.embedding_;
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}

// .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
inline bool NeuralNetworkLayer::has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
inline void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
inline void NeuralNetworkLayer::clear_batchnorm() {
  if (has_batchnorm()) {
    delete layer_.batchnorm_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return has_batchnorm()
      ? *layer_.batchnorm_
      : ::CoreML::Specification::BatchnormLayerParams::default_instance();
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  if (!has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = new ::CoreML::Specification::BatchnormLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return layer_.batchnorm_;
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}

// .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
inline bool NeuralNetworkLayer::has_mvn() const {
  return layer_case() == kMvn;
}
inline void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
inline void NeuralNetworkLayer::clear_mvn() {
  if (has_mvn()) {
    delete layer_.mvn_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return has_mvn()
      ? *layer_.mvn_
      : ::CoreML::Specification::MeanVarianceNormalizeLayerParams::default_instance();
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  if (!has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = new ::CoreML::Specification::MeanVarianceNormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return layer_.mvn_;
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}

// .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
inline bool NeuralNetworkLayer::has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
inline void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
inline void NeuralNetworkLayer::clear_l2normalize() {
  if (has_l2normalize()) {
    delete layer_.l2normalize_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return has_l2normalize()
      ? *layer_.l2normalize_
      : ::CoreML::Specification::L2NormalizeLayerParams::default_instance();
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  if (!has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = new ::CoreML::Specification::L2NormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return layer_.l2normalize_;
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}

// .CoreML.Specification.SoftmaxLayerParams softmax = 175;
inline bool NeuralNetworkLayer::has_softmax() const {
  return layer_case() == kSoftmax;
}
inline void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
inline void NeuralNetworkLayer::clear_softmax() {
  if (has_softmax()) {
    delete layer_.softmax_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return has_softmax()
      ? *layer_.softmax_
      : ::CoreML::Specification::SoftmaxLayerParams::default_instance();
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  if (!has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = new ::CoreML::Specification::SoftmaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return layer_.softmax_;
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}

// .CoreML.Specification.LRNLayerParams lrn = 180;
inline bool NeuralNetworkLayer::has_lrn() const {
  return layer_case() == kLrn;
}
inline void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
inline void NeuralNetworkLayer::clear_lrn() {
  if (has_lrn()) {
    delete layer_.lrn_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return has_lrn()
      ? *layer_.lrn_
      : ::CoreML::Specification::LRNLayerParams::default_instance();
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  if (!has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = new ::CoreML::Specification::LRNLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return layer_.lrn_;
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}

// .CoreML.Specification.CropLayerParams crop = 190;
inline bool NeuralNetworkLayer::has_crop() const {
  return layer_case() == kCrop;
}
inline void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
inline void NeuralNetworkLayer::clear_crop() {
  if (has_crop()) {
    delete layer_.crop_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return has_crop()
      ? *layer_.crop_
      : ::CoreML::Specification::CropLayerParams::default_instance();
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  if (!has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = new ::CoreML::Specification::CropLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return layer_.crop_;
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}

// .CoreML.Specification.PaddingLayerParams padding = 200;
inline bool NeuralNetworkLayer::has_padding() const {
  return layer_case() == kPadding;
}
inline void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
inline void NeuralNetworkLayer::clear_padding() {
  if (has_padding()) {
    delete layer_.padding_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return has_padding()
      ? *layer_.padding_
      : ::CoreML::Specification::PaddingLayerParams::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  if (!has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = new ::CoreML::Specification::PaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return layer_.padding_;
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}

// .CoreML.Specification.UpsampleLayerParams upsample = 210;
inline bool NeuralNetworkLayer::has_upsample() const {
  return layer_case() == kUpsample;
}
inline void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
inline void NeuralNetworkLayer::clear_upsample() {
  if (has_upsample()) {
    delete layer_.upsample_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return has_upsample()
      ? *layer_.upsample_
      : ::CoreML::Specification::UpsampleLayerParams::default_instance();
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  if (!has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = new ::CoreML::Specification::UpsampleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return layer_.upsample_;
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}

// .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
inline bool NeuralNetworkLayer::has_resizebilinear() const {
  return layer_case() == kResizeBilinear;
}
inline void NeuralNetworkLayer::set_has_resizebilinear() {
  _oneof_case_[0] = kResizeBilinear;
}
inline void NeuralNetworkLayer::clear_resizebilinear() {
  if (has_resizebilinear()) {
    delete layer_.resizebilinear_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ResizeBilinearLayerParams& NeuralNetworkLayer::resizebilinear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return has_resizebilinear()
      ? *layer_.resizebilinear_
      : ::CoreML::Specification::ResizeBilinearLayerParams::default_instance();
}
inline ::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::mutable_resizebilinear() {
  if (!has_resizebilinear()) {
    clear_layer();
    set_has_resizebilinear();
    layer_.resizebilinear_ = new ::CoreML::Specification::ResizeBilinearLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return layer_.resizebilinear_;
}
inline ::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::release_resizebilinear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  if (has_resizebilinear()) {
    clear_has_layer();
    ::CoreML::Specification::ResizeBilinearLayerParams* temp = layer_.resizebilinear_;
    layer_.resizebilinear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_resizebilinear(::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear) {
  clear_layer();
  if (resizebilinear) {
    set_has_resizebilinear();
    layer_.resizebilinear_ = resizebilinear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
}

// .CoreML.Specification.CropResizeLayerParams cropResize = 212;
inline bool NeuralNetworkLayer::has_cropresize() const {
  return layer_case() == kCropResize;
}
inline void NeuralNetworkLayer::set_has_cropresize() {
  _oneof_case_[0] = kCropResize;
}
inline void NeuralNetworkLayer::clear_cropresize() {
  if (has_cropresize()) {
    delete layer_.cropresize_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CropResizeLayerParams& NeuralNetworkLayer::cropresize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return has_cropresize()
      ? *layer_.cropresize_
      : ::CoreML::Specification::CropResizeLayerParams::default_instance();
}
inline ::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::mutable_cropresize() {
  if (!has_cropresize()) {
    clear_layer();
    set_has_cropresize();
    layer_.cropresize_ = new ::CoreML::Specification::CropResizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return layer_.cropresize_;
}
inline ::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::release_cropresize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cropResize)
  if (has_cropresize()) {
    clear_has_layer();
    ::CoreML::Specification::CropResizeLayerParams* temp = layer_.cropresize_;
    layer_.cropresize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_cropresize(::CoreML::Specification::CropResizeLayerParams* cropresize) {
  clear_layer();
  if (cropresize) {
    set_has_cropresize();
    layer_.cropresize_ = cropresize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cropResize)
}

// .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
inline bool NeuralNetworkLayer::has_unary() const {
  return layer_case() == kUnary;
}
inline void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
inline void NeuralNetworkLayer::clear_unary() {
  if (has_unary()) {
    delete layer_.unary_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return has_unary()
      ? *layer_.unary_
      : ::CoreML::Specification::UnaryFunctionLayerParams::default_instance();
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  if (!has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = new ::CoreML::Specification::UnaryFunctionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return layer_.unary_;
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}

// .CoreML.Specification.AddLayerParams add = 230;
inline bool NeuralNetworkLayer::has_add() const {
  return layer_case() == kAdd;
}
inline void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
inline void NeuralNetworkLayer::clear_add() {
  if (has_add()) {
    delete layer_.add_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return has_add()
      ? *layer_.add_
      : ::CoreML::Specification::AddLayerParams::default_instance();
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  if (!has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = new ::CoreML::Specification::AddLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return layer_.add_;
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}

// .CoreML.Specification.MultiplyLayerParams multiply = 231;
inline bool NeuralNetworkLayer::has_multiply() const {
  return layer_case() == kMultiply;
}
inline void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
inline void NeuralNetworkLayer::clear_multiply() {
  if (has_multiply()) {
    delete layer_.multiply_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return has_multiply()
      ? *layer_.multiply_
      : ::CoreML::Specification::MultiplyLayerParams::default_instance();
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  if (!has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = new ::CoreML::Specification::MultiplyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return layer_.multiply_;
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}

// .CoreML.Specification.AverageLayerParams average = 240;
inline bool NeuralNetworkLayer::has_average() const {
  return layer_case() == kAverage;
}
inline void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
inline void NeuralNetworkLayer::clear_average() {
  if (has_average()) {
    delete layer_.average_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return has_average()
      ? *layer_.average_
      : ::CoreML::Specification::AverageLayerParams::default_instance();
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  if (!has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = new ::CoreML::Specification::AverageLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return layer_.average_;
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}

// .CoreML.Specification.ScaleLayerParams scale = 245;
inline bool NeuralNetworkLayer::has_scale() const {
  return layer_case() == kScale;
}
inline void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
inline void NeuralNetworkLayer::clear_scale() {
  if (has_scale()) {
    delete layer_.scale_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return has_scale()
      ? *layer_.scale_
      : ::CoreML::Specification::ScaleLayerParams::default_instance();
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  if (!has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = new ::CoreML::Specification::ScaleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return layer_.scale_;
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}

// .CoreML.Specification.BiasLayerParams bias = 250;
inline bool NeuralNetworkLayer::has_bias() const {
  return layer_case() == kBias;
}
inline void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
inline void NeuralNetworkLayer::clear_bias() {
  if (has_bias()) {
    delete layer_.bias_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return has_bias()
      ? *layer_.bias_
      : ::CoreML::Specification::BiasLayerParams::default_instance();
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  if (!has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = new ::CoreML::Specification::BiasLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return layer_.bias_;
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}

// .CoreML.Specification.MaxLayerParams max = 260;
inline bool NeuralNetworkLayer::has_max() const {
  return layer_case() == kMax;
}
inline void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
inline void NeuralNetworkLayer::clear_max() {
  if (has_max()) {
    delete layer_.max_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return has_max()
      ? *layer_.max_
      : ::CoreML::Specification::MaxLayerParams::default_instance();
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  if (!has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = new ::CoreML::Specification::MaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return layer_.max_;
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}

// .CoreML.Specification.MinLayerParams min = 261;
inline bool NeuralNetworkLayer::has_min() const {
  return layer_case() == kMin;
}
inline void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
inline void NeuralNetworkLayer::clear_min() {
  if (has_min()) {
    delete layer_.min_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return has_min()
      ? *layer_.min_
      : ::CoreML::Specification::MinLayerParams::default_instance();
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  if (!has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = new ::CoreML::Specification::MinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return layer_.min_;
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}

// .CoreML.Specification.DotProductLayerParams dot = 270;
inline bool NeuralNetworkLayer::has_dot() const {
  return layer_case() == kDot;
}
inline void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
inline void NeuralNetworkLayer::clear_dot() {
  if (has_dot()) {
    delete layer_.dot_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return has_dot()
      ? *layer_.dot_
      : ::CoreML::Specification::DotProductLayerParams::default_instance();
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  if (!has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = new ::CoreML::Specification::DotProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return layer_.dot_;
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}

// .CoreML.Specification.ReduceLayerParams reduce = 280;
inline bool NeuralNetworkLayer::has_reduce() const {
  return layer_case() == kReduce;
}
inline void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
inline void NeuralNetworkLayer::clear_reduce() {
  if (has_reduce()) {
    delete layer_.reduce_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return has_reduce()
      ? *layer_.reduce_
      : ::CoreML::Specification::ReduceLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  if (!has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = new ::CoreML::Specification::ReduceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return layer_.reduce_;
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}

// .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
inline bool NeuralNetworkLayer::has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
inline void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
inline void NeuralNetworkLayer::clear_loadconstant() {
  if (has_loadconstant()) {
    delete layer_.loadconstant_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return has_loadconstant()
      ? *layer_.loadconstant_
      : ::CoreML::Specification::LoadConstantLayerParams::default_instance();
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  if (!has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = new ::CoreML::Specification::LoadConstantLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return layer_.loadconstant_;
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}

// .CoreML.Specification.ReshapeLayerParams reshape = 300;
inline bool NeuralNetworkLayer::has_reshape() const {
  return layer_case() == kReshape;
}
inline void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
inline void NeuralNetworkLayer::clear_reshape() {
  if (has_reshape()) {
    delete layer_.reshape_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return has_reshape()
      ? *layer_.reshape_
      : ::CoreML::Specification::ReshapeLayerParams::default_instance();
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  if (!has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = new ::CoreML::Specification::ReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return layer_.reshape_;
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}

// .CoreML.Specification.FlattenLayerParams flatten = 301;
inline bool NeuralNetworkLayer::has_flatten() const {
  return layer_case() == kFlatten;
}
inline void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
inline void NeuralNetworkLayer::clear_flatten() {
  if (has_flatten()) {
    delete layer_.flatten_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return has_flatten()
      ? *layer_.flatten_
      : ::CoreML::Specification::FlattenLayerParams::default_instance();
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  if (!has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = new ::CoreML::Specification::FlattenLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return layer_.flatten_;
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}

// .CoreML.Specification.PermuteLayerParams permute = 310;
inline bool NeuralNetworkLayer::has_permute() const {
  return layer_case() == kPermute;
}
inline void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
inline void NeuralNetworkLayer::clear_permute() {
  if (has_permute()) {
    delete layer_.permute_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return has_permute()
      ? *layer_.permute_
      : ::CoreML::Specification::PermuteLayerParams::default_instance();
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  if (!has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = new ::CoreML::Specification::PermuteLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return layer_.permute_;
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}

// .CoreML.Specification.ConcatLayerParams concat = 320;
inline bool NeuralNetworkLayer::has_concat() const {
  return layer_case() == kConcat;
}
inline void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
inline void NeuralNetworkLayer::clear_concat() {
  if (has_concat()) {
    delete layer_.concat_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return has_concat()
      ? *layer_.concat_
      : ::CoreML::Specification::ConcatLayerParams::default_instance();
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  if (!has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = new ::CoreML::Specification::ConcatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return layer_.concat_;
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}

// .CoreML.Specification.SplitLayerParams split = 330;
inline bool NeuralNetworkLayer::has_split() const {
  return layer_case() == kSplit;
}
inline void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
inline void NeuralNetworkLayer::clear_split() {
  if (has_split()) {
    delete layer_.split_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return has_split()
      ? *layer_.split_
      : ::CoreML::Specification::SplitLayerParams::default_instance();
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  if (!has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = new ::CoreML::Specification::SplitLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return layer_.split_;
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}

// .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
inline bool NeuralNetworkLayer::has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
inline void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
inline void NeuralNetworkLayer::clear_sequencerepeat() {
  if (has_sequencerepeat()) {
    delete layer_.sequencerepeat_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : ::CoreML::Specification::SequenceRepeatLayerParams::default_instance();
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  if (!has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = new ::CoreML::Specification::SequenceRepeatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return layer_.sequencerepeat_;
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}

// .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
inline bool NeuralNetworkLayer::has_reorganizedata() const {
  return layer_case() == kReorganizeData;
}
inline void NeuralNetworkLayer::set_has_reorganizedata() {
  _oneof_case_[0] = kReorganizeData;
}
inline void NeuralNetworkLayer::clear_reorganizedata() {
  if (has_reorganizedata()) {
    delete layer_.reorganizedata_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReorganizeDataLayerParams& NeuralNetworkLayer::reorganizedata() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return has_reorganizedata()
      ? *layer_.reorganizedata_
      : ::CoreML::Specification::ReorganizeDataLayerParams::default_instance();
}
inline ::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::mutable_reorganizedata() {
  if (!has_reorganizedata()) {
    clear_layer();
    set_has_reorganizedata();
    layer_.reorganizedata_ = new ::CoreML::Specification::ReorganizeDataLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return layer_.reorganizedata_;
}
inline ::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::release_reorganizedata() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  if (has_reorganizedata()) {
    clear_has_layer();
    ::CoreML::Specification::ReorganizeDataLayerParams* temp = layer_.reorganizedata_;
    layer_.reorganizedata_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata) {
  clear_layer();
  if (reorganizedata) {
    set_has_reorganizedata();
    layer_.reorganizedata_ = reorganizedata;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
}

// .CoreML.Specification.SliceLayerParams slice = 350;
inline bool NeuralNetworkLayer::has_slice() const {
  return layer_case() == kSlice;
}
inline void NeuralNetworkLayer::set_has_slice() {
  _oneof_case_[0] = kSlice;
}
inline void NeuralNetworkLayer::clear_slice() {
  if (has_slice()) {
    delete layer_.slice_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SliceLayerParams& NeuralNetworkLayer::slice() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slice)
  return has_slice()
      ? *layer_.slice_
      : ::CoreML::Specification::SliceLayerParams::default_instance();
}
inline ::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::mutable_slice() {
  if (!has_slice()) {
    clear_layer();
    set_has_slice();
    layer_.slice_ = new ::CoreML::Specification::SliceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slice)
  return layer_.slice_;
}
inline ::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::release_slice() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slice)
  if (has_slice()) {
    clear_has_layer();
    ::CoreML::Specification::SliceLayerParams* temp = layer_.slice_;
    layer_.slice_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice) {
  clear_layer();
  if (slice) {
    set_has_slice();
    layer_.slice_ = slice;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slice)
}

// .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
inline bool NeuralNetworkLayer::has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
inline void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
inline void NeuralNetworkLayer::clear_simplerecurrent() {
  if (has_simplerecurrent()) {
    delete layer_.simplerecurrent_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : ::CoreML::Specification::SimpleRecurrentLayerParams::default_instance();
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  if (!has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = new ::CoreML::Specification::SimpleRecurrentLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return layer_.simplerecurrent_;
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}

// .CoreML.Specification.GRULayerParams gru = 410;
inline bool NeuralNetworkLayer::has_gru() const {
  return layer_case() == kGru;
}
inline void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
inline void NeuralNetworkLayer::clear_gru() {
  if (has_gru()) {
    delete layer_.gru_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return has_gru()
      ? *layer_.gru_
      : ::CoreML::Specification::GRULayerParams::default_instance();
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  if (!has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = new ::CoreML::Specification::GRULayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return layer_.gru_;
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}

// .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
inline bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
inline void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
inline void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (has_unidirectionallstm()) {
    delete layer_.unidirectionallstm_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : ::CoreML::Specification::UniDirectionalLSTMLayerParams::default_instance();
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  if (!has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = new ::CoreML::Specification::UniDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return layer_.unidirectionallstm_;
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}

// .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
inline bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
inline void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
inline void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (has_bidirectionallstm()) {
    delete layer_.bidirectionallstm_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : ::CoreML::Specification::BiDirectionalLSTMLayerParams::default_instance();
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  if (!has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = new ::CoreML::Specification::BiDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return layer_.bidirectionallstm_;
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}

// .CoreML.Specification.CustomLayerParams custom = 500;
inline bool NeuralNetworkLayer::has_custom() const {
  return layer_case() == kCustom;
}
inline void NeuralNetworkLayer::set_has_custom() {
  _oneof_case_[0] = kCustom;
}
inline void NeuralNetworkLayer::clear_custom() {
  if (has_custom()) {
    delete layer_.custom_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CustomLayerParams& NeuralNetworkLayer::custom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.custom)
  return has_custom()
      ? *layer_.custom_
      : ::CoreML::Specification::CustomLayerParams::default_instance();
}
inline ::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::mutable_custom() {
  if (!has_custom()) {
    clear_layer();
    set_has_custom();
    layer_.custom_ = new ::CoreML::Specification::CustomLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.custom)
  return layer_.custom_;
}
inline ::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::release_custom() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.custom)
  if (has_custom()) {
    clear_has_layer();
    ::CoreML::Specification::CustomLayerParams* temp = layer_.custom_;
    layer_.custom_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom) {
  clear_layer();
  if (custom) {
    set_has_custom();
    layer_.custom_ = custom;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.custom)
}

// .CoreML.Specification.CopyLayerParams copy = 600;
inline bool NeuralNetworkLayer::has_copy() const {
  return layer_case() == kCopy;
}
inline void NeuralNetworkLayer::set_has_copy() {
  _oneof_case_[0] = kCopy;
}
inline void NeuralNetworkLayer::clear_copy() {
  if (has_copy()) {
    delete layer_.copy_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CopyLayerParams& NeuralNetworkLayer::copy() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.copy)
  return has_copy()
      ? *layer_.copy_
      : ::CoreML::Specification::CopyLayerParams::default_instance();
}
inline ::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::mutable_copy() {
  if (!has_copy()) {
    clear_layer();
    set_has_copy();
    layer_.copy_ = new ::CoreML::Specification::CopyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.copy)
  return layer_.copy_;
}
inline ::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::release_copy() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.copy)
  if (has_copy()) {
    clear_has_layer();
    ::CoreML::Specification::CopyLayerParams* temp = layer_.copy_;
    layer_.copy_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_copy(::CoreML::Specification::CopyLayerParams* copy) {
  clear_layer();
  if (copy) {
    set_has_copy();
    layer_.copy_ = copy;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.copy)
}

// .CoreML.Specification.BranchLayerParams branch = 605;
inline bool NeuralNetworkLayer::has_branch() const {
  return layer_case() == kBranch;
}
inline void NeuralNetworkLayer::set_has_branch() {
  _oneof_case_[0] = kBranch;
}
inline void NeuralNetworkLayer::clear_branch() {
  if (has_branch()) {
    delete layer_.branch_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BranchLayerParams& NeuralNetworkLayer::branch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.branch)
  return has_branch()
      ? *layer_.branch_
      : ::CoreML::Specification::BranchLayerParams::default_instance();
}
inline ::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::mutable_branch() {
  if (!has_branch()) {
    clear_layer();
    set_has_branch();
    layer_.branch_ = new ::CoreML::Specification::BranchLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.branch)
  return layer_.branch_;
}
inline ::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::release_branch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.branch)
  if (has_branch()) {
    clear_has_layer();
    ::CoreML::Specification::BranchLayerParams* temp = layer_.branch_;
    layer_.branch_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_branch(::CoreML::Specification::BranchLayerParams* branch) {
  clear_layer();
  if (branch) {
    set_has_branch();
    layer_.branch_ = branch;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.branch)
}

// .CoreML.Specification.LoopLayerParams loop = 615;
inline bool NeuralNetworkLayer::has_loop() const {
  return layer_case() == kLoop;
}
inline void NeuralNetworkLayer::set_has_loop() {
  _oneof_case_[0] = kLoop;
}
inline void NeuralNetworkLayer::clear_loop() {
  if (has_loop()) {
    delete layer_.loop_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LoopLayerParams& NeuralNetworkLayer::loop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loop)
  return has_loop()
      ? *layer_.loop_
      : ::CoreML::Specification::LoopLayerParams::default_instance();
}
inline ::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::mutable_loop() {
  if (!has_loop()) {
    clear_layer();
    set_has_loop();
    layer_.loop_ = new ::CoreML::Specification::LoopLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loop)
  return layer_.loop_;
}
inline ::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::release_loop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loop)
  if (has_loop()) {
    clear_has_layer();
    ::CoreML::Specification::LoopLayerParams* temp = layer_.loop_;
    layer_.loop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_loop(::CoreML::Specification::LoopLayerParams* loop) {
  clear_layer();
  if (loop) {
    set_has_loop();
    layer_.loop_ = loop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loop)
}

// .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
inline bool NeuralNetworkLayer::has_loopbreak() const {
  return layer_case() == kLoopBreak;
}
inline void NeuralNetworkLayer::set_has_loopbreak() {
  _oneof_case_[0] = kLoopBreak;
}
inline void NeuralNetworkLayer::clear_loopbreak() {
  if (has_loopbreak()) {
    delete layer_.loopbreak_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LoopBreakLayerParams& NeuralNetworkLayer::loopbreak() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return has_loopbreak()
      ? *layer_.loopbreak_
      : ::CoreML::Specification::LoopBreakLayerParams::default_instance();
}
inline ::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::mutable_loopbreak() {
  if (!has_loopbreak()) {
    clear_layer();
    set_has_loopbreak();
    layer_.loopbreak_ = new ::CoreML::Specification::LoopBreakLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return layer_.loopbreak_;
}
inline ::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::release_loopbreak() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  if (has_loopbreak()) {
    clear_has_layer();
    ::CoreML::Specification::LoopBreakLayerParams* temp = layer_.loopbreak_;
    layer_.loopbreak_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_loopbreak(::CoreML::Specification::LoopBreakLayerParams* loopbreak) {
  clear_layer();
  if (loopbreak) {
    set_has_loopbreak();
    layer_.loopbreak_ = loopbreak;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopBreak)
}

// .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
inline bool NeuralNetworkLayer::has_loopcontinue() const {
  return layer_case() == kLoopContinue;
}
inline void NeuralNetworkLayer::set_has_loopcontinue() {
  _oneof_case_[0] = kLoopContinue;
}
inline void NeuralNetworkLayer::clear_loopcontinue() {
  if (has_loopcontinue()) {
    delete layer_.loopcontinue_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LoopContinueLayerParams& NeuralNetworkLayer::loopcontinue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return has_loopcontinue()
      ? *layer_.loopcontinue_
      : ::CoreML::Specification::LoopContinueLayerParams::default_instance();
}
inline ::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::mutable_loopcontinue() {
  if (!has_loopcontinue()) {
    clear_layer();
    set_has_loopcontinue();
    layer_.loopcontinue_ = new ::CoreML::Specification::LoopContinueLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return layer_.loopcontinue_;
}
inline ::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::release_loopcontinue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  if (has_loopcontinue()) {
    clear_has_layer();
    ::CoreML::Specification::LoopContinueLayerParams* temp = layer_.loopcontinue_;
    layer_.loopcontinue_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_loopcontinue(::CoreML::Specification::LoopContinueLayerParams* loopcontinue) {
  clear_layer();
  if (loopcontinue) {
    set_has_loopcontinue();
    layer_.loopcontinue_ = loopcontinue;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopContinue)
}

// .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
inline bool NeuralNetworkLayer::has_rangestatic() const {
  return layer_case() == kRangeStatic;
}
inline void NeuralNetworkLayer::set_has_rangestatic() {
  _oneof_case_[0] = kRangeStatic;
}
inline void NeuralNetworkLayer::clear_rangestatic() {
  if (has_rangestatic()) {
    delete layer_.rangestatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RangeStaticLayerParams& NeuralNetworkLayer::rangestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  return has_rangestatic()
      ? *layer_.rangestatic_
      : ::CoreML::Specification::RangeStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::mutable_rangestatic() {
  if (!has_rangestatic()) {
    clear_layer();
    set_has_rangestatic();
    layer_.rangestatic_ = new ::CoreML::Specification::RangeStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  return layer_.rangestatic_;
}
inline ::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::release_rangestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  if (has_rangestatic()) {
    clear_has_layer();
    ::CoreML::Specification::RangeStaticLayerParams* temp = layer_.rangestatic_;
    layer_.rangestatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_rangestatic(::CoreML::Specification::RangeStaticLayerParams* rangestatic) {
  clear_layer();
  if (rangestatic) {
    set_has_rangestatic();
    layer_.rangestatic_ = rangestatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
}

// .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
inline bool NeuralNetworkLayer::has_rangedynamic() const {
  return layer_case() == kRangeDynamic;
}
inline void NeuralNetworkLayer::set_has_rangedynamic() {
  _oneof_case_[0] = kRangeDynamic;
}
inline void NeuralNetworkLayer::clear_rangedynamic() {
  if (has_rangedynamic()) {
    delete layer_.rangedynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RangeDynamicLayerParams& NeuralNetworkLayer::rangedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  return has_rangedynamic()
      ? *layer_.rangedynamic_
      : ::CoreML::Specification::RangeDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::mutable_rangedynamic() {
  if (!has_rangedynamic()) {
    clear_layer();
    set_has_rangedynamic();
    layer_.rangedynamic_ = new ::CoreML::Specification::RangeDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  return layer_.rangedynamic_;
}
inline ::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::release_rangedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  if (has_rangedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RangeDynamicLayerParams* temp = layer_.rangedynamic_;
    layer_.rangedynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_rangedynamic(::CoreML::Specification::RangeDynamicLayerParams* rangedynamic) {
  clear_layer();
  if (rangedynamic) {
    set_has_rangedynamic();
    layer_.rangedynamic_ = rangedynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
}

// .CoreML.Specification.ClipLayerParams clip = 660;
inline bool NeuralNetworkLayer::has_clip() const {
  return layer_case() == kClip;
}
inline void NeuralNetworkLayer::set_has_clip() {
  _oneof_case_[0] = kClip;
}
inline void NeuralNetworkLayer::clear_clip() {
  if (has_clip()) {
    delete layer_.clip_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ClipLayerParams& NeuralNetworkLayer::clip() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.clip)
  return has_clip()
      ? *layer_.clip_
      : ::CoreML::Specification::ClipLayerParams::default_instance();
}
inline ::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::mutable_clip() {
  if (!has_clip()) {
    clear_layer();
    set_has_clip();
    layer_.clip_ = new ::CoreML::Specification::ClipLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.clip)
  return layer_.clip_;
}
inline ::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::release_clip() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.clip)
  if (has_clip()) {
    clear_has_layer();
    ::CoreML::Specification::ClipLayerParams* temp = layer_.clip_;
    layer_.clip_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_clip(::CoreML::Specification::ClipLayerParams* clip) {
  clear_layer();
  if (clip) {
    set_has_clip();
    layer_.clip_ = clip;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.clip)
}

// .CoreML.Specification.CeilLayerParams ceil = 665;
inline bool NeuralNetworkLayer::has_ceil() const {
  return layer_case() == kCeil;
}
inline void NeuralNetworkLayer::set_has_ceil() {
  _oneof_case_[0] = kCeil;
}
inline void NeuralNetworkLayer::clear_ceil() {
  if (has_ceil()) {
    delete layer_.ceil_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CeilLayerParams& NeuralNetworkLayer::ceil() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.ceil)
  return has_ceil()
      ? *layer_.ceil_
      : ::CoreML::Specification::CeilLayerParams::default_instance();
}
inline ::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::mutable_ceil() {
  if (!has_ceil()) {
    clear_layer();
    set_has_ceil();
    layer_.ceil_ = new ::CoreML::Specification::CeilLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.ceil)
  return layer_.ceil_;
}
inline ::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::release_ceil() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.ceil)
  if (has_ceil()) {
    clear_has_layer();
    ::CoreML::Specification::CeilLayerParams* temp = layer_.ceil_;
    layer_.ceil_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_ceil(::CoreML::Specification::CeilLayerParams* ceil) {
  clear_layer();
  if (ceil) {
    set_has_ceil();
    layer_.ceil_ = ceil;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.ceil)
}

// .CoreML.Specification.FloorLayerParams floor = 670;
inline bool NeuralNetworkLayer::has_floor() const {
  return layer_case() == kFloor;
}
inline void NeuralNetworkLayer::set_has_floor() {
  _oneof_case_[0] = kFloor;
}
inline void NeuralNetworkLayer::clear_floor() {
  if (has_floor()) {
    delete layer_.floor_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FloorLayerParams& NeuralNetworkLayer::floor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floor)
  return has_floor()
      ? *layer_.floor_
      : ::CoreML::Specification::FloorLayerParams::default_instance();
}
inline ::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::mutable_floor() {
  if (!has_floor()) {
    clear_layer();
    set_has_floor();
    layer_.floor_ = new ::CoreML::Specification::FloorLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floor)
  return layer_.floor_;
}
inline ::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::release_floor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floor)
  if (has_floor()) {
    clear_has_layer();
    ::CoreML::Specification::FloorLayerParams* temp = layer_.floor_;
    layer_.floor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_floor(::CoreML::Specification::FloorLayerParams* floor) {
  clear_layer();
  if (floor) {
    set_has_floor();
    layer_.floor_ = floor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.floor)
}

// .CoreML.Specification.SignLayerParams sign = 680;
inline bool NeuralNetworkLayer::has_sign() const {
  return layer_case() == kSign;
}
inline void NeuralNetworkLayer::set_has_sign() {
  _oneof_case_[0] = kSign;
}
inline void NeuralNetworkLayer::clear_sign() {
  if (has_sign()) {
    delete layer_.sign_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SignLayerParams& NeuralNetworkLayer::sign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sign)
  return has_sign()
      ? *layer_.sign_
      : ::CoreML::Specification::SignLayerParams::default_instance();
}
inline ::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::mutable_sign() {
  if (!has_sign()) {
    clear_layer();
    set_has_sign();
    layer_.sign_ = new ::CoreML::Specification::SignLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sign)
  return layer_.sign_;
}
inline ::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::release_sign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sign)
  if (has_sign()) {
    clear_has_layer();
    ::CoreML::Specification::SignLayerParams* temp = layer_.sign_;
    layer_.sign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_sign(::CoreML::Specification::SignLayerParams* sign) {
  clear_layer();
  if (sign) {
    set_has_sign();
    layer_.sign_ = sign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sign)
}

// .CoreML.Specification.RoundLayerParams round = 685;
inline bool NeuralNetworkLayer::has_round() const {
  return layer_case() == kRound;
}
inline void NeuralNetworkLayer::set_has_round() {
  _oneof_case_[0] = kRound;
}
inline void NeuralNetworkLayer::clear_round() {
  if (has_round()) {
    delete layer_.round_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RoundLayerParams& NeuralNetworkLayer::round() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.round)
  return has_round()
      ? *layer_.round_
      : ::CoreML::Specification::RoundLayerParams::default_instance();
}
inline ::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::mutable_round() {
  if (!has_round()) {
    clear_layer();
    set_has_round();
    layer_.round_ = new ::CoreML::Specification::RoundLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.round)
  return layer_.round_;
}
inline ::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::release_round() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.round)
  if (has_round()) {
    clear_has_layer();
    ::CoreML::Specification::RoundLayerParams* temp = layer_.round_;
    layer_.round_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_round(::CoreML::Specification::RoundLayerParams* round) {
  clear_layer();
  if (round) {
    set_has_round();
    layer_.round_ = round;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.round)
}

// .CoreML.Specification.Exp2LayerParams exp2 = 700;
inline bool NeuralNetworkLayer::has_exp2() const {
  return layer_case() == kExp2;
}
inline void NeuralNetworkLayer::set_has_exp2() {
  _oneof_case_[0] = kExp2;
}
inline void NeuralNetworkLayer::clear_exp2() {
  if (has_exp2()) {
    delete layer_.exp2_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::Exp2LayerParams& NeuralNetworkLayer::exp2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.exp2)
  return has_exp2()
      ? *layer_.exp2_
      : ::CoreML::Specification::Exp2LayerParams::default_instance();
}
inline ::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::mutable_exp2() {
  if (!has_exp2()) {
    clear_layer();
    set_has_exp2();
    layer_.exp2_ = new ::CoreML::Specification::Exp2LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.exp2)
  return layer_.exp2_;
}
inline ::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::release_exp2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.exp2)
  if (has_exp2()) {
    clear_has_layer();
    ::CoreML::Specification::Exp2LayerParams* temp = layer_.exp2_;
    layer_.exp2_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_exp2(::CoreML::Specification::Exp2LayerParams* exp2) {
  clear_layer();
  if (exp2) {
    set_has_exp2();
    layer_.exp2_ = exp2;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.exp2)
}

// .CoreML.Specification.SinLayerParams sin = 710;
inline bool NeuralNetworkLayer::has_sin() const {
  return layer_case() == kSin;
}
inline void NeuralNetworkLayer::set_has_sin() {
  _oneof_case_[0] = kSin;
}
inline void NeuralNetworkLayer::clear_sin() {
  if (has_sin()) {
    delete layer_.sin_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SinLayerParams& NeuralNetworkLayer::sin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sin)
  return has_sin()
      ? *layer_.sin_
      : ::CoreML::Specification::SinLayerParams::default_instance();
}
inline ::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::mutable_sin() {
  if (!has_sin()) {
    clear_layer();
    set_has_sin();
    layer_.sin_ = new ::CoreML::Specification::SinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sin)
  return layer_.sin_;
}
inline ::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::release_sin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sin)
  if (has_sin()) {
    clear_has_layer();
    ::CoreML::Specification::SinLayerParams* temp = layer_.sin_;
    layer_.sin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_sin(::CoreML::Specification::SinLayerParams* sin) {
  clear_layer();
  if (sin) {
    set_has_sin();
    layer_.sin_ = sin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sin)
}

// .CoreML.Specification.CosLayerParams cos = 715;
inline bool NeuralNetworkLayer::has_cos() const {
  return layer_case() == kCos;
}
inline void NeuralNetworkLayer::set_has_cos() {
  _oneof_case_[0] = kCos;
}
inline void NeuralNetworkLayer::clear_cos() {
  if (has_cos()) {
    delete layer_.cos_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CosLayerParams& NeuralNetworkLayer::cos() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cos)
  return has_cos()
      ? *layer_.cos_
      : ::CoreML::Specification::CosLayerParams::default_instance();
}
inline ::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::mutable_cos() {
  if (!has_cos()) {
    clear_layer();
    set_has_cos();
    layer_.cos_ = new ::CoreML::Specification::CosLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cos)
  return layer_.cos_;
}
inline ::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::release_cos() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cos)
  if (has_cos()) {
    clear_has_layer();
    ::CoreML::Specification::CosLayerParams* temp = layer_.cos_;
    layer_.cos_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_cos(::CoreML::Specification::CosLayerParams* cos) {
  clear_layer();
  if (cos) {
    set_has_cos();
    layer_.cos_ = cos;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cos)
}

// .CoreML.Specification.TanLayerParams tan = 720;
inline bool NeuralNetworkLayer::has_tan() const {
  return layer_case() == kTan;
}
inline void NeuralNetworkLayer::set_has_tan() {
  _oneof_case_[0] = kTan;
}
inline void NeuralNetworkLayer::clear_tan() {
  if (has_tan()) {
    delete layer_.tan_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::TanLayerParams& NeuralNetworkLayer::tan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tan)
  return has_tan()
      ? *layer_.tan_
      : ::CoreML::Specification::TanLayerParams::default_instance();
}
inline ::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::mutable_tan() {
  if (!has_tan()) {
    clear_layer();
    set_has_tan();
    layer_.tan_ = new ::CoreML::Specification::TanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tan)
  return layer_.tan_;
}
inline ::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::release_tan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tan)
  if (has_tan()) {
    clear_has_layer();
    ::CoreML::Specification::TanLayerParams* temp = layer_.tan_;
    layer_.tan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_tan(::CoreML::Specification::TanLayerParams* tan) {
  clear_layer();
  if (tan) {
    set_has_tan();
    layer_.tan_ = tan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tan)
}

// .CoreML.Specification.AsinLayerParams asin = 730;
inline bool NeuralNetworkLayer::has_asin() const {
  return layer_case() == kAsin;
}
inline void NeuralNetworkLayer::set_has_asin() {
  _oneof_case_[0] = kAsin;
}
inline void NeuralNetworkLayer::clear_asin() {
  if (has_asin()) {
    delete layer_.asin_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AsinLayerParams& NeuralNetworkLayer::asin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.asin)
  return has_asin()
      ? *layer_.asin_
      : ::CoreML::Specification::AsinLayerParams::default_instance();
}
inline ::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::mutable_asin() {
  if (!has_asin()) {
    clear_layer();
    set_has_asin();
    layer_.asin_ = new ::CoreML::Specification::AsinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.asin)
  return layer_.asin_;
}
inline ::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::release_asin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.asin)
  if (has_asin()) {
    clear_has_layer();
    ::CoreML::Specification::AsinLayerParams* temp = layer_.asin_;
    layer_.asin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_asin(::CoreML::Specification::AsinLayerParams* asin) {
  clear_layer();
  if (asin) {
    set_has_asin();
    layer_.asin_ = asin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.asin)
}

// .CoreML.Specification.AcosLayerParams acos = 735;
inline bool NeuralNetworkLayer::has_acos() const {
  return layer_case() == kAcos;
}
inline void NeuralNetworkLayer::set_has_acos() {
  _oneof_case_[0] = kAcos;
}
inline void NeuralNetworkLayer::clear_acos() {
  if (has_acos()) {
    delete layer_.acos_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AcosLayerParams& NeuralNetworkLayer::acos() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.acos)
  return has_acos()
      ? *layer_.acos_
      : ::CoreML::Specification::AcosLayerParams::default_instance();
}
inline ::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::mutable_acos() {
  if (!has_acos()) {
    clear_layer();
    set_has_acos();
    layer_.acos_ = new ::CoreML::Specification::AcosLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.acos)
  return layer_.acos_;
}
inline ::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::release_acos() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.acos)
  if (has_acos()) {
    clear_has_layer();
    ::CoreML::Specification::AcosLayerParams* temp = layer_.acos_;
    layer_.acos_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_acos(::CoreML::Specification::AcosLayerParams* acos) {
  clear_layer();
  if (acos) {
    set_has_acos();
    layer_.acos_ = acos;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.acos)
}

// .CoreML.Specification.AtanLayerParams atan = 740;
inline bool NeuralNetworkLayer::has_atan() const {
  return layer_case() == kAtan;
}
inline void NeuralNetworkLayer::set_has_atan() {
  _oneof_case_[0] = kAtan;
}
inline void NeuralNetworkLayer::clear_atan() {
  if (has_atan()) {
    delete layer_.atan_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AtanLayerParams& NeuralNetworkLayer::atan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.atan)
  return has_atan()
      ? *layer_.atan_
      : ::CoreML::Specification::AtanLayerParams::default_instance();
}
inline ::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::mutable_atan() {
  if (!has_atan()) {
    clear_layer();
    set_has_atan();
    layer_.atan_ = new ::CoreML::Specification::AtanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.atan)
  return layer_.atan_;
}
inline ::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::release_atan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.atan)
  if (has_atan()) {
    clear_has_layer();
    ::CoreML::Specification::AtanLayerParams* temp = layer_.atan_;
    layer_.atan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_atan(::CoreML::Specification::AtanLayerParams* atan) {
  clear_layer();
  if (atan) {
    set_has_atan();
    layer_.atan_ = atan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.atan)
}

// .CoreML.Specification.SinhLayerParams sinh = 750;
inline bool NeuralNetworkLayer::has_sinh() const {
  return layer_case() == kSinh;
}
inline void NeuralNetworkLayer::set_has_sinh() {
  _oneof_case_[0] = kSinh;
}
inline void NeuralNetworkLayer::clear_sinh() {
  if (has_sinh()) {
    delete layer_.sinh_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SinhLayerParams& NeuralNetworkLayer::sinh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sinh)
  return has_sinh()
      ? *layer_.sinh_
      : ::CoreML::Specification::SinhLayerParams::default_instance();
}
inline ::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::mutable_sinh() {
  if (!has_sinh()) {
    clear_layer();
    set_has_sinh();
    layer_.sinh_ = new ::CoreML::Specification::SinhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sinh)
  return layer_.sinh_;
}
inline ::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::release_sinh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sinh)
  if (has_sinh()) {
    clear_has_layer();
    ::CoreML::Specification::SinhLayerParams* temp = layer_.sinh_;
    layer_.sinh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_sinh(::CoreML::Specification::SinhLayerParams* sinh) {
  clear_layer();
  if (sinh) {
    set_has_sinh();
    layer_.sinh_ = sinh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sinh)
}

// .CoreML.Specification.CoshLayerParams cosh = 755;
inline bool NeuralNetworkLayer::has_cosh() const {
  return layer_case() == kCosh;
}
inline void NeuralNetworkLayer::set_has_cosh() {
  _oneof_case_[0] = kCosh;
}
inline void NeuralNetworkLayer::clear_cosh() {
  if (has_cosh()) {
    delete layer_.cosh_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CoshLayerParams& NeuralNetworkLayer::cosh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cosh)
  return has_cosh()
      ? *layer_.cosh_
      : ::CoreML::Specification::CoshLayerParams::default_instance();
}
inline ::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::mutable_cosh() {
  if (!has_cosh()) {
    clear_layer();
    set_has_cosh();
    layer_.cosh_ = new ::CoreML::Specification::CoshLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cosh)
  return layer_.cosh_;
}
inline ::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::release_cosh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cosh)
  if (has_cosh()) {
    clear_has_layer();
    ::CoreML::Specification::CoshLayerParams* temp = layer_.cosh_;
    layer_.cosh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_cosh(::CoreML::Specification::CoshLayerParams* cosh) {
  clear_layer();
  if (cosh) {
    set_has_cosh();
    layer_.cosh_ = cosh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cosh)
}

// .CoreML.Specification.TanhLayerParams tanh = 760;
inline bool NeuralNetworkLayer::has_tanh() const {
  return layer_case() == kTanh;
}
inline void NeuralNetworkLayer::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
inline void NeuralNetworkLayer::clear_tanh() {
  if (has_tanh()) {
    delete layer_.tanh_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::TanhLayerParams& NeuralNetworkLayer::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tanh)
  return has_tanh()
      ? *layer_.tanh_
      : ::CoreML::Specification::TanhLayerParams::default_instance();
}
inline ::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::mutable_tanh() {
  if (!has_tanh()) {
    clear_layer();
    set_has_tanh();
    layer_.tanh_ = new ::CoreML::Specification::TanhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tanh)
  return layer_.tanh_;
}
inline ::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tanh)
  if (has_tanh()) {
    clear_has_layer();
    ::CoreML::Specification::TanhLayerParams* temp = layer_.tanh_;
    layer_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_tanh(::CoreML::Specification::TanhLayerParams* tanh) {
  clear_layer();
  if (tanh) {
    set_has_tanh();
    layer_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tanh)
}

// .CoreML.Specification.AsinhLayerParams asinh = 770;
inline bool NeuralNetworkLayer::has_asinh() const {
  return layer_case() == kAsinh;
}
inline void NeuralNetworkLayer::set_has_asinh() {
  _oneof_case_[0] = kAsinh;
}
inline void NeuralNetworkLayer::clear_asinh() {
  if (has_asinh()) {
    delete layer_.asinh_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AsinhLayerParams& NeuralNetworkLayer::asinh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.asinh)
  return has_asinh()
      ? *layer_.asinh_
      : ::CoreML::Specification::AsinhLayerParams::default_instance();
}
inline ::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::mutable_asinh() {
  if (!has_asinh()) {
    clear_layer();
    set_has_asinh();
    layer_.asinh_ = new ::CoreML::Specification::AsinhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.asinh)
  return layer_.asinh_;
}
inline ::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::release_asinh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.asinh)
  if (has_asinh()) {
    clear_has_layer();
    ::CoreML::Specification::AsinhLayerParams* temp = layer_.asinh_;
    layer_.asinh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_asinh(::CoreML::Specification::AsinhLayerParams* asinh) {
  clear_layer();
  if (asinh) {
    set_has_asinh();
    layer_.asinh_ = asinh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.asinh)
}

// .CoreML.Specification.AcoshLayerParams acosh = 775;
inline bool NeuralNetworkLayer::has_acosh() const {
  return layer_case() == kAcosh;
}
inline void NeuralNetworkLayer::set_has_acosh() {
  _oneof_case_[0] = kAcosh;
}
inline void NeuralNetworkLayer::clear_acosh() {
  if (has_acosh()) {
    delete layer_.acosh_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AcoshLayerParams& NeuralNetworkLayer::acosh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.acosh)
  return has_acosh()
      ? *layer_.acosh_
      : ::CoreML::Specification::AcoshLayerParams::default_instance();
}
inline ::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::mutable_acosh() {
  if (!has_acosh()) {
    clear_layer();
    set_has_acosh();
    layer_.acosh_ = new ::CoreML::Specification::AcoshLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.acosh)
  return layer_.acosh_;
}
inline ::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::release_acosh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.acosh)
  if (has_acosh()) {
    clear_has_layer();
    ::CoreML::Specification::AcoshLayerParams* temp = layer_.acosh_;
    layer_.acosh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_acosh(::CoreML::Specification::AcoshLayerParams* acosh) {
  clear_layer();
  if (acosh) {
    set_has_acosh();
    layer_.acosh_ = acosh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.acosh)
}

// .CoreML.Specification.AtanhLayerParams atanh = 780;
inline bool NeuralNetworkLayer::has_atanh() const {
  return layer_case() == kAtanh;
}
inline void NeuralNetworkLayer::set_has_atanh() {
  _oneof_case_[0] = kAtanh;
}
inline void NeuralNetworkLayer::clear_atanh() {
  if (has_atanh()) {
    delete layer_.atanh_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AtanhLayerParams& NeuralNetworkLayer::atanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.atanh)
  return has_atanh()
      ? *layer_.atanh_
      : ::CoreML::Specification::AtanhLayerParams::default_instance();
}
inline ::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::mutable_atanh() {
  if (!has_atanh()) {
    clear_layer();
    set_has_atanh();
    layer_.atanh_ = new ::CoreML::Specification::AtanhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.atanh)
  return layer_.atanh_;
}
inline ::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::release_atanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.atanh)
  if (has_atanh()) {
    clear_has_layer();
    ::CoreML::Specification::AtanhLayerParams* temp = layer_.atanh_;
    layer_.atanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_atanh(::CoreML::Specification::AtanhLayerParams* atanh) {
  clear_layer();
  if (atanh) {
    set_has_atanh();
    layer_.atanh_ = atanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.atanh)
}

// .CoreML.Specification.ErfLayerParams erf = 790;
inline bool NeuralNetworkLayer::has_erf() const {
  return layer_case() == kErf;
}
inline void NeuralNetworkLayer::set_has_erf() {
  _oneof_case_[0] = kErf;
}
inline void NeuralNetworkLayer::clear_erf() {
  if (has_erf()) {
    delete layer_.erf_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ErfLayerParams& NeuralNetworkLayer::erf() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.erf)
  return has_erf()
      ? *layer_.erf_
      : ::CoreML::Specification::ErfLayerParams::default_instance();
}
inline ::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::mutable_erf() {
  if (!has_erf()) {
    clear_layer();
    set_has_erf();
    layer_.erf_ = new ::CoreML::Specification::ErfLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.erf)
  return layer_.erf_;
}
inline ::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::release_erf() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.erf)
  if (has_erf()) {
    clear_has_layer();
    ::CoreML::Specification::ErfLayerParams* temp = layer_.erf_;
    layer_.erf_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_erf(::CoreML::Specification::ErfLayerParams* erf) {
  clear_layer();
  if (erf) {
    set_has_erf();
    layer_.erf_ = erf;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.erf)
}

// .CoreML.Specification.GeluLayerParams gelu = 795;
inline bool NeuralNetworkLayer::has_gelu() const {
  return layer_case() == kGelu;
}
inline void NeuralNetworkLayer::set_has_gelu() {
  _oneof_case_[0] = kGelu;
}
inline void NeuralNetworkLayer::clear_gelu() {
  if (has_gelu()) {
    delete layer_.gelu_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GeluLayerParams& NeuralNetworkLayer::gelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gelu)
  return has_gelu()
      ? *layer_.gelu_
      : ::CoreML::Specification::GeluLayerParams::default_instance();
}
inline ::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::mutable_gelu() {
  if (!has_gelu()) {
    clear_layer();
    set_has_gelu();
    layer_.gelu_ = new ::CoreML::Specification::GeluLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gelu)
  return layer_.gelu_;
}
inline ::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::release_gelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gelu)
  if (has_gelu()) {
    clear_has_layer();
    ::CoreML::Specification::GeluLayerParams* temp = layer_.gelu_;
    layer_.gelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_gelu(::CoreML::Specification::GeluLayerParams* gelu) {
  clear_layer();
  if (gelu) {
    set_has_gelu();
    layer_.gelu_ = gelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gelu)
}

// .CoreML.Specification.EqualLayerParams equal = 815;
inline bool NeuralNetworkLayer::has_equal() const {
  return layer_case() == kEqual;
}
inline void NeuralNetworkLayer::set_has_equal() {
  _oneof_case_[0] = kEqual;
}
inline void NeuralNetworkLayer::clear_equal() {
  if (has_equal()) {
    delete layer_.equal_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::EqualLayerParams& NeuralNetworkLayer::equal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.equal)
  return has_equal()
      ? *layer_.equal_
      : ::CoreML::Specification::EqualLayerParams::default_instance();
}
inline ::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::mutable_equal() {
  if (!has_equal()) {
    clear_layer();
    set_has_equal();
    layer_.equal_ = new ::CoreML::Specification::EqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.equal)
  return layer_.equal_;
}
inline ::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::release_equal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.equal)
  if (has_equal()) {
    clear_has_layer();
    ::CoreML::Specification::EqualLayerParams* temp = layer_.equal_;
    layer_.equal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_equal(::CoreML::Specification::EqualLayerParams* equal) {
  clear_layer();
  if (equal) {
    set_has_equal();
    layer_.equal_ = equal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.equal)
}

// .CoreML.Specification.NotEqualLayerParams notEqual = 820;
inline bool NeuralNetworkLayer::has_notequal() const {
  return layer_case() == kNotEqual;
}
inline void NeuralNetworkLayer::set_has_notequal() {
  _oneof_case_[0] = kNotEqual;
}
inline void NeuralNetworkLayer::clear_notequal() {
  if (has_notequal()) {
    delete layer_.notequal_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::NotEqualLayerParams& NeuralNetworkLayer::notequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return has_notequal()
      ? *layer_.notequal_
      : ::CoreML::Specification::NotEqualLayerParams::default_instance();
}
inline ::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::mutable_notequal() {
  if (!has_notequal()) {
    clear_layer();
    set_has_notequal();
    layer_.notequal_ = new ::CoreML::Specification::NotEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return layer_.notequal_;
}
inline ::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::release_notequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.notEqual)
  if (has_notequal()) {
    clear_has_layer();
    ::CoreML::Specification::NotEqualLayerParams* temp = layer_.notequal_;
    layer_.notequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_notequal(::CoreML::Specification::NotEqualLayerParams* notequal) {
  clear_layer();
  if (notequal) {
    set_has_notequal();
    layer_.notequal_ = notequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.notEqual)
}

// .CoreML.Specification.LessThanLayerParams lessThan = 825;
inline bool NeuralNetworkLayer::has_lessthan() const {
  return layer_case() == kLessThan;
}
inline void NeuralNetworkLayer::set_has_lessthan() {
  _oneof_case_[0] = kLessThan;
}
inline void NeuralNetworkLayer::clear_lessthan() {
  if (has_lessthan()) {
    delete layer_.lessthan_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LessThanLayerParams& NeuralNetworkLayer::lessthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return has_lessthan()
      ? *layer_.lessthan_
      : ::CoreML::Specification::LessThanLayerParams::default_instance();
}
inline ::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::mutable_lessthan() {
  if (!has_lessthan()) {
    clear_layer();
    set_has_lessthan();
    layer_.lessthan_ = new ::CoreML::Specification::LessThanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return layer_.lessthan_;
}
inline ::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::release_lessthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessThan)
  if (has_lessthan()) {
    clear_has_layer();
    ::CoreML::Specification::LessThanLayerParams* temp = layer_.lessthan_;
    layer_.lessthan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_lessthan(::CoreML::Specification::LessThanLayerParams* lessthan) {
  clear_layer();
  if (lessthan) {
    set_has_lessthan();
    layer_.lessthan_ = lessthan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessThan)
}

// .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
inline bool NeuralNetworkLayer::has_lessequal() const {
  return layer_case() == kLessEqual;
}
inline void NeuralNetworkLayer::set_has_lessequal() {
  _oneof_case_[0] = kLessEqual;
}
inline void NeuralNetworkLayer::clear_lessequal() {
  if (has_lessequal()) {
    delete layer_.lessequal_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LessEqualLayerParams& NeuralNetworkLayer::lessequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  return has_lessequal()
      ? *layer_.lessequal_
      : ::CoreML::Specification::LessEqualLayerParams::default_instance();
}
inline ::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::mutable_lessequal() {
  if (!has_lessequal()) {
    clear_layer();
    set_has_lessequal();
    layer_.lessequal_ = new ::CoreML::Specification::LessEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  return layer_.lessequal_;
}
inline ::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::release_lessequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  if (has_lessequal()) {
    clear_has_layer();
    ::CoreML::Specification::LessEqualLayerParams* temp = layer_.lessequal_;
    layer_.lessequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_lessequal(::CoreML::Specification::LessEqualLayerParams* lessequal) {
  clear_layer();
  if (lessequal) {
    set_has_lessequal();
    layer_.lessequal_ = lessequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessEqual)
}

// .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
inline bool NeuralNetworkLayer::has_greaterthan() const {
  return layer_case() == kGreaterThan;
}
inline void NeuralNetworkLayer::set_has_greaterthan() {
  _oneof_case_[0] = kGreaterThan;
}
inline void NeuralNetworkLayer::clear_greaterthan() {
  if (has_greaterthan()) {
    delete layer_.greaterthan_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GreaterThanLayerParams& NeuralNetworkLayer::greaterthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return has_greaterthan()
      ? *layer_.greaterthan_
      : ::CoreML::Specification::GreaterThanLayerParams::default_instance();
}
inline ::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::mutable_greaterthan() {
  if (!has_greaterthan()) {
    clear_layer();
    set_has_greaterthan();
    layer_.greaterthan_ = new ::CoreML::Specification::GreaterThanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return layer_.greaterthan_;
}
inline ::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::release_greaterthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  if (has_greaterthan()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterThanLayerParams* temp = layer_.greaterthan_;
    layer_.greaterthan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_greaterthan(::CoreML::Specification::GreaterThanLayerParams* greaterthan) {
  clear_layer();
  if (greaterthan) {
    set_has_greaterthan();
    layer_.greaterthan_ = greaterthan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterThan)
}

// .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
inline bool NeuralNetworkLayer::has_greaterequal() const {
  return layer_case() == kGreaterEqual;
}
inline void NeuralNetworkLayer::set_has_greaterequal() {
  _oneof_case_[0] = kGreaterEqual;
}
inline void NeuralNetworkLayer::clear_greaterequal() {
  if (has_greaterequal()) {
    delete layer_.greaterequal_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GreaterEqualLayerParams& NeuralNetworkLayer::greaterequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  return has_greaterequal()
      ? *layer_.greaterequal_
      : ::CoreML::Specification::GreaterEqualLayerParams::default_instance();
}
inline ::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::mutable_greaterequal() {
  if (!has_greaterequal()) {
    clear_layer();
    set_has_greaterequal();
    layer_.greaterequal_ = new ::CoreML::Specification::GreaterEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  return layer_.greaterequal_;
}
inline ::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::release_greaterequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  if (has_greaterequal()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterEqualLayerParams* temp = layer_.greaterequal_;
    layer_.greaterequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_greaterequal(::CoreML::Specification::GreaterEqualLayerParams* greaterequal) {
  clear_layer();
  if (greaterequal) {
    set_has_greaterequal();
    layer_.greaterequal_ = greaterequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
}

// .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
inline bool NeuralNetworkLayer::has_logicalor() const {
  return layer_case() == kLogicalOr;
}
inline void NeuralNetworkLayer::set_has_logicalor() {
  _oneof_case_[0] = kLogicalOr;
}
inline void NeuralNetworkLayer::clear_logicalor() {
  if (has_logicalor()) {
    delete layer_.logicalor_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LogicalOrLayerParams& NeuralNetworkLayer::logicalor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return has_logicalor()
      ? *layer_.logicalor_
      : ::CoreML::Specification::LogicalOrLayerParams::default_instance();
}
inline ::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::mutable_logicalor() {
  if (!has_logicalor()) {
    clear_layer();
    set_has_logicalor();
    layer_.logicalor_ = new ::CoreML::Specification::LogicalOrLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return layer_.logicalor_;
}
inline ::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::release_logicalor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  if (has_logicalor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalOrLayerParams* temp = layer_.logicalor_;
    layer_.logicalor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_logicalor(::CoreML::Specification::LogicalOrLayerParams* logicalor) {
  clear_layer();
  if (logicalor) {
    set_has_logicalor();
    layer_.logicalor_ = logicalor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalOr)
}

// .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
inline bool NeuralNetworkLayer::has_logicalxor() const {
  return layer_case() == kLogicalXor;
}
inline void NeuralNetworkLayer::set_has_logicalxor() {
  _oneof_case_[0] = kLogicalXor;
}
inline void NeuralNetworkLayer::clear_logicalxor() {
  if (has_logicalxor()) {
    delete layer_.logicalxor_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LogicalXorLayerParams& NeuralNetworkLayer::logicalxor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return has_logicalxor()
      ? *layer_.logicalxor_
      : ::CoreML::Specification::LogicalXorLayerParams::default_instance();
}
inline ::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::mutable_logicalxor() {
  if (!has_logicalxor()) {
    clear_layer();
    set_has_logicalxor();
    layer_.logicalxor_ = new ::CoreML::Specification::LogicalXorLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return layer_.logicalxor_;
}
inline ::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::release_logicalxor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  if (has_logicalxor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalXorLayerParams* temp = layer_.logicalxor_;
    layer_.logicalxor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_logicalxor(::CoreML::Specification::LogicalXorLayerParams* logicalxor) {
  clear_layer();
  if (logicalxor) {
    set_has_logicalxor();
    layer_.logicalxor_ = logicalxor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalXor)
}

// .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
inline bool NeuralNetworkLayer::has_logicalnot() const {
  return layer_case() == kLogicalNot;
}
inline void NeuralNetworkLayer::set_has_logicalnot() {
  _oneof_case_[0] = kLogicalNot;
}
inline void NeuralNetworkLayer::clear_logicalnot() {
  if (has_logicalnot()) {
    delete layer_.logicalnot_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LogicalNotLayerParams& NeuralNetworkLayer::logicalnot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return has_logicalnot()
      ? *layer_.logicalnot_
      : ::CoreML::Specification::LogicalNotLayerParams::default_instance();
}
inline ::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::mutable_logicalnot() {
  if (!has_logicalnot()) {
    clear_layer();
    set_has_logicalnot();
    layer_.logicalnot_ = new ::CoreML::Specification::LogicalNotLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return layer_.logicalnot_;
}
inline ::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::release_logicalnot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  if (has_logicalnot()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalNotLayerParams* temp = layer_.logicalnot_;
    layer_.logicalnot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_logicalnot(::CoreML::Specification::LogicalNotLayerParams* logicalnot) {
  clear_layer();
  if (logicalnot) {
    set_has_logicalnot();
    layer_.logicalnot_ = logicalnot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalNot)
}

// .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
inline bool NeuralNetworkLayer::has_logicaland() const {
  return layer_case() == kLogicalAnd;
}
inline void NeuralNetworkLayer::set_has_logicaland() {
  _oneof_case_[0] = kLogicalAnd;
}
inline void NeuralNetworkLayer::clear_logicaland() {
  if (has_logicaland()) {
    delete layer_.logicaland_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LogicalAndLayerParams& NeuralNetworkLayer::logicaland() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return has_logicaland()
      ? *layer_.logicaland_
      : ::CoreML::Specification::LogicalAndLayerParams::default_instance();
}
inline ::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::mutable_logicaland() {
  if (!has_logicaland()) {
    clear_layer();
    set_has_logicaland();
    layer_.logicaland_ = new ::CoreML::Specification::LogicalAndLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return layer_.logicaland_;
}
inline ::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::release_logicaland() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  if (has_logicaland()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalAndLayerParams* temp = layer_.logicaland_;
    layer_.logicaland_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_logicaland(::CoreML::Specification::LogicalAndLayerParams* logicaland) {
  clear_layer();
  if (logicaland) {
    set_has_logicaland();
    layer_.logicaland_ = logicaland;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
}

// .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
inline bool NeuralNetworkLayer::has_modbroadcastable() const {
  return layer_case() == kModBroadcastable;
}
inline void NeuralNetworkLayer::set_has_modbroadcastable() {
  _oneof_case_[0] = kModBroadcastable;
}
inline void NeuralNetworkLayer::clear_modbroadcastable() {
  if (has_modbroadcastable()) {
    delete layer_.modbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ModBroadcastableLayerParams& NeuralNetworkLayer::modbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  return has_modbroadcastable()
      ? *layer_.modbroadcastable_
      : ::CoreML::Specification::ModBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::mutable_modbroadcastable() {
  if (!has_modbroadcastable()) {
    clear_layer();
    set_has_modbroadcastable();
    layer_.modbroadcastable_ = new ::CoreML::Specification::ModBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  return layer_.modbroadcastable_;
}
inline ::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::release_modbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  if (has_modbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::ModBroadcastableLayerParams* temp = layer_.modbroadcastable_;
    layer_.modbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_modbroadcastable(::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable) {
  clear_layer();
  if (modbroadcastable) {
    set_has_modbroadcastable();
    layer_.modbroadcastable_ = modbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
}

// .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
inline bool NeuralNetworkLayer::has_minbroadcastable() const {
  return layer_case() == kMinBroadcastable;
}
inline void NeuralNetworkLayer::set_has_minbroadcastable() {
  _oneof_case_[0] = kMinBroadcastable;
}
inline void NeuralNetworkLayer::clear_minbroadcastable() {
  if (has_minbroadcastable()) {
    delete layer_.minbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MinBroadcastableLayerParams& NeuralNetworkLayer::minbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  return has_minbroadcastable()
      ? *layer_.minbroadcastable_
      : ::CoreML::Specification::MinBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::mutable_minbroadcastable() {
  if (!has_minbroadcastable()) {
    clear_layer();
    set_has_minbroadcastable();
    layer_.minbroadcastable_ = new ::CoreML::Specification::MinBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  return layer_.minbroadcastable_;
}
inline ::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::release_minbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  if (has_minbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MinBroadcastableLayerParams* temp = layer_.minbroadcastable_;
    layer_.minbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_minbroadcastable(::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable) {
  clear_layer();
  if (minbroadcastable) {
    set_has_minbroadcastable();
    layer_.minbroadcastable_ = minbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
}

// .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
inline bool NeuralNetworkLayer::has_maxbroadcastable() const {
  return layer_case() == kMaxBroadcastable;
}
inline void NeuralNetworkLayer::set_has_maxbroadcastable() {
  _oneof_case_[0] = kMaxBroadcastable;
}
inline void NeuralNetworkLayer::clear_maxbroadcastable() {
  if (has_maxbroadcastable()) {
    delete layer_.maxbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MaxBroadcastableLayerParams& NeuralNetworkLayer::maxbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  return has_maxbroadcastable()
      ? *layer_.maxbroadcastable_
      : ::CoreML::Specification::MaxBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::mutable_maxbroadcastable() {
  if (!has_maxbroadcastable()) {
    clear_layer();
    set_has_maxbroadcastable();
    layer_.maxbroadcastable_ = new ::CoreML::Specification::MaxBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  return layer_.maxbroadcastable_;
}
inline ::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::release_maxbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  if (has_maxbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MaxBroadcastableLayerParams* temp = layer_.maxbroadcastable_;
    layer_.maxbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_maxbroadcastable(::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable) {
  clear_layer();
  if (maxbroadcastable) {
    set_has_maxbroadcastable();
    layer_.maxbroadcastable_ = maxbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
}

// .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
inline bool NeuralNetworkLayer::has_addbroadcastable() const {
  return layer_case() == kAddBroadcastable;
}
inline void NeuralNetworkLayer::set_has_addbroadcastable() {
  _oneof_case_[0] = kAddBroadcastable;
}
inline void NeuralNetworkLayer::clear_addbroadcastable() {
  if (has_addbroadcastable()) {
    delete layer_.addbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AddBroadcastableLayerParams& NeuralNetworkLayer::addbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return has_addbroadcastable()
      ? *layer_.addbroadcastable_
      : ::CoreML::Specification::AddBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::mutable_addbroadcastable() {
  if (!has_addbroadcastable()) {
    clear_layer();
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = new ::CoreML::Specification::AddBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return layer_.addbroadcastable_;
}
inline ::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::release_addbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  if (has_addbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::AddBroadcastableLayerParams* temp = layer_.addbroadcastable_;
    layer_.addbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_addbroadcastable(::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable) {
  clear_layer();
  if (addbroadcastable) {
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = addbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
}

// .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
inline bool NeuralNetworkLayer::has_powbroadcastable() const {
  return layer_case() == kPowBroadcastable;
}
inline void NeuralNetworkLayer::set_has_powbroadcastable() {
  _oneof_case_[0] = kPowBroadcastable;
}
inline void NeuralNetworkLayer::clear_powbroadcastable() {
  if (has_powbroadcastable()) {
    delete layer_.powbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PowBroadcastableLayerParams& NeuralNetworkLayer::powbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return has_powbroadcastable()
      ? *layer_.powbroadcastable_
      : ::CoreML::Specification::PowBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::mutable_powbroadcastable() {
  if (!has_powbroadcastable()) {
    clear_layer();
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = new ::CoreML::Specification::PowBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return layer_.powbroadcastable_;
}
inline ::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::release_powbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  if (has_powbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::PowBroadcastableLayerParams* temp = layer_.powbroadcastable_;
    layer_.powbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_powbroadcastable(::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable) {
  clear_layer();
  if (powbroadcastable) {
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = powbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
}

// .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
inline bool NeuralNetworkLayer::has_dividebroadcastable() const {
  return layer_case() == kDivideBroadcastable;
}
inline void NeuralNetworkLayer::set_has_dividebroadcastable() {
  _oneof_case_[0] = kDivideBroadcastable;
}
inline void NeuralNetworkLayer::clear_dividebroadcastable() {
  if (has_dividebroadcastable()) {
    delete layer_.dividebroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::DivideBroadcastableLayerParams& NeuralNetworkLayer::dividebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return has_dividebroadcastable()
      ? *layer_.dividebroadcastable_
      : ::CoreML::Specification::DivideBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::mutable_dividebroadcastable() {
  if (!has_dividebroadcastable()) {
    clear_layer();
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = new ::CoreML::Specification::DivideBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return layer_.dividebroadcastable_;
}
inline ::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::release_dividebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  if (has_dividebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::DivideBroadcastableLayerParams* temp = layer_.dividebroadcastable_;
    layer_.dividebroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_dividebroadcastable(::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable) {
  clear_layer();
  if (dividebroadcastable) {
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = dividebroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
}

// .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
inline bool NeuralNetworkLayer::has_floordivbroadcastable() const {
  return layer_case() == kFloorDivBroadcastable;
}
inline void NeuralNetworkLayer::set_has_floordivbroadcastable() {
  _oneof_case_[0] = kFloorDivBroadcastable;
}
inline void NeuralNetworkLayer::clear_floordivbroadcastable() {
  if (has_floordivbroadcastable()) {
    delete layer_.floordivbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FloorDivBroadcastableLayerParams& NeuralNetworkLayer::floordivbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  return has_floordivbroadcastable()
      ? *layer_.floordivbroadcastable_
      : ::CoreML::Specification::FloorDivBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::mutable_floordivbroadcastable() {
  if (!has_floordivbroadcastable()) {
    clear_layer();
    set_has_floordivbroadcastable();
    layer_.floordivbroadcastable_ = new ::CoreML::Specification::FloorDivBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  return layer_.floordivbroadcastable_;
}
inline ::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::release_floordivbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  if (has_floordivbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::FloorDivBroadcastableLayerParams* temp = layer_.floordivbroadcastable_;
    layer_.floordivbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_floordivbroadcastable(::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable) {
  clear_layer();
  if (floordivbroadcastable) {
    set_has_floordivbroadcastable();
    layer_.floordivbroadcastable_ = floordivbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
}

// .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
inline bool NeuralNetworkLayer::has_multiplybroadcastable() const {
  return layer_case() == kMultiplyBroadcastable;
}
inline void NeuralNetworkLayer::set_has_multiplybroadcastable() {
  _oneof_case_[0] = kMultiplyBroadcastable;
}
inline void NeuralNetworkLayer::clear_multiplybroadcastable() {
  if (has_multiplybroadcastable()) {
    delete layer_.multiplybroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MultiplyBroadcastableLayerParams& NeuralNetworkLayer::multiplybroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return has_multiplybroadcastable()
      ? *layer_.multiplybroadcastable_
      : ::CoreML::Specification::MultiplyBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::mutable_multiplybroadcastable() {
  if (!has_multiplybroadcastable()) {
    clear_layer();
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = new ::CoreML::Specification::MultiplyBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return layer_.multiplybroadcastable_;
}
inline ::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::release_multiplybroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  if (has_multiplybroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyBroadcastableLayerParams* temp = layer_.multiplybroadcastable_;
    layer_.multiplybroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_multiplybroadcastable(::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable) {
  clear_layer();
  if (multiplybroadcastable) {
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = multiplybroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
}

// .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
inline bool NeuralNetworkLayer::has_subtractbroadcastable() const {
  return layer_case() == kSubtractBroadcastable;
}
inline void NeuralNetworkLayer::set_has_subtractbroadcastable() {
  _oneof_case_[0] = kSubtractBroadcastable;
}
inline void NeuralNetworkLayer::clear_subtractbroadcastable() {
  if (has_subtractbroadcastable()) {
    delete layer_.subtractbroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SubtractBroadcastableLayerParams& NeuralNetworkLayer::subtractbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return has_subtractbroadcastable()
      ? *layer_.subtractbroadcastable_
      : ::CoreML::Specification::SubtractBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::mutable_subtractbroadcastable() {
  if (!has_subtractbroadcastable()) {
    clear_layer();
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = new ::CoreML::Specification::SubtractBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return layer_.subtractbroadcastable_;
}
inline ::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::release_subtractbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  if (has_subtractbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::SubtractBroadcastableLayerParams* temp = layer_.subtractbroadcastable_;
    layer_.subtractbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_subtractbroadcastable(::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable) {
  clear_layer();
  if (subtractbroadcastable) {
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = subtractbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
}

// .CoreML.Specification.TileLayerParams tile = 920;
inline bool NeuralNetworkLayer::has_tile() const {
  return layer_case() == kTile;
}
inline void NeuralNetworkLayer::set_has_tile() {
  _oneof_case_[0] = kTile;
}
inline void NeuralNetworkLayer::clear_tile() {
  if (has_tile()) {
    delete layer_.tile_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::TileLayerParams& NeuralNetworkLayer::tile() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tile)
  return has_tile()
      ? *layer_.tile_
      : ::CoreML::Specification::TileLayerParams::default_instance();
}
inline ::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::mutable_tile() {
  if (!has_tile()) {
    clear_layer();
    set_has_tile();
    layer_.tile_ = new ::CoreML::Specification::TileLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tile)
  return layer_.tile_;
}
inline ::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::release_tile() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tile)
  if (has_tile()) {
    clear_has_layer();
    ::CoreML::Specification::TileLayerParams* temp = layer_.tile_;
    layer_.tile_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_tile(::CoreML::Specification::TileLayerParams* tile) {
  clear_layer();
  if (tile) {
    set_has_tile();
    layer_.tile_ = tile;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tile)
}

// .CoreML.Specification.StackLayerParams stack = 925;
inline bool NeuralNetworkLayer::has_stack() const {
  return layer_case() == kStack;
}
inline void NeuralNetworkLayer::set_has_stack() {
  _oneof_case_[0] = kStack;
}
inline void NeuralNetworkLayer::clear_stack() {
  if (has_stack()) {
    delete layer_.stack_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::StackLayerParams& NeuralNetworkLayer::stack() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.stack)
  return has_stack()
      ? *layer_.stack_
      : ::CoreML::Specification::StackLayerParams::default_instance();
}
inline ::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::mutable_stack() {
  if (!has_stack()) {
    clear_layer();
    set_has_stack();
    layer_.stack_ = new ::CoreML::Specification::StackLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.stack)
  return layer_.stack_;
}
inline ::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::release_stack() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.stack)
  if (has_stack()) {
    clear_has_layer();
    ::CoreML::Specification::StackLayerParams* temp = layer_.stack_;
    layer_.stack_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_stack(::CoreML::Specification::StackLayerParams* stack) {
  clear_layer();
  if (stack) {
    set_has_stack();
    layer_.stack_ = stack;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.stack)
}

// .CoreML.Specification.GatherLayerParams gather = 930;
inline bool NeuralNetworkLayer::has_gather() const {
  return layer_case() == kGather;
}
inline void NeuralNetworkLayer::set_has_gather() {
  _oneof_case_[0] = kGather;
}
inline void NeuralNetworkLayer::clear_gather() {
  if (has_gather()) {
    delete layer_.gather_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GatherLayerParams& NeuralNetworkLayer::gather() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gather)
  return has_gather()
      ? *layer_.gather_
      : ::CoreML::Specification::GatherLayerParams::default_instance();
}
inline ::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::mutable_gather() {
  if (!has_gather()) {
    clear_layer();
    set_has_gather();
    layer_.gather_ = new ::CoreML::Specification::GatherLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gather)
  return layer_.gather_;
}
inline ::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::release_gather() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gather)
  if (has_gather()) {
    clear_has_layer();
    ::CoreML::Specification::GatherLayerParams* temp = layer_.gather_;
    layer_.gather_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_gather(::CoreML::Specification::GatherLayerParams* gather) {
  clear_layer();
  if (gather) {
    set_has_gather();
    layer_.gather_ = gather;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gather)
}

// .CoreML.Specification.ScatterLayerParams scatter = 935;
inline bool NeuralNetworkLayer::has_scatter() const {
  return layer_case() == kScatter;
}
inline void NeuralNetworkLayer::set_has_scatter() {
  _oneof_case_[0] = kScatter;
}
inline void NeuralNetworkLayer::clear_scatter() {
  if (has_scatter()) {
    delete layer_.scatter_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ScatterLayerParams& NeuralNetworkLayer::scatter() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatter)
  return has_scatter()
      ? *layer_.scatter_
      : ::CoreML::Specification::ScatterLayerParams::default_instance();
}
inline ::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::mutable_scatter() {
  if (!has_scatter()) {
    clear_layer();
    set_has_scatter();
    layer_.scatter_ = new ::CoreML::Specification::ScatterLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatter)
  return layer_.scatter_;
}
inline ::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::release_scatter() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatter)
  if (has_scatter()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterLayerParams* temp = layer_.scatter_;
    layer_.scatter_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_scatter(::CoreML::Specification::ScatterLayerParams* scatter) {
  clear_layer();
  if (scatter) {
    set_has_scatter();
    layer_.scatter_ = scatter;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatter)
}

// .CoreML.Specification.GatherNDLayerParams gatherND = 940;
inline bool NeuralNetworkLayer::has_gathernd() const {
  return layer_case() == kGatherND;
}
inline void NeuralNetworkLayer::set_has_gathernd() {
  _oneof_case_[0] = kGatherND;
}
inline void NeuralNetworkLayer::clear_gathernd() {
  if (has_gathernd()) {
    delete layer_.gathernd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GatherNDLayerParams& NeuralNetworkLayer::gathernd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gatherND)
  return has_gathernd()
      ? *layer_.gathernd_
      : ::CoreML::Specification::GatherNDLayerParams::default_instance();
}
inline ::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::mutable_gathernd() {
  if (!has_gathernd()) {
    clear_layer();
    set_has_gathernd();
    layer_.gathernd_ = new ::CoreML::Specification::GatherNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gatherND)
  return layer_.gathernd_;
}
inline ::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::release_gathernd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gatherND)
  if (has_gathernd()) {
    clear_has_layer();
    ::CoreML::Specification::GatherNDLayerParams* temp = layer_.gathernd_;
    layer_.gathernd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_gathernd(::CoreML::Specification::GatherNDLayerParams* gathernd) {
  clear_layer();
  if (gathernd) {
    set_has_gathernd();
    layer_.gathernd_ = gathernd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gatherND)
}

// .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
inline bool NeuralNetworkLayer::has_scatternd() const {
  return layer_case() == kScatterND;
}
inline void NeuralNetworkLayer::set_has_scatternd() {
  _oneof_case_[0] = kScatterND;
}
inline void NeuralNetworkLayer::clear_scatternd() {
  if (has_scatternd()) {
    delete layer_.scatternd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ScatterNDLayerParams& NeuralNetworkLayer::scatternd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatterND)
  return has_scatternd()
      ? *layer_.scatternd_
      : ::CoreML::Specification::ScatterNDLayerParams::default_instance();
}
inline ::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::mutable_scatternd() {
  if (!has_scatternd()) {
    clear_layer();
    set_has_scatternd();
    layer_.scatternd_ = new ::CoreML::Specification::ScatterNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatterND)
  return layer_.scatternd_;
}
inline ::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::release_scatternd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatterND)
  if (has_scatternd()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterNDLayerParams* temp = layer_.scatternd_;
    layer_.scatternd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_scatternd(::CoreML::Specification::ScatterNDLayerParams* scatternd) {
  clear_layer();
  if (scatternd) {
    set_has_scatternd();
    layer_.scatternd_ = scatternd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatterND)
}

// .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
inline bool NeuralNetworkLayer::has_softmaxnd() const {
  return layer_case() == kSoftmaxND;
}
inline void NeuralNetworkLayer::set_has_softmaxnd() {
  _oneof_case_[0] = kSoftmaxND;
}
inline void NeuralNetworkLayer::clear_softmaxnd() {
  if (has_softmaxnd()) {
    delete layer_.softmaxnd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SoftmaxNDLayerParams& NeuralNetworkLayer::softmaxnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return has_softmaxnd()
      ? *layer_.softmaxnd_
      : ::CoreML::Specification::SoftmaxNDLayerParams::default_instance();
}
inline ::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::mutable_softmaxnd() {
  if (!has_softmaxnd()) {
    clear_layer();
    set_has_softmaxnd();
    layer_.softmaxnd_ = new ::CoreML::Specification::SoftmaxNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return layer_.softmaxnd_;
}
inline ::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::release_softmaxnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  if (has_softmaxnd()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxNDLayerParams* temp = layer_.softmaxnd_;
    layer_.softmaxnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_softmaxnd(::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd) {
  clear_layer();
  if (softmaxnd) {
    set_has_softmaxnd();
    layer_.softmaxnd_ = softmaxnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmaxND)
}

// .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
inline bool NeuralNetworkLayer::has_gatheralongaxis() const {
  return layer_case() == kGatherAlongAxis;
}
inline void NeuralNetworkLayer::set_has_gatheralongaxis() {
  _oneof_case_[0] = kGatherAlongAxis;
}
inline void NeuralNetworkLayer::clear_gatheralongaxis() {
  if (has_gatheralongaxis()) {
    delete layer_.gatheralongaxis_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GatherAlongAxisLayerParams& NeuralNetworkLayer::gatheralongaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  return has_gatheralongaxis()
      ? *layer_.gatheralongaxis_
      : ::CoreML::Specification::GatherAlongAxisLayerParams::default_instance();
}
inline ::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::mutable_gatheralongaxis() {
  if (!has_gatheralongaxis()) {
    clear_layer();
    set_has_gatheralongaxis();
    layer_.gatheralongaxis_ = new ::CoreML::Specification::GatherAlongAxisLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  return layer_.gatheralongaxis_;
}
inline ::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::release_gatheralongaxis() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  if (has_gatheralongaxis()) {
    clear_has_layer();
    ::CoreML::Specification::GatherAlongAxisLayerParams* temp = layer_.gatheralongaxis_;
    layer_.gatheralongaxis_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_gatheralongaxis(::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis) {
  clear_layer();
  if (gatheralongaxis) {
    set_has_gatheralongaxis();
    layer_.gatheralongaxis_ = gatheralongaxis;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
}

// .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
inline bool NeuralNetworkLayer::has_scatteralongaxis() const {
  return layer_case() == kScatterAlongAxis;
}
inline void NeuralNetworkLayer::set_has_scatteralongaxis() {
  _oneof_case_[0] = kScatterAlongAxis;
}
inline void NeuralNetworkLayer::clear_scatteralongaxis() {
  if (has_scatteralongaxis()) {
    delete layer_.scatteralongaxis_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ScatterAlongAxisLayerParams& NeuralNetworkLayer::scatteralongaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  return has_scatteralongaxis()
      ? *layer_.scatteralongaxis_
      : ::CoreML::Specification::ScatterAlongAxisLayerParams::default_instance();
}
inline ::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::mutable_scatteralongaxis() {
  if (!has_scatteralongaxis()) {
    clear_layer();
    set_has_scatteralongaxis();
    layer_.scatteralongaxis_ = new ::CoreML::Specification::ScatterAlongAxisLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  return layer_.scatteralongaxis_;
}
inline ::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::release_scatteralongaxis() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  if (has_scatteralongaxis()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterAlongAxisLayerParams* temp = layer_.scatteralongaxis_;
    layer_.scatteralongaxis_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_scatteralongaxis(::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis) {
  clear_layer();
  if (scatteralongaxis) {
    set_has_scatteralongaxis();
    layer_.scatteralongaxis_ = scatteralongaxis;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
}

// .CoreML.Specification.ReverseLayerParams reverse = 960;
inline bool NeuralNetworkLayer::has_reverse() const {
  return layer_case() == kReverse;
}
inline void NeuralNetworkLayer::set_has_reverse() {
  _oneof_case_[0] = kReverse;
}
inline void NeuralNetworkLayer::clear_reverse() {
  if (has_reverse()) {
    delete layer_.reverse_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReverseLayerParams& NeuralNetworkLayer::reverse() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reverse)
  return has_reverse()
      ? *layer_.reverse_
      : ::CoreML::Specification::ReverseLayerParams::default_instance();
}
inline ::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::mutable_reverse() {
  if (!has_reverse()) {
    clear_layer();
    set_has_reverse();
    layer_.reverse_ = new ::CoreML::Specification::ReverseLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reverse)
  return layer_.reverse_;
}
inline ::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::release_reverse() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reverse)
  if (has_reverse()) {
    clear_has_layer();
    ::CoreML::Specification::ReverseLayerParams* temp = layer_.reverse_;
    layer_.reverse_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reverse(::CoreML::Specification::ReverseLayerParams* reverse) {
  clear_layer();
  if (reverse) {
    set_has_reverse();
    layer_.reverse_ = reverse;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reverse)
}

// .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
inline bool NeuralNetworkLayer::has_reverseseq() const {
  return layer_case() == kReverseSeq;
}
inline void NeuralNetworkLayer::set_has_reverseseq() {
  _oneof_case_[0] = kReverseSeq;
}
inline void NeuralNetworkLayer::clear_reverseseq() {
  if (has_reverseseq()) {
    delete layer_.reverseseq_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReverseSeqLayerParams& NeuralNetworkLayer::reverseseq() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  return has_reverseseq()
      ? *layer_.reverseseq_
      : ::CoreML::Specification::ReverseSeqLayerParams::default_instance();
}
inline ::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::mutable_reverseseq() {
  if (!has_reverseseq()) {
    clear_layer();
    set_has_reverseseq();
    layer_.reverseseq_ = new ::CoreML::Specification::ReverseSeqLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  return layer_.reverseseq_;
}
inline ::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::release_reverseseq() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  if (has_reverseseq()) {
    clear_has_layer();
    ::CoreML::Specification::ReverseSeqLayerParams* temp = layer_.reverseseq_;
    layer_.reverseseq_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reverseseq(::CoreML::Specification::ReverseSeqLayerParams* reverseseq) {
  clear_layer();
  if (reverseseq) {
    set_has_reverseseq();
    layer_.reverseseq_ = reverseseq;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
}

// .CoreML.Specification.SplitNDLayerParams splitND = 975;
inline bool NeuralNetworkLayer::has_splitnd() const {
  return layer_case() == kSplitND;
}
inline void NeuralNetworkLayer::set_has_splitnd() {
  _oneof_case_[0] = kSplitND;
}
inline void NeuralNetworkLayer::clear_splitnd() {
  if (has_splitnd()) {
    delete layer_.splitnd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SplitNDLayerParams& NeuralNetworkLayer::splitnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.splitND)
  return has_splitnd()
      ? *layer_.splitnd_
      : ::CoreML::Specification::SplitNDLayerParams::default_instance();
}
inline ::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::mutable_splitnd() {
  if (!has_splitnd()) {
    clear_layer();
    set_has_splitnd();
    layer_.splitnd_ = new ::CoreML::Specification::SplitNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.splitND)
  return layer_.splitnd_;
}
inline ::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::release_splitnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.splitND)
  if (has_splitnd()) {
    clear_has_layer();
    ::CoreML::Specification::SplitNDLayerParams* temp = layer_.splitnd_;
    layer_.splitnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_splitnd(::CoreML::Specification::SplitNDLayerParams* splitnd) {
  clear_layer();
  if (splitnd) {
    set_has_splitnd();
    layer_.splitnd_ = splitnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.splitND)
}

// .CoreML.Specification.ConcatNDLayerParams concatND = 980;
inline bool NeuralNetworkLayer::has_concatnd() const {
  return layer_case() == kConcatND;
}
inline void NeuralNetworkLayer::set_has_concatnd() {
  _oneof_case_[0] = kConcatND;
}
inline void NeuralNetworkLayer::clear_concatnd() {
  if (has_concatnd()) {
    delete layer_.concatnd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ConcatNDLayerParams& NeuralNetworkLayer::concatnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concatND)
  return has_concatnd()
      ? *layer_.concatnd_
      : ::CoreML::Specification::ConcatNDLayerParams::default_instance();
}
inline ::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::mutable_concatnd() {
  if (!has_concatnd()) {
    clear_layer();
    set_has_concatnd();
    layer_.concatnd_ = new ::CoreML::Specification::ConcatNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concatND)
  return layer_.concatnd_;
}
inline ::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::release_concatnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concatND)
  if (has_concatnd()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatNDLayerParams* temp = layer_.concatnd_;
    layer_.concatnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_concatnd(::CoreML::Specification::ConcatNDLayerParams* concatnd) {
  clear_layer();
  if (concatnd) {
    set_has_concatnd();
    layer_.concatnd_ = concatnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concatND)
}

// .CoreML.Specification.TransposeLayerParams transpose = 985;
inline bool NeuralNetworkLayer::has_transpose() const {
  return layer_case() == kTranspose;
}
inline void NeuralNetworkLayer::set_has_transpose() {
  _oneof_case_[0] = kTranspose;
}
inline void NeuralNetworkLayer::clear_transpose() {
  if (has_transpose()) {
    delete layer_.transpose_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::TransposeLayerParams& NeuralNetworkLayer::transpose() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.transpose)
  return has_transpose()
      ? *layer_.transpose_
      : ::CoreML::Specification::TransposeLayerParams::default_instance();
}
inline ::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::mutable_transpose() {
  if (!has_transpose()) {
    clear_layer();
    set_has_transpose();
    layer_.transpose_ = new ::CoreML::Specification::TransposeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.transpose)
  return layer_.transpose_;
}
inline ::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::release_transpose() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.transpose)
  if (has_transpose()) {
    clear_has_layer();
    ::CoreML::Specification::TransposeLayerParams* temp = layer_.transpose_;
    layer_.transpose_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_transpose(::CoreML::Specification::TransposeLayerParams* transpose) {
  clear_layer();
  if (transpose) {
    set_has_transpose();
    layer_.transpose_ = transpose;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.transpose)
}

// .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
inline bool NeuralNetworkLayer::has_slicestatic() const {
  return layer_case() == kSliceStatic;
}
inline void NeuralNetworkLayer::set_has_slicestatic() {
  _oneof_case_[0] = kSliceStatic;
}
inline void NeuralNetworkLayer::clear_slicestatic() {
  if (has_slicestatic()) {
    delete layer_.slicestatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SliceStaticLayerParams& NeuralNetworkLayer::slicestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  return has_slicestatic()
      ? *layer_.slicestatic_
      : ::CoreML::Specification::SliceStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::mutable_slicestatic() {
  if (!has_slicestatic()) {
    clear_layer();
    set_has_slicestatic();
    layer_.slicestatic_ = new ::CoreML::Specification::SliceStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  return layer_.slicestatic_;
}
inline ::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::release_slicestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  if (has_slicestatic()) {
    clear_has_layer();
    ::CoreML::Specification::SliceStaticLayerParams* temp = layer_.slicestatic_;
    layer_.slicestatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_slicestatic(::CoreML::Specification::SliceStaticLayerParams* slicestatic) {
  clear_layer();
  if (slicestatic) {
    set_has_slicestatic();
    layer_.slicestatic_ = slicestatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
}

// .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
inline bool NeuralNetworkLayer::has_slicedynamic() const {
  return layer_case() == kSliceDynamic;
}
inline void NeuralNetworkLayer::set_has_slicedynamic() {
  _oneof_case_[0] = kSliceDynamic;
}
inline void NeuralNetworkLayer::clear_slicedynamic() {
  if (has_slicedynamic()) {
    delete layer_.slicedynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SliceDynamicLayerParams& NeuralNetworkLayer::slicedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  return has_slicedynamic()
      ? *layer_.slicedynamic_
      : ::CoreML::Specification::SliceDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::mutable_slicedynamic() {
  if (!has_slicedynamic()) {
    clear_layer();
    set_has_slicedynamic();
    layer_.slicedynamic_ = new ::CoreML::Specification::SliceDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  return layer_.slicedynamic_;
}
inline ::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::release_slicedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  if (has_slicedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::SliceDynamicLayerParams* temp = layer_.slicedynamic_;
    layer_.slicedynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_slicedynamic(::CoreML::Specification::SliceDynamicLayerParams* slicedynamic) {
  clear_layer();
  if (slicedynamic) {
    set_has_slicedynamic();
    layer_.slicedynamic_ = slicedynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
}

// .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
inline bool NeuralNetworkLayer::has_slidingwindows() const {
  return layer_case() == kSlidingWindows;
}
inline void NeuralNetworkLayer::set_has_slidingwindows() {
  _oneof_case_[0] = kSlidingWindows;
}
inline void NeuralNetworkLayer::clear_slidingwindows() {
  if (has_slidingwindows()) {
    delete layer_.slidingwindows_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SlidingWindowsLayerParams& NeuralNetworkLayer::slidingwindows() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return has_slidingwindows()
      ? *layer_.slidingwindows_
      : ::CoreML::Specification::SlidingWindowsLayerParams::default_instance();
}
inline ::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::mutable_slidingwindows() {
  if (!has_slidingwindows()) {
    clear_layer();
    set_has_slidingwindows();
    layer_.slidingwindows_ = new ::CoreML::Specification::SlidingWindowsLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return layer_.slidingwindows_;
}
inline ::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::release_slidingwindows() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  if (has_slidingwindows()) {
    clear_has_layer();
    ::CoreML::Specification::SlidingWindowsLayerParams* temp = layer_.slidingwindows_;
    layer_.slidingwindows_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_slidingwindows(::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows) {
  clear_layer();
  if (slidingwindows) {
    set_has_slidingwindows();
    layer_.slidingwindows_ = slidingwindows;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
}

// .CoreML.Specification.TopKLayerParams topK = 1015;
inline bool NeuralNetworkLayer::has_topk() const {
  return layer_case() == kTopK;
}
inline void NeuralNetworkLayer::set_has_topk() {
  _oneof_case_[0] = kTopK;
}
inline void NeuralNetworkLayer::clear_topk() {
  if (has_topk()) {
    delete layer_.topk_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::TopKLayerParams& NeuralNetworkLayer::topk() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.topK)
  return has_topk()
      ? *layer_.topk_
      : ::CoreML::Specification::TopKLayerParams::default_instance();
}
inline ::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::mutable_topk() {
  if (!has_topk()) {
    clear_layer();
    set_has_topk();
    layer_.topk_ = new ::CoreML::Specification::TopKLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.topK)
  return layer_.topk_;
}
inline ::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::release_topk() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.topK)
  if (has_topk()) {
    clear_has_layer();
    ::CoreML::Specification::TopKLayerParams* temp = layer_.topk_;
    layer_.topk_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_topk(::CoreML::Specification::TopKLayerParams* topk) {
  clear_layer();
  if (topk) {
    set_has_topk();
    layer_.topk_ = topk;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.topK)
}

// .CoreML.Specification.ArgMinLayerParams argMin = 1020;
inline bool NeuralNetworkLayer::has_argmin() const {
  return layer_case() == kArgMin;
}
inline void NeuralNetworkLayer::set_has_argmin() {
  _oneof_case_[0] = kArgMin;
}
inline void NeuralNetworkLayer::clear_argmin() {
  if (has_argmin()) {
    delete layer_.argmin_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ArgMinLayerParams& NeuralNetworkLayer::argmin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argMin)
  return has_argmin()
      ? *layer_.argmin_
      : ::CoreML::Specification::ArgMinLayerParams::default_instance();
}
inline ::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::mutable_argmin() {
  if (!has_argmin()) {
    clear_layer();
    set_has_argmin();
    layer_.argmin_ = new ::CoreML::Specification::ArgMinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argMin)
  return layer_.argmin_;
}
inline ::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::release_argmin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argMin)
  if (has_argmin()) {
    clear_has_layer();
    ::CoreML::Specification::ArgMinLayerParams* temp = layer_.argmin_;
    layer_.argmin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_argmin(::CoreML::Specification::ArgMinLayerParams* argmin) {
  clear_layer();
  if (argmin) {
    set_has_argmin();
    layer_.argmin_ = argmin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.argMin)
}

// .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
inline bool NeuralNetworkLayer::has_argmax() const {
  return layer_case() == kArgMax;
}
inline void NeuralNetworkLayer::set_has_argmax() {
  _oneof_case_[0] = kArgMax;
}
inline void NeuralNetworkLayer::clear_argmax() {
  if (has_argmax()) {
    delete layer_.argmax_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ArgMaxLayerParams& NeuralNetworkLayer::argmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argMax)
  return has_argmax()
      ? *layer_.argmax_
      : ::CoreML::Specification::ArgMaxLayerParams::default_instance();
}
inline ::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::mutable_argmax() {
  if (!has_argmax()) {
    clear_layer();
    set_has_argmax();
    layer_.argmax_ = new ::CoreML::Specification::ArgMaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argMax)
  return layer_.argmax_;
}
inline ::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::release_argmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argMax)
  if (has_argmax()) {
    clear_has_layer();
    ::CoreML::Specification::ArgMaxLayerParams* temp = layer_.argmax_;
    layer_.argmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_argmax(::CoreML::Specification::ArgMaxLayerParams* argmax) {
  clear_layer();
  if (argmax) {
    set_has_argmax();
    layer_.argmax_ = argmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.argMax)
}

// .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
inline bool NeuralNetworkLayer::has_embeddingnd() const {
  return layer_case() == kEmbeddingND;
}
inline void NeuralNetworkLayer::set_has_embeddingnd() {
  _oneof_case_[0] = kEmbeddingND;
}
inline void NeuralNetworkLayer::clear_embeddingnd() {
  if (has_embeddingnd()) {
    delete layer_.embeddingnd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::EmbeddingNDLayerParams& NeuralNetworkLayer::embeddingnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return has_embeddingnd()
      ? *layer_.embeddingnd_
      : ::CoreML::Specification::EmbeddingNDLayerParams::default_instance();
}
inline ::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::mutable_embeddingnd() {
  if (!has_embeddingnd()) {
    clear_layer();
    set_has_embeddingnd();
    layer_.embeddingnd_ = new ::CoreML::Specification::EmbeddingNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return layer_.embeddingnd_;
}
inline ::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::release_embeddingnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  if (has_embeddingnd()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingNDLayerParams* temp = layer_.embeddingnd_;
    layer_.embeddingnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_embeddingnd(::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd) {
  clear_layer();
  if (embeddingnd) {
    set_has_embeddingnd();
    layer_.embeddingnd_ = embeddingnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embeddingND)
}

// .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
inline bool NeuralNetworkLayer::has_batchedmatmul() const {
  return layer_case() == kBatchedMatmul;
}
inline void NeuralNetworkLayer::set_has_batchedmatmul() {
  _oneof_case_[0] = kBatchedMatmul;
}
inline void NeuralNetworkLayer::clear_batchedmatmul() {
  if (has_batchedmatmul()) {
    delete layer_.batchedmatmul_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BatchedMatMulLayerParams& NeuralNetworkLayer::batchedmatmul() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return has_batchedmatmul()
      ? *layer_.batchedmatmul_
      : ::CoreML::Specification::BatchedMatMulLayerParams::default_instance();
}
inline ::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::mutable_batchedmatmul() {
  if (!has_batchedmatmul()) {
    clear_layer();
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = new ::CoreML::Specification::BatchedMatMulLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return layer_.batchedmatmul_;
}
inline ::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::release_batchedmatmul() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  if (has_batchedmatmul()) {
    clear_has_layer();
    ::CoreML::Specification::BatchedMatMulLayerParams* temp = layer_.batchedmatmul_;
    layer_.batchedmatmul_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_batchedmatmul(::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul) {
  clear_layer();
  if (batchedmatmul) {
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = batchedmatmul;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
}

// .CoreML.Specification.GetShapeLayerParams getShape = 1065;
inline bool NeuralNetworkLayer::has_getshape() const {
  return layer_case() == kGetShape;
}
inline void NeuralNetworkLayer::set_has_getshape() {
  _oneof_case_[0] = kGetShape;
}
inline void NeuralNetworkLayer::clear_getshape() {
  if (has_getshape()) {
    delete layer_.getshape_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GetShapeLayerParams& NeuralNetworkLayer::getshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.getShape)
  return has_getshape()
      ? *layer_.getshape_
      : ::CoreML::Specification::GetShapeLayerParams::default_instance();
}
inline ::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::mutable_getshape() {
  if (!has_getshape()) {
    clear_layer();
    set_has_getshape();
    layer_.getshape_ = new ::CoreML::Specification::GetShapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.getShape)
  return layer_.getshape_;
}
inline ::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::release_getshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.getShape)
  if (has_getshape()) {
    clear_has_layer();
    ::CoreML::Specification::GetShapeLayerParams* temp = layer_.getshape_;
    layer_.getshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_getshape(::CoreML::Specification::GetShapeLayerParams* getshape) {
  clear_layer();
  if (getshape) {
    set_has_getshape();
    layer_.getshape_ = getshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.getShape)
}

// .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
inline bool NeuralNetworkLayer::has_loadconstantnd() const {
  return layer_case() == kLoadConstantND;
}
inline void NeuralNetworkLayer::set_has_loadconstantnd() {
  _oneof_case_[0] = kLoadConstantND;
}
inline void NeuralNetworkLayer::clear_loadconstantnd() {
  if (has_loadconstantnd()) {
    delete layer_.loadconstantnd_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LoadConstantNDLayerParams& NeuralNetworkLayer::loadconstantnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return has_loadconstantnd()
      ? *layer_.loadconstantnd_
      : ::CoreML::Specification::LoadConstantNDLayerParams::default_instance();
}
inline ::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::mutable_loadconstantnd() {
  if (!has_loadconstantnd()) {
    clear_layer();
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = new ::CoreML::Specification::LoadConstantNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return layer_.loadconstantnd_;
}
inline ::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::release_loadconstantnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  if (has_loadconstantnd()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantNDLayerParams* temp = layer_.loadconstantnd_;
    layer_.loadconstantnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_loadconstantnd(::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd) {
  clear_layer();
  if (loadconstantnd) {
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = loadconstantnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
}

// .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
inline bool NeuralNetworkLayer::has_filllike() const {
  return layer_case() == kFillLike;
}
inline void NeuralNetworkLayer::set_has_filllike() {
  _oneof_case_[0] = kFillLike;
}
inline void NeuralNetworkLayer::clear_filllike() {
  if (has_filllike()) {
    delete layer_.filllike_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FillLikeLayerParams& NeuralNetworkLayer::filllike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return has_filllike()
      ? *layer_.filllike_
      : ::CoreML::Specification::FillLikeLayerParams::default_instance();
}
inline ::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::mutable_filllike() {
  if (!has_filllike()) {
    clear_layer();
    set_has_filllike();
    layer_.filllike_ = new ::CoreML::Specification::FillLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return layer_.filllike_;
}
inline ::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::release_filllike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillLike)
  if (has_filllike()) {
    clear_has_layer();
    ::CoreML::Specification::FillLikeLayerParams* temp = layer_.filllike_;
    layer_.filllike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_filllike(::CoreML::Specification::FillLikeLayerParams* filllike) {
  clear_layer();
  if (filllike) {
    set_has_filllike();
    layer_.filllike_ = filllike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillLike)
}

// .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
inline bool NeuralNetworkLayer::has_fillstatic() const {
  return layer_case() == kFillStatic;
}
inline void NeuralNetworkLayer::set_has_fillstatic() {
  _oneof_case_[0] = kFillStatic;
}
inline void NeuralNetworkLayer::clear_fillstatic() {
  if (has_fillstatic()) {
    delete layer_.fillstatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FillStaticLayerParams& NeuralNetworkLayer::fillstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return has_fillstatic()
      ? *layer_.fillstatic_
      : ::CoreML::Specification::FillStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::mutable_fillstatic() {
  if (!has_fillstatic()) {
    clear_layer();
    set_has_fillstatic();
    layer_.fillstatic_ = new ::CoreML::Specification::FillStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return layer_.fillstatic_;
}
inline ::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::release_fillstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  if (has_fillstatic()) {
    clear_has_layer();
    ::CoreML::Specification::FillStaticLayerParams* temp = layer_.fillstatic_;
    layer_.fillstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_fillstatic(::CoreML::Specification::FillStaticLayerParams* fillstatic) {
  clear_layer();
  if (fillstatic) {
    set_has_fillstatic();
    layer_.fillstatic_ = fillstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillStatic)
}

// .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
inline bool NeuralNetworkLayer::has_filldynamic() const {
  return layer_case() == kFillDynamic;
}
inline void NeuralNetworkLayer::set_has_filldynamic() {
  _oneof_case_[0] = kFillDynamic;
}
inline void NeuralNetworkLayer::clear_filldynamic() {
  if (has_filldynamic()) {
    delete layer_.filldynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FillDynamicLayerParams& NeuralNetworkLayer::filldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return has_filldynamic()
      ? *layer_.filldynamic_
      : ::CoreML::Specification::FillDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::mutable_filldynamic() {
  if (!has_filldynamic()) {
    clear_layer();
    set_has_filldynamic();
    layer_.filldynamic_ = new ::CoreML::Specification::FillDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return layer_.filldynamic_;
}
inline ::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::release_filldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  if (has_filldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::FillDynamicLayerParams* temp = layer_.filldynamic_;
    layer_.filldynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_filldynamic(::CoreML::Specification::FillDynamicLayerParams* filldynamic) {
  clear_layer();
  if (filldynamic) {
    set_has_filldynamic();
    layer_.filldynamic_ = filldynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
}

// .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
inline bool NeuralNetworkLayer::has_broadcasttolike() const {
  return layer_case() == kBroadcastToLike;
}
inline void NeuralNetworkLayer::set_has_broadcasttolike() {
  _oneof_case_[0] = kBroadcastToLike;
}
inline void NeuralNetworkLayer::clear_broadcasttolike() {
  if (has_broadcasttolike()) {
    delete layer_.broadcasttolike_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BroadcastToLikeLayerParams& NeuralNetworkLayer::broadcasttolike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return has_broadcasttolike()
      ? *layer_.broadcasttolike_
      : ::CoreML::Specification::BroadcastToLikeLayerParams::default_instance();
}
inline ::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::mutable_broadcasttolike() {
  if (!has_broadcasttolike()) {
    clear_layer();
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = new ::CoreML::Specification::BroadcastToLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return layer_.broadcasttolike_;
}
inline ::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::release_broadcasttolike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  if (has_broadcasttolike()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToLikeLayerParams* temp = layer_.broadcasttolike_;
    layer_.broadcasttolike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_broadcasttolike(::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike) {
  clear_layer();
  if (broadcasttolike) {
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = broadcasttolike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
}

// .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
inline bool NeuralNetworkLayer::has_broadcasttostatic() const {
  return layer_case() == kBroadcastToStatic;
}
inline void NeuralNetworkLayer::set_has_broadcasttostatic() {
  _oneof_case_[0] = kBroadcastToStatic;
}
inline void NeuralNetworkLayer::clear_broadcasttostatic() {
  if (has_broadcasttostatic()) {
    delete layer_.broadcasttostatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BroadcastToStaticLayerParams& NeuralNetworkLayer::broadcasttostatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return has_broadcasttostatic()
      ? *layer_.broadcasttostatic_
      : ::CoreML::Specification::BroadcastToStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::mutable_broadcasttostatic() {
  if (!has_broadcasttostatic()) {
    clear_layer();
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = new ::CoreML::Specification::BroadcastToStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return layer_.broadcasttostatic_;
}
inline ::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::release_broadcasttostatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  if (has_broadcasttostatic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToStaticLayerParams* temp = layer_.broadcasttostatic_;
    layer_.broadcasttostatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_broadcasttostatic(::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic) {
  clear_layer();
  if (broadcasttostatic) {
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = broadcasttostatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
}

// .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
inline bool NeuralNetworkLayer::has_broadcasttodynamic() const {
  return layer_case() == kBroadcastToDynamic;
}
inline void NeuralNetworkLayer::set_has_broadcasttodynamic() {
  _oneof_case_[0] = kBroadcastToDynamic;
}
inline void NeuralNetworkLayer::clear_broadcasttodynamic() {
  if (has_broadcasttodynamic()) {
    delete layer_.broadcasttodynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BroadcastToDynamicLayerParams& NeuralNetworkLayer::broadcasttodynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return has_broadcasttodynamic()
      ? *layer_.broadcasttodynamic_
      : ::CoreML::Specification::BroadcastToDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::mutable_broadcasttodynamic() {
  if (!has_broadcasttodynamic()) {
    clear_layer();
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = new ::CoreML::Specification::BroadcastToDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return layer_.broadcasttodynamic_;
}
inline ::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::release_broadcasttodynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  if (has_broadcasttodynamic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToDynamicLayerParams* temp = layer_.broadcasttodynamic_;
    layer_.broadcasttodynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_broadcasttodynamic(::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic) {
  clear_layer();
  if (broadcasttodynamic) {
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = broadcasttodynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
}

// .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
inline bool NeuralNetworkLayer::has_squeeze() const {
  return layer_case() == kSqueeze;
}
inline void NeuralNetworkLayer::set_has_squeeze() {
  _oneof_case_[0] = kSqueeze;
}
inline void NeuralNetworkLayer::clear_squeeze() {
  if (has_squeeze()) {
    delete layer_.squeeze_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SqueezeLayerParams& NeuralNetworkLayer::squeeze() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return has_squeeze()
      ? *layer_.squeeze_
      : ::CoreML::Specification::SqueezeLayerParams::default_instance();
}
inline ::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::mutable_squeeze() {
  if (!has_squeeze()) {
    clear_layer();
    set_has_squeeze();
    layer_.squeeze_ = new ::CoreML::Specification::SqueezeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return layer_.squeeze_;
}
inline ::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::release_squeeze() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.squeeze)
  if (has_squeeze()) {
    clear_has_layer();
    ::CoreML::Specification::SqueezeLayerParams* temp = layer_.squeeze_;
    layer_.squeeze_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_squeeze(::CoreML::Specification::SqueezeLayerParams* squeeze) {
  clear_layer();
  if (squeeze) {
    set_has_squeeze();
    layer_.squeeze_ = squeeze;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.squeeze)
}

// .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
inline bool NeuralNetworkLayer::has_expanddims() const {
  return layer_case() == kExpandDims;
}
inline void NeuralNetworkLayer::set_has_expanddims() {
  _oneof_case_[0] = kExpandDims;
}
inline void NeuralNetworkLayer::clear_expanddims() {
  if (has_expanddims()) {
    delete layer_.expanddims_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ExpandDimsLayerParams& NeuralNetworkLayer::expanddims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return has_expanddims()
      ? *layer_.expanddims_
      : ::CoreML::Specification::ExpandDimsLayerParams::default_instance();
}
inline ::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::mutable_expanddims() {
  if (!has_expanddims()) {
    clear_layer();
    set_has_expanddims();
    layer_.expanddims_ = new ::CoreML::Specification::ExpandDimsLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return layer_.expanddims_;
}
inline ::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::release_expanddims() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.expandDims)
  if (has_expanddims()) {
    clear_has_layer();
    ::CoreML::Specification::ExpandDimsLayerParams* temp = layer_.expanddims_;
    layer_.expanddims_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_expanddims(::CoreML::Specification::ExpandDimsLayerParams* expanddims) {
  clear_layer();
  if (expanddims) {
    set_has_expanddims();
    layer_.expanddims_ = expanddims;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.expandDims)
}

// .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
inline bool NeuralNetworkLayer::has_flattento2d() const {
  return layer_case() == kFlattenTo2D;
}
inline void NeuralNetworkLayer::set_has_flattento2d() {
  _oneof_case_[0] = kFlattenTo2D;
}
inline void NeuralNetworkLayer::clear_flattento2d() {
  if (has_flattento2d()) {
    delete layer_.flattento2d_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FlattenTo2DLayerParams& NeuralNetworkLayer::flattento2d() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  return has_flattento2d()
      ? *layer_.flattento2d_
      : ::CoreML::Specification::FlattenTo2DLayerParams::default_instance();
}
inline ::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::mutable_flattento2d() {
  if (!has_flattento2d()) {
    clear_layer();
    set_has_flattento2d();
    layer_.flattento2d_ = new ::CoreML::Specification::FlattenTo2DLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  return layer_.flattento2d_;
}
inline ::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::release_flattento2d() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  if (has_flattento2d()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenTo2DLayerParams* temp = layer_.flattento2d_;
    layer_.flattento2d_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_flattento2d(::CoreML::Specification::FlattenTo2DLayerParams* flattento2d) {
  clear_layer();
  if (flattento2d) {
    set_has_flattento2d();
    layer_.flattento2d_ = flattento2d;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
}

// .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
inline bool NeuralNetworkLayer::has_reshapelike() const {
  return layer_case() == kReshapeLike;
}
inline void NeuralNetworkLayer::set_has_reshapelike() {
  _oneof_case_[0] = kReshapeLike;
}
inline void NeuralNetworkLayer::clear_reshapelike() {
  if (has_reshapelike()) {
    delete layer_.reshapelike_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReshapeLikeLayerParams& NeuralNetworkLayer::reshapelike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  return has_reshapelike()
      ? *layer_.reshapelike_
      : ::CoreML::Specification::ReshapeLikeLayerParams::default_instance();
}
inline ::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::mutable_reshapelike() {
  if (!has_reshapelike()) {
    clear_layer();
    set_has_reshapelike();
    layer_.reshapelike_ = new ::CoreML::Specification::ReshapeLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  return layer_.reshapelike_;
}
inline ::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::release_reshapelike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  if (has_reshapelike()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLikeLayerParams* temp = layer_.reshapelike_;
    layer_.reshapelike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reshapelike(::CoreML::Specification::ReshapeLikeLayerParams* reshapelike) {
  clear_layer();
  if (reshapelike) {
    set_has_reshapelike();
    layer_.reshapelike_ = reshapelike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
}

// .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
inline bool NeuralNetworkLayer::has_reshapestatic() const {
  return layer_case() == kReshapeStatic;
}
inline void NeuralNetworkLayer::set_has_reshapestatic() {
  _oneof_case_[0] = kReshapeStatic;
}
inline void NeuralNetworkLayer::clear_reshapestatic() {
  if (has_reshapestatic()) {
    delete layer_.reshapestatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReshapeStaticLayerParams& NeuralNetworkLayer::reshapestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  return has_reshapestatic()
      ? *layer_.reshapestatic_
      : ::CoreML::Specification::ReshapeStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::mutable_reshapestatic() {
  if (!has_reshapestatic()) {
    clear_layer();
    set_has_reshapestatic();
    layer_.reshapestatic_ = new ::CoreML::Specification::ReshapeStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  return layer_.reshapestatic_;
}
inline ::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::release_reshapestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  if (has_reshapestatic()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeStaticLayerParams* temp = layer_.reshapestatic_;
    layer_.reshapestatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reshapestatic(::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic) {
  clear_layer();
  if (reshapestatic) {
    set_has_reshapestatic();
    layer_.reshapestatic_ = reshapestatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
}

// .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
inline bool NeuralNetworkLayer::has_reshapedynamic() const {
  return layer_case() == kReshapeDynamic;
}
inline void NeuralNetworkLayer::set_has_reshapedynamic() {
  _oneof_case_[0] = kReshapeDynamic;
}
inline void NeuralNetworkLayer::clear_reshapedynamic() {
  if (has_reshapedynamic()) {
    delete layer_.reshapedynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReshapeDynamicLayerParams& NeuralNetworkLayer::reshapedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  return has_reshapedynamic()
      ? *layer_.reshapedynamic_
      : ::CoreML::Specification::ReshapeDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::mutable_reshapedynamic() {
  if (!has_reshapedynamic()) {
    clear_layer();
    set_has_reshapedynamic();
    layer_.reshapedynamic_ = new ::CoreML::Specification::ReshapeDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  return layer_.reshapedynamic_;
}
inline ::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::release_reshapedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  if (has_reshapedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeDynamicLayerParams* temp = layer_.reshapedynamic_;
    layer_.reshapedynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reshapedynamic(::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic) {
  clear_layer();
  if (reshapedynamic) {
    set_has_reshapedynamic();
    layer_.reshapedynamic_ = reshapedynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
}

// .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
inline bool NeuralNetworkLayer::has_rankpreservingreshape() const {
  return layer_case() == kRankPreservingReshape;
}
inline void NeuralNetworkLayer::set_has_rankpreservingreshape() {
  _oneof_case_[0] = kRankPreservingReshape;
}
inline void NeuralNetworkLayer::clear_rankpreservingreshape() {
  if (has_rankpreservingreshape()) {
    delete layer_.rankpreservingreshape_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RankPreservingReshapeLayerParams& NeuralNetworkLayer::rankpreservingreshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return has_rankpreservingreshape()
      ? *layer_.rankpreservingreshape_
      : ::CoreML::Specification::RankPreservingReshapeLayerParams::default_instance();
}
inline ::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::mutable_rankpreservingreshape() {
  if (!has_rankpreservingreshape()) {
    clear_layer();
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = new ::CoreML::Specification::RankPreservingReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return layer_.rankpreservingreshape_;
}
inline ::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::release_rankpreservingreshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  if (has_rankpreservingreshape()) {
    clear_has_layer();
    ::CoreML::Specification::RankPreservingReshapeLayerParams* temp = layer_.rankpreservingreshape_;
    layer_.rankpreservingreshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_rankpreservingreshape(::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape) {
  clear_layer();
  if (rankpreservingreshape) {
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = rankpreservingreshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
}

// .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
inline bool NeuralNetworkLayer::has_constantpad() const {
  return layer_case() == kConstantPad;
}
inline void NeuralNetworkLayer::set_has_constantpad() {
  _oneof_case_[0] = kConstantPad;
}
inline void NeuralNetworkLayer::clear_constantpad() {
  if (has_constantpad()) {
    delete layer_.constantpad_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ConstantPaddingLayerParams& NeuralNetworkLayer::constantpad() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.constantPad)
  return has_constantpad()
      ? *layer_.constantpad_
      : ::CoreML::Specification::ConstantPaddingLayerParams::default_instance();
}
inline ::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::mutable_constantpad() {
  if (!has_constantpad()) {
    clear_layer();
    set_has_constantpad();
    layer_.constantpad_ = new ::CoreML::Specification::ConstantPaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.constantPad)
  return layer_.constantpad_;
}
inline ::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::release_constantpad() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.constantPad)
  if (has_constantpad()) {
    clear_has_layer();
    ::CoreML::Specification::ConstantPaddingLayerParams* temp = layer_.constantpad_;
    layer_.constantpad_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_constantpad(::CoreML::Specification::ConstantPaddingLayerParams* constantpad) {
  clear_layer();
  if (constantpad) {
    set_has_constantpad();
    layer_.constantpad_ = constantpad;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.constantPad)
}

// .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
inline bool NeuralNetworkLayer::has_randomnormallike() const {
  return layer_case() == kRandomNormalLike;
}
inline void NeuralNetworkLayer::set_has_randomnormallike() {
  _oneof_case_[0] = kRandomNormalLike;
}
inline void NeuralNetworkLayer::clear_randomnormallike() {
  if (has_randomnormallike()) {
    delete layer_.randomnormallike_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomNormalLikeLayerParams& NeuralNetworkLayer::randomnormallike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  return has_randomnormallike()
      ? *layer_.randomnormallike_
      : ::CoreML::Specification::RandomNormalLikeLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::mutable_randomnormallike() {
  if (!has_randomnormallike()) {
    clear_layer();
    set_has_randomnormallike();
    layer_.randomnormallike_ = new ::CoreML::Specification::RandomNormalLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  return layer_.randomnormallike_;
}
inline ::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::release_randomnormallike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  if (has_randomnormallike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalLikeLayerParams* temp = layer_.randomnormallike_;
    layer_.randomnormallike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randomnormallike(::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike) {
  clear_layer();
  if (randomnormallike) {
    set_has_randomnormallike();
    layer_.randomnormallike_ = randomnormallike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
}

// .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
inline bool NeuralNetworkLayer::has_randomnormalstatic() const {
  return layer_case() == kRandomNormalStatic;
}
inline void NeuralNetworkLayer::set_has_randomnormalstatic() {
  _oneof_case_[0] = kRandomNormalStatic;
}
inline void NeuralNetworkLayer::clear_randomnormalstatic() {
  if (has_randomnormalstatic()) {
    delete layer_.randomnormalstatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomNormalStaticLayerParams& NeuralNetworkLayer::randomnormalstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  return has_randomnormalstatic()
      ? *layer_.randomnormalstatic_
      : ::CoreML::Specification::RandomNormalStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::mutable_randomnormalstatic() {
  if (!has_randomnormalstatic()) {
    clear_layer();
    set_has_randomnormalstatic();
    layer_.randomnormalstatic_ = new ::CoreML::Specification::RandomNormalStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  return layer_.randomnormalstatic_;
}
inline ::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::release_randomnormalstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  if (has_randomnormalstatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalStaticLayerParams* temp = layer_.randomnormalstatic_;
    layer_.randomnormalstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randomnormalstatic(::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic) {
  clear_layer();
  if (randomnormalstatic) {
    set_has_randomnormalstatic();
    layer_.randomnormalstatic_ = randomnormalstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
}

// .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
inline bool NeuralNetworkLayer::has_randomnormaldynamic() const {
  return layer_case() == kRandomNormalDynamic;
}
inline void NeuralNetworkLayer::set_has_randomnormaldynamic() {
  _oneof_case_[0] = kRandomNormalDynamic;
}
inline void NeuralNetworkLayer::clear_randomnormaldynamic() {
  if (has_randomnormaldynamic()) {
    delete layer_.randomnormaldynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomNormalDynamicLayerParams& NeuralNetworkLayer::randomnormaldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  return has_randomnormaldynamic()
      ? *layer_.randomnormaldynamic_
      : ::CoreML::Specification::RandomNormalDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::mutable_randomnormaldynamic() {
  if (!has_randomnormaldynamic()) {
    clear_layer();
    set_has_randomnormaldynamic();
    layer_.randomnormaldynamic_ = new ::CoreML::Specification::RandomNormalDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  return layer_.randomnormaldynamic_;
}
inline ::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::release_randomnormaldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  if (has_randomnormaldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalDynamicLayerParams* temp = layer_.randomnormaldynamic_;
    layer_.randomnormaldynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randomnormaldynamic(::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic) {
  clear_layer();
  if (randomnormaldynamic) {
    set_has_randomnormaldynamic();
    layer_.randomnormaldynamic_ = randomnormaldynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
}

// .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
inline bool NeuralNetworkLayer::has_randomuniformlike() const {
  return layer_case() == kRandomUniformLike;
}
inline void NeuralNetworkLayer::set_has_randomuniformlike() {
  _oneof_case_[0] = kRandomUniformLike;
}
inline void NeuralNetworkLayer::clear_randomuniformlike() {
  if (has_randomuniformlike()) {
    delete layer_.randomuniformlike_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomUniformLikeLayerParams& NeuralNetworkLayer::randomuniformlike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  return has_randomuniformlike()
      ? *layer_.randomuniformlike_
      : ::CoreML::Specification::RandomUniformLikeLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::mutable_randomuniformlike() {
  if (!has_randomuniformlike()) {
    clear_layer();
    set_has_randomuniformlike();
    layer_.randomuniformlike_ = new ::CoreML::Specification::RandomUniformLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  return layer_.randomuniformlike_;
}
inline ::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::release_randomuniformlike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  if (has_randomuniformlike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformLikeLayerParams* temp = layer_.randomuniformlike_;
    layer_.randomuniformlike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randomuniformlike(::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike) {
  clear_layer();
  if (randomuniformlike) {
    set_has_randomuniformlike();
    layer_.randomuniformlike_ = randomuniformlike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
}

// .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
inline bool NeuralNetworkLayer::has_randomuniformstatic() const {
  return layer_case() == kRandomUniformStatic;
}
inline void NeuralNetworkLayer::set_has_randomuniformstatic() {
  _oneof_case_[0] = kRandomUniformStatic;
}
inline void NeuralNetworkLayer::clear_randomuniformstatic() {
  if (has_randomuniformstatic()) {
    delete layer_.randomuniformstatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomUniformStaticLayerParams& NeuralNetworkLayer::randomuniformstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  return has_randomuniformstatic()
      ? *layer_.randomuniformstatic_
      : ::CoreML::Specification::RandomUniformStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::mutable_randomuniformstatic() {
  if (!has_randomuniformstatic()) {
    clear_layer();
    set_has_randomuniformstatic();
    layer_.randomuniformstatic_ = new ::CoreML::Specification::RandomUniformStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  return layer_.randomuniformstatic_;
}
inline ::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::release_randomuniformstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  if (has_randomuniformstatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformStaticLayerParams* temp = layer_.randomuniformstatic_;
    layer_.randomuniformstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randomuniformstatic(::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic) {
  clear_layer();
  if (randomuniformstatic) {
    set_has_randomuniformstatic();
    layer_.randomuniformstatic_ = randomuniformstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
}

// .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
inline bool NeuralNetworkLayer::has_randomuniformdynamic() const {
  return layer_case() == kRandomUniformDynamic;
}
inline void NeuralNetworkLayer::set_has_randomuniformdynamic() {
  _oneof_case_[0] = kRandomUniformDynamic;
}
inline void NeuralNetworkLayer::clear_randomuniformdynamic() {
  if (has_randomuniformdynamic()) {
    delete layer_.randomuniformdynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomUniformDynamicLayerParams& NeuralNetworkLayer::randomuniformdynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  return has_randomuniformdynamic()
      ? *layer_.randomuniformdynamic_
      : ::CoreML::Specification::RandomUniformDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::mutable_randomuniformdynamic() {
  if (!has_randomuniformdynamic()) {
    clear_layer();
    set_has_randomuniformdynamic();
    layer_.randomuniformdynamic_ = new ::CoreML::Specification::RandomUniformDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  return layer_.randomuniformdynamic_;
}
inline ::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::release_randomuniformdynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  if (has_randomuniformdynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformDynamicLayerParams* temp = layer_.randomuniformdynamic_;
    layer_.randomuniformdynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randomuniformdynamic(::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic) {
  clear_layer();
  if (randomuniformdynamic) {
    set_has_randomuniformdynamic();
    layer_.randomuniformdynamic_ = randomuniformdynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
}

// .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
inline bool NeuralNetworkLayer::has_randombernoullilike() const {
  return layer_case() == kRandomBernoulliLike;
}
inline void NeuralNetworkLayer::set_has_randombernoullilike() {
  _oneof_case_[0] = kRandomBernoulliLike;
}
inline void NeuralNetworkLayer::clear_randombernoullilike() {
  if (has_randombernoullilike()) {
    delete layer_.randombernoullilike_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomBernoulliLikeLayerParams& NeuralNetworkLayer::randombernoullilike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  return has_randombernoullilike()
      ? *layer_.randombernoullilike_
      : ::CoreML::Specification::RandomBernoulliLikeLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::mutable_randombernoullilike() {
  if (!has_randombernoullilike()) {
    clear_layer();
    set_has_randombernoullilike();
    layer_.randombernoullilike_ = new ::CoreML::Specification::RandomBernoulliLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  return layer_.randombernoullilike_;
}
inline ::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::release_randombernoullilike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  if (has_randombernoullilike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliLikeLayerParams* temp = layer_.randombernoullilike_;
    layer_.randombernoullilike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randombernoullilike(::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike) {
  clear_layer();
  if (randombernoullilike) {
    set_has_randombernoullilike();
    layer_.randombernoullilike_ = randombernoullilike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
}

// .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
inline bool NeuralNetworkLayer::has_randombernoullistatic() const {
  return layer_case() == kRandomBernoulliStatic;
}
inline void NeuralNetworkLayer::set_has_randombernoullistatic() {
  _oneof_case_[0] = kRandomBernoulliStatic;
}
inline void NeuralNetworkLayer::clear_randombernoullistatic() {
  if (has_randombernoullistatic()) {
    delete layer_.randombernoullistatic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomBernoulliStaticLayerParams& NeuralNetworkLayer::randombernoullistatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  return has_randombernoullistatic()
      ? *layer_.randombernoullistatic_
      : ::CoreML::Specification::RandomBernoulliStaticLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::mutable_randombernoullistatic() {
  if (!has_randombernoullistatic()) {
    clear_layer();
    set_has_randombernoullistatic();
    layer_.randombernoullistatic_ = new ::CoreML::Specification::RandomBernoulliStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  return layer_.randombernoullistatic_;
}
inline ::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::release_randombernoullistatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  if (has_randombernoullistatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliStaticLayerParams* temp = layer_.randombernoullistatic_;
    layer_.randombernoullistatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randombernoullistatic(::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic) {
  clear_layer();
  if (randombernoullistatic) {
    set_has_randombernoullistatic();
    layer_.randombernoullistatic_ = randombernoullistatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
}

// .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
inline bool NeuralNetworkLayer::has_randombernoullidynamic() const {
  return layer_case() == kRandomBernoulliDynamic;
}
inline void NeuralNetworkLayer::set_has_randombernoullidynamic() {
  _oneof_case_[0] = kRandomBernoulliDynamic;
}
inline void NeuralNetworkLayer::clear_randombernoullidynamic() {
  if (has_randombernoullidynamic()) {
    delete layer_.randombernoullidynamic_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& NeuralNetworkLayer::randombernoullidynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  return has_randombernoullidynamic()
      ? *layer_.randombernoullidynamic_
      : ::CoreML::Specification::RandomBernoulliDynamicLayerParams::default_instance();
}
inline ::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::mutable_randombernoullidynamic() {
  if (!has_randombernoullidynamic()) {
    clear_layer();
    set_has_randombernoullidynamic();
    layer_.randombernoullidynamic_ = new ::CoreML::Specification::RandomBernoulliDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  return layer_.randombernoullidynamic_;
}
inline ::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::release_randombernoullidynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  if (has_randombernoullidynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliDynamicLayerParams* temp = layer_.randombernoullidynamic_;
    layer_.randombernoullidynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_randombernoullidynamic(::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic) {
  clear_layer();
  if (randombernoullidynamic) {
    set_has_randombernoullidynamic();
    layer_.randombernoullidynamic_ = randombernoullidynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
}

// .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
inline bool NeuralNetworkLayer::has_categoricaldistribution() const {
  return layer_case() == kCategoricalDistribution;
}
inline void NeuralNetworkLayer::set_has_categoricaldistribution() {
  _oneof_case_[0] = kCategoricalDistribution;
}
inline void NeuralNetworkLayer::clear_categoricaldistribution() {
  if (has_categoricaldistribution()) {
    delete layer_.categoricaldistribution_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CategoricalDistributionLayerParams& NeuralNetworkLayer::categoricaldistribution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  return has_categoricaldistribution()
      ? *layer_.categoricaldistribution_
      : ::CoreML::Specification::CategoricalDistributionLayerParams::default_instance();
}
inline ::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::mutable_categoricaldistribution() {
  if (!has_categoricaldistribution()) {
    clear_layer();
    set_has_categoricaldistribution();
    layer_.categoricaldistribution_ = new ::CoreML::Specification::CategoricalDistributionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  return layer_.categoricaldistribution_;
}
inline ::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::release_categoricaldistribution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  if (has_categoricaldistribution()) {
    clear_has_layer();
    ::CoreML::Specification::CategoricalDistributionLayerParams* temp = layer_.categoricaldistribution_;
    layer_.categoricaldistribution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_categoricaldistribution(::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution) {
  clear_layer();
  if (categoricaldistribution) {
    set_has_categoricaldistribution();
    layer_.categoricaldistribution_ = categoricaldistribution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
}

// .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
inline bool NeuralNetworkLayer::has_reducel1() const {
  return layer_case() == kReduceL1;
}
inline void NeuralNetworkLayer::set_has_reducel1() {
  _oneof_case_[0] = kReduceL1;
}
inline void NeuralNetworkLayer::clear_reducel1() {
  if (has_reducel1()) {
    delete layer_.reducel1_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceL1LayerParams& NeuralNetworkLayer::reducel1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  return has_reducel1()
      ? *layer_.reducel1_
      : ::CoreML::Specification::ReduceL1LayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::mutable_reducel1() {
  if (!has_reducel1()) {
    clear_layer();
    set_has_reducel1();
    layer_.reducel1_ = new ::CoreML::Specification::ReduceL1LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  return layer_.reducel1_;
}
inline ::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::release_reducel1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  if (has_reducel1()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceL1LayerParams* temp = layer_.reducel1_;
    layer_.reducel1_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducel1(::CoreML::Specification::ReduceL1LayerParams* reducel1) {
  clear_layer();
  if (reducel1) {
    set_has_reducel1();
    layer_.reducel1_ = reducel1;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceL1)
}

// .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
inline bool NeuralNetworkLayer::has_reducel2() const {
  return layer_case() == kReduceL2;
}
inline void NeuralNetworkLayer::set_has_reducel2() {
  _oneof_case_[0] = kReduceL2;
}
inline void NeuralNetworkLayer::clear_reducel2() {
  if (has_reducel2()) {
    delete layer_.reducel2_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceL2LayerParams& NeuralNetworkLayer::reducel2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  return has_reducel2()
      ? *layer_.reducel2_
      : ::CoreML::Specification::ReduceL2LayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::mutable_reducel2() {
  if (!has_reducel2()) {
    clear_layer();
    set_has_reducel2();
    layer_.reducel2_ = new ::CoreML::Specification::ReduceL2LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  return layer_.reducel2_;
}
inline ::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::release_reducel2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  if (has_reducel2()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceL2LayerParams* temp = layer_.reducel2_;
    layer_.reducel2_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducel2(::CoreML::Specification::ReduceL2LayerParams* reducel2) {
  clear_layer();
  if (reducel2) {
    set_has_reducel2();
    layer_.reducel2_ = reducel2;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceL2)
}

// .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
inline bool NeuralNetworkLayer::has_reducemax() const {
  return layer_case() == kReduceMax;
}
inline void NeuralNetworkLayer::set_has_reducemax() {
  _oneof_case_[0] = kReduceMax;
}
inline void NeuralNetworkLayer::clear_reducemax() {
  if (has_reducemax()) {
    delete layer_.reducemax_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceMaxLayerParams& NeuralNetworkLayer::reducemax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  return has_reducemax()
      ? *layer_.reducemax_
      : ::CoreML::Specification::ReduceMaxLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::mutable_reducemax() {
  if (!has_reducemax()) {
    clear_layer();
    set_has_reducemax();
    layer_.reducemax_ = new ::CoreML::Specification::ReduceMaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  return layer_.reducemax_;
}
inline ::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::release_reducemax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  if (has_reducemax()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMaxLayerParams* temp = layer_.reducemax_;
    layer_.reducemax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducemax(::CoreML::Specification::ReduceMaxLayerParams* reducemax) {
  clear_layer();
  if (reducemax) {
    set_has_reducemax();
    layer_.reducemax_ = reducemax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMax)
}

// .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
inline bool NeuralNetworkLayer::has_reducemin() const {
  return layer_case() == kReduceMin;
}
inline void NeuralNetworkLayer::set_has_reducemin() {
  _oneof_case_[0] = kReduceMin;
}
inline void NeuralNetworkLayer::clear_reducemin() {
  if (has_reducemin()) {
    delete layer_.reducemin_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceMinLayerParams& NeuralNetworkLayer::reducemin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  return has_reducemin()
      ? *layer_.reducemin_
      : ::CoreML::Specification::ReduceMinLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::mutable_reducemin() {
  if (!has_reducemin()) {
    clear_layer();
    set_has_reducemin();
    layer_.reducemin_ = new ::CoreML::Specification::ReduceMinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  return layer_.reducemin_;
}
inline ::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::release_reducemin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  if (has_reducemin()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMinLayerParams* temp = layer_.reducemin_;
    layer_.reducemin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducemin(::CoreML::Specification::ReduceMinLayerParams* reducemin) {
  clear_layer();
  if (reducemin) {
    set_has_reducemin();
    layer_.reducemin_ = reducemin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMin)
}

// .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
inline bool NeuralNetworkLayer::has_reducesum() const {
  return layer_case() == kReduceSum;
}
inline void NeuralNetworkLayer::set_has_reducesum() {
  _oneof_case_[0] = kReduceSum;
}
inline void NeuralNetworkLayer::clear_reducesum() {
  if (has_reducesum()) {
    delete layer_.reducesum_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceSumLayerParams& NeuralNetworkLayer::reducesum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  return has_reducesum()
      ? *layer_.reducesum_
      : ::CoreML::Specification::ReduceSumLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::mutable_reducesum() {
  if (!has_reducesum()) {
    clear_layer();
    set_has_reducesum();
    layer_.reducesum_ = new ::CoreML::Specification::ReduceSumLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  return layer_.reducesum_;
}
inline ::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::release_reducesum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  if (has_reducesum()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceSumLayerParams* temp = layer_.reducesum_;
    layer_.reducesum_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducesum(::CoreML::Specification::ReduceSumLayerParams* reducesum) {
  clear_layer();
  if (reducesum) {
    set_has_reducesum();
    layer_.reducesum_ = reducesum;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceSum)
}

// .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
inline bool NeuralNetworkLayer::has_reduceprod() const {
  return layer_case() == kReduceProd;
}
inline void NeuralNetworkLayer::set_has_reduceprod() {
  _oneof_case_[0] = kReduceProd;
}
inline void NeuralNetworkLayer::clear_reduceprod() {
  if (has_reduceprod()) {
    delete layer_.reduceprod_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceProdLayerParams& NeuralNetworkLayer::reduceprod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  return has_reduceprod()
      ? *layer_.reduceprod_
      : ::CoreML::Specification::ReduceProdLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::mutable_reduceprod() {
  if (!has_reduceprod()) {
    clear_layer();
    set_has_reduceprod();
    layer_.reduceprod_ = new ::CoreML::Specification::ReduceProdLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  return layer_.reduceprod_;
}
inline ::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::release_reduceprod() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  if (has_reduceprod()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceProdLayerParams* temp = layer_.reduceprod_;
    layer_.reduceprod_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reduceprod(::CoreML::Specification::ReduceProdLayerParams* reduceprod) {
  clear_layer();
  if (reduceprod) {
    set_has_reduceprod();
    layer_.reduceprod_ = reduceprod;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceProd)
}

// .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
inline bool NeuralNetworkLayer::has_reducemean() const {
  return layer_case() == kReduceMean;
}
inline void NeuralNetworkLayer::set_has_reducemean() {
  _oneof_case_[0] = kReduceMean;
}
inline void NeuralNetworkLayer::clear_reducemean() {
  if (has_reducemean()) {
    delete layer_.reducemean_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceMeanLayerParams& NeuralNetworkLayer::reducemean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  return has_reducemean()
      ? *layer_.reducemean_
      : ::CoreML::Specification::ReduceMeanLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::mutable_reducemean() {
  if (!has_reducemean()) {
    clear_layer();
    set_has_reducemean();
    layer_.reducemean_ = new ::CoreML::Specification::ReduceMeanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  return layer_.reducemean_;
}
inline ::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::release_reducemean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  if (has_reducemean()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMeanLayerParams* temp = layer_.reducemean_;
    layer_.reducemean_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducemean(::CoreML::Specification::ReduceMeanLayerParams* reducemean) {
  clear_layer();
  if (reducemean) {
    set_has_reducemean();
    layer_.reducemean_ = reducemean;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMean)
}

// .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
inline bool NeuralNetworkLayer::has_reducelogsum() const {
  return layer_case() == kReduceLogSum;
}
inline void NeuralNetworkLayer::set_has_reducelogsum() {
  _oneof_case_[0] = kReduceLogSum;
}
inline void NeuralNetworkLayer::clear_reducelogsum() {
  if (has_reducelogsum()) {
    delete layer_.reducelogsum_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceLogSumLayerParams& NeuralNetworkLayer::reducelogsum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  return has_reducelogsum()
      ? *layer_.reducelogsum_
      : ::CoreML::Specification::ReduceLogSumLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::mutable_reducelogsum() {
  if (!has_reducelogsum()) {
    clear_layer();
    set_has_reducelogsum();
    layer_.reducelogsum_ = new ::CoreML::Specification::ReduceLogSumLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  return layer_.reducelogsum_;
}
inline ::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::release_reducelogsum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  if (has_reducelogsum()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLogSumLayerParams* temp = layer_.reducelogsum_;
    layer_.reducelogsum_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducelogsum(::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum) {
  clear_layer();
  if (reducelogsum) {
    set_has_reducelogsum();
    layer_.reducelogsum_ = reducelogsum;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
}

// .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
inline bool NeuralNetworkLayer::has_reducesumsquare() const {
  return layer_case() == kReduceSumSquare;
}
inline void NeuralNetworkLayer::set_has_reducesumsquare() {
  _oneof_case_[0] = kReduceSumSquare;
}
inline void NeuralNetworkLayer::clear_reducesumsquare() {
  if (has_reducesumsquare()) {
    delete layer_.reducesumsquare_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceSumSquareLayerParams& NeuralNetworkLayer::reducesumsquare() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  return has_reducesumsquare()
      ? *layer_.reducesumsquare_
      : ::CoreML::Specification::ReduceSumSquareLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::mutable_reducesumsquare() {
  if (!has_reducesumsquare()) {
    clear_layer();
    set_has_reducesumsquare();
    layer_.reducesumsquare_ = new ::CoreML::Specification::ReduceSumSquareLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  return layer_.reducesumsquare_;
}
inline ::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::release_reducesumsquare() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  if (has_reducesumsquare()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceSumSquareLayerParams* temp = layer_.reducesumsquare_;
    layer_.reducesumsquare_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducesumsquare(::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare) {
  clear_layer();
  if (reducesumsquare) {
    set_has_reducesumsquare();
    layer_.reducesumsquare_ = reducesumsquare;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
}

// .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
inline bool NeuralNetworkLayer::has_reducelogsumexp() const {
  return layer_case() == kReduceLogSumExp;
}
inline void NeuralNetworkLayer::set_has_reducelogsumexp() {
  _oneof_case_[0] = kReduceLogSumExp;
}
inline void NeuralNetworkLayer::clear_reducelogsumexp() {
  if (has_reducelogsumexp()) {
    delete layer_.reducelogsumexp_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceLogSumExpLayerParams& NeuralNetworkLayer::reducelogsumexp() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  return has_reducelogsumexp()
      ? *layer_.reducelogsumexp_
      : ::CoreML::Specification::ReduceLogSumExpLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::mutable_reducelogsumexp() {
  if (!has_reducelogsumexp()) {
    clear_layer();
    set_has_reducelogsumexp();
    layer_.reducelogsumexp_ = new ::CoreML::Specification::ReduceLogSumExpLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  return layer_.reducelogsumexp_;
}
inline ::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::release_reducelogsumexp() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  if (has_reducelogsumexp()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLogSumExpLayerParams* temp = layer_.reducelogsumexp_;
    layer_.reducelogsumexp_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reducelogsumexp(::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp) {
  clear_layer();
  if (reducelogsumexp) {
    set_has_reducelogsumexp();
    layer_.reducelogsumexp_ = reducelogsumexp;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
}

// .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
inline bool NeuralNetworkLayer::has_wherenonzero() const {
  return layer_case() == kWhereNonZero;
}
inline void NeuralNetworkLayer::set_has_wherenonzero() {
  _oneof_case_[0] = kWhereNonZero;
}
inline void NeuralNetworkLayer::clear_wherenonzero() {
  if (has_wherenonzero()) {
    delete layer_.wherenonzero_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::WhereNonZeroLayerParams& NeuralNetworkLayer::wherenonzero() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  return has_wherenonzero()
      ? *layer_.wherenonzero_
      : ::CoreML::Specification::WhereNonZeroLayerParams::default_instance();
}
inline ::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::mutable_wherenonzero() {
  if (!has_wherenonzero()) {
    clear_layer();
    set_has_wherenonzero();
    layer_.wherenonzero_ = new ::CoreML::Specification::WhereNonZeroLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  return layer_.wherenonzero_;
}
inline ::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::release_wherenonzero() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  if (has_wherenonzero()) {
    clear_has_layer();
    ::CoreML::Specification::WhereNonZeroLayerParams* temp = layer_.wherenonzero_;
    layer_.wherenonzero_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_wherenonzero(::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero) {
  clear_layer();
  if (wherenonzero) {
    set_has_wherenonzero();
    layer_.wherenonzero_ = wherenonzero;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
}

// .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
inline bool NeuralNetworkLayer::has_matrixbandpart() const {
  return layer_case() == kMatrixBandPart;
}
inline void NeuralNetworkLayer::set_has_matrixbandpart() {
  _oneof_case_[0] = kMatrixBandPart;
}
inline void NeuralNetworkLayer::clear_matrixbandpart() {
  if (has_matrixbandpart()) {
    delete layer_.matrixbandpart_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MatrixBandPartLayerParams& NeuralNetworkLayer::matrixbandpart() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  return has_matrixbandpart()
      ? *layer_.matrixbandpart_
      : ::CoreML::Specification::MatrixBandPartLayerParams::default_instance();
}
inline ::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::mutable_matrixbandpart() {
  if (!has_matrixbandpart()) {
    clear_layer();
    set_has_matrixbandpart();
    layer_.matrixbandpart_ = new ::CoreML::Specification::MatrixBandPartLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  return layer_.matrixbandpart_;
}
inline ::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::release_matrixbandpart() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  if (has_matrixbandpart()) {
    clear_has_layer();
    ::CoreML::Specification::MatrixBandPartLayerParams* temp = layer_.matrixbandpart_;
    layer_.matrixbandpart_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_matrixbandpart(::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart) {
  clear_layer();
  if (matrixbandpart) {
    set_has_matrixbandpart();
    layer_.matrixbandpart_ = matrixbandpart;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
}

// .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
inline bool NeuralNetworkLayer::has_lowertriangular() const {
  return layer_case() == kLowerTriangular;
}
inline void NeuralNetworkLayer::set_has_lowertriangular() {
  _oneof_case_[0] = kLowerTriangular;
}
inline void NeuralNetworkLayer::clear_lowertriangular() {
  if (has_lowertriangular()) {
    delete layer_.lowertriangular_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LowerTriangularLayerParams& NeuralNetworkLayer::lowertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return has_lowertriangular()
      ? *layer_.lowertriangular_
      : ::CoreML::Specification::LowerTriangularLayerParams::default_instance();
}
inline ::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::mutable_lowertriangular() {
  if (!has_lowertriangular()) {
    clear_layer();
    set_has_lowertriangular();
    layer_.lowertriangular_ = new ::CoreML::Specification::LowerTriangularLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return layer_.lowertriangular_;
}
inline ::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::release_lowertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  if (has_lowertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::LowerTriangularLayerParams* temp = layer_.lowertriangular_;
    layer_.lowertriangular_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_lowertriangular(::CoreML::Specification::LowerTriangularLayerParams* lowertriangular) {
  clear_layer();
  if (lowertriangular) {
    set_has_lowertriangular();
    layer_.lowertriangular_ = lowertriangular;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
}

// .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
inline bool NeuralNetworkLayer::has_uppertriangular() const {
  return layer_case() == kUpperTriangular;
}
inline void NeuralNetworkLayer::set_has_uppertriangular() {
  _oneof_case_[0] = kUpperTriangular;
}
inline void NeuralNetworkLayer::clear_uppertriangular() {
  if (has_uppertriangular()) {
    delete layer_.uppertriangular_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UpperTriangularLayerParams& NeuralNetworkLayer::uppertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return has_uppertriangular()
      ? *layer_.uppertriangular_
      : ::CoreML::Specification::UpperTriangularLayerParams::default_instance();
}
inline ::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::mutable_uppertriangular() {
  if (!has_uppertriangular()) {
    clear_layer();
    set_has_uppertriangular();
    layer_.uppertriangular_ = new ::CoreML::Specification::UpperTriangularLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return layer_.uppertriangular_;
}
inline ::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::release_uppertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  if (has_uppertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::UpperTriangularLayerParams* temp = layer_.uppertriangular_;
    layer_.uppertriangular_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_uppertriangular(::CoreML::Specification::UpperTriangularLayerParams* uppertriangular) {
  clear_layer();
  if (uppertriangular) {
    set_has_uppertriangular();
    layer_.uppertriangular_ = uppertriangular;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
}

// .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
inline bool NeuralNetworkLayer::has_wherebroadcastable() const {
  return layer_case() == kWhereBroadcastable;
}
inline void NeuralNetworkLayer::set_has_wherebroadcastable() {
  _oneof_case_[0] = kWhereBroadcastable;
}
inline void NeuralNetworkLayer::clear_wherebroadcastable() {
  if (has_wherebroadcastable()) {
    delete layer_.wherebroadcastable_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::WhereBroadcastableLayerParams& NeuralNetworkLayer::wherebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  return has_wherebroadcastable()
      ? *layer_.wherebroadcastable_
      : ::CoreML::Specification::WhereBroadcastableLayerParams::default_instance();
}
inline ::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::mutable_wherebroadcastable() {
  if (!has_wherebroadcastable()) {
    clear_layer();
    set_has_wherebroadcastable();
    layer_.wherebroadcastable_ = new ::CoreML::Specification::WhereBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  return layer_.wherebroadcastable_;
}
inline ::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::release_wherebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  if (has_wherebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::WhereBroadcastableLayerParams* temp = layer_.wherebroadcastable_;
    layer_.wherebroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_wherebroadcastable(::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable) {
  clear_layer();
  if (wherebroadcastable) {
    set_has_wherebroadcastable();
    layer_.wherebroadcastable_ = wherebroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
}

// .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
inline bool NeuralNetworkLayer::has_layernormalization() const {
  return layer_case() == kLayerNormalization;
}
inline void NeuralNetworkLayer::set_has_layernormalization() {
  _oneof_case_[0] = kLayerNormalization;
}
inline void NeuralNetworkLayer::clear_layernormalization() {
  if (has_layernormalization()) {
    delete layer_.layernormalization_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LayerNormalizationLayerParams& NeuralNetworkLayer::layernormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  return has_layernormalization()
      ? *layer_.layernormalization_
      : ::CoreML::Specification::LayerNormalizationLayerParams::default_instance();
}
inline ::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::mutable_layernormalization() {
  if (!has_layernormalization()) {
    clear_layer();
    set_has_layernormalization();
    layer_.layernormalization_ = new ::CoreML::Specification::LayerNormalizationLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  return layer_.layernormalization_;
}
inline ::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::release_layernormalization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  if (has_layernormalization()) {
    clear_has_layer();
    ::CoreML::Specification::LayerNormalizationLayerParams* temp = layer_.layernormalization_;
    layer_.layernormalization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_layernormalization(::CoreML::Specification::LayerNormalizationLayerParams* layernormalization) {
  clear_layer();
  if (layernormalization) {
    set_has_layernormalization();
    layer_.layernormalization_ = layernormalization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
}

// .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
inline bool NeuralNetworkLayer::has_nonmaximumsuppression() const {
  return layer_case() == kNonMaximumSuppression;
}
inline void NeuralNetworkLayer::set_has_nonmaximumsuppression() {
  _oneof_case_[0] = kNonMaximumSuppression;
}
inline void NeuralNetworkLayer::clear_nonmaximumsuppression() {
  if (has_nonmaximumsuppression()) {
    delete layer_.nonmaximumsuppression_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::NonMaximumSuppressionLayerParams& NeuralNetworkLayer::nonmaximumsuppression() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  return has_nonmaximumsuppression()
      ? *layer_.nonmaximumsuppression_
      : ::CoreML::Specification::NonMaximumSuppressionLayerParams::default_instance();
}
inline ::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::mutable_nonmaximumsuppression() {
  if (!has_nonmaximumsuppression()) {
    clear_layer();
    set_has_nonmaximumsuppression();
    layer_.nonmaximumsuppression_ = new ::CoreML::Specification::NonMaximumSuppressionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  return layer_.nonmaximumsuppression_;
}
inline ::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::release_nonmaximumsuppression() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  if (has_nonmaximumsuppression()) {
    clear_has_layer();
    ::CoreML::Specification::NonMaximumSuppressionLayerParams* temp = layer_.nonmaximumsuppression_;
    layer_.nonmaximumsuppression_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_nonmaximumsuppression(::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression) {
  clear_layer();
  if (nonmaximumsuppression) {
    set_has_nonmaximumsuppression();
    layer_.nonmaximumsuppression_ = nonmaximumsuppression;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
}

inline bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
inline void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
inline NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// BranchLayerParams

// .CoreML.Specification.NeuralNetwork ifBranch = 1;
inline bool BranchLayerParams::has_ifbranch() const {
  return this != internal_default_instance() && ifbranch_ != NULL;
}
inline void BranchLayerParams::clear_ifbranch() {
  if (GetArenaNoVirtual() == NULL && ifbranch_ != NULL) delete ifbranch_;
  ifbranch_ = NULL;
}
inline const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::ifbranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.ifBranch)
  return ifbranch_ != NULL ? *ifbranch_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_ifbranch() {
  
  if (ifbranch_ == NULL) {
    ifbranch_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.ifBranch)
  return ifbranch_;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_ifbranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.ifBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = ifbranch_;
  ifbranch_ = NULL;
  return temp;
}
inline void BranchLayerParams::set_allocated_ifbranch(::CoreML::Specification::NeuralNetwork* ifbranch) {
  delete ifbranch_;
  ifbranch_ = ifbranch;
  if (ifbranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.ifBranch)
}

// .CoreML.Specification.NeuralNetwork elseBranch = 2;
inline bool BranchLayerParams::has_elsebranch() const {
  return this != internal_default_instance() && elsebranch_ != NULL;
}
inline void BranchLayerParams::clear_elsebranch() {
  if (GetArenaNoVirtual() == NULL && elsebranch_ != NULL) delete elsebranch_;
  elsebranch_ = NULL;
}
inline const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::elsebranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.elseBranch)
  return elsebranch_ != NULL ? *elsebranch_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_elsebranch() {
  
  if (elsebranch_ == NULL) {
    elsebranch_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.elseBranch)
  return elsebranch_;
}
inline ::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_elsebranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.elseBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = elsebranch_;
  elsebranch_ = NULL;
  return temp;
}
inline void BranchLayerParams::set_allocated_elsebranch(::CoreML::Specification::NeuralNetwork* elsebranch) {
  delete elsebranch_;
  elsebranch_ = elsebranch;
  if (elsebranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.elseBranch)
}

// -------------------------------------------------------------------

// LoopLayerParams

// uint64 maxLoopIterations = 1;
inline void LoopLayerParams::clear_maxloopiterations() {
  maxloopiterations_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 LoopLayerParams::maxloopiterations() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.maxLoopIterations)
  return maxloopiterations_;
}
inline void LoopLayerParams::set_maxloopiterations(::google::protobuf::uint64 value) {
  
  maxloopiterations_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.maxLoopIterations)
}

// string conditionVar = 2;
inline void LoopLayerParams::clear_conditionvar() {
  conditionvar_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& LoopLayerParams::conditionvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.GetNoArena();
}
inline void LoopLayerParams::set_conditionvar(const ::std::string& value) {
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.conditionVar)
}
#if LANG_CXX11
inline void LoopLayerParams::set_conditionvar(::std::string&& value) {
  
  conditionvar_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.LoopLayerParams.conditionVar)
}
#endif
inline void LoopLayerParams::set_conditionvar(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.LoopLayerParams.conditionVar)
}
inline void LoopLayerParams::set_conditionvar(const char* value, size_t size) {
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.LoopLayerParams.conditionVar)
}
inline ::std::string* LoopLayerParams::mutable_conditionvar() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* LoopLayerParams::release_conditionvar() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionVar)
  
  return conditionvar_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void LoopLayerParams::set_allocated_conditionvar(::std::string* conditionvar) {
  if (conditionvar != NULL) {
    
  } else {
    
  }
  conditionvar_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), conditionvar);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionVar)
}

// .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
inline bool LoopLayerParams::has_conditionnetwork() const {
  return this != internal_default_instance() && conditionnetwork_ != NULL;
}
inline void LoopLayerParams::clear_conditionnetwork() {
  if (GetArenaNoVirtual() == NULL && conditionnetwork_ != NULL) delete conditionnetwork_;
  conditionnetwork_ = NULL;
}
inline const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::conditionnetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return conditionnetwork_ != NULL ? *conditionnetwork_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_conditionnetwork() {
  
  if (conditionnetwork_ == NULL) {
    conditionnetwork_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return conditionnetwork_;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_conditionnetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = conditionnetwork_;
  conditionnetwork_ = NULL;
  return temp;
}
inline void LoopLayerParams::set_allocated_conditionnetwork(::CoreML::Specification::NeuralNetwork* conditionnetwork) {
  delete conditionnetwork_;
  conditionnetwork_ = conditionnetwork;
  if (conditionnetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionNetwork)
}

// .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
inline bool LoopLayerParams::has_bodynetwork() const {
  return this != internal_default_instance() && bodynetwork_ != NULL;
}
inline void LoopLayerParams::clear_bodynetwork() {
  if (GetArenaNoVirtual() == NULL && bodynetwork_ != NULL) delete bodynetwork_;
  bodynetwork_ = NULL;
}
inline const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::bodynetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return bodynetwork_ != NULL ? *bodynetwork_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_bodynetwork() {
  
  if (bodynetwork_ == NULL) {
    bodynetwork_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return bodynetwork_;
}
inline ::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_bodynetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.bodyNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = bodynetwork_;
  bodynetwork_ = NULL;
  return temp;
}
inline void LoopLayerParams::set_allocated_bodynetwork(::CoreML::Specification::NeuralNetwork* bodynetwork) {
  delete bodynetwork_;
  bodynetwork_ = bodynetwork;
  if (bodynetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.bodyNetwork)
}

// -------------------------------------------------------------------

// LoopBreakLayerParams

// -------------------------------------------------------------------

// LoopContinueLayerParams

// -------------------------------------------------------------------

// CopyLayerParams

// -------------------------------------------------------------------

// GreaterThanLayerParams

// float alpha = 2;
inline void GreaterThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float GreaterThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterThanLayerParams.alpha)
  return alpha_;
}
inline void GreaterThanLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterThanLayerParams.alpha)
}

// -------------------------------------------------------------------

// GreaterEqualLayerParams

// float alpha = 2;
inline void GreaterEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float GreaterEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterEqualLayerParams.alpha)
  return alpha_;
}
inline void GreaterEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterEqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// LessThanLayerParams

// float alpha = 2;
inline void LessThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LessThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessThanLayerParams.alpha)
  return alpha_;
}
inline void LessThanLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessThanLayerParams.alpha)
}

// -------------------------------------------------------------------

// LessEqualLayerParams

// float alpha = 2;
inline void LessEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LessEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessEqualLayerParams.alpha)
  return alpha_;
}
inline void LessEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessEqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// EqualLayerParams

// float alpha = 1;
inline void EqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float EqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EqualLayerParams.alpha)
  return alpha_;
}
inline void EqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// NotEqualLayerParams

// float alpha = 1;
inline void NotEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float NotEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NotEqualLayerParams.alpha)
  return alpha_;
}
inline void NotEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NotEqualLayerParams.alpha)
}

// -------------------------------------------------------------------

// LogicalAndLayerParams

// -------------------------------------------------------------------

// LogicalOrLayerParams

// -------------------------------------------------------------------

// LogicalXorLayerParams

// -------------------------------------------------------------------

// LogicalNotLayerParams

// -------------------------------------------------------------------

// BorderAmounts_EdgeSizes

// uint64 startEdgeSize = 1;
inline void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return startedgesize_;
}
inline void BorderAmounts_EdgeSizes::set_startedgesize(::google::protobuf::uint64 value) {
  
  startedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// uint64 endEdgeSize = 2;
inline void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return endedgesize_;
}
inline void BorderAmounts_EdgeSizes::set_endedgesize(::google::protobuf::uint64 value) {
  
  endedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

// -------------------------------------------------------------------

// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
inline int BorderAmounts::borderamounts_size() const {
  return borderamounts_.size();
}
inline void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
inline const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Get(index);
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

// -------------------------------------------------------------------

// ValidPadding

// .CoreML.Specification.BorderAmounts paddingAmounts = 1;
inline bool ValidPadding::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
inline void ValidPadding::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
inline const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_;
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
inline void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

// -------------------------------------------------------------------

// SamePadding

// .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
inline void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
inline ::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
inline void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  
  asymmetrymode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

// -------------------------------------------------------------------

// SamplingMode

// .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
inline void SamplingMode::clear_samplingmethod() {
  samplingmethod_ = 0;
}
inline ::CoreML::Specification::SamplingMode_Method SamplingMode::samplingmethod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamplingMode.samplingMethod)
  return static_cast< ::CoreML::Specification::SamplingMode_Method >(samplingmethod_);
}
inline void SamplingMode::set_samplingmethod(::CoreML::Specification::SamplingMode_Method value) {
  
  samplingmethod_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamplingMode.samplingMethod)
}

// -------------------------------------------------------------------

// BoxCoordinatesMode

// .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
inline void BoxCoordinatesMode::clear_boxmode() {
  boxmode_ = 0;
}
inline ::CoreML::Specification::BoxCoordinatesMode_Coordinates BoxCoordinatesMode::boxmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BoxCoordinatesMode.boxMode)
  return static_cast< ::CoreML::Specification::BoxCoordinatesMode_Coordinates >(boxmode_);
}
inline void BoxCoordinatesMode::set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value) {
  
  boxmode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BoxCoordinatesMode.boxMode)
}

// -------------------------------------------------------------------

// WeightParams

// repeated float floatValue = 1;
inline int WeightParams::floatvalue_size() const {
  return floatvalue_.size();
}
inline void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
inline float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_.Get(index);
}
inline void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
inline void WeightParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
inline const ::google::protobuf::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_;
}
inline ::google::protobuf::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return &floatvalue_;
}

// bytes float16Value = 2;
inline void WeightParams::clear_float16value() {
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& WeightParams::float16value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.float16Value)
  return float16value_.GetNoArena();
}
inline void WeightParams::set_float16value(const ::std::string& value) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.float16Value)
}
#if LANG_CXX11
inline void WeightParams::set_float16value(::std::string&& value) {
  
  float16value_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.float16Value)
}
#endif
inline void WeightParams::set_float16value(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.float16Value)
}
inline void WeightParams::set_float16value(const void* value, size_t size) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.float16Value)
}
inline ::std::string* WeightParams::mutable_float16value() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.float16Value)
  return float16value_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* WeightParams::release_float16value() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.float16Value)
  
  return float16value_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void WeightParams::set_allocated_float16value(::std::string* float16value) {
  if (float16value != NULL) {
    
  } else {
    
  }
  float16value_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), float16value);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.float16Value)
}

// bytes rawValue = 30;
inline void WeightParams::clear_rawvalue() {
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& WeightParams::rawvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.GetNoArena();
}
inline void WeightParams::set_rawvalue(const ::std::string& value) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.rawValue)
}
#if LANG_CXX11
inline void WeightParams::set_rawvalue(::std::string&& value) {
  
  rawvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.rawValue)
}
#endif
inline void WeightParams::set_rawvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.rawValue)
}
inline void WeightParams::set_rawvalue(const void* value, size_t size) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.rawValue)
}
inline ::std::string* WeightParams::mutable_rawvalue() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* WeightParams::release_rawvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.rawValue)
  
  return rawvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void WeightParams::set_allocated_rawvalue(::std::string* rawvalue) {
  if (rawvalue != NULL) {
    
  } else {
    
  }
  rawvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), rawvalue);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.rawValue)
}

// .CoreML.Specification.QuantizationParams quantization = 40;
inline bool WeightParams::has_quantization() const {
  return this != internal_default_instance() && quantization_ != NULL;
}
inline void WeightParams::clear_quantization() {
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) delete quantization_;
  quantization_ = NULL;
}
inline const ::CoreML::Specification::QuantizationParams& WeightParams::quantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.quantization)
  return quantization_ != NULL ? *quantization_
                         : *::CoreML::Specification::QuantizationParams::internal_default_instance();
}
inline ::CoreML::Specification::QuantizationParams* WeightParams::mutable_quantization() {
  
  if (quantization_ == NULL) {
    quantization_ = new ::CoreML::Specification::QuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.quantization)
  return quantization_;
}
inline ::CoreML::Specification::QuantizationParams* WeightParams::release_quantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.quantization)
  
  ::CoreML::Specification::QuantizationParams* temp = quantization_;
  quantization_ = NULL;
  return temp;
}
inline void WeightParams::set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization) {
  delete quantization_;
  quantization_ = quantization;
  if (quantization) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.quantization)
}

// bool isUpdatable = 50;
inline void WeightParams::clear_isupdatable() {
  isupdatable_ = false;
}
inline bool WeightParams::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.isUpdatable)
  return isupdatable_;
}
inline void WeightParams::set_isupdatable(bool value) {
  
  isupdatable_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.isUpdatable)
}

// -------------------------------------------------------------------

// QuantizationParams

// uint64 numberOfBits = 1;
inline void QuantizationParams::clear_numberofbits() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 QuantizationParams::numberofbits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.numberOfBits)
  return numberofbits_;
}
inline void QuantizationParams::set_numberofbits(::google::protobuf::uint64 value) {
  
  numberofbits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.QuantizationParams.numberOfBits)
}

// .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
inline bool QuantizationParams::has_linearquantization() const {
  return QuantizationType_case() == kLinearQuantization;
}
inline void QuantizationParams::set_has_linearquantization() {
  _oneof_case_[0] = kLinearQuantization;
}
inline void QuantizationParams::clear_linearquantization() {
  if (has_linearquantization()) {
    delete QuantizationType_.linearquantization_;
    clear_has_QuantizationType();
  }
}
inline  const ::CoreML::Specification::LinearQuantizationParams& QuantizationParams::linearquantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.linearQuantization)
  return has_linearquantization()
      ? *QuantizationType_.linearquantization_
      : ::CoreML::Specification::LinearQuantizationParams::default_instance();
}
inline ::CoreML::Specification::LinearQuantizationParams* QuantizationParams::mutable_linearquantization() {
  if (!has_linearquantization()) {
    clear_QuantizationType();
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = new ::CoreML::Specification::LinearQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.linearQuantization)
  return QuantizationType_.linearquantization_;
}
inline ::CoreML::Specification::LinearQuantizationParams* QuantizationParams::release_linearquantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.linearQuantization)
  if (has_linearquantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LinearQuantizationParams* temp = QuantizationType_.linearquantization_;
    QuantizationType_.linearquantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void QuantizationParams::set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization) {
  clear_QuantizationType();
  if (linearquantization) {
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = linearquantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.linearQuantization)
}

// .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
inline bool QuantizationParams::has_lookuptablequantization() const {
  return QuantizationType_case() == kLookupTableQuantization;
}
inline void QuantizationParams::set_has_lookuptablequantization() {
  _oneof_case_[0] = kLookupTableQuantization;
}
inline void QuantizationParams::clear_lookuptablequantization() {
  if (has_lookuptablequantization()) {
    delete QuantizationType_.lookuptablequantization_;
    clear_has_QuantizationType();
  }
}
inline  const ::CoreML::Specification::LookUpTableQuantizationParams& QuantizationParams::lookuptablequantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return has_lookuptablequantization()
      ? *QuantizationType_.lookuptablequantization_
      : ::CoreML::Specification::LookUpTableQuantizationParams::default_instance();
}
inline ::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::mutable_lookuptablequantization() {
  if (!has_lookuptablequantization()) {
    clear_QuantizationType();
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = new ::CoreML::Specification::LookUpTableQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return QuantizationType_.lookuptablequantization_;
}
inline ::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::release_lookuptablequantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  if (has_lookuptablequantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LookUpTableQuantizationParams* temp = QuantizationType_.lookuptablequantization_;
    QuantizationType_.lookuptablequantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void QuantizationParams::set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization) {
  clear_QuantizationType();
  if (lookuptablequantization) {
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = lookuptablequantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.lookupTableQuantization)
}

inline bool QuantizationParams::has_QuantizationType() const {
  return QuantizationType_case() != QUANTIZATIONTYPE_NOT_SET;
}
inline void QuantizationParams::clear_has_QuantizationType() {
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}
inline QuantizationParams::QuantizationTypeCase QuantizationParams::QuantizationType_case() const {
  return QuantizationParams::QuantizationTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// LinearQuantizationParams

// repeated float scale = 1;
inline int LinearQuantizationParams::scale_size() const {
  return scale_.size();
}
inline void LinearQuantizationParams::clear_scale() {
  scale_.Clear();
}
inline float LinearQuantizationParams::scale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_.Get(index);
}
inline void LinearQuantizationParams::set_scale(int index, float value) {
  scale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.scale)
}
inline void LinearQuantizationParams::add_scale(float value) {
  scale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.scale)
}
inline const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::scale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_;
}
inline ::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_scale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.scale)
  return &scale_;
}

// repeated float bias = 2;
inline int LinearQuantizationParams::bias_size() const {
  return bias_.size();
}
inline void LinearQuantizationParams::clear_bias() {
  bias_.Clear();
}
inline float LinearQuantizationParams::bias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_.Get(index);
}
inline void LinearQuantizationParams::set_bias(int index, float value) {
  bias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.bias)
}
inline void LinearQuantizationParams::add_bias(float value) {
  bias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.bias)
}
inline const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::bias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_;
}
inline ::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_bias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.bias)
  return &bias_;
}

// -------------------------------------------------------------------

// LookUpTableQuantizationParams

// repeated float floatValue = 1;
inline int LookUpTableQuantizationParams::floatvalue_size() const {
  return floatvalue_.size();
}
inline void LookUpTableQuantizationParams::clear_floatvalue() {
  floatvalue_.Clear();
}
inline float LookUpTableQuantizationParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_.Get(index);
}
inline void LookUpTableQuantizationParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
inline void LookUpTableQuantizationParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
inline const ::google::protobuf::RepeatedField< float >&
LookUpTableQuantizationParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_;
}
inline ::google::protobuf::RepeatedField< float >*
LookUpTableQuantizationParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return &floatvalue_;
}

// -------------------------------------------------------------------

// ConvolutionLayerParams

// uint64 outputChannels = 1;
inline void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return outputchannels_;
}
inline void ConvolutionLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// uint64 kernelChannels = 2;
inline void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return kernelchannels_;
}
inline void ConvolutionLayerParams::set_kernelchannels(::google::protobuf::uint64 value) {
  
  kernelchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// uint64 nGroups = 10;
inline void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return ngroups_;
}
inline void ConvolutionLayerParams::set_ngroups(::google::protobuf::uint64 value) {
  
  ngroups_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
inline int ConvolutionLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
inline void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
inline void ConvolutionLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
inline void ConvolutionLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 30;
inline int ConvolutionLayerParams::stride_size() const {
  return stride_.size();
}
inline void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_.Get(index);
}
inline void ConvolutionLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
inline void ConvolutionLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return &stride_;
}

// repeated uint64 dilationFactor = 40;
inline int ConvolutionLayerParams::dilationfactor_size() const {
  return dilationfactor_.size();
}
inline void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_.Get(index);
}
inline void ConvolutionLayerParams::set_dilationfactor(int index, ::google::protobuf::uint64 value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
inline void ConvolutionLayerParams::add_dilationfactor(::google::protobuf::uint64 value) {
  dilationfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return &dilationfactor_;
}

// .CoreML.Specification.ValidPadding valid = 50;
inline bool ConvolutionLayerParams::has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
inline void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
inline void ConvolutionLayerParams::clear_valid() {
  if (has_valid()) {
    delete ConvolutionPaddingType_.valid_;
    clear_has_ConvolutionPaddingType();
  }
}
inline  const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return has_valid()
      ? *ConvolutionPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return ConvolutionPaddingType_.valid_;
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ConvolutionLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 51;
inline bool ConvolutionLayerParams::has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
inline void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
inline void ConvolutionLayerParams::clear_same() {
  if (has_same()) {
    delete ConvolutionPaddingType_.same_;
    clear_has_ConvolutionPaddingType();
  }
}
inline  const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return has_same()
      ? *ConvolutionPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  if (!has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return ConvolutionPaddingType_.same_;
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ConvolutionLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}

// bool isDeconvolution = 60;
inline void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
inline bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return isdeconvolution_;
}
inline void ConvolutionLayerParams::set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// bool hasBias = 70;
inline void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return hasbias_;
}
inline void ConvolutionLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 90;
inline bool ConvolutionLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void ConvolutionLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 91;
inline bool ConvolutionLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void ConvolutionLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
inline int ConvolutionLayerParams::outputshape_size() const {
  return outputshape_.size();
}
inline void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_.Get(index);
}
inline void ConvolutionLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
inline void ConvolutionLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return &outputshape_;
}

inline bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
inline void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
inline ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// InnerProductLayerParams

// uint64 inputChannels = 1;
inline void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return inputchannels_;
}
inline void InnerProductLayerParams::set_inputchannels(::google::protobuf::uint64 value) {
  
  inputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// uint64 outputChannels = 2;
inline void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return outputchannels_;
}
inline void InnerProductLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// bool hasBias = 10;
inline void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return hasbias_;
}
inline void InnerProductLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
inline bool InnerProductLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void InnerProductLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
inline bool InnerProductLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void InnerProductLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

// -------------------------------------------------------------------

// EmbeddingLayerParams

// uint64 inputDim = 1;
inline void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return inputdim_;
}
inline void EmbeddingLayerParams::set_inputdim(::google::protobuf::uint64 value) {
  
  inputdim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// uint64 outputChannels = 2;
inline void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return outputchannels_;
}
inline void EmbeddingLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// bool hasBias = 10;
inline void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return hasbias_;
}
inline void EmbeddingLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
inline bool EmbeddingLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void EmbeddingLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
inline bool EmbeddingLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void EmbeddingLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

// -------------------------------------------------------------------

// EmbeddingNDLayerParams

// uint64 vocabSize = 1;
inline void EmbeddingNDLayerParams::clear_vocabsize() {
  vocabsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 EmbeddingNDLayerParams::vocabsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
  return vocabsize_;
}
inline void EmbeddingNDLayerParams::set_vocabsize(::google::protobuf::uint64 value) {
  
  vocabsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
}

// uint64 embeddingSize = 2;
inline void EmbeddingNDLayerParams::clear_embeddingsize() {
  embeddingsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 EmbeddingNDLayerParams::embeddingsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
  return embeddingsize_;
}
inline void EmbeddingNDLayerParams::set_embeddingsize(::google::protobuf::uint64 value) {
  
  embeddingsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
}

// bool hasBias = 3;
inline void EmbeddingNDLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool EmbeddingNDLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
  return hasbias_;
}
inline void EmbeddingNDLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
inline bool EmbeddingNDLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void EmbeddingNDLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void EmbeddingNDLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
inline bool EmbeddingNDLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void EmbeddingNDLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void EmbeddingNDLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.bias)
}

// -------------------------------------------------------------------

// BatchnormLayerParams

// uint64 channels = 1;
inline void BatchnormLayerParams::clear_channels() {
  channels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return channels_;
}
inline void BatchnormLayerParams::set_channels(::google::protobuf::uint64 value) {
  
  channels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// bool computeMeanVar = 5;
inline void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
inline bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return computemeanvar_;
}
inline void BatchnormLayerParams::set_computemeanvar(bool value) {
  
  computemeanvar_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// bool instanceNormalization = 6;
inline void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
inline bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return instancenormalization_;
}
inline void BatchnormLayerParams::set_instancenormalization(bool value) {
  
  instancenormalization_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// float epsilon = 10;
inline void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return epsilon_;
}
inline void BatchnormLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// .CoreML.Specification.WeightParams gamma = 15;
inline bool BatchnormLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
inline void BatchnormLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 16;
inline bool BatchnormLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
inline void BatchnormLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// .CoreML.Specification.WeightParams mean = 17;
inline bool BatchnormLayerParams::has_mean() const {
  return this != internal_default_instance() && mean_ != NULL;
}
inline void BatchnormLayerParams::clear_mean() {
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_ != NULL ? *mean_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {
  
  if (mean_ == NULL) {
    mean_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  delete mean_;
  mean_ = mean;
  if (mean) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// .CoreML.Specification.WeightParams variance = 18;
inline bool BatchnormLayerParams::has_variance() const {
  return this != internal_default_instance() && variance_ != NULL;
}
inline void BatchnormLayerParams::clear_variance() {
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_ != NULL ? *variance_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {
  
  if (variance_ == NULL) {
    variance_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  delete variance_;
  variance_ = variance;
  if (variance) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

// -------------------------------------------------------------------

// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
inline int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return paddingamounts_.size();
}
inline void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
inline ::google::protobuf::uint64 PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_.Get(index);
}
inline void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, ::google::protobuf::uint64 value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
inline void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(::google::protobuf::uint64 value) {
  paddingamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return &paddingamounts_;
}

// -------------------------------------------------------------------

// PoolingLayerParams

// .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
inline void PoolingLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
inline void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
inline int PoolingLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
inline void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
inline ::google::protobuf::uint64 PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
inline void PoolingLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
inline void PoolingLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 20;
inline int PoolingLayerParams::stride_size() const {
  return stride_.size();
}
inline void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
inline ::google::protobuf::uint64 PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return stride_.Get(index);
}
inline void PoolingLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
inline void PoolingLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return stride_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return &stride_;
}

// .CoreML.Specification.ValidPadding valid = 30;
inline bool PoolingLayerParams::has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
inline void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
inline void PoolingLayerParams::clear_valid() {
  if (has_valid()) {
    delete PoolingPaddingType_.valid_;
    clear_has_PoolingPaddingType();
  }
}
inline  const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return has_valid()
      ? *PoolingPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return PoolingPaddingType_.valid_;
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PoolingLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 31;
inline bool PoolingLayerParams::has_same() const {
  return PoolingPaddingType_case() == kSame;
}
inline void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
inline void PoolingLayerParams::clear_same() {
  if (has_same()) {
    delete PoolingPaddingType_.same_;
    clear_has_PoolingPaddingType();
  }
}
inline  const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return has_same()
      ? *PoolingPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  if (!has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return PoolingPaddingType_.same_;
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PoolingLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}

// .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
inline bool PoolingLayerParams::has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
inline void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
inline void PoolingLayerParams::clear_includelastpixel() {
  if (has_includelastpixel()) {
    delete PoolingPaddingType_.includelastpixel_;
    clear_has_PoolingPaddingType();
  }
}
inline  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::default_instance();
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  if (!has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = new ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return PoolingPaddingType_.includelastpixel_;
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PoolingLayerParams::set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}

// bool avgPoolExcludePadding = 50;
inline void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
inline bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return avgpoolexcludepadding_;
}
inline void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {
  
  avgpoolexcludepadding_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// bool globalPooling = 60;
inline void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
inline bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return globalpooling_;
}
inline void PoolingLayerParams::set_globalpooling(bool value) {
  
  globalpooling_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

inline bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
inline void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
inline PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// PaddingLayerParams_PaddingConstant

// float value = 1;
inline void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
inline float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return value_;
}
inline void PaddingLayerParams_PaddingConstant::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReflection

// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReplication

// -------------------------------------------------------------------

// PaddingLayerParams

// .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
inline bool PaddingLayerParams::has_constant() const {
  return PaddingType_case() == kConstant;
}
inline void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
inline void PaddingLayerParams::clear_constant() {
  if (has_constant()) {
    delete PaddingType_.constant_;
    clear_has_PaddingType();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return has_constant()
      ? *PaddingType_.constant_
      : ::CoreML::Specification::PaddingLayerParams_PaddingConstant::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  if (!has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = new ::CoreML::Specification::PaddingLayerParams_PaddingConstant;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return PaddingType_.constant_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PaddingLayerParams::set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
inline bool PaddingLayerParams::has_reflection() const {
  return PaddingType_case() == kReflection;
}
inline void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
inline void PaddingLayerParams::clear_reflection() {
  if (has_reflection()) {
    delete PaddingType_.reflection_;
    clear_has_PaddingType();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return has_reflection()
      ? *PaddingType_.reflection_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReflection::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  if (!has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReflection;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return PaddingType_.reflection_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PaddingLayerParams::set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
inline bool PaddingLayerParams::has_replication() const {
  return PaddingType_case() == kReplication;
}
inline void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
inline void PaddingLayerParams::clear_replication() {
  if (has_replication()) {
    delete PaddingType_.replication_;
    clear_has_PaddingType();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return has_replication()
      ? *PaddingType_.replication_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReplication::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  if (!has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReplication;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return PaddingType_.replication_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PaddingLayerParams::set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}

// .CoreML.Specification.BorderAmounts paddingAmounts = 10;
inline bool PaddingLayerParams::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
inline void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
inline const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_;
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
inline void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

inline bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
inline void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
inline PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// ConcatLayerParams

// bool sequenceConcat = 100;
inline void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
inline bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return sequenceconcat_;
}
inline void ConcatLayerParams::set_sequenceconcat(bool value) {
  
  sequenceconcat_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

// -------------------------------------------------------------------

// LRNLayerParams

// float alpha = 1;
inline void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return alpha_;
}
inline void LRNLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// float beta = 2;
inline void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
inline float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return beta_;
}
inline void LRNLayerParams::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// uint64 localSize = 3;
inline void LRNLayerParams::clear_localsize() {
  localsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return localsize_;
}
inline void LRNLayerParams::set_localsize(::google::protobuf::uint64 value) {
  
  localsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// float k = 4;
inline void LRNLayerParams::clear_k() {
  k_ = 0;
}
inline float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return k_;
}
inline void LRNLayerParams::set_k(float value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

// -------------------------------------------------------------------

// SoftmaxLayerParams

// -------------------------------------------------------------------

// SplitLayerParams

// uint64 nOutputs = 1;
inline void SplitLayerParams::clear_noutputs() {
  noutputs_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return noutputs_;
}
inline void SplitLayerParams::set_noutputs(::google::protobuf::uint64 value) {
  
  noutputs_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

// -------------------------------------------------------------------

// AddLayerParams

// float alpha = 1;
inline void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return alpha_;
}
inline void AddLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

// -------------------------------------------------------------------

// MultiplyLayerParams

// float alpha = 1;
inline void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return alpha_;
}
inline void MultiplyLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

// -------------------------------------------------------------------

// UnaryFunctionLayerParams

// .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
inline void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
inline void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// float alpha = 2;
inline void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return alpha_;
}
inline void UnaryFunctionLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// float epsilon = 3;
inline void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return epsilon_;
}
inline void UnaryFunctionLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// float shift = 4;
inline void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
inline float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return shift_;
}
inline void UnaryFunctionLayerParams::set_shift(float value) {
  
  shift_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// float scale = 5;
inline void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
inline float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return scale_;
}
inline void UnaryFunctionLayerParams::set_scale(float value) {
  
  scale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

// -------------------------------------------------------------------

// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
inline int UpsampleLayerParams::scalingfactor_size() const {
  return scalingfactor_.size();
}
inline void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
inline ::google::protobuf::uint64 UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_.Get(index);
}
inline void UpsampleLayerParams::set_scalingfactor(int index, ::google::protobuf::uint64 value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
inline void UpsampleLayerParams::add_scalingfactor(::google::protobuf::uint64 value) {
  scalingfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return &scalingfactor_;
}

// .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
inline void UpsampleLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::UpsampleLayerParams_InterpolationMode UpsampleLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.mode)
  return static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(mode_);
}
inline void UpsampleLayerParams::set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.mode)
}

// -------------------------------------------------------------------

// ResizeBilinearLayerParams

// repeated uint64 targetSize = 1;
inline int ResizeBilinearLayerParams::targetsize_size() const {
  return targetsize_.size();
}
inline void ResizeBilinearLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
inline ::google::protobuf::uint64 ResizeBilinearLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return targetsize_.Get(index);
}
inline void ResizeBilinearLayerParams::set_targetsize(int index, ::google::protobuf::uint64 value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
inline void ResizeBilinearLayerParams::add_targetsize(::google::protobuf::uint64 value) {
  targetsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ResizeBilinearLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return targetsize_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ResizeBilinearLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return &targetsize_;
}

// .CoreML.Specification.SamplingMode mode = 2;
inline bool ResizeBilinearLayerParams::has_mode() const {
  return this != internal_default_instance() && mode_ != NULL;
}
inline void ResizeBilinearLayerParams::clear_mode() {
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) delete mode_;
  mode_ = NULL;
}
inline const ::CoreML::Specification::SamplingMode& ResizeBilinearLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return mode_ != NULL ? *mode_
                         : *::CoreML::Specification::SamplingMode::internal_default_instance();
}
inline ::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::mutable_mode() {
  
  if (mode_ == NULL) {
    mode_ = new ::CoreML::Specification::SamplingMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return mode_;
}
inline ::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ResizeBilinearLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = NULL;
  return temp;
}
inline void ResizeBilinearLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  delete mode_;
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ResizeBilinearLayerParams.mode)
}

// -------------------------------------------------------------------

// CropResizeLayerParams

// repeated uint64 targetSize = 1;
inline int CropResizeLayerParams::targetsize_size() const {
  return targetsize_.size();
}
inline void CropResizeLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
inline ::google::protobuf::uint64 CropResizeLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.targetSize)
  return targetsize_.Get(index);
}
inline void CropResizeLayerParams::set_targetsize(int index, ::google::protobuf::uint64 value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.targetSize)
}
inline void CropResizeLayerParams::add_targetsize(::google::protobuf::uint64 value) {
  targetsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropResizeLayerParams.targetSize)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropResizeLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return targetsize_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropResizeLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return &targetsize_;
}

// bool normalizedCoordinates = 2;
inline void CropResizeLayerParams::clear_normalizedcoordinates() {
  normalizedcoordinates_ = false;
}
inline bool CropResizeLayerParams::normalizedcoordinates() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
  return normalizedcoordinates_;
}
inline void CropResizeLayerParams::set_normalizedcoordinates(bool value) {
  
  normalizedcoordinates_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
}

// .CoreML.Specification.SamplingMode mode = 3;
inline bool CropResizeLayerParams::has_mode() const {
  return this != internal_default_instance() && mode_ != NULL;
}
inline void CropResizeLayerParams::clear_mode() {
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) delete mode_;
  mode_ = NULL;
}
inline const ::CoreML::Specification::SamplingMode& CropResizeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.mode)
  return mode_ != NULL ? *mode_
                         : *::CoreML::Specification::SamplingMode::internal_default_instance();
}
inline ::CoreML::Specification::SamplingMode* CropResizeLayerParams::mutable_mode() {
  
  if (mode_ == NULL) {
    mode_ = new ::CoreML::Specification::SamplingMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.mode)
  return mode_;
}
inline ::CoreML::Specification::SamplingMode* CropResizeLayerParams::release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = NULL;
  return temp;
}
inline void CropResizeLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  delete mode_;
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.mode)
}

// .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
inline bool CropResizeLayerParams::has_boxindicesmode() const {
  return this != internal_default_instance() && boxindicesmode_ != NULL;
}
inline void CropResizeLayerParams::clear_boxindicesmode() {
  if (GetArenaNoVirtual() == NULL && boxindicesmode_ != NULL) delete boxindicesmode_;
  boxindicesmode_ = NULL;
}
inline const ::CoreML::Specification::BoxCoordinatesMode& CropResizeLayerParams::boxindicesmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return boxindicesmode_ != NULL ? *boxindicesmode_
                         : *::CoreML::Specification::BoxCoordinatesMode::internal_default_instance();
}
inline ::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::mutable_boxindicesmode() {
  
  if (boxindicesmode_ == NULL) {
    boxindicesmode_ = new ::CoreML::Specification::BoxCoordinatesMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return boxindicesmode_;
}
inline ::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::release_boxindicesmode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  
  ::CoreML::Specification::BoxCoordinatesMode* temp = boxindicesmode_;
  boxindicesmode_ = NULL;
  return temp;
}
inline void CropResizeLayerParams::set_allocated_boxindicesmode(::CoreML::Specification::BoxCoordinatesMode* boxindicesmode) {
  delete boxindicesmode_;
  boxindicesmode_ = boxindicesmode;
  if (boxindicesmode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
}

// float spatialScale = 5;
inline void CropResizeLayerParams::clear_spatialscale() {
  spatialscale_ = 0;
}
inline float CropResizeLayerParams::spatialscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.spatialScale)
  return spatialscale_;
}
inline void CropResizeLayerParams::set_spatialscale(float value) {
  
  spatialscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.spatialScale)
}

// -------------------------------------------------------------------

// BiasLayerParams

// repeated uint64 shape = 1;
inline int BiasLayerParams::shape_size() const {
  return shape_.size();
}
inline void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
inline ::google::protobuf::uint64 BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return shape_.Get(index);
}
inline void BiasLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
inline void BiasLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return shape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams bias = 2;
inline bool BiasLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void BiasLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

// -------------------------------------------------------------------

// ScaleLayerParams

// repeated uint64 shapeScale = 1;
inline int ScaleLayerParams::shapescale_size() const {
  return shapescale_.size();
}
inline void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
inline ::google::protobuf::uint64 ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_.Get(index);
}
inline void ScaleLayerParams::set_shapescale(int index, ::google::protobuf::uint64 value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
inline void ScaleLayerParams::add_shapescale(::google::protobuf::uint64 value) {
  shapescale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return &shapescale_;
}

// .CoreML.Specification.WeightParams scale = 2;
inline bool ScaleLayerParams::has_scale() const {
  return this != internal_default_instance() && scale_ != NULL;
}
inline void ScaleLayerParams::clear_scale() {
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return scale_ != NULL ? *scale_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {
  
  if (scale_ == NULL) {
    scale_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return scale_;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = NULL;
  return temp;
}
inline void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  delete scale_;
  scale_ = scale;
  if (scale) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// bool hasBias = 3;
inline void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return hasbias_;
}
inline void ScaleLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
inline int ScaleLayerParams::shapebias_size() const {
  return shapebias_.size();
}
inline void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
inline ::google::protobuf::uint64 ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_.Get(index);
}
inline void ScaleLayerParams::set_shapebias(int index, ::google::protobuf::uint64 value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
inline void ScaleLayerParams::add_shapebias(::google::protobuf::uint64 value) {
  shapebias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return &shapebias_;
}

// .CoreML.Specification.WeightParams bias = 5;
inline bool ScaleLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void ScaleLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

// -------------------------------------------------------------------

// LoadConstantLayerParams

// repeated uint64 shape = 1;
inline int LoadConstantLayerParams::shape_size() const {
  return shape_.size();
}
inline void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
inline ::google::protobuf::uint64 LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_.Get(index);
}
inline void LoadConstantLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
inline void LoadConstantLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
inline bool LoadConstantLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
inline void LoadConstantLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return data_;
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
inline void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

// -------------------------------------------------------------------

// L2NormalizeLayerParams

// float epsilon = 1;
inline void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return epsilon_;
}
inline void L2NormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

// -------------------------------------------------------------------

// FlattenLayerParams

// .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
inline void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
inline void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

// -------------------------------------------------------------------

// ReshapeLayerParams

// repeated int64 targetShape = 1;
inline int ReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
inline void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline ::google::protobuf::int64 ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
inline void ReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
inline void ReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return &targetshape_;
}

// .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
inline void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
inline void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

// -------------------------------------------------------------------

// PermuteLayerParams

// repeated uint64 axis = 1;
inline int PermuteLayerParams::axis_size() const {
  return axis_.size();
}
inline void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
inline ::google::protobuf::uint64 PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return axis_.Get(index);
}
inline void PermuteLayerParams::set_axis(int index, ::google::protobuf::uint64 value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
inline void PermuteLayerParams::add_axis(::google::protobuf::uint64 value) {
  axis_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return axis_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return &axis_;
}

// -------------------------------------------------------------------

// ReorganizeDataLayerParams

// .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
inline void ReorganizeDataLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(mode_);
}
inline void ReorganizeDataLayerParams::set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.mode)
}

// uint64 blockSize = 2;
inline void ReorganizeDataLayerParams::clear_blocksize() {
  blocksize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ReorganizeDataLayerParams::blocksize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
  return blocksize_;
}
inline void ReorganizeDataLayerParams::set_blocksize(::google::protobuf::uint64 value) {
  
  blocksize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
}

// -------------------------------------------------------------------

// SliceLayerParams

// int64 startIndex = 1;
inline void SliceLayerParams::clear_startindex() {
  startindex_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 SliceLayerParams::startindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.startIndex)
  return startindex_;
}
inline void SliceLayerParams::set_startindex(::google::protobuf::int64 value) {
  
  startindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.startIndex)
}

// int64 endIndex = 2;
inline void SliceLayerParams::clear_endindex() {
  endindex_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 SliceLayerParams::endindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.endIndex)
  return endindex_;
}
inline void SliceLayerParams::set_endindex(::google::protobuf::int64 value) {
  
  endindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.endIndex)
}

// uint64 stride = 3;
inline void SliceLayerParams::clear_stride() {
  stride_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SliceLayerParams::stride() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.stride)
  return stride_;
}
inline void SliceLayerParams::set_stride(::google::protobuf::uint64 value) {
  
  stride_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.stride)
}

// .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
inline void SliceLayerParams::clear_axis() {
  axis_ = 0;
}
inline ::CoreML::Specification::SliceLayerParams_SliceAxis SliceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.axis)
  return static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(axis_);
}
inline void SliceLayerParams::set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.axis)
}

// -------------------------------------------------------------------

// ReduceLayerParams

// .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
inline void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
inline void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// float epsilon = 2;
inline void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return epsilon_;
}
inline void ReduceLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

// .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
inline void ReduceLayerParams::clear_axis() {
  axis_ = 0;
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceAxis ReduceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.axis)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(axis_);
}
inline void ReduceLayerParams::set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.axis)
}

// -------------------------------------------------------------------

// CropLayerParams

// .CoreML.Specification.BorderAmounts cropAmounts = 1;
inline bool CropLayerParams::has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != NULL;
}
inline void CropLayerParams::clear_cropamounts() {
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
}
inline const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_ != NULL ? *cropamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {
  
  if (cropamounts_ == NULL) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_;
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = NULL;
  return temp;
}
inline void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  delete cropamounts_;
  cropamounts_ = cropamounts;
  if (cropamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
inline int CropLayerParams::offset_size() const {
  return offset_.size();
}
inline void CropLayerParams::clear_offset() {
  offset_.Clear();
}
inline ::google::protobuf::uint64 CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return offset_.Get(index);
}
inline void CropLayerParams::set_offset(int index, ::google::protobuf::uint64 value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
inline void CropLayerParams::add_offset(::google::protobuf::uint64 value) {
  offset_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return offset_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return &offset_;
}

// -------------------------------------------------------------------

// AverageLayerParams

// -------------------------------------------------------------------

// MaxLayerParams

// -------------------------------------------------------------------

// MinLayerParams

// -------------------------------------------------------------------

// DotProductLayerParams

// bool cosineSimilarity = 1;
inline void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
inline bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return cosinesimilarity_;
}
inline void DotProductLayerParams::set_cosinesimilarity(bool value) {
  
  cosinesimilarity_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

// -------------------------------------------------------------------

// MeanVarianceNormalizeLayerParams

// bool acrossChannels = 1;
inline void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
inline bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return acrosschannels_;
}
inline void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {
  
  acrosschannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// bool normalizeVariance = 2;
inline void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
inline bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return normalizevariance_;
}
inline void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {
  
  normalizevariance_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// float epsilon = 3;
inline void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return epsilon_;
}
inline void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

// -------------------------------------------------------------------

// SequenceRepeatLayerParams

// uint64 nRepetitions = 1;
inline void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return nrepetitions_;
}
inline void SequenceRepeatLayerParams::set_nrepetitions(::google::protobuf::uint64 value) {
  
  nrepetitions_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

// -------------------------------------------------------------------

// SimpleRecurrentLayerParams

// uint64 inputVectorSize = 1;
inline void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void SimpleRecurrentLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void SimpleRecurrentLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// .CoreML.Specification.ActivationParams activation = 10;
inline bool SimpleRecurrentLayerParams::has_activation() const {
  return this != internal_default_instance() && activation_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
}
inline const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_ != NULL ? *activation_
                         : *::CoreML::Specification::ActivationParams::internal_default_instance();
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {
  
  if (activation_ == NULL) {
    activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_;
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  delete activation_;
  activation_ = activation;
  if (activation) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// bool sequenceOutput = 15;
inline void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return sequenceoutput_;
}
inline void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// bool hasBiasVector = 20;
inline void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
inline bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return hasbiasvector_;
}
inline void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {
  
  hasbiasvector_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// .CoreML.Specification.WeightParams weightMatrix = 30;
inline bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_ != NULL ? *weightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {
  
  if (weightmatrix_ == NULL) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  delete weightmatrix_;
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// .CoreML.Specification.WeightParams recursionMatrix = 31;
inline bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_ != NULL ? *recursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {
  
  if (recursionmatrix_ == NULL) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  delete recursionmatrix_;
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// .CoreML.Specification.WeightParams biasVector = 32;
inline bool SimpleRecurrentLayerParams::has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_ != NULL ? *biasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {
  
  if (biasvector_ == NULL) {
    biasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  delete biasvector_;
  biasvector_ = biasvector;
  if (biasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// bool reverseInput = 100;
inline void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return reverseinput_;
}
inline void SimpleRecurrentLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

// -------------------------------------------------------------------

// GRULayerParams

// uint64 inputVectorSize = 1;
inline void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void GRULayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void GRULayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
inline int GRULayerParams::activations_size() const {
  return activations_.size();
}
inline void GRULayerParams::clear_activations() {
  activations_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return activations_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return activations_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// bool sequenceOutput = 15;
inline void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return sequenceoutput_;
}
inline void GRULayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
inline void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
inline bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return hasbiasvectors_;
}
inline void GRULayerParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
inline bool GRULayerParams::has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != NULL;
}
inline void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_ != NULL ? *updategateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {
  
  if (updategateweightmatrix_ == NULL) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  delete updategateweightmatrix_;
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
inline bool GRULayerParams::has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != NULL;
}
inline void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_ != NULL ? *resetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {
  
  if (resetgateweightmatrix_ == NULL) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  delete resetgateweightmatrix_;
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
inline bool GRULayerParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
inline void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
inline bool GRULayerParams::has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != NULL;
}
inline void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_ != NULL ? *updategaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {
  
  if (updategaterecursionmatrix_ == NULL) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
inline bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != NULL;
}
inline void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_ != NULL ? *resetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {
  
  if (resetgaterecursionmatrix_ == NULL) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
inline bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
inline void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams updateGateBiasVector = 70;
inline bool GRULayerParams::has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != NULL;
}
inline void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_ != NULL ? *updategatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {
  
  if (updategatebiasvector_ == NULL) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  delete updategatebiasvector_;
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// .CoreML.Specification.WeightParams resetGateBiasVector = 71;
inline bool GRULayerParams::has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != NULL;
}
inline void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_ != NULL ? *resetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {
  
  if (resetgatebiasvector_ == NULL) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  delete resetgatebiasvector_;
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 72;
inline bool GRULayerParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
inline void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// bool reverseInput = 100;
inline void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return reverseinput_;
}
inline void GRULayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

// -------------------------------------------------------------------

// LSTMParams

// bool sequenceOutput = 10;
inline void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return sequenceoutput_;
}
inline void LSTMParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
inline void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
inline bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return hasbiasvectors_;
}
inline void LSTMParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// bool forgetBias = 30;
inline void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
inline bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return forgetbias_;
}
inline void LSTMParams::set_forgetbias(bool value) {
  
  forgetbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// bool hasPeepholeVectors = 40;
inline void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
inline bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return haspeepholevectors_;
}
inline void LSTMParams::set_haspeepholevectors(bool value) {
  
  haspeepholevectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// bool coupledInputAndForgetGate = 50;
inline void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
inline bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return coupledinputandforgetgate_;
}
inline void LSTMParams::set_coupledinputandforgetgate(bool value) {
  
  coupledinputandforgetgate_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// float cellClipThreshold = 60;
inline void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
inline float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return cellclipthreshold_;
}
inline void LSTMParams::set_cellclipthreshold(float value) {
  
  cellclipthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

// -------------------------------------------------------------------

// LSTMWeightParams

// .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
inline bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_ != NULL ? *inputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {
  
  if (inputgateweightmatrix_ == NULL) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  delete inputgateweightmatrix_;
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
inline bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_ != NULL ? *forgetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {
  
  if (forgetgateweightmatrix_ == NULL) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
inline bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_ != NULL ? *blockinputweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {
  
  if (blockinputweightmatrix_ == NULL) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  delete blockinputweightmatrix_;
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
inline bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
inline bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_ != NULL ? *inputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {
  
  if (inputgaterecursionmatrix_ == NULL) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
inline bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_ != NULL ? *forgetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {
  
  if (forgetgaterecursionmatrix_ == NULL) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
inline bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_ != NULL ? *blockinputrecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {
  
  if (blockinputrecursionmatrix_ == NULL) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
inline bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams inputGateBiasVector = 40;
inline bool LSTMWeightParams::has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_ != NULL ? *inputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {
  
  if (inputgatebiasvector_ == NULL) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  delete inputgatebiasvector_;
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
inline bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_ != NULL ? *forgetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {
  
  if (forgetgatebiasvector_ == NULL) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  delete forgetgatebiasvector_;
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// .CoreML.Specification.WeightParams blockInputBiasVector = 42;
inline bool LSTMWeightParams::has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_ != NULL ? *blockinputbiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {
  
  if (blockinputbiasvector_ == NULL) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  delete blockinputbiasvector_;
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 43;
inline bool LSTMWeightParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
inline bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != NULL;
}
inline void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_ != NULL ? *inputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {
  
  if (inputgatepeepholevector_ == NULL) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  delete inputgatepeepholevector_;
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
inline bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_ != NULL ? *forgetgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {
  
  if (forgetgatepeepholevector_ == NULL) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
inline bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != NULL;
}
inline void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_ != NULL ? *outputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {
  
  if (outputgatepeepholevector_ == NULL) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  delete outputgatepeepholevector_;
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

// -------------------------------------------------------------------

// UniDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
inline void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void UniDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void UniDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
inline int UniDirectionalLSTMLayerParams::activations_size() const {
  return activations_.size();
}
inline void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// .CoreML.Specification.LSTMParams params = 15;
inline bool UniDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
inline void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
inline const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_;
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
inline void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// .CoreML.Specification.LSTMWeightParams weightParams = 20;
inline bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != NULL;
}
inline void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;
}
inline const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_ != NULL ? *weightparams_
                         : *::CoreML::Specification::LSTMWeightParams::internal_default_instance();
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {
  
  if (weightparams_ == NULL) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = NULL;
  return temp;
}
inline void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  delete weightparams_;
  weightparams_ = weightparams;
  if (weightparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// bool reverseInput = 100;
inline void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return reverseinput_;
}
inline void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

// -------------------------------------------------------------------

// BiDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
inline void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void BiDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
inline void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void BiDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
inline int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
inline void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
inline int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
inline void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// .CoreML.Specification.LSTMParams params = 15;
inline bool BiDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
inline void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
inline const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_;
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
inline void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
inline int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return weightparams_.size();
}
inline void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
inline const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Get(index);
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

// -------------------------------------------------------------------

// CustomLayerParams_CustomLayerParamValue

// double doubleValue = 10;
inline bool CustomLayerParams_CustomLayerParamValue::has_doublevalue() const {
  return value_case() == kDoubleValue;
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_doublevalue() {
  _oneof_case_[0] = kDoubleValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_doublevalue() {
  if (has_doublevalue()) {
    value_.doublevalue_ = 0;
    clear_has_value();
  }
}
inline double CustomLayerParams_CustomLayerParamValue::doublevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
  if (has_doublevalue()) {
    return value_.doublevalue_;
  }
  return 0;
}
inline void CustomLayerParams_CustomLayerParamValue::set_doublevalue(double value) {
  if (!has_doublevalue()) {
    clear_value();
    set_has_doublevalue();
  }
  value_.doublevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
}

// string stringValue = 20;
inline bool CustomLayerParams_CustomLayerParamValue::has_stringvalue() const {
  return value_case() == kStringValue;
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_stringvalue() {
  _oneof_case_[0] = kStringValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_stringvalue() {
  if (has_stringvalue()) {
    value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
    clear_has_value();
  }
}
inline const ::std::string& CustomLayerParams_CustomLayerParamValue::stringvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    return value_.stringvalue_.GetNoArena();
  }
  return *&::google::protobuf::internal::GetEmptyStringAlreadyInited();
}
inline void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#if LANG_CXX11
inline void CustomLayerParams_CustomLayerParamValue::set_stringvalue(::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#endif
inline void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
inline void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value, size_t size) {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
inline ::std::string* CustomLayerParams_CustomLayerParamValue::mutable_stringvalue() {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  return value_.stringvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* CustomLayerParams_CustomLayerParamValue::release_stringvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    clear_has_value();
    return value_.stringvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  } else {
    return NULL;
  }
}
inline void CustomLayerParams_CustomLayerParamValue::set_allocated_stringvalue(::std::string* stringvalue) {
  if (!has_stringvalue()) {
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  clear_value();
  if (stringvalue != NULL) {
    set_has_stringvalue();
    value_.stringvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
        stringvalue);
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}

// int32 intValue = 30;
inline bool CustomLayerParams_CustomLayerParamValue::has_intvalue() const {
  return value_case() == kIntValue;
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_intvalue() {
  _oneof_case_[0] = kIntValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_intvalue() {
  if (has_intvalue()) {
    value_.intvalue_ = 0;
    clear_has_value();
  }
}
inline ::google::protobuf::int32 CustomLayerParams_CustomLayerParamValue::intvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
  if (has_intvalue()) {
    return value_.intvalue_;
  }
  return 0;
}
inline void CustomLayerParams_CustomLayerParamValue::set_intvalue(::google::protobuf::int32 value) {
  if (!has_intvalue()) {
    clear_value();
    set_has_intvalue();
  }
  value_.intvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
}

// int64 longValue = 40;
inline bool CustomLayerParams_CustomLayerParamValue::has_longvalue() const {
  return value_case() == kLongValue;
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_longvalue() {
  _oneof_case_[0] = kLongValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_longvalue() {
  if (has_longvalue()) {
    value_.longvalue_ = GOOGLE_LONGLONG(0);
    clear_has_value();
  }
}
inline ::google::protobuf::int64 CustomLayerParams_CustomLayerParamValue::longvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
  if (has_longvalue()) {
    return value_.longvalue_;
  }
  return GOOGLE_LONGLONG(0);
}
inline void CustomLayerParams_CustomLayerParamValue::set_longvalue(::google::protobuf::int64 value) {
  if (!has_longvalue()) {
    clear_value();
    set_has_longvalue();
  }
  value_.longvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
}

// bool boolValue = 50;
inline bool CustomLayerParams_CustomLayerParamValue::has_boolvalue() const {
  return value_case() == kBoolValue;
}
inline void CustomLayerParams_CustomLayerParamValue::set_has_boolvalue() {
  _oneof_case_[0] = kBoolValue;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_boolvalue() {
  if (has_boolvalue()) {
    value_.boolvalue_ = false;
    clear_has_value();
  }
}
inline bool CustomLayerParams_CustomLayerParamValue::boolvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
  if (has_boolvalue()) {
    return value_.boolvalue_;
  }
  return false;
}
inline void CustomLayerParams_CustomLayerParamValue::set_boolvalue(bool value) {
  if (!has_boolvalue()) {
    clear_value();
    set_has_boolvalue();
  }
  value_.boolvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
}

inline bool CustomLayerParams_CustomLayerParamValue::has_value() const {
  return value_case() != VALUE_NOT_SET;
}
inline void CustomLayerParams_CustomLayerParamValue::clear_has_value() {
  _oneof_case_[0] = VALUE_NOT_SET;
}
inline CustomLayerParams_CustomLayerParamValue::ValueCase CustomLayerParams_CustomLayerParamValue::value_case() const {
  return CustomLayerParams_CustomLayerParamValue::ValueCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// CustomLayerParams

// string className = 10;
inline void CustomLayerParams::clear_classname() {
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& CustomLayerParams::classname() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.className)
  return classname_.GetNoArena();
}
inline void CustomLayerParams::set_classname(const ::std::string& value) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.className)
}
#if LANG_CXX11
inline void CustomLayerParams::set_classname(::std::string&& value) {
  
  classname_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.className)
}
#endif
inline void CustomLayerParams::set_classname(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.className)
}
inline void CustomLayerParams::set_classname(const char* value, size_t size) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.className)
}
inline ::std::string* CustomLayerParams::mutable_classname() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.className)
  return classname_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* CustomLayerParams::release_classname() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.className)
  
  return classname_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void CustomLayerParams::set_allocated_classname(::std::string* classname) {
  if (classname != NULL) {
    
  } else {
    
  }
  classname_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), classname);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.className)
}

// repeated .CoreML.Specification.WeightParams weights = 20;
inline int CustomLayerParams::weights_size() const {
  return weights_.size();
}
inline void CustomLayerParams::clear_weights() {
  weights_.Clear();
}
inline const ::CoreML::Specification::WeightParams& CustomLayerParams::weights(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Get(index);
}
inline ::CoreML::Specification::WeightParams* CustomLayerParams::mutable_weights(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Mutable(index);
}
inline ::CoreML::Specification::WeightParams* CustomLayerParams::add_weights() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
CustomLayerParams::mutable_weights() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CustomLayerParams.weights)
  return &weights_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
CustomLayerParams::weights() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CustomLayerParams.weights)
  return weights_;
}

// map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
inline int CustomLayerParams::parameters_size() const {
  return parameters_.size();
}
inline void CustomLayerParams::clear_parameters() {
  parameters_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
CustomLayerParams::parameters() const {
  // @@protoc_insertion_point(field_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
CustomLayerParams::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.MutableMap();
}

// string description = 40;
inline void CustomLayerParams::clear_description() {
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& CustomLayerParams::description() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.description)
  return description_.GetNoArena();
}
inline void CustomLayerParams::set_description(const ::std::string& value) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.description)
}
#if LANG_CXX11
inline void CustomLayerParams::set_description(::std::string&& value) {
  
  description_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.description)
}
#endif
inline void CustomLayerParams::set_description(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.description)
}
inline void CustomLayerParams::set_description(const char* value, size_t size) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.description)
}
inline ::std::string* CustomLayerParams::mutable_description() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.description)
  return description_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* CustomLayerParams::release_description() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.description)
  
  return description_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void CustomLayerParams::set_allocated_description(::std::string* description) {
  if (description != NULL) {
    
  } else {
    
  }
  description_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), description);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.description)
}

// -------------------------------------------------------------------

// TransposeLayerParams

// repeated uint64 axes = 1;
inline int TransposeLayerParams::axes_size() const {
  return axes_.size();
}
inline void TransposeLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::uint64 TransposeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TransposeLayerParams.axes)
  return axes_.Get(index);
}
inline void TransposeLayerParams::set_axes(int index, ::google::protobuf::uint64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TransposeLayerParams.axes)
}
inline void TransposeLayerParams::add_axes(::google::protobuf::uint64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TransposeLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
TransposeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TransposeLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
TransposeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TransposeLayerParams.axes)
  return &axes_;
}

// -------------------------------------------------------------------

// BatchedMatMulLayerParams

// bool transposeA = 1;
inline void BatchedMatMulLayerParams::clear_transposea() {
  transposea_ = false;
}
inline bool BatchedMatMulLayerParams::transposea() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.transposeA)
  return transposea_;
}
inline void BatchedMatMulLayerParams::set_transposea(bool value) {
  
  transposea_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.transposeA)
}

// bool transposeB = 2;
inline void BatchedMatMulLayerParams::clear_transposeb() {
  transposeb_ = false;
}
inline bool BatchedMatMulLayerParams::transposeb() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.transposeB)
  return transposeb_;
}
inline void BatchedMatMulLayerParams::set_transposeb(bool value) {
  
  transposeb_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.transposeB)
}

// uint64 weightMatrixFirstDimension = 5;
inline void BatchedMatMulLayerParams::clear_weightmatrixfirstdimension() {
  weightmatrixfirstdimension_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BatchedMatMulLayerParams::weightmatrixfirstdimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixFirstDimension)
  return weightmatrixfirstdimension_;
}
inline void BatchedMatMulLayerParams::set_weightmatrixfirstdimension(::google::protobuf::uint64 value) {
  
  weightmatrixfirstdimension_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixFirstDimension)
}

// uint64 weightMatrixSecondDimension = 6;
inline void BatchedMatMulLayerParams::clear_weightmatrixseconddimension() {
  weightmatrixseconddimension_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BatchedMatMulLayerParams::weightmatrixseconddimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixSecondDimension)
  return weightmatrixseconddimension_;
}
inline void BatchedMatMulLayerParams::set_weightmatrixseconddimension(::google::protobuf::uint64 value) {
  
  weightmatrixseconddimension_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixSecondDimension)
}

// bool hasBias = 7;
inline void BatchedMatMulLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool BatchedMatMulLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.hasBias)
  return hasbias_;
}
inline void BatchedMatMulLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 8;
inline bool BatchedMatMulLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void BatchedMatMulLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void BatchedMatMulLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 9;
inline bool BatchedMatMulLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void BatchedMatMulLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void BatchedMatMulLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.bias)
}

// -------------------------------------------------------------------

// ConcatNDLayerParams

// int64 axis = 1;
inline void ConcatNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ConcatNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatNDLayerParams.axis)
  return axis_;
}
inline void ConcatNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatNDLayerParams.axis)
}

// -------------------------------------------------------------------

// SoftmaxNDLayerParams

// int64 axis = 1;
inline void SoftmaxNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 SoftmaxNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SoftmaxNDLayerParams.axis)
  return axis_;
}
inline void SoftmaxNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SoftmaxNDLayerParams.axis)
}

// -------------------------------------------------------------------

// ReverseLayerParams

// repeated bool reverseDim = 1;
inline int ReverseLayerParams::reversedim_size() const {
  return reversedim_.size();
}
inline void ReverseLayerParams::clear_reversedim() {
  reversedim_.Clear();
}
inline bool ReverseLayerParams::reversedim(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseLayerParams.reverseDim)
  return reversedim_.Get(index);
}
inline void ReverseLayerParams::set_reversedim(int index, bool value) {
  reversedim_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseLayerParams.reverseDim)
}
inline void ReverseLayerParams::add_reversedim(bool value) {
  reversedim_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReverseLayerParams.reverseDim)
}
inline const ::google::protobuf::RepeatedField< bool >&
ReverseLayerParams::reversedim() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReverseLayerParams.reverseDim)
  return reversedim_;
}
inline ::google::protobuf::RepeatedField< bool >*
ReverseLayerParams::mutable_reversedim() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReverseLayerParams.reverseDim)
  return &reversedim_;
}

// -------------------------------------------------------------------

// ReverseSeqLayerParams

// int64 batchAxis = 1;
inline void ReverseSeqLayerParams::clear_batchaxis() {
  batchaxis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ReverseSeqLayerParams::batchaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseSeqLayerParams.batchAxis)
  return batchaxis_;
}
inline void ReverseSeqLayerParams::set_batchaxis(::google::protobuf::int64 value) {
  
  batchaxis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseSeqLayerParams.batchAxis)
}

// int64 sequenceAxis = 2;
inline void ReverseSeqLayerParams::clear_sequenceaxis() {
  sequenceaxis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ReverseSeqLayerParams::sequenceaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseSeqLayerParams.sequenceAxis)
  return sequenceaxis_;
}
inline void ReverseSeqLayerParams::set_sequenceaxis(::google::protobuf::int64 value) {
  
  sequenceaxis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseSeqLayerParams.sequenceAxis)
}

// -------------------------------------------------------------------

// LoadConstantNDLayerParams

// repeated uint64 shape = 1;
inline int LoadConstantNDLayerParams::shape_size() const {
  return shape_.size();
}
inline void LoadConstantNDLayerParams::clear_shape() {
  shape_.Clear();
}
inline ::google::protobuf::uint64 LoadConstantNDLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return shape_.Get(index);
}
inline void LoadConstantNDLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
inline void LoadConstantNDLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantNDLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return shape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantNDLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
inline bool LoadConstantNDLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
inline void LoadConstantNDLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LoadConstantNDLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantNDLayerParams.data)
  return data_;
}
inline ::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantNDLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
inline void LoadConstantNDLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantNDLayerParams.data)
}

// -------------------------------------------------------------------

// FillLikeLayerParams

// float value = 1;
inline void FillLikeLayerParams::clear_value() {
  value_ = 0;
}
inline float FillLikeLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillLikeLayerParams.value)
  return value_;
}
inline void FillLikeLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillLikeLayerParams.value)
}

// -------------------------------------------------------------------

// FillStaticLayerParams

// float value = 1;
inline void FillStaticLayerParams::clear_value() {
  value_ = 0;
}
inline float FillStaticLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.value)
  return value_;
}
inline void FillStaticLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.value)
}

// repeated uint64 targetShape = 2;
inline int FillStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
inline void FillStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline ::google::protobuf::uint64 FillStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
inline void FillStaticLayerParams::set_targetshape(int index, ::google::protobuf::uint64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.targetShape)
}
inline void FillStaticLayerParams::add_targetshape(::google::protobuf::uint64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.FillStaticLayerParams.targetShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
FillStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return targetshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
FillStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return &targetshape_;
}

// -------------------------------------------------------------------

// FillDynamicLayerParams

// float value = 1;
inline void FillDynamicLayerParams::clear_value() {
  value_ = 0;
}
inline float FillDynamicLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillDynamicLayerParams.value)
  return value_;
}
inline void FillDynamicLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillDynamicLayerParams.value)
}

// -------------------------------------------------------------------

// WhereBroadcastableLayerParams

// -------------------------------------------------------------------

// SinLayerParams

// -------------------------------------------------------------------

// CosLayerParams

// -------------------------------------------------------------------

// TanLayerParams

// -------------------------------------------------------------------

// AsinLayerParams

// -------------------------------------------------------------------

// AcosLayerParams

// -------------------------------------------------------------------

// AtanLayerParams

// -------------------------------------------------------------------

// SinhLayerParams

// -------------------------------------------------------------------

// CoshLayerParams

// -------------------------------------------------------------------

// TanhLayerParams

// -------------------------------------------------------------------

// AsinhLayerParams

// -------------------------------------------------------------------

// AcoshLayerParams

// -------------------------------------------------------------------

// AtanhLayerParams

// -------------------------------------------------------------------

// PowBroadcastableLayerParams

// -------------------------------------------------------------------

// Exp2LayerParams

// -------------------------------------------------------------------

// WhereNonZeroLayerParams

// -------------------------------------------------------------------

// MatrixBandPartLayerParams

// int64 numLower = 1;
inline void MatrixBandPartLayerParams::clear_numlower() {
  numlower_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 MatrixBandPartLayerParams::numlower() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MatrixBandPartLayerParams.numLower)
  return numlower_;
}
inline void MatrixBandPartLayerParams::set_numlower(::google::protobuf::int64 value) {
  
  numlower_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MatrixBandPartLayerParams.numLower)
}

// int64 numUpper = 2;
inline void MatrixBandPartLayerParams::clear_numupper() {
  numupper_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 MatrixBandPartLayerParams::numupper() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MatrixBandPartLayerParams.numUpper)
  return numupper_;
}
inline void MatrixBandPartLayerParams::set_numupper(::google::protobuf::int64 value) {
  
  numupper_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MatrixBandPartLayerParams.numUpper)
}

// -------------------------------------------------------------------

// UpperTriangularLayerParams

// int64 k = 1;
inline void UpperTriangularLayerParams::clear_k() {
  k_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 UpperTriangularLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpperTriangularLayerParams.k)
  return k_;
}
inline void UpperTriangularLayerParams::set_k(::google::protobuf::int64 value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpperTriangularLayerParams.k)
}

// -------------------------------------------------------------------

// LowerTriangularLayerParams

// int64 k = 1;
inline void LowerTriangularLayerParams::clear_k() {
  k_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 LowerTriangularLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LowerTriangularLayerParams.k)
  return k_;
}
inline void LowerTriangularLayerParams::set_k(::google::protobuf::int64 value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LowerTriangularLayerParams.k)
}

// -------------------------------------------------------------------

// BroadcastToLikeLayerParams

// -------------------------------------------------------------------

// BroadcastToStaticLayerParams

// repeated uint64 targetShape = 1;
inline int BroadcastToStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
inline void BroadcastToStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline ::google::protobuf::uint64 BroadcastToStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
inline void BroadcastToStaticLayerParams::set_targetshape(int index, ::google::protobuf::uint64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
inline void BroadcastToStaticLayerParams::add_targetshape(::google::protobuf::uint64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BroadcastToStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return targetshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BroadcastToStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return &targetshape_;
}

// -------------------------------------------------------------------

// BroadcastToDynamicLayerParams

// -------------------------------------------------------------------

// AddBroadcastableLayerParams

// -------------------------------------------------------------------

// MaxBroadcastableLayerParams

// -------------------------------------------------------------------

// MinBroadcastableLayerParams

// -------------------------------------------------------------------

// ModBroadcastableLayerParams

// -------------------------------------------------------------------

// FloorDivBroadcastableLayerParams

// -------------------------------------------------------------------

// SubtractBroadcastableLayerParams

// -------------------------------------------------------------------

// MultiplyBroadcastableLayerParams

// -------------------------------------------------------------------

// DivideBroadcastableLayerParams

// -------------------------------------------------------------------

// GatherLayerParams

// int64 axis = 1;
inline void GatherLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 GatherLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherLayerParams.axis)
  return axis_;
}
inline void GatherLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherLayerParams.axis)
}

// -------------------------------------------------------------------

// ScatterLayerParams

// int64 axis = 1;
inline void ScatterLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ScatterLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterLayerParams.axis)
  return axis_;
}
inline void ScatterLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterLayerParams.axis)
}

// .CoreML.Specification.ScatterMode mode = 2;
inline void ScatterLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ScatterMode ScatterLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterLayerParams.mode)
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
inline void ScatterLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterLayerParams.mode)
}

// -------------------------------------------------------------------

// GatherNDLayerParams

// -------------------------------------------------------------------

// ScatterNDLayerParams

// .CoreML.Specification.ScatterMode mode = 1;
inline void ScatterNDLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ScatterMode ScatterNDLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterNDLayerParams.mode)
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
inline void ScatterNDLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterNDLayerParams.mode)
}

// -------------------------------------------------------------------

// GatherAlongAxisLayerParams

// int64 axis = 1;
inline void GatherAlongAxisLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 GatherAlongAxisLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherAlongAxisLayerParams.axis)
  return axis_;
}
inline void GatherAlongAxisLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherAlongAxisLayerParams.axis)
}

// -------------------------------------------------------------------

// ScatterAlongAxisLayerParams

// int64 axis = 1;
inline void ScatterAlongAxisLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ScatterAlongAxisLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterAlongAxisLayerParams.axis)
  return axis_;
}
inline void ScatterAlongAxisLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterAlongAxisLayerParams.axis)
}

// .CoreML.Specification.ScatterMode mode = 2;
inline void ScatterAlongAxisLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ScatterMode ScatterAlongAxisLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterAlongAxisLayerParams.mode)
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
inline void ScatterAlongAxisLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterAlongAxisLayerParams.mode)
}

// -------------------------------------------------------------------

// StackLayerParams

// int64 axis = 1;
inline void StackLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 StackLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.StackLayerParams.axis)
  return axis_;
}
inline void StackLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.StackLayerParams.axis)
}

// -------------------------------------------------------------------

// RankPreservingReshapeLayerParams

// repeated int64 targetShape = 1;
inline int RankPreservingReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
inline void RankPreservingReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline ::google::protobuf::int64 RankPreservingReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
inline void RankPreservingReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
inline void RankPreservingReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
RankPreservingReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return targetshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
RankPreservingReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return &targetshape_;
}

// -------------------------------------------------------------------

// ConstantPaddingLayerParams

// float value = 1;
inline void ConstantPaddingLayerParams::clear_value() {
  value_ = 0;
}
inline float ConstantPaddingLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.value)
  return value_;
}
inline void ConstantPaddingLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.value)
}

// repeated uint64 padAmounts = 2;
inline int ConstantPaddingLayerParams::padamounts_size() const {
  return padamounts_.size();
}
inline void ConstantPaddingLayerParams::clear_padamounts() {
  padamounts_.Clear();
}
inline ::google::protobuf::uint64 ConstantPaddingLayerParams::padamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return padamounts_.Get(index);
}
inline void ConstantPaddingLayerParams::set_padamounts(int index, ::google::protobuf::uint64 value) {
  padamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
}
inline void ConstantPaddingLayerParams::add_padamounts(::google::protobuf::uint64 value) {
  padamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConstantPaddingLayerParams::padamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return padamounts_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConstantPaddingLayerParams::mutable_padamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return &padamounts_;
}

// bool padToGivenOutputSizeMode = 3;
inline void ConstantPaddingLayerParams::clear_padtogivenoutputsizemode() {
  padtogivenoutputsizemode_ = false;
}
inline bool ConstantPaddingLayerParams::padtogivenoutputsizemode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.padToGivenOutputSizeMode)
  return padtogivenoutputsizemode_;
}
inline void ConstantPaddingLayerParams::set_padtogivenoutputsizemode(bool value) {
  
  padtogivenoutputsizemode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.padToGivenOutputSizeMode)
}

// -------------------------------------------------------------------

// RandomNormalLikeLayerParams

// int64 seed = 1;
inline void RandomNormalLikeLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomNormalLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.seed)
  return seed_;
}
inline void RandomNormalLikeLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.seed)
}

// float mean = 2;
inline void RandomNormalLikeLayerParams::clear_mean() {
  mean_ = 0;
}
inline float RandomNormalLikeLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.mean)
  return mean_;
}
inline void RandomNormalLikeLayerParams::set_mean(float value) {
  
  mean_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.mean)
}

// float stdDev = 3;
inline void RandomNormalLikeLayerParams::clear_stddev() {
  stddev_ = 0;
}
inline float RandomNormalLikeLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.stdDev)
  return stddev_;
}
inline void RandomNormalLikeLayerParams::set_stddev(float value) {
  
  stddev_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.stdDev)
}

// -------------------------------------------------------------------

// RandomNormalStaticLayerParams

// int64 seed = 1;
inline void RandomNormalStaticLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomNormalStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.seed)
  return seed_;
}
inline void RandomNormalStaticLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.seed)
}

// float mean = 2;
inline void RandomNormalStaticLayerParams::clear_mean() {
  mean_ = 0;
}
inline float RandomNormalStaticLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.mean)
  return mean_;
}
inline void RandomNormalStaticLayerParams::set_mean(float value) {
  
  mean_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.mean)
}

// float stdDev = 3;
inline void RandomNormalStaticLayerParams::clear_stddev() {
  stddev_ = 0;
}
inline float RandomNormalStaticLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.stdDev)
  return stddev_;
}
inline void RandomNormalStaticLayerParams::set_stddev(float value) {
  
  stddev_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.stdDev)
}

// repeated uint64 outputShape = 4;
inline int RandomNormalStaticLayerParams::outputshape_size() const {
  return outputshape_.size();
}
inline void RandomNormalStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline ::google::protobuf::uint64 RandomNormalStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return outputshape_.Get(index);
}
inline void RandomNormalStaticLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
}
inline void RandomNormalStaticLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
RandomNormalStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return outputshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
RandomNormalStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return &outputshape_;
}

// -------------------------------------------------------------------

// RandomNormalDynamicLayerParams

// int64 seed = 1;
inline void RandomNormalDynamicLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomNormalDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.seed)
  return seed_;
}
inline void RandomNormalDynamicLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.seed)
}

// float mean = 2;
inline void RandomNormalDynamicLayerParams::clear_mean() {
  mean_ = 0;
}
inline float RandomNormalDynamicLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.mean)
  return mean_;
}
inline void RandomNormalDynamicLayerParams::set_mean(float value) {
  
  mean_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.mean)
}

// float stdDev = 3;
inline void RandomNormalDynamicLayerParams::clear_stddev() {
  stddev_ = 0;
}
inline float RandomNormalDynamicLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.stdDev)
  return stddev_;
}
inline void RandomNormalDynamicLayerParams::set_stddev(float value) {
  
  stddev_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.stdDev)
}

// -------------------------------------------------------------------

// RandomUniformLikeLayerParams

// int64 seed = 1;
inline void RandomUniformLikeLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomUniformLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.seed)
  return seed_;
}
inline void RandomUniformLikeLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.seed)
}

// float minVal = 2;
inline void RandomUniformLikeLayerParams::clear_minval() {
  minval_ = 0;
}
inline float RandomUniformLikeLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.minVal)
  return minval_;
}
inline void RandomUniformLikeLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.minVal)
}

// float maxVal = 3;
inline void RandomUniformLikeLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float RandomUniformLikeLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.maxVal)
  return maxval_;
}
inline void RandomUniformLikeLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.maxVal)
}

// -------------------------------------------------------------------

// RandomUniformStaticLayerParams

// int64 seed = 1;
inline void RandomUniformStaticLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomUniformStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.seed)
  return seed_;
}
inline void RandomUniformStaticLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.seed)
}

// float minVal = 2;
inline void RandomUniformStaticLayerParams::clear_minval() {
  minval_ = 0;
}
inline float RandomUniformStaticLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.minVal)
  return minval_;
}
inline void RandomUniformStaticLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.minVal)
}

// float maxVal = 3;
inline void RandomUniformStaticLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float RandomUniformStaticLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.maxVal)
  return maxval_;
}
inline void RandomUniformStaticLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.maxVal)
}

// repeated uint64 outputShape = 4;
inline int RandomUniformStaticLayerParams::outputshape_size() const {
  return outputshape_.size();
}
inline void RandomUniformStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline ::google::protobuf::uint64 RandomUniformStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return outputshape_.Get(index);
}
inline void RandomUniformStaticLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
}
inline void RandomUniformStaticLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
RandomUniformStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return outputshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
RandomUniformStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return &outputshape_;
}

// -------------------------------------------------------------------

// RandomUniformDynamicLayerParams

// int64 seed = 1;
inline void RandomUniformDynamicLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomUniformDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.seed)
  return seed_;
}
inline void RandomUniformDynamicLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.seed)
}

// float minVal = 2;
inline void RandomUniformDynamicLayerParams::clear_minval() {
  minval_ = 0;
}
inline float RandomUniformDynamicLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.minVal)
  return minval_;
}
inline void RandomUniformDynamicLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.minVal)
}

// float maxVal = 3;
inline void RandomUniformDynamicLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float RandomUniformDynamicLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.maxVal)
  return maxval_;
}
inline void RandomUniformDynamicLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.maxVal)
}

// -------------------------------------------------------------------

// RandomBernoulliLikeLayerParams

// int64 seed = 1;
inline void RandomBernoulliLikeLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomBernoulliLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliLikeLayerParams.seed)
  return seed_;
}
inline void RandomBernoulliLikeLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliLikeLayerParams.seed)
}

// float prob = 2;
inline void RandomBernoulliLikeLayerParams::clear_prob() {
  prob_ = 0;
}
inline float RandomBernoulliLikeLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliLikeLayerParams.prob)
  return prob_;
}
inline void RandomBernoulliLikeLayerParams::set_prob(float value) {
  
  prob_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliLikeLayerParams.prob)
}

// -------------------------------------------------------------------

// RandomBernoulliStaticLayerParams

// int64 seed = 1;
inline void RandomBernoulliStaticLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomBernoulliStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.seed)
  return seed_;
}
inline void RandomBernoulliStaticLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.seed)
}

// float prob = 2;
inline void RandomBernoulliStaticLayerParams::clear_prob() {
  prob_ = 0;
}
inline float RandomBernoulliStaticLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.prob)
  return prob_;
}
inline void RandomBernoulliStaticLayerParams::set_prob(float value) {
  
  prob_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.prob)
}

// repeated uint64 outputShape = 3;
inline int RandomBernoulliStaticLayerParams::outputshape_size() const {
  return outputshape_.size();
}
inline void RandomBernoulliStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline ::google::protobuf::uint64 RandomBernoulliStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return outputshape_.Get(index);
}
inline void RandomBernoulliStaticLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
}
inline void RandomBernoulliStaticLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
RandomBernoulliStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return outputshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
RandomBernoulliStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return &outputshape_;
}

// -------------------------------------------------------------------

// RandomBernoulliDynamicLayerParams

// int64 seed = 1;
inline void RandomBernoulliDynamicLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 RandomBernoulliDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliDynamicLayerParams.seed)
  return seed_;
}
inline void RandomBernoulliDynamicLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliDynamicLayerParams.seed)
}

// float prob = 2;
inline void RandomBernoulliDynamicLayerParams::clear_prob() {
  prob_ = 0;
}
inline float RandomBernoulliDynamicLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliDynamicLayerParams.prob)
  return prob_;
}
inline void RandomBernoulliDynamicLayerParams::set_prob(float value) {
  
  prob_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliDynamicLayerParams.prob)
}

// -------------------------------------------------------------------

// CategoricalDistributionLayerParams

// int64 seed = 1;
inline void CategoricalDistributionLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 CategoricalDistributionLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.seed)
  return seed_;
}
inline void CategoricalDistributionLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.seed)
}

// int64 numSamples = 2;
inline void CategoricalDistributionLayerParams::clear_numsamples() {
  numsamples_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 CategoricalDistributionLayerParams::numsamples() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.numSamples)
  return numsamples_;
}
inline void CategoricalDistributionLayerParams::set_numsamples(::google::protobuf::int64 value) {
  
  numsamples_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.numSamples)
}

// bool isLogits = 3;
inline void CategoricalDistributionLayerParams::clear_islogits() {
  islogits_ = false;
}
inline bool CategoricalDistributionLayerParams::islogits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.isLogits)
  return islogits_;
}
inline void CategoricalDistributionLayerParams::set_islogits(bool value) {
  
  islogits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.isLogits)
}

// float eps = 4;
inline void CategoricalDistributionLayerParams::clear_eps() {
  eps_ = 0;
}
inline float CategoricalDistributionLayerParams::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.eps)
  return eps_;
}
inline void CategoricalDistributionLayerParams::set_eps(float value) {
  
  eps_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.eps)
}

// float temperature = 5;
inline void CategoricalDistributionLayerParams::clear_temperature() {
  temperature_ = 0;
}
inline float CategoricalDistributionLayerParams::temperature() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.temperature)
  return temperature_;
}
inline void CategoricalDistributionLayerParams::set_temperature(float value) {
  
  temperature_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.temperature)
}

// -------------------------------------------------------------------

// ReduceL1LayerParams

// repeated int64 axes = 1;
inline int ReduceL1LayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceL1LayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceL1LayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceL1LayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.axes)
}
inline void ReduceL1LayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceL1LayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceL1LayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceL1LayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceL1LayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceL1LayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceL1LayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceL1LayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.keepDims)
  return keepdims_;
}
inline void ReduceL1LayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceL1LayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceL1LayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceL1LayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceL2LayerParams

// repeated int64 axes = 1;
inline int ReduceL2LayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceL2LayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceL2LayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceL2LayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.axes)
}
inline void ReduceL2LayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceL2LayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceL2LayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceL2LayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceL2LayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceL2LayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceL2LayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceL2LayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.keepDims)
  return keepdims_;
}
inline void ReduceL2LayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceL2LayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceL2LayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceL2LayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceMaxLayerParams

// repeated int64 axes = 1;
inline int ReduceMaxLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceMaxLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceMaxLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceMaxLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.axes)
}
inline void ReduceMaxLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMaxLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceMaxLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMaxLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceMaxLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMaxLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceMaxLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceMaxLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceMaxLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceMaxLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceMaxLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceMaxLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceMinLayerParams

// repeated int64 axes = 1;
inline int ReduceMinLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceMinLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceMinLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceMinLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.axes)
}
inline void ReduceMinLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMinLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceMinLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMinLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceMinLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMinLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceMinLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceMinLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceMinLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceMinLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceMinLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceMinLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceSumLayerParams

// repeated int64 axes = 1;
inline int ReduceSumLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceSumLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceSumLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceSumLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.axes)
}
inline void ReduceSumLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceSumLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceSumLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceSumLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceSumLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceSumLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceSumLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceSumLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceSumLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceSumLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceSumLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceSumLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceProdLayerParams

// repeated int64 axes = 1;
inline int ReduceProdLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceProdLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceProdLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceProdLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.axes)
}
inline void ReduceProdLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceProdLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceProdLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceProdLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceProdLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceProdLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceProdLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceProdLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceProdLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceProdLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceProdLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceProdLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceMeanLayerParams

// repeated int64 axes = 1;
inline int ReduceMeanLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceMeanLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceMeanLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceMeanLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.axes)
}
inline void ReduceMeanLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMeanLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceMeanLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMeanLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceMeanLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMeanLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceMeanLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceMeanLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceMeanLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceMeanLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceMeanLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceMeanLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceLogSumLayerParams

// repeated int64 axes = 1;
inline int ReduceLogSumLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceLogSumLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceLogSumLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceLogSumLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.axes)
}
inline void ReduceLogSumLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceLogSumLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceLogSumLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceLogSumLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceLogSumLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceLogSumLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceLogSumLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceLogSumLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceLogSumLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceLogSumLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceSumSquareLayerParams

// repeated int64 axes = 1;
inline int ReduceSumSquareLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceSumSquareLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceSumSquareLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceSumSquareLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.axes)
}
inline void ReduceSumSquareLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceSumSquareLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceSumSquareLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceSumSquareLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceSumSquareLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceSumSquareLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceSumSquareLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceSumSquareLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceSumSquareLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceSumSquareLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ReduceLogSumExpLayerParams

// repeated int64 axes = 1;
inline int ReduceLogSumExpLayerParams::axes_size() const {
  return axes_.size();
}
inline void ReduceLogSumExpLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ReduceLogSumExpLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return axes_.Get(index);
}
inline void ReduceLogSumExpLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
}
inline void ReduceLogSumExpLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceLogSumExpLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceLogSumExpLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
inline void ReduceLogSumExpLayerParams::clear_keepdims() {
  keepdims_ = false;
}
inline bool ReduceLogSumExpLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.keepDims)
  return keepdims_;
}
inline void ReduceLogSumExpLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.keepDims)
}

// bool reduceAll = 3;
inline void ReduceLogSumExpLayerParams::clear_reduceall() {
  reduceall_ = false;
}
inline bool ReduceLogSumExpLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.reduceAll)
  return reduceall_;
}
inline void ReduceLogSumExpLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.reduceAll)
}

// -------------------------------------------------------------------

// ExpandDimsLayerParams

// repeated int64 axes = 1;
inline int ExpandDimsLayerParams::axes_size() const {
  return axes_.size();
}
inline void ExpandDimsLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 ExpandDimsLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ExpandDimsLayerParams.axes)
  return axes_.Get(index);
}
inline void ExpandDimsLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ExpandDimsLayerParams.axes)
}
inline void ExpandDimsLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ExpandDimsLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ExpandDimsLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ExpandDimsLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return &axes_;
}

// -------------------------------------------------------------------

// FlattenTo2DLayerParams

// int64 axis = 1;
inline void FlattenTo2DLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 FlattenTo2DLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenTo2DLayerParams.axis)
  return axis_;
}
inline void FlattenTo2DLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenTo2DLayerParams.axis)
}

// -------------------------------------------------------------------

// ReshapeStaticLayerParams

// repeated int64 targetShape = 1;
inline int ReshapeStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
inline void ReshapeStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline ::google::protobuf::int64 ReshapeStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
inline void ReshapeStaticLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
}
inline void ReshapeStaticLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return targetshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return &targetshape_;
}

// -------------------------------------------------------------------

// ReshapeLikeLayerParams

// -------------------------------------------------------------------

// ReshapeDynamicLayerParams

// -------------------------------------------------------------------

// SqueezeLayerParams

// repeated int64 axes = 1;
inline int SqueezeLayerParams::axes_size() const {
  return axes_.size();
}
inline void SqueezeLayerParams::clear_axes() {
  axes_.Clear();
}
inline ::google::protobuf::int64 SqueezeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.axes)
  return axes_.Get(index);
}
inline void SqueezeLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.axes)
}
inline void SqueezeLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SqueezeLayerParams.axes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SqueezeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SqueezeLayerParams.axes)
  return axes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SqueezeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SqueezeLayerParams.axes)
  return &axes_;
}

// bool squeezeAll = 2;
inline void SqueezeLayerParams::clear_squeezeall() {
  squeezeall_ = false;
}
inline bool SqueezeLayerParams::squeezeall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.squeezeAll)
  return squeezeall_;
}
inline void SqueezeLayerParams::set_squeezeall(bool value) {
  
  squeezeall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.squeezeAll)
}

// -------------------------------------------------------------------

// TopKLayerParams

// int64 axis = 1;
inline void TopKLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 TopKLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.axis)
  return axis_;
}
inline void TopKLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.axis)
}

// uint64 K = 2;
inline void TopKLayerParams::clear_k() {
  k_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 TopKLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.K)
  return k_;
}
inline void TopKLayerParams::set_k(::google::protobuf::uint64 value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.K)
}

// bool useBottomK = 3;
inline void TopKLayerParams::clear_usebottomk() {
  usebottomk_ = false;
}
inline bool TopKLayerParams::usebottomk() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.useBottomK)
  return usebottomk_;
}
inline void TopKLayerParams::set_usebottomk(bool value) {
  
  usebottomk_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.useBottomK)
}

// -------------------------------------------------------------------

// ArgMaxLayerParams

// int64 axis = 1;
inline void ArgMaxLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ArgMaxLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMaxLayerParams.axis)
  return axis_;
}
inline void ArgMaxLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMaxLayerParams.axis)
}

// bool removeDim = 2;
inline void ArgMaxLayerParams::clear_removedim() {
  removedim_ = false;
}
inline bool ArgMaxLayerParams::removedim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMaxLayerParams.removeDim)
  return removedim_;
}
inline void ArgMaxLayerParams::set_removedim(bool value) {
  
  removedim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMaxLayerParams.removeDim)
}

// -------------------------------------------------------------------

// ArgMinLayerParams

// int64 axis = 1;
inline void ArgMinLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ArgMinLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMinLayerParams.axis)
  return axis_;
}
inline void ArgMinLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMinLayerParams.axis)
}

// bool removeDim = 2;
inline void ArgMinLayerParams::clear_removedim() {
  removedim_ = false;
}
inline bool ArgMinLayerParams::removedim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMinLayerParams.removeDim)
  return removedim_;
}
inline void ArgMinLayerParams::set_removedim(bool value) {
  
  removedim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMinLayerParams.removeDim)
}

// -------------------------------------------------------------------

// SplitNDLayerParams

// int64 axis = 1;
inline void SplitNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 SplitNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.axis)
  return axis_;
}
inline void SplitNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.axis)
}

// uint64 numSplits = 2;
inline void SplitNDLayerParams::clear_numsplits() {
  numsplits_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SplitNDLayerParams::numsplits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.numSplits)
  return numsplits_;
}
inline void SplitNDLayerParams::set_numsplits(::google::protobuf::uint64 value) {
  
  numsplits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.numSplits)
}

// repeated uint64 splitSizes = 3;
inline int SplitNDLayerParams::splitsizes_size() const {
  return splitsizes_.size();
}
inline void SplitNDLayerParams::clear_splitsizes() {
  splitsizes_.Clear();
}
inline ::google::protobuf::uint64 SplitNDLayerParams::splitsizes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return splitsizes_.Get(index);
}
inline void SplitNDLayerParams::set_splitsizes(int index, ::google::protobuf::uint64 value) {
  splitsizes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
inline void SplitNDLayerParams::add_splitsizes(::google::protobuf::uint64 value) {
  splitsizes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
SplitNDLayerParams::splitsizes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return splitsizes_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
SplitNDLayerParams::mutable_splitsizes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return &splitsizes_;
}

// -------------------------------------------------------------------

// CeilLayerParams

// -------------------------------------------------------------------

// RoundLayerParams

// -------------------------------------------------------------------

// FloorLayerParams

// -------------------------------------------------------------------

// SignLayerParams

// -------------------------------------------------------------------

// ClipLayerParams

// float minVal = 1;
inline void ClipLayerParams::clear_minval() {
  minval_ = 0;
}
inline float ClipLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.minVal)
  return minval_;
}
inline void ClipLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.minVal)
}

// float maxVal = 2;
inline void ClipLayerParams::clear_maxval() {
  maxval_ = 0;
}
inline float ClipLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.maxVal)
  return maxval_;
}
inline void ClipLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.maxVal)
}

// -------------------------------------------------------------------

// SliceStaticLayerParams

// repeated int64 beginIds = 1;
inline int SliceStaticLayerParams::beginids_size() const {
  return beginids_.size();
}
inline void SliceStaticLayerParams::clear_beginids() {
  beginids_.Clear();
}
inline ::google::protobuf::int64 SliceStaticLayerParams::beginids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return beginids_.Get(index);
}
inline void SliceStaticLayerParams::set_beginids(int index, ::google::protobuf::int64 value) {
  beginids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.beginIds)
}
inline void SliceStaticLayerParams::add_beginids(::google::protobuf::int64 value) {
  beginids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.beginIds)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceStaticLayerParams::beginids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return beginids_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceStaticLayerParams::mutable_beginids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return &beginids_;
}

// repeated bool beginMasks = 2;
inline int SliceStaticLayerParams::beginmasks_size() const {
  return beginmasks_.size();
}
inline void SliceStaticLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
inline bool SliceStaticLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return beginmasks_.Get(index);
}
inline void SliceStaticLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.beginMasks)
}
inline void SliceStaticLayerParams::add_beginmasks(bool value) {
  beginmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.beginMasks)
}
inline const ::google::protobuf::RepeatedField< bool >&
SliceStaticLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return beginmasks_;
}
inline ::google::protobuf::RepeatedField< bool >*
SliceStaticLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return &beginmasks_;
}

// repeated int64 endIds = 3;
inline int SliceStaticLayerParams::endids_size() const {
  return endids_.size();
}
inline void SliceStaticLayerParams::clear_endids() {
  endids_.Clear();
}
inline ::google::protobuf::int64 SliceStaticLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.endIds)
  return endids_.Get(index);
}
inline void SliceStaticLayerParams::set_endids(int index, ::google::protobuf::int64 value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.endIds)
}
inline void SliceStaticLayerParams::add_endids(::google::protobuf::int64 value) {
  endids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.endIds)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceStaticLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.endIds)
  return endids_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceStaticLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.endIds)
  return &endids_;
}

// repeated bool endMasks = 4;
inline int SliceStaticLayerParams::endmasks_size() const {
  return endmasks_.size();
}
inline void SliceStaticLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
inline bool SliceStaticLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return endmasks_.Get(index);
}
inline void SliceStaticLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.endMasks)
}
inline void SliceStaticLayerParams::add_endmasks(bool value) {
  endmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.endMasks)
}
inline const ::google::protobuf::RepeatedField< bool >&
SliceStaticLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return endmasks_;
}
inline ::google::protobuf::RepeatedField< bool >*
SliceStaticLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return &endmasks_;
}

// repeated int64 strides = 5;
inline int SliceStaticLayerParams::strides_size() const {
  return strides_.size();
}
inline void SliceStaticLayerParams::clear_strides() {
  strides_.Clear();
}
inline ::google::protobuf::int64 SliceStaticLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.strides)
  return strides_.Get(index);
}
inline void SliceStaticLayerParams::set_strides(int index, ::google::protobuf::int64 value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.strides)
}
inline void SliceStaticLayerParams::add_strides(::google::protobuf::int64 value) {
  strides_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.strides)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceStaticLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.strides)
  return strides_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceStaticLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.strides)
  return &strides_;
}

// -------------------------------------------------------------------

// SliceDynamicLayerParams

// repeated bool beginMasks = 2;
inline int SliceDynamicLayerParams::beginmasks_size() const {
  return beginmasks_.size();
}
inline void SliceDynamicLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
inline bool SliceDynamicLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return beginmasks_.Get(index);
}
inline void SliceDynamicLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
}
inline void SliceDynamicLayerParams::add_beginmasks(bool value) {
  beginmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
}
inline const ::google::protobuf::RepeatedField< bool >&
SliceDynamicLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return beginmasks_;
}
inline ::google::protobuf::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return &beginmasks_;
}

// repeated int64 endIds = 3;
inline int SliceDynamicLayerParams::endids_size() const {
  return endids_.size();
}
inline void SliceDynamicLayerParams::clear_endids() {
  endids_.Clear();
}
inline ::google::protobuf::int64 SliceDynamicLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return endids_.Get(index);
}
inline void SliceDynamicLayerParams::set_endids(int index, ::google::protobuf::int64 value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.endIds)
}
inline void SliceDynamicLayerParams::add_endids(::google::protobuf::int64 value) {
  endids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.endIds)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceDynamicLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return endids_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceDynamicLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return &endids_;
}

// repeated bool endMasks = 4;
inline int SliceDynamicLayerParams::endmasks_size() const {
  return endmasks_.size();
}
inline void SliceDynamicLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
inline bool SliceDynamicLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return endmasks_.Get(index);
}
inline void SliceDynamicLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.endMasks)
}
inline void SliceDynamicLayerParams::add_endmasks(bool value) {
  endmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.endMasks)
}
inline const ::google::protobuf::RepeatedField< bool >&
SliceDynamicLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return endmasks_;
}
inline ::google::protobuf::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return &endmasks_;
}

// repeated int64 strides = 5;
inline int SliceDynamicLayerParams::strides_size() const {
  return strides_.size();
}
inline void SliceDynamicLayerParams::clear_strides() {
  strides_.Clear();
}
inline ::google::protobuf::int64 SliceDynamicLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.strides)
  return strides_.Get(index);
}
inline void SliceDynamicLayerParams::set_strides(int index, ::google::protobuf::int64 value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.strides)
}
inline void SliceDynamicLayerParams::add_strides(::google::protobuf::int64 value) {
  strides_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.strides)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceDynamicLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.strides)
  return strides_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceDynamicLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.strides)
  return &strides_;
}

// -------------------------------------------------------------------

// TileLayerParams

// repeated uint64 reps = 1;
inline int TileLayerParams::reps_size() const {
  return reps_.size();
}
inline void TileLayerParams::clear_reps() {
  reps_.Clear();
}
inline ::google::protobuf::uint64 TileLayerParams::reps(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TileLayerParams.reps)
  return reps_.Get(index);
}
inline void TileLayerParams::set_reps(int index, ::google::protobuf::uint64 value) {
  reps_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TileLayerParams.reps)
}
inline void TileLayerParams::add_reps(::google::protobuf::uint64 value) {
  reps_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TileLayerParams.reps)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
TileLayerParams::reps() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TileLayerParams.reps)
  return reps_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
TileLayerParams::mutable_reps() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TileLayerParams.reps)
  return &reps_;
}

// -------------------------------------------------------------------

// GetShapeLayerParams

// -------------------------------------------------------------------

// ErfLayerParams

// -------------------------------------------------------------------

// GeluLayerParams

// .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
inline void GeluLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::GeluLayerParams_GeluMode GeluLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GeluLayerParams.mode)
  return static_cast< ::CoreML::Specification::GeluLayerParams_GeluMode >(mode_);
}
inline void GeluLayerParams::set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GeluLayerParams.mode)
}

// -------------------------------------------------------------------

// RangeStaticLayerParams

// float endValue = 1;
inline void RangeStaticLayerParams::clear_endvalue() {
  endvalue_ = 0;
}
inline float RangeStaticLayerParams::endvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.endValue)
  return endvalue_;
}
inline void RangeStaticLayerParams::set_endvalue(float value) {
  
  endvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.endValue)
}

// float startValue = 2;
inline void RangeStaticLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
inline float RangeStaticLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.startValue)
  return startvalue_;
}
inline void RangeStaticLayerParams::set_startvalue(float value) {
  
  startvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.startValue)
}

// float stepSizeValue = 3;
inline void RangeStaticLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
inline float RangeStaticLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.stepSizeValue)
  return stepsizevalue_;
}
inline void RangeStaticLayerParams::set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.stepSizeValue)
}

// -------------------------------------------------------------------

// RangeDynamicLayerParams

// float startValue = 2;
inline void RangeDynamicLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
inline float RangeDynamicLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeDynamicLayerParams.startValue)
  return startvalue_;
}
inline void RangeDynamicLayerParams::set_startvalue(float value) {
  
  startvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeDynamicLayerParams.startValue)
}

// float stepSizeValue = 3;
inline void RangeDynamicLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
inline float RangeDynamicLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeDynamicLayerParams.stepSizeValue)
  return stepsizevalue_;
}
inline void RangeDynamicLayerParams::set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeDynamicLayerParams.stepSizeValue)
}

// -------------------------------------------------------------------

// SlidingWindowsLayerParams

// int64 axis = 1;
inline void SlidingWindowsLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 SlidingWindowsLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.axis)
  return axis_;
}
inline void SlidingWindowsLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.axis)
}

// uint64 windowSize = 2;
inline void SlidingWindowsLayerParams::clear_windowsize() {
  windowsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SlidingWindowsLayerParams::windowsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
  return windowsize_;
}
inline void SlidingWindowsLayerParams::set_windowsize(::google::protobuf::uint64 value) {
  
  windowsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
}

// uint64 step = 3;
inline void SlidingWindowsLayerParams::clear_step() {
  step_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SlidingWindowsLayerParams::step() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.step)
  return step_;
}
inline void SlidingWindowsLayerParams::set_step(::google::protobuf::uint64 value) {
  
  step_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.step)
}

// -------------------------------------------------------------------

// LayerNormalizationLayerParams

// repeated int64 normalizedShape = 1;
inline int LayerNormalizationLayerParams::normalizedshape_size() const {
  return normalizedshape_.size();
}
inline void LayerNormalizationLayerParams::clear_normalizedshape() {
  normalizedshape_.Clear();
}
inline ::google::protobuf::int64 LayerNormalizationLayerParams::normalizedshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return normalizedshape_.Get(index);
}
inline void LayerNormalizationLayerParams::set_normalizedshape(int index, ::google::protobuf::int64 value) {
  normalizedshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
}
inline void LayerNormalizationLayerParams::add_normalizedshape(::google::protobuf::int64 value) {
  normalizedshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
LayerNormalizationLayerParams::normalizedshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return normalizedshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
LayerNormalizationLayerParams::mutable_normalizedshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return &normalizedshape_;
}

// float eps = 2;
inline void LayerNormalizationLayerParams::clear_eps() {
  eps_ = 0;
}
inline float LayerNormalizationLayerParams::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.eps)
  return eps_;
}
inline void LayerNormalizationLayerParams::set_eps(float value) {
  
  eps_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LayerNormalizationLayerParams.eps)
}

// .CoreML.Specification.WeightParams gamma = 3;
inline bool LayerNormalizationLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
inline void LayerNormalizationLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  return gamma_;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
inline void LayerNormalizationLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 4;
inline bool LayerNormalizationLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
inline void LayerNormalizationLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LayerNormalizationLayerParams.beta)
  return beta_;
}
inline ::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LayerNormalizationLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
inline void LayerNormalizationLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.beta)
}

// -------------------------------------------------------------------

// NonMaximumSuppressionLayerParams

// float iouThreshold = 1;
inline void NonMaximumSuppressionLayerParams::clear_iouthreshold() {
  iouthreshold_ = 0;
}
inline float NonMaximumSuppressionLayerParams::iouthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.iouThreshold)
  return iouthreshold_;
}
inline void NonMaximumSuppressionLayerParams::set_iouthreshold(float value) {
  
  iouthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.iouThreshold)
}

// float scoreThreshold = 2;
inline void NonMaximumSuppressionLayerParams::clear_scorethreshold() {
  scorethreshold_ = 0;
}
inline float NonMaximumSuppressionLayerParams::scorethreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.scoreThreshold)
  return scorethreshold_;
}
inline void NonMaximumSuppressionLayerParams::set_scorethreshold(float value) {
  
  scorethreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.scoreThreshold)
}

// uint64 maxBoxes = 3;
inline void NonMaximumSuppressionLayerParams::clear_maxboxes() {
  maxboxes_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 NonMaximumSuppressionLayerParams::maxboxes() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.maxBoxes)
  return maxboxes_;
}
inline void NonMaximumSuppressionLayerParams::set_maxboxes(::google::protobuf::uint64 value) {
  
  maxboxes_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.maxBoxes)
}

// bool perClassSuppression = 4;
inline void NonMaximumSuppressionLayerParams::clear_perclasssuppression() {
  perclasssuppression_ = false;
}
inline bool NonMaximumSuppressionLayerParams::perclasssuppression() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.perClassSuppression)
  return perclasssuppression_;
}
inline void NonMaximumSuppressionLayerParams::set_perclasssuppression(bool value) {
  
  perclasssuppression_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.perClassSuppression)
}

// -------------------------------------------------------------------

// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetworkClassifier::layers_size() const {
  return layers_.size();
}
inline void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetworkClassifier::preprocessing_size() const {
  return preprocessing_.size();
}
inline void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
inline void NeuralNetworkClassifier::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkClassifier::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
inline void NeuralNetworkClassifier::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
inline void NeuralNetworkClassifier::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkClassifier::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
inline void NeuralNetworkClassifier::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
inline bool NeuralNetworkClassifier::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
inline void NeuralNetworkClassifier::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkClassifier::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return updateparams_;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
inline void NeuralNetworkClassifier::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.updateParams)
}

// .CoreML.Specification.StringVector stringClassLabels = 100;
inline bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
inline void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
inline void NeuralNetworkClassifier::clear_stringclasslabels() {
  if (has_stringclasslabels()) {
    delete ClassLabels_.stringclasslabels_;
    clear_has_ClassLabels();
  }
}
inline  const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : ::CoreML::Specification::StringVector::default_instance();
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  if (!has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = new ::CoreML::Specification::StringVector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return ClassLabels_.stringclasslabels_;
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkClassifier::set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}

// .CoreML.Specification.Int64Vector int64ClassLabels = 101;
inline bool NeuralNetworkClassifier::has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
inline void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
inline void NeuralNetworkClassifier::clear_int64classlabels() {
  if (has_int64classlabels()) {
    delete ClassLabels_.int64classlabels_;
    clear_has_ClassLabels();
  }
}
inline  const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : ::CoreML::Specification::Int64Vector::default_instance();
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  if (!has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = new ::CoreML::Specification::Int64Vector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return ClassLabels_.int64classlabels_;
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkClassifier::set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}

// string labelProbabilityLayerName = 200;
inline void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.GetNoArena();
}
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(const ::std::string& value) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#if LANG_CXX11
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(::std::string&& value) {
  
  labelprobabilitylayername_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#endif
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value, size_t size) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
inline ::std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  
  return labelprobabilitylayername_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != NULL) {
    
  } else {
    
  }
  labelprobabilitylayername_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

inline bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
inline void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
inline NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetworkRegressor::layers_size() const {
  return layers_.size();
}
inline void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetworkRegressor::preprocessing_size() const {
  return preprocessing_.size();
}
inline void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
inline void NeuralNetworkRegressor::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkRegressor::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
inline void NeuralNetworkRegressor::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
inline void NeuralNetworkRegressor::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
inline ::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkRegressor::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
inline void NeuralNetworkRegressor::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
inline bool NeuralNetworkRegressor::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
inline void NeuralNetworkRegressor::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
inline const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkRegressor::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return updateparams_;
}
inline ::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
inline void NeuralNetworkRegressor::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkRegressor.updateParams)
}

// -------------------------------------------------------------------

// NetworkUpdateParameters

// repeated .CoreML.Specification.LossLayer lossLayers = 1;
inline int NetworkUpdateParameters::losslayers_size() const {
  return losslayers_.size();
}
inline void NetworkUpdateParameters::clear_losslayers() {
  losslayers_.Clear();
}
inline const ::CoreML::Specification::LossLayer& NetworkUpdateParameters::losslayers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Get(index);
}
inline ::CoreML::Specification::LossLayer* NetworkUpdateParameters::mutable_losslayers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Mutable(index);
}
inline ::CoreML::Specification::LossLayer* NetworkUpdateParameters::add_losslayers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >*
NetworkUpdateParameters::mutable_losslayers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return &losslayers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >&
NetworkUpdateParameters::losslayers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_;
}

// .CoreML.Specification.Optimizer optimizer = 2;
inline bool NetworkUpdateParameters::has_optimizer() const {
  return this != internal_default_instance() && optimizer_ != NULL;
}
inline void NetworkUpdateParameters::clear_optimizer() {
  if (GetArenaNoVirtual() == NULL && optimizer_ != NULL) delete optimizer_;
  optimizer_ = NULL;
}
inline const ::CoreML::Specification::Optimizer& NetworkUpdateParameters::optimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return optimizer_ != NULL ? *optimizer_
                         : *::CoreML::Specification::Optimizer::internal_default_instance();
}
inline ::CoreML::Specification::Optimizer* NetworkUpdateParameters::mutable_optimizer() {
  
  if (optimizer_ == NULL) {
    optimizer_ = new ::CoreML::Specification::Optimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return optimizer_;
}
inline ::CoreML::Specification::Optimizer* NetworkUpdateParameters::release_optimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.optimizer)
  
  ::CoreML::Specification::Optimizer* temp = optimizer_;
  optimizer_ = NULL;
  return temp;
}
inline void NetworkUpdateParameters::set_allocated_optimizer(::CoreML::Specification::Optimizer* optimizer) {
  delete optimizer_;
  optimizer_ = optimizer;
  if (optimizer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.optimizer)
}

// .CoreML.Specification.Int64Parameter epochs = 3;
inline bool NetworkUpdateParameters::has_epochs() const {
  return this != internal_default_instance() && epochs_ != NULL;
}
inline void NetworkUpdateParameters::clear_epochs() {
  if (GetArenaNoVirtual() == NULL && epochs_ != NULL) delete epochs_;
  epochs_ = NULL;
}
inline const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::epochs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.epochs)
  return epochs_ != NULL ? *epochs_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_epochs() {
  
  if (epochs_ == NULL) {
    epochs_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.epochs)
  return epochs_;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_epochs() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.epochs)
  
  ::CoreML::Specification::Int64Parameter* temp = epochs_;
  epochs_ = NULL;
  return temp;
}
inline void NetworkUpdateParameters::set_allocated_epochs(::CoreML::Specification::Int64Parameter* epochs) {
  delete epochs_;
  epochs_ = epochs;
  if (epochs) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.epochs)
}

// .CoreML.Specification.BoolParameter shuffle = 10;
inline bool NetworkUpdateParameters::has_shuffle() const {
  return this != internal_default_instance() && shuffle_ != NULL;
}
inline void NetworkUpdateParameters::clear_shuffle() {
  if (GetArenaNoVirtual() == NULL && shuffle_ != NULL) delete shuffle_;
  shuffle_ = NULL;
}
inline const ::CoreML::Specification::BoolParameter& NetworkUpdateParameters::shuffle() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.shuffle)
  return shuffle_ != NULL ? *shuffle_
                         : *::CoreML::Specification::BoolParameter::internal_default_instance();
}
inline ::CoreML::Specification::BoolParameter* NetworkUpdateParameters::mutable_shuffle() {
  
  if (shuffle_ == NULL) {
    shuffle_ = new ::CoreML::Specification::BoolParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.shuffle)
  return shuffle_;
}
inline ::CoreML::Specification::BoolParameter* NetworkUpdateParameters::release_shuffle() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.shuffle)
  
  ::CoreML::Specification::BoolParameter* temp = shuffle_;
  shuffle_ = NULL;
  return temp;
}
inline void NetworkUpdateParameters::set_allocated_shuffle(::CoreML::Specification::BoolParameter* shuffle) {
  delete shuffle_;
  shuffle_ = shuffle;
  if (shuffle) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.shuffle)
}

// .CoreML.Specification.Int64Parameter seed = 20;
inline bool NetworkUpdateParameters::has_seed() const {
  return this != internal_default_instance() && seed_ != NULL;
}
inline void NetworkUpdateParameters::clear_seed() {
  if (GetArenaNoVirtual() == NULL && seed_ != NULL) delete seed_;
  seed_ = NULL;
}
inline const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.seed)
  return seed_ != NULL ? *seed_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_seed() {
  
  if (seed_ == NULL) {
    seed_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.seed)
  return seed_;
}
inline ::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_seed() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.seed)
  
  ::CoreML::Specification::Int64Parameter* temp = seed_;
  seed_ = NULL;
  return temp;
}
inline void NetworkUpdateParameters::set_allocated_seed(::CoreML::Specification::Int64Parameter* seed) {
  delete seed_;
  seed_ = seed;
  if (seed) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.seed)
}

// -------------------------------------------------------------------

// LossLayer

// string name = 1;
inline void LossLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& LossLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.name)
  return name_.GetNoArena();
}
inline void LossLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LossLayer.name)
}
#if LANG_CXX11
inline void LossLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.LossLayer.name)
}
#endif
inline void LossLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.LossLayer.name)
}
inline void LossLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.LossLayer.name)
}
inline ::std::string* LossLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* LossLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void LossLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.name)
}

// .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
inline bool LossLayer::has_categoricalcrossentropylosslayer() const {
  return LossLayerType_case() == kCategoricalCrossEntropyLossLayer;
}
inline void LossLayer::set_has_categoricalcrossentropylosslayer() {
  _oneof_case_[0] = kCategoricalCrossEntropyLossLayer;
}
inline void LossLayer::clear_categoricalcrossentropylosslayer() {
  if (has_categoricalcrossentropylosslayer()) {
    delete LossLayerType_.categoricalcrossentropylosslayer_;
    clear_has_LossLayerType();
  }
}
inline  const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& LossLayer::categoricalcrossentropylosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  return has_categoricalcrossentropylosslayer()
      ? *LossLayerType_.categoricalcrossentropylosslayer_
      : ::CoreML::Specification::CategoricalCrossEntropyLossLayer::default_instance();
}
inline ::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::mutable_categoricalcrossentropylosslayer() {
  if (!has_categoricalcrossentropylosslayer()) {
    clear_LossLayerType();
    set_has_categoricalcrossentropylosslayer();
    LossLayerType_.categoricalcrossentropylosslayer_ = new ::CoreML::Specification::CategoricalCrossEntropyLossLayer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  return LossLayerType_.categoricalcrossentropylosslayer_;
}
inline ::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::release_categoricalcrossentropylosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  if (has_categoricalcrossentropylosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::CategoricalCrossEntropyLossLayer* temp = LossLayerType_.categoricalcrossentropylosslayer_;
    LossLayerType_.categoricalcrossentropylosslayer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void LossLayer::set_allocated_categoricalcrossentropylosslayer(::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer) {
  clear_LossLayerType();
  if (categoricalcrossentropylosslayer) {
    set_has_categoricalcrossentropylosslayer();
    LossLayerType_.categoricalcrossentropylosslayer_ = categoricalcrossentropylosslayer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
}

// .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
inline bool LossLayer::has_meansquarederrorlosslayer() const {
  return LossLayerType_case() == kMeanSquaredErrorLossLayer;
}
inline void LossLayer::set_has_meansquarederrorlosslayer() {
  _oneof_case_[0] = kMeanSquaredErrorLossLayer;
}
inline void LossLayer::clear_meansquarederrorlosslayer() {
  if (has_meansquarederrorlosslayer()) {
    delete LossLayerType_.meansquarederrorlosslayer_;
    clear_has_LossLayerType();
  }
}
inline  const ::CoreML::Specification::MeanSquaredErrorLossLayer& LossLayer::meansquarederrorlosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  return has_meansquarederrorlosslayer()
      ? *LossLayerType_.meansquarederrorlosslayer_
      : ::CoreML::Specification::MeanSquaredErrorLossLayer::default_instance();
}
inline ::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::mutable_meansquarederrorlosslayer() {
  if (!has_meansquarederrorlosslayer()) {
    clear_LossLayerType();
    set_has_meansquarederrorlosslayer();
    LossLayerType_.meansquarederrorlosslayer_ = new ::CoreML::Specification::MeanSquaredErrorLossLayer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  return LossLayerType_.meansquarederrorlosslayer_;
}
inline ::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::release_meansquarederrorlosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  if (has_meansquarederrorlosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::MeanSquaredErrorLossLayer* temp = LossLayerType_.meansquarederrorlosslayer_;
    LossLayerType_.meansquarederrorlosslayer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void LossLayer::set_allocated_meansquarederrorlosslayer(::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer) {
  clear_LossLayerType();
  if (meansquarederrorlosslayer) {
    set_has_meansquarederrorlosslayer();
    LossLayerType_.meansquarederrorlosslayer_ = meansquarederrorlosslayer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
}

inline bool LossLayer::has_LossLayerType() const {
  return LossLayerType_case() != LOSSLAYERTYPE_NOT_SET;
}
inline void LossLayer::clear_has_LossLayerType() {
  _oneof_case_[0] = LOSSLAYERTYPE_NOT_SET;
}
inline LossLayer::LossLayerTypeCase LossLayer::LossLayerType_case() const {
  return LossLayer::LossLayerTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// CategoricalCrossEntropyLossLayer

// string input = 1;
inline void CategoricalCrossEntropyLossLayer::clear_input() {
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& CategoricalCrossEntropyLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return input_.GetNoArena();
}
inline void CategoricalCrossEntropyLossLayer::set_input(const ::std::string& value) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
#if LANG_CXX11
inline void CategoricalCrossEntropyLossLayer::set_input(::std::string&& value) {
  
  input_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
#endif
inline void CategoricalCrossEntropyLossLayer::set_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
inline void CategoricalCrossEntropyLossLayer::set_input(const char* value, size_t size) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
inline ::std::string* CategoricalCrossEntropyLossLayer::mutable_input() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return input_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* CategoricalCrossEntropyLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  
  return input_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void CategoricalCrossEntropyLossLayer::set_allocated_input(::std::string* input) {
  if (input != NULL) {
    
  } else {
    
  }
  input_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}

// string target = 2;
inline void CategoricalCrossEntropyLossLayer::clear_target() {
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& CategoricalCrossEntropyLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return target_.GetNoArena();
}
inline void CategoricalCrossEntropyLossLayer::set_target(const ::std::string& value) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
#if LANG_CXX11
inline void CategoricalCrossEntropyLossLayer::set_target(::std::string&& value) {
  
  target_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
#endif
inline void CategoricalCrossEntropyLossLayer::set_target(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
inline void CategoricalCrossEntropyLossLayer::set_target(const char* value, size_t size) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
inline ::std::string* CategoricalCrossEntropyLossLayer::mutable_target() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return target_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* CategoricalCrossEntropyLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  
  return target_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void CategoricalCrossEntropyLossLayer::set_allocated_target(::std::string* target) {
  if (target != NULL) {
    
  } else {
    
  }
  target_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), target);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}

// -------------------------------------------------------------------

// MeanSquaredErrorLossLayer

// string input = 1;
inline void MeanSquaredErrorLossLayer::clear_input() {
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& MeanSquaredErrorLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return input_.GetNoArena();
}
inline void MeanSquaredErrorLossLayer::set_input(const ::std::string& value) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
#if LANG_CXX11
inline void MeanSquaredErrorLossLayer::set_input(::std::string&& value) {
  
  input_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
#endif
inline void MeanSquaredErrorLossLayer::set_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
inline void MeanSquaredErrorLossLayer::set_input(const char* value, size_t size) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
inline ::std::string* MeanSquaredErrorLossLayer::mutable_input() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return input_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* MeanSquaredErrorLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  
  return input_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void MeanSquaredErrorLossLayer::set_allocated_input(::std::string* input) {
  if (input != NULL) {
    
  } else {
    
  }
  input_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}

// string target = 2;
inline void MeanSquaredErrorLossLayer::clear_target() {
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& MeanSquaredErrorLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return target_.GetNoArena();
}
inline void MeanSquaredErrorLossLayer::set_target(const ::std::string& value) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
#if LANG_CXX11
inline void MeanSquaredErrorLossLayer::set_target(::std::string&& value) {
  
  target_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
#endif
inline void MeanSquaredErrorLossLayer::set_target(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
inline void MeanSquaredErrorLossLayer::set_target(const char* value, size_t size) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
inline ::std::string* MeanSquaredErrorLossLayer::mutable_target() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return target_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* MeanSquaredErrorLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  
  return target_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void MeanSquaredErrorLossLayer::set_allocated_target(::std::string* target) {
  if (target != NULL) {
    
  } else {
    
  }
  target_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), target);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}

// -------------------------------------------------------------------

// Optimizer

// .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
inline bool Optimizer::has_sgdoptimizer() const {
  return OptimizerType_case() == kSgdOptimizer;
}
inline void Optimizer::set_has_sgdoptimizer() {
  _oneof_case_[0] = kSgdOptimizer;
}
inline void Optimizer::clear_sgdoptimizer() {
  if (has_sgdoptimizer()) {
    delete OptimizerType_.sgdoptimizer_;
    clear_has_OptimizerType();
  }
}
inline  const ::CoreML::Specification::SGDOptimizer& Optimizer::sgdoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.sgdOptimizer)
  return has_sgdoptimizer()
      ? *OptimizerType_.sgdoptimizer_
      : ::CoreML::Specification::SGDOptimizer::default_instance();
}
inline ::CoreML::Specification::SGDOptimizer* Optimizer::mutable_sgdoptimizer() {
  if (!has_sgdoptimizer()) {
    clear_OptimizerType();
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = new ::CoreML::Specification::SGDOptimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.sgdOptimizer)
  return OptimizerType_.sgdoptimizer_;
}
inline ::CoreML::Specification::SGDOptimizer* Optimizer::release_sgdoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.sgdOptimizer)
  if (has_sgdoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::SGDOptimizer* temp = OptimizerType_.sgdoptimizer_;
    OptimizerType_.sgdoptimizer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void Optimizer::set_allocated_sgdoptimizer(::CoreML::Specification::SGDOptimizer* sgdoptimizer) {
  clear_OptimizerType();
  if (sgdoptimizer) {
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = sgdoptimizer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Optimizer.sgdOptimizer)
}

// .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
inline bool Optimizer::has_adamoptimizer() const {
  return OptimizerType_case() == kAdamOptimizer;
}
inline void Optimizer::set_has_adamoptimizer() {
  _oneof_case_[0] = kAdamOptimizer;
}
inline void Optimizer::clear_adamoptimizer() {
  if (has_adamoptimizer()) {
    delete OptimizerType_.adamoptimizer_;
    clear_has_OptimizerType();
  }
}
inline  const ::CoreML::Specification::AdamOptimizer& Optimizer::adamoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.adamOptimizer)
  return has_adamoptimizer()
      ? *OptimizerType_.adamoptimizer_
      : ::CoreML::Specification::AdamOptimizer::default_instance();
}
inline ::CoreML::Specification::AdamOptimizer* Optimizer::mutable_adamoptimizer() {
  if (!has_adamoptimizer()) {
    clear_OptimizerType();
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = new ::CoreML::Specification::AdamOptimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.adamOptimizer)
  return OptimizerType_.adamoptimizer_;
}
inline ::CoreML::Specification::AdamOptimizer* Optimizer::release_adamoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.adamOptimizer)
  if (has_adamoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::AdamOptimizer* temp = OptimizerType_.adamoptimizer_;
    OptimizerType_.adamoptimizer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void Optimizer::set_allocated_adamoptimizer(::CoreML::Specification::AdamOptimizer* adamoptimizer) {
  clear_OptimizerType();
  if (adamoptimizer) {
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = adamoptimizer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Optimizer.adamOptimizer)
}

inline bool Optimizer::has_OptimizerType() const {
  return OptimizerType_case() != OPTIMIZERTYPE_NOT_SET;
}
inline void Optimizer::clear_has_OptimizerType() {
  _oneof_case_[0] = OPTIMIZERTYPE_NOT_SET;
}
inline Optimizer::OptimizerTypeCase Optimizer::OptimizerType_case() const {
  return Optimizer::OptimizerTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// SGDOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
inline bool SGDOptimizer::has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != NULL;
}
inline void SGDOptimizer::clear_learningrate() {
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) delete learningrate_;
  learningrate_ = NULL;
}
inline const ::CoreML::Specification::DoubleParameter& SGDOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.learningRate)
  return learningrate_ != NULL ? *learningrate_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_learningrate() {
  
  if (learningrate_ == NULL) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.learningRate)
  return learningrate_;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = NULL;
  return temp;
}
inline void SGDOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  delete learningrate_;
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
inline bool SGDOptimizer::has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != NULL;
}
inline void SGDOptimizer::clear_minibatchsize() {
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) delete minibatchsize_;
  minibatchsize_ = NULL;
}
inline const ::CoreML::Specification::Int64Parameter& SGDOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return minibatchsize_ != NULL ? *minibatchsize_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
inline ::CoreML::Specification::Int64Parameter* SGDOptimizer::mutable_minibatchsize() {
  
  if (minibatchsize_ == NULL) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return minibatchsize_;
}
inline ::CoreML::Specification::Int64Parameter* SGDOptimizer::release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = NULL;
  return temp;
}
inline void SGDOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  delete minibatchsize_;
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter momentum = 3;
inline bool SGDOptimizer::has_momentum() const {
  return this != internal_default_instance() && momentum_ != NULL;
}
inline void SGDOptimizer::clear_momentum() {
  if (GetArenaNoVirtual() == NULL && momentum_ != NULL) delete momentum_;
  momentum_ = NULL;
}
inline const ::CoreML::Specification::DoubleParameter& SGDOptimizer::momentum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.momentum)
  return momentum_ != NULL ? *momentum_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_momentum() {
  
  if (momentum_ == NULL) {
    momentum_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.momentum)
  return momentum_;
}
inline ::CoreML::Specification::DoubleParameter* SGDOptimizer::release_momentum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.momentum)
  
  ::CoreML::Specification::DoubleParameter* temp = momentum_;
  momentum_ = NULL;
  return temp;
}
inline void SGDOptimizer::set_allocated_momentum(::CoreML::Specification::DoubleParameter* momentum) {
  delete momentum_;
  momentum_ = momentum;
  if (momentum) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.momentum)
}

// -------------------------------------------------------------------

// AdamOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
inline bool AdamOptimizer::has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != NULL;
}
inline void AdamOptimizer::clear_learningrate() {
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) delete learningrate_;
  learningrate_ = NULL;
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.learningRate)
  return learningrate_ != NULL ? *learningrate_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_learningrate() {
  
  if (learningrate_ == NULL) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.learningRate)
  return learningrate_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = NULL;
  return temp;
}
inline void AdamOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  delete learningrate_;
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
inline bool AdamOptimizer::has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != NULL;
}
inline void AdamOptimizer::clear_minibatchsize() {
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) delete minibatchsize_;
  minibatchsize_ = NULL;
}
inline const ::CoreML::Specification::Int64Parameter& AdamOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return minibatchsize_ != NULL ? *minibatchsize_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
inline ::CoreML::Specification::Int64Parameter* AdamOptimizer::mutable_minibatchsize() {
  
  if (minibatchsize_ == NULL) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return minibatchsize_;
}
inline ::CoreML::Specification::Int64Parameter* AdamOptimizer::release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = NULL;
  return temp;
}
inline void AdamOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  delete minibatchsize_;
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter beta1 = 3;
inline bool AdamOptimizer::has_beta1() const {
  return this != internal_default_instance() && beta1_ != NULL;
}
inline void AdamOptimizer::clear_beta1() {
  if (GetArenaNoVirtual() == NULL && beta1_ != NULL) delete beta1_;
  beta1_ = NULL;
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta1)
  return beta1_ != NULL ? *beta1_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta1() {
  
  if (beta1_ == NULL) {
    beta1_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta1)
  return beta1_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta1)
  
  ::CoreML::Specification::DoubleParameter* temp = beta1_;
  beta1_ = NULL;
  return temp;
}
inline void AdamOptimizer::set_allocated_beta1(::CoreML::Specification::DoubleParameter* beta1) {
  delete beta1_;
  beta1_ = beta1;
  if (beta1) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta1)
}

// .CoreML.Specification.DoubleParameter beta2 = 4;
inline bool AdamOptimizer::has_beta2() const {
  return this != internal_default_instance() && beta2_ != NULL;
}
inline void AdamOptimizer::clear_beta2() {
  if (GetArenaNoVirtual() == NULL && beta2_ != NULL) delete beta2_;
  beta2_ = NULL;
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta2)
  return beta2_ != NULL ? *beta2_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta2() {
  
  if (beta2_ == NULL) {
    beta2_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta2)
  return beta2_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta2)
  
  ::CoreML::Specification::DoubleParameter* temp = beta2_;
  beta2_ = NULL;
  return temp;
}
inline void AdamOptimizer::set_allocated_beta2(::CoreML::Specification::DoubleParameter* beta2) {
  delete beta2_;
  beta2_ = beta2;
  if (beta2) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta2)
}

// .CoreML.Specification.DoubleParameter eps = 5;
inline bool AdamOptimizer::has_eps() const {
  return this != internal_default_instance() && eps_ != NULL;
}
inline void AdamOptimizer::clear_eps() {
  if (GetArenaNoVirtual() == NULL && eps_ != NULL) delete eps_;
  eps_ = NULL;
}
inline const ::CoreML::Specification::DoubleParameter& AdamOptimizer::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.eps)
  return eps_ != NULL ? *eps_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_eps() {
  
  if (eps_ == NULL) {
    eps_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.eps)
  return eps_;
}
inline ::CoreML::Specification::DoubleParameter* AdamOptimizer::release_eps() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.eps)
  
  ::CoreML::Specification::DoubleParameter* temp = eps_;
  eps_ = NULL;
  return temp;
}
inline void AdamOptimizer::set_allocated_eps(::CoreML::Specification::DoubleParameter* eps) {
  delete eps_;
  eps_ = eps;
  if (eps) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.eps)
}

#endif  // !PROTOBUF_INLINE_NOT_IN_HEADERS
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)


}  // namespace Specification
}  // namespace CoreML

#ifndef SWIG
namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::CoreML::Specification::SamePadding_SamePaddingMode> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::SamplingMode_Method> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::BoxCoordinatesMode_Coordinates> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::PoolingLayerParams_PoolingType> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::UnaryFunctionLayerParams_Operation> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::FlattenLayerParams_FlattenOrder> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::SliceLayerParams_SliceAxis> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReduceLayerParams_ReduceOperation> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReduceLayerParams_ReduceAxis> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::GeluLayerParams_GeluMode> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::NeuralNetworkImageShapeMapping> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ScatterMode> : ::google::protobuf::internal::true_type {};

}  // namespace protobuf
}  // namespace google
#endif  // SWIG

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_NeuralNetwork_2eproto__INCLUDED
