// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "NeuralNetwork.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/io/zero_copy_stream_impl_lite.h>
// @@protoc_insertion_point(includes)

namespace CoreML {
namespace Specification {
class NeuralNetworkDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetwork> {
} _NeuralNetwork_default_instance_;
class NeuralNetworkImageScalerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkImageScaler> {
} _NeuralNetworkImageScaler_default_instance_;
class NeuralNetworkMeanImageDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkMeanImage> {
} _NeuralNetworkMeanImage_default_instance_;
class NeuralNetworkPreprocessingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkPreprocessing> {
  public:
  const ::CoreML::Specification::NeuralNetworkImageScaler* scaler_;
  const ::CoreML::Specification::NeuralNetworkMeanImage* meanimage_;
} _NeuralNetworkPreprocessing_default_instance_;
class ActivationReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationReLU> {
} _ActivationReLU_default_instance_;
class ActivationLeakyReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationLeakyReLU> {
} _ActivationLeakyReLU_default_instance_;
class ActivationTanhDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationTanh> {
} _ActivationTanh_default_instance_;
class ActivationScaledTanhDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationScaledTanh> {
} _ActivationScaledTanh_default_instance_;
class ActivationSigmoidDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoid> {
} _ActivationSigmoid_default_instance_;
class ActivationLinearDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationLinear> {
} _ActivationLinear_default_instance_;
class ActivationSigmoidHardDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoidHard> {
} _ActivationSigmoidHard_default_instance_;
class ActivationPReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationPReLU> {
} _ActivationPReLU_default_instance_;
class ActivationELUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationELU> {
} _ActivationELU_default_instance_;
class ActivationThresholdedReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationThresholdedReLU> {
} _ActivationThresholdedReLU_default_instance_;
class ActivationSoftsignDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftsign> {
} _ActivationSoftsign_default_instance_;
class ActivationSoftplusDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftplus> {
} _ActivationSoftplus_default_instance_;
class ActivationParametricSoftplusDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationParametricSoftplus> {
} _ActivationParametricSoftplus_default_instance_;
class ActivationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationParams> {
  public:
  const ::CoreML::Specification::ActivationLinear* linear_;
  const ::CoreML::Specification::ActivationReLU* relu_;
  const ::CoreML::Specification::ActivationLeakyReLU* leakyrelu_;
  const ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu_;
  const ::CoreML::Specification::ActivationPReLU* prelu_;
  const ::CoreML::Specification::ActivationTanh* tanh_;
  const ::CoreML::Specification::ActivationScaledTanh* scaledtanh_;
  const ::CoreML::Specification::ActivationSigmoid* sigmoid_;
  const ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard_;
  const ::CoreML::Specification::ActivationELU* elu_;
  const ::CoreML::Specification::ActivationSoftsign* softsign_;
  const ::CoreML::Specification::ActivationSoftplus* softplus_;
  const ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus_;
} _ActivationParams_default_instance_;
class TensorDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<Tensor> {
} _Tensor_default_instance_;
class NeuralNetworkLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkLayer> {
  public:
  const ::CoreML::Specification::ConvolutionLayerParams* convolution_;
  const ::CoreML::Specification::PoolingLayerParams* pooling_;
  const ::CoreML::Specification::ActivationParams* activation_;
  const ::CoreML::Specification::InnerProductLayerParams* innerproduct_;
  const ::CoreML::Specification::EmbeddingLayerParams* embedding_;
  const ::CoreML::Specification::BatchnormLayerParams* batchnorm_;
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn_;
  const ::CoreML::Specification::L2NormalizeLayerParams* l2normalize_;
  const ::CoreML::Specification::SoftmaxLayerParams* softmax_;
  const ::CoreML::Specification::LRNLayerParams* lrn_;
  const ::CoreML::Specification::CropLayerParams* crop_;
  const ::CoreML::Specification::PaddingLayerParams* padding_;
  const ::CoreML::Specification::UpsampleLayerParams* upsample_;
  const ::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear_;
  const ::CoreML::Specification::CropResizeLayerParams* cropresize_;
  const ::CoreML::Specification::UnaryFunctionLayerParams* unary_;
  const ::CoreML::Specification::AddLayerParams* add_;
  const ::CoreML::Specification::MultiplyLayerParams* multiply_;
  const ::CoreML::Specification::AverageLayerParams* average_;
  const ::CoreML::Specification::ScaleLayerParams* scale_;
  const ::CoreML::Specification::BiasLayerParams* bias_;
  const ::CoreML::Specification::MaxLayerParams* max_;
  const ::CoreML::Specification::MinLayerParams* min_;
  const ::CoreML::Specification::DotProductLayerParams* dot_;
  const ::CoreML::Specification::ReduceLayerParams* reduce_;
  const ::CoreML::Specification::LoadConstantLayerParams* loadconstant_;
  const ::CoreML::Specification::ReshapeLayerParams* reshape_;
  const ::CoreML::Specification::FlattenLayerParams* flatten_;
  const ::CoreML::Specification::PermuteLayerParams* permute_;
  const ::CoreML::Specification::ConcatLayerParams* concat_;
  const ::CoreML::Specification::SplitLayerParams* split_;
  const ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat_;
  const ::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata_;
  const ::CoreML::Specification::SliceLayerParams* slice_;
  const ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent_;
  const ::CoreML::Specification::GRULayerParams* gru_;
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm_;
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm_;
  const ::CoreML::Specification::CustomLayerParams* custom_;
  const ::CoreML::Specification::CopyLayerParams* copy_;
  const ::CoreML::Specification::BranchLayerParams* branch_;
  const ::CoreML::Specification::LoopLayerParams* loop_;
  const ::CoreML::Specification::LoopBreakLayerParams* loopbreak_;
  const ::CoreML::Specification::LoopContinueLayerParams* loopcontinue_;
  const ::CoreML::Specification::RangeStaticLayerParams* rangestatic_;
  const ::CoreML::Specification::RangeDynamicLayerParams* rangedynamic_;
  const ::CoreML::Specification::ClipLayerParams* clip_;
  const ::CoreML::Specification::CeilLayerParams* ceil_;
  const ::CoreML::Specification::FloorLayerParams* floor_;
  const ::CoreML::Specification::SignLayerParams* sign_;
  const ::CoreML::Specification::RoundLayerParams* round_;
  const ::CoreML::Specification::Exp2LayerParams* exp2_;
  const ::CoreML::Specification::SinLayerParams* sin_;
  const ::CoreML::Specification::CosLayerParams* cos_;
  const ::CoreML::Specification::TanLayerParams* tan_;
  const ::CoreML::Specification::AsinLayerParams* asin_;
  const ::CoreML::Specification::AcosLayerParams* acos_;
  const ::CoreML::Specification::AtanLayerParams* atan_;
  const ::CoreML::Specification::SinhLayerParams* sinh_;
  const ::CoreML::Specification::CoshLayerParams* cosh_;
  const ::CoreML::Specification::TanhLayerParams* tanh_;
  const ::CoreML::Specification::AsinhLayerParams* asinh_;
  const ::CoreML::Specification::AcoshLayerParams* acosh_;
  const ::CoreML::Specification::AtanhLayerParams* atanh_;
  const ::CoreML::Specification::ErfLayerParams* erf_;
  const ::CoreML::Specification::GeluLayerParams* gelu_;
  const ::CoreML::Specification::EqualLayerParams* equal_;
  const ::CoreML::Specification::NotEqualLayerParams* notequal_;
  const ::CoreML::Specification::LessThanLayerParams* lessthan_;
  const ::CoreML::Specification::LessEqualLayerParams* lessequal_;
  const ::CoreML::Specification::GreaterThanLayerParams* greaterthan_;
  const ::CoreML::Specification::GreaterEqualLayerParams* greaterequal_;
  const ::CoreML::Specification::LogicalOrLayerParams* logicalor_;
  const ::CoreML::Specification::LogicalXorLayerParams* logicalxor_;
  const ::CoreML::Specification::LogicalNotLayerParams* logicalnot_;
  const ::CoreML::Specification::LogicalAndLayerParams* logicaland_;
  const ::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable_;
  const ::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable_;
  const ::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable_;
  const ::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable_;
  const ::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable_;
  const ::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable_;
  const ::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable_;
  const ::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable_;
  const ::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable_;
  const ::CoreML::Specification::TileLayerParams* tile_;
  const ::CoreML::Specification::StackLayerParams* stack_;
  const ::CoreML::Specification::GatherLayerParams* gather_;
  const ::CoreML::Specification::ScatterLayerParams* scatter_;
  const ::CoreML::Specification::GatherNDLayerParams* gathernd_;
  const ::CoreML::Specification::ScatterNDLayerParams* scatternd_;
  const ::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd_;
  const ::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis_;
  const ::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis_;
  const ::CoreML::Specification::ReverseLayerParams* reverse_;
  const ::CoreML::Specification::ReverseSeqLayerParams* reverseseq_;
  const ::CoreML::Specification::SplitNDLayerParams* splitnd_;
  const ::CoreML::Specification::ConcatNDLayerParams* concatnd_;
  const ::CoreML::Specification::TransposeLayerParams* transpose_;
  const ::CoreML::Specification::SliceStaticLayerParams* slicestatic_;
  const ::CoreML::Specification::SliceDynamicLayerParams* slicedynamic_;
  const ::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows_;
  const ::CoreML::Specification::TopKLayerParams* topk_;
  const ::CoreML::Specification::ArgMinLayerParams* argmin_;
  const ::CoreML::Specification::ArgMaxLayerParams* argmax_;
  const ::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd_;
  const ::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul_;
  const ::CoreML::Specification::GetShapeLayerParams* getshape_;
  const ::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd_;
  const ::CoreML::Specification::FillLikeLayerParams* filllike_;
  const ::CoreML::Specification::FillStaticLayerParams* fillstatic_;
  const ::CoreML::Specification::FillDynamicLayerParams* filldynamic_;
  const ::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike_;
  const ::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic_;
  const ::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic_;
  const ::CoreML::Specification::SqueezeLayerParams* squeeze_;
  const ::CoreML::Specification::ExpandDimsLayerParams* expanddims_;
  const ::CoreML::Specification::FlattenTo2DLayerParams* flattento2d_;
  const ::CoreML::Specification::ReshapeLikeLayerParams* reshapelike_;
  const ::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic_;
  const ::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic_;
  const ::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape_;
  const ::CoreML::Specification::ConstantPaddingLayerParams* constantpad_;
  const ::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike_;
  const ::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic_;
  const ::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic_;
  const ::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike_;
  const ::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic_;
  const ::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic_;
  const ::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike_;
  const ::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic_;
  const ::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic_;
  const ::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution_;
  const ::CoreML::Specification::ReduceL1LayerParams* reducel1_;
  const ::CoreML::Specification::ReduceL2LayerParams* reducel2_;
  const ::CoreML::Specification::ReduceMaxLayerParams* reducemax_;
  const ::CoreML::Specification::ReduceMinLayerParams* reducemin_;
  const ::CoreML::Specification::ReduceSumLayerParams* reducesum_;
  const ::CoreML::Specification::ReduceProdLayerParams* reduceprod_;
  const ::CoreML::Specification::ReduceMeanLayerParams* reducemean_;
  const ::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum_;
  const ::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare_;
  const ::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp_;
  const ::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero_;
  const ::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart_;
  const ::CoreML::Specification::LowerTriangularLayerParams* lowertriangular_;
  const ::CoreML::Specification::UpperTriangularLayerParams* uppertriangular_;
  const ::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable_;
  const ::CoreML::Specification::LayerNormalizationLayerParams* layernormalization_;
  const ::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression_;
} _NeuralNetworkLayer_default_instance_;
class BranchLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BranchLayerParams> {
} _BranchLayerParams_default_instance_;
class LoopLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoopLayerParams> {
} _LoopLayerParams_default_instance_;
class LoopBreakLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoopBreakLayerParams> {
} _LoopBreakLayerParams_default_instance_;
class LoopContinueLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoopContinueLayerParams> {
} _LoopContinueLayerParams_default_instance_;
class CopyLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CopyLayerParams> {
} _CopyLayerParams_default_instance_;
class GreaterThanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GreaterThanLayerParams> {
} _GreaterThanLayerParams_default_instance_;
class GreaterEqualLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GreaterEqualLayerParams> {
} _GreaterEqualLayerParams_default_instance_;
class LessThanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LessThanLayerParams> {
} _LessThanLayerParams_default_instance_;
class LessEqualLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LessEqualLayerParams> {
} _LessEqualLayerParams_default_instance_;
class EqualLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EqualLayerParams> {
} _EqualLayerParams_default_instance_;
class NotEqualLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NotEqualLayerParams> {
} _NotEqualLayerParams_default_instance_;
class LogicalAndLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalAndLayerParams> {
} _LogicalAndLayerParams_default_instance_;
class LogicalOrLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalOrLayerParams> {
} _LogicalOrLayerParams_default_instance_;
class LogicalXorLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalXorLayerParams> {
} _LogicalXorLayerParams_default_instance_;
class LogicalNotLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalNotLayerParams> {
} _LogicalNotLayerParams_default_instance_;
class BorderAmounts_EdgeSizesDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts_EdgeSizes> {
} _BorderAmounts_EdgeSizes_default_instance_;
class BorderAmountsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts> {
} _BorderAmounts_default_instance_;
class ValidPaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ValidPadding> {
} _ValidPadding_default_instance_;
class SamePaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SamePadding> {
} _SamePadding_default_instance_;
class SamplingModeDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SamplingMode> {
} _SamplingMode_default_instance_;
class BoxCoordinatesModeDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BoxCoordinatesMode> {
} _BoxCoordinatesMode_default_instance_;
class WeightParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<WeightParams> {
} _WeightParams_default_instance_;
class QuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<QuantizationParams> {
  public:
  const ::CoreML::Specification::LinearQuantizationParams* linearquantization_;
  const ::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization_;
} _QuantizationParams_default_instance_;
class LinearQuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LinearQuantizationParams> {
} _LinearQuantizationParams_default_instance_;
class LookUpTableQuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LookUpTableQuantizationParams> {
} _LookUpTableQuantizationParams_default_instance_;
class ConvolutionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConvolutionLayerParams> {
  public:
  const ::CoreML::Specification::ValidPadding* valid_;
  const ::CoreML::Specification::SamePadding* same_;
} _ConvolutionLayerParams_default_instance_;
class InnerProductLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<InnerProductLayerParams> {
} _InnerProductLayerParams_default_instance_;
class EmbeddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EmbeddingLayerParams> {
} _EmbeddingLayerParams_default_instance_;
class EmbeddingNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EmbeddingNDLayerParams> {
} _EmbeddingNDLayerParams_default_instance_;
class BatchnormLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BatchnormLayerParams> {
} _BatchnormLayerParams_default_instance_;
class PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams_ValidCompletePadding> {
} _PoolingLayerParams_ValidCompletePadding_default_instance_;
class PoolingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams> {
  public:
  const ::CoreML::Specification::ValidPadding* valid_;
  const ::CoreML::Specification::SamePadding* same_;
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel_;
} _PoolingLayerParams_default_instance_;
class PaddingLayerParams_PaddingConstantDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingConstant> {
} _PaddingLayerParams_PaddingConstant_default_instance_;
class PaddingLayerParams_PaddingReflectionDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReflection> {
} _PaddingLayerParams_PaddingReflection_default_instance_;
class PaddingLayerParams_PaddingReplicationDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReplication> {
} _PaddingLayerParams_PaddingReplication_default_instance_;
class PaddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams> {
  public:
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant_;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection_;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication_;
} _PaddingLayerParams_default_instance_;
class ConcatLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConcatLayerParams> {
} _ConcatLayerParams_default_instance_;
class LRNLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LRNLayerParams> {
} _LRNLayerParams_default_instance_;
class SoftmaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SoftmaxLayerParams> {
} _SoftmaxLayerParams_default_instance_;
class SplitLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SplitLayerParams> {
} _SplitLayerParams_default_instance_;
class AddLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AddLayerParams> {
} _AddLayerParams_default_instance_;
class MultiplyLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MultiplyLayerParams> {
} _MultiplyLayerParams_default_instance_;
class UnaryFunctionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UnaryFunctionLayerParams> {
} _UnaryFunctionLayerParams_default_instance_;
class UpsampleLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UpsampleLayerParams> {
} _UpsampleLayerParams_default_instance_;
class ResizeBilinearLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ResizeBilinearLayerParams> {
} _ResizeBilinearLayerParams_default_instance_;
class CropResizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CropResizeLayerParams> {
} _CropResizeLayerParams_default_instance_;
class BiasLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BiasLayerParams> {
} _BiasLayerParams_default_instance_;
class ScaleLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScaleLayerParams> {
} _ScaleLayerParams_default_instance_;
class LoadConstantLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoadConstantLayerParams> {
} _LoadConstantLayerParams_default_instance_;
class L2NormalizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<L2NormalizeLayerParams> {
} _L2NormalizeLayerParams_default_instance_;
class FlattenLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FlattenLayerParams> {
} _FlattenLayerParams_default_instance_;
class ReshapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReshapeLayerParams> {
} _ReshapeLayerParams_default_instance_;
class PermuteLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PermuteLayerParams> {
} _PermuteLayerParams_default_instance_;
class ReorganizeDataLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReorganizeDataLayerParams> {
} _ReorganizeDataLayerParams_default_instance_;
class SliceLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SliceLayerParams> {
} _SliceLayerParams_default_instance_;
class ReduceLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceLayerParams> {
} _ReduceLayerParams_default_instance_;
class CropLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CropLayerParams> {
} _CropLayerParams_default_instance_;
class AverageLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AverageLayerParams> {
} _AverageLayerParams_default_instance_;
class MaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MaxLayerParams> {
} _MaxLayerParams_default_instance_;
class MinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MinLayerParams> {
} _MinLayerParams_default_instance_;
class DotProductLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<DotProductLayerParams> {
} _DotProductLayerParams_default_instance_;
class MeanVarianceNormalizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MeanVarianceNormalizeLayerParams> {
} _MeanVarianceNormalizeLayerParams_default_instance_;
class SequenceRepeatLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SequenceRepeatLayerParams> {
} _SequenceRepeatLayerParams_default_instance_;
class SimpleRecurrentLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SimpleRecurrentLayerParams> {
} _SimpleRecurrentLayerParams_default_instance_;
class GRULayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GRULayerParams> {
} _GRULayerParams_default_instance_;
class LSTMParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LSTMParams> {
} _LSTMParams_default_instance_;
class LSTMWeightParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LSTMWeightParams> {
} _LSTMWeightParams_default_instance_;
class UniDirectionalLSTMLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UniDirectionalLSTMLayerParams> {
} _UniDirectionalLSTMLayerParams_default_instance_;
class BiDirectionalLSTMLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BiDirectionalLSTMLayerParams> {
} _BiDirectionalLSTMLayerParams_default_instance_;
class CustomLayerParams_CustomLayerParamValueDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams_CustomLayerParamValue> {
  public:
  double doublevalue_;
  ::google::protobuf::internal::ArenaStringPtr stringvalue_;
  ::google::protobuf::int32 intvalue_;
  ::google::protobuf::int64 longvalue_;
  bool boolvalue_;
} _CustomLayerParams_CustomLayerParamValue_default_instance_;
class CustomLayerParams_ParametersEntryDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams::CustomLayerParams_ParametersEntry> {
} _CustomLayerParams_ParametersEntry_default_instance_;
class CustomLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams> {
} _CustomLayerParams_default_instance_;
class TransposeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TransposeLayerParams> {
} _TransposeLayerParams_default_instance_;
class BatchedMatMulLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BatchedMatMulLayerParams> {
} _BatchedMatMulLayerParams_default_instance_;
class ConcatNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConcatNDLayerParams> {
} _ConcatNDLayerParams_default_instance_;
class SoftmaxNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SoftmaxNDLayerParams> {
} _SoftmaxNDLayerParams_default_instance_;
class ReverseLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReverseLayerParams> {
} _ReverseLayerParams_default_instance_;
class ReverseSeqLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReverseSeqLayerParams> {
} _ReverseSeqLayerParams_default_instance_;
class LoadConstantNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoadConstantNDLayerParams> {
} _LoadConstantNDLayerParams_default_instance_;
class FillLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FillLikeLayerParams> {
} _FillLikeLayerParams_default_instance_;
class FillStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FillStaticLayerParams> {
} _FillStaticLayerParams_default_instance_;
class FillDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FillDynamicLayerParams> {
} _FillDynamicLayerParams_default_instance_;
class WhereBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<WhereBroadcastableLayerParams> {
} _WhereBroadcastableLayerParams_default_instance_;
class SinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SinLayerParams> {
} _SinLayerParams_default_instance_;
class CosLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CosLayerParams> {
} _CosLayerParams_default_instance_;
class TanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TanLayerParams> {
} _TanLayerParams_default_instance_;
class AsinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AsinLayerParams> {
} _AsinLayerParams_default_instance_;
class AcosLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AcosLayerParams> {
} _AcosLayerParams_default_instance_;
class AtanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AtanLayerParams> {
} _AtanLayerParams_default_instance_;
class SinhLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SinhLayerParams> {
} _SinhLayerParams_default_instance_;
class CoshLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CoshLayerParams> {
} _CoshLayerParams_default_instance_;
class TanhLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TanhLayerParams> {
} _TanhLayerParams_default_instance_;
class AsinhLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AsinhLayerParams> {
} _AsinhLayerParams_default_instance_;
class AcoshLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AcoshLayerParams> {
} _AcoshLayerParams_default_instance_;
class AtanhLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AtanhLayerParams> {
} _AtanhLayerParams_default_instance_;
class PowBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PowBroadcastableLayerParams> {
} _PowBroadcastableLayerParams_default_instance_;
class Exp2LayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<Exp2LayerParams> {
} _Exp2LayerParams_default_instance_;
class WhereNonZeroLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<WhereNonZeroLayerParams> {
} _WhereNonZeroLayerParams_default_instance_;
class MatrixBandPartLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MatrixBandPartLayerParams> {
} _MatrixBandPartLayerParams_default_instance_;
class UpperTriangularLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UpperTriangularLayerParams> {
} _UpperTriangularLayerParams_default_instance_;
class LowerTriangularLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LowerTriangularLayerParams> {
} _LowerTriangularLayerParams_default_instance_;
class BroadcastToLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BroadcastToLikeLayerParams> {
} _BroadcastToLikeLayerParams_default_instance_;
class BroadcastToStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BroadcastToStaticLayerParams> {
} _BroadcastToStaticLayerParams_default_instance_;
class BroadcastToDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BroadcastToDynamicLayerParams> {
} _BroadcastToDynamicLayerParams_default_instance_;
class AddBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AddBroadcastableLayerParams> {
} _AddBroadcastableLayerParams_default_instance_;
class MaxBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MaxBroadcastableLayerParams> {
} _MaxBroadcastableLayerParams_default_instance_;
class MinBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MinBroadcastableLayerParams> {
} _MinBroadcastableLayerParams_default_instance_;
class ModBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ModBroadcastableLayerParams> {
} _ModBroadcastableLayerParams_default_instance_;
class FloorDivBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FloorDivBroadcastableLayerParams> {
} _FloorDivBroadcastableLayerParams_default_instance_;
class SubtractBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SubtractBroadcastableLayerParams> {
} _SubtractBroadcastableLayerParams_default_instance_;
class MultiplyBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MultiplyBroadcastableLayerParams> {
} _MultiplyBroadcastableLayerParams_default_instance_;
class DivideBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<DivideBroadcastableLayerParams> {
} _DivideBroadcastableLayerParams_default_instance_;
class GatherLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GatherLayerParams> {
} _GatherLayerParams_default_instance_;
class ScatterLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScatterLayerParams> {
} _ScatterLayerParams_default_instance_;
class GatherNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GatherNDLayerParams> {
} _GatherNDLayerParams_default_instance_;
class ScatterNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScatterNDLayerParams> {
} _ScatterNDLayerParams_default_instance_;
class GatherAlongAxisLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GatherAlongAxisLayerParams> {
} _GatherAlongAxisLayerParams_default_instance_;
class ScatterAlongAxisLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScatterAlongAxisLayerParams> {
} _ScatterAlongAxisLayerParams_default_instance_;
class StackLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<StackLayerParams> {
} _StackLayerParams_default_instance_;
class RankPreservingReshapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RankPreservingReshapeLayerParams> {
} _RankPreservingReshapeLayerParams_default_instance_;
class ConstantPaddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConstantPaddingLayerParams> {
} _ConstantPaddingLayerParams_default_instance_;
class RandomNormalLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomNormalLikeLayerParams> {
} _RandomNormalLikeLayerParams_default_instance_;
class RandomNormalStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomNormalStaticLayerParams> {
} _RandomNormalStaticLayerParams_default_instance_;
class RandomNormalDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomNormalDynamicLayerParams> {
} _RandomNormalDynamicLayerParams_default_instance_;
class RandomUniformLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomUniformLikeLayerParams> {
} _RandomUniformLikeLayerParams_default_instance_;
class RandomUniformStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomUniformStaticLayerParams> {
} _RandomUniformStaticLayerParams_default_instance_;
class RandomUniformDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomUniformDynamicLayerParams> {
} _RandomUniformDynamicLayerParams_default_instance_;
class RandomBernoulliLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomBernoulliLikeLayerParams> {
} _RandomBernoulliLikeLayerParams_default_instance_;
class RandomBernoulliStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomBernoulliStaticLayerParams> {
} _RandomBernoulliStaticLayerParams_default_instance_;
class RandomBernoulliDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RandomBernoulliDynamicLayerParams> {
} _RandomBernoulliDynamicLayerParams_default_instance_;
class CategoricalDistributionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CategoricalDistributionLayerParams> {
} _CategoricalDistributionLayerParams_default_instance_;
class ReduceL1LayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceL1LayerParams> {
} _ReduceL1LayerParams_default_instance_;
class ReduceL2LayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceL2LayerParams> {
} _ReduceL2LayerParams_default_instance_;
class ReduceMaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceMaxLayerParams> {
} _ReduceMaxLayerParams_default_instance_;
class ReduceMinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceMinLayerParams> {
} _ReduceMinLayerParams_default_instance_;
class ReduceSumLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceSumLayerParams> {
} _ReduceSumLayerParams_default_instance_;
class ReduceProdLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceProdLayerParams> {
} _ReduceProdLayerParams_default_instance_;
class ReduceMeanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceMeanLayerParams> {
} _ReduceMeanLayerParams_default_instance_;
class ReduceLogSumLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceLogSumLayerParams> {
} _ReduceLogSumLayerParams_default_instance_;
class ReduceSumSquareLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceSumSquareLayerParams> {
} _ReduceSumSquareLayerParams_default_instance_;
class ReduceLogSumExpLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceLogSumExpLayerParams> {
} _ReduceLogSumExpLayerParams_default_instance_;
class ExpandDimsLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ExpandDimsLayerParams> {
} _ExpandDimsLayerParams_default_instance_;
class FlattenTo2DLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FlattenTo2DLayerParams> {
} _FlattenTo2DLayerParams_default_instance_;
class ReshapeStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReshapeStaticLayerParams> {
} _ReshapeStaticLayerParams_default_instance_;
class ReshapeLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReshapeLikeLayerParams> {
} _ReshapeLikeLayerParams_default_instance_;
class ReshapeDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReshapeDynamicLayerParams> {
} _ReshapeDynamicLayerParams_default_instance_;
class SqueezeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SqueezeLayerParams> {
} _SqueezeLayerParams_default_instance_;
class TopKLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TopKLayerParams> {
} _TopKLayerParams_default_instance_;
class ArgMaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ArgMaxLayerParams> {
} _ArgMaxLayerParams_default_instance_;
class ArgMinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ArgMinLayerParams> {
} _ArgMinLayerParams_default_instance_;
class SplitNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SplitNDLayerParams> {
} _SplitNDLayerParams_default_instance_;
class CeilLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CeilLayerParams> {
} _CeilLayerParams_default_instance_;
class RoundLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RoundLayerParams> {
} _RoundLayerParams_default_instance_;
class FloorLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FloorLayerParams> {
} _FloorLayerParams_default_instance_;
class SignLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SignLayerParams> {
} _SignLayerParams_default_instance_;
class ClipLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ClipLayerParams> {
} _ClipLayerParams_default_instance_;
class SliceStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SliceStaticLayerParams> {
} _SliceStaticLayerParams_default_instance_;
class SliceDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SliceDynamicLayerParams> {
} _SliceDynamicLayerParams_default_instance_;
class TileLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TileLayerParams> {
} _TileLayerParams_default_instance_;
class GetShapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GetShapeLayerParams> {
} _GetShapeLayerParams_default_instance_;
class ErfLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ErfLayerParams> {
} _ErfLayerParams_default_instance_;
class GeluLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GeluLayerParams> {
} _GeluLayerParams_default_instance_;
class RangeStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RangeStaticLayerParams> {
} _RangeStaticLayerParams_default_instance_;
class RangeDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RangeDynamicLayerParams> {
} _RangeDynamicLayerParams_default_instance_;
class SlidingWindowsLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SlidingWindowsLayerParams> {
} _SlidingWindowsLayerParams_default_instance_;
class LayerNormalizationLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LayerNormalizationLayerParams> {
} _LayerNormalizationLayerParams_default_instance_;
class NonMaximumSuppressionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NonMaximumSuppressionLayerParams> {
} _NonMaximumSuppressionLayerParams_default_instance_;
class NeuralNetworkClassifierDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkClassifier> {
  public:
  const ::CoreML::Specification::StringVector* stringclasslabels_;
  const ::CoreML::Specification::Int64Vector* int64classlabels_;
} _NeuralNetworkClassifier_default_instance_;
class NeuralNetworkRegressorDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkRegressor> {
} _NeuralNetworkRegressor_default_instance_;
class NetworkUpdateParametersDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NetworkUpdateParameters> {
} _NetworkUpdateParameters_default_instance_;
class LossLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LossLayer> {
  public:
  const ::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer_;
  const ::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer_;
} _LossLayer_default_instance_;
class CategoricalCrossEntropyLossLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CategoricalCrossEntropyLossLayer> {
} _CategoricalCrossEntropyLossLayer_default_instance_;
class MeanSquaredErrorLossLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MeanSquaredErrorLossLayer> {
} _MeanSquaredErrorLossLayer_default_instance_;
class OptimizerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<Optimizer> {
  public:
  const ::CoreML::Specification::SGDOptimizer* sgdoptimizer_;
  const ::CoreML::Specification::AdamOptimizer* adamoptimizer_;
} _Optimizer_default_instance_;
class SGDOptimizerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SGDOptimizer> {
} _SGDOptimizer_default_instance_;
class AdamOptimizerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AdamOptimizer> {
} _AdamOptimizer_default_instance_;

namespace protobuf_NeuralNetwork_2eproto {

PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::ParseTableField
    const TableStruct::entries[] = {
  {0, 0, 0, ::google::protobuf::internal::kInvalidMask, 0, 0},
};

PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::AuxillaryParseTableField
    const TableStruct::aux[] = {
  ::google::protobuf::internal::AuxillaryParseTableField(),
};
PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::ParseTable const
    TableStruct::schema[] = {
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
};


void TableStruct::Shutdown() {
  _NeuralNetwork_default_instance_.Shutdown();
  _NeuralNetworkImageScaler_default_instance_.Shutdown();
  _NeuralNetworkMeanImage_default_instance_.Shutdown();
  _NeuralNetworkPreprocessing_default_instance_.Shutdown();
  _ActivationReLU_default_instance_.Shutdown();
  _ActivationLeakyReLU_default_instance_.Shutdown();
  _ActivationTanh_default_instance_.Shutdown();
  _ActivationScaledTanh_default_instance_.Shutdown();
  _ActivationSigmoid_default_instance_.Shutdown();
  _ActivationLinear_default_instance_.Shutdown();
  _ActivationSigmoidHard_default_instance_.Shutdown();
  _ActivationPReLU_default_instance_.Shutdown();
  _ActivationELU_default_instance_.Shutdown();
  _ActivationThresholdedReLU_default_instance_.Shutdown();
  _ActivationSoftsign_default_instance_.Shutdown();
  _ActivationSoftplus_default_instance_.Shutdown();
  _ActivationParametricSoftplus_default_instance_.Shutdown();
  _ActivationParams_default_instance_.Shutdown();
  _Tensor_default_instance_.Shutdown();
  _NeuralNetworkLayer_default_instance_.Shutdown();
  _BranchLayerParams_default_instance_.Shutdown();
  _LoopLayerParams_default_instance_.Shutdown();
  _LoopBreakLayerParams_default_instance_.Shutdown();
  _LoopContinueLayerParams_default_instance_.Shutdown();
  _CopyLayerParams_default_instance_.Shutdown();
  _GreaterThanLayerParams_default_instance_.Shutdown();
  _GreaterEqualLayerParams_default_instance_.Shutdown();
  _LessThanLayerParams_default_instance_.Shutdown();
  _LessEqualLayerParams_default_instance_.Shutdown();
  _EqualLayerParams_default_instance_.Shutdown();
  _NotEqualLayerParams_default_instance_.Shutdown();
  _LogicalAndLayerParams_default_instance_.Shutdown();
  _LogicalOrLayerParams_default_instance_.Shutdown();
  _LogicalXorLayerParams_default_instance_.Shutdown();
  _LogicalNotLayerParams_default_instance_.Shutdown();
  _BorderAmounts_EdgeSizes_default_instance_.Shutdown();
  _BorderAmounts_default_instance_.Shutdown();
  _ValidPadding_default_instance_.Shutdown();
  _SamePadding_default_instance_.Shutdown();
  _SamplingMode_default_instance_.Shutdown();
  _BoxCoordinatesMode_default_instance_.Shutdown();
  _WeightParams_default_instance_.Shutdown();
  _QuantizationParams_default_instance_.Shutdown();
  _LinearQuantizationParams_default_instance_.Shutdown();
  _LookUpTableQuantizationParams_default_instance_.Shutdown();
  _ConvolutionLayerParams_default_instance_.Shutdown();
  _InnerProductLayerParams_default_instance_.Shutdown();
  _EmbeddingLayerParams_default_instance_.Shutdown();
  _EmbeddingNDLayerParams_default_instance_.Shutdown();
  _BatchnormLayerParams_default_instance_.Shutdown();
  _PoolingLayerParams_ValidCompletePadding_default_instance_.Shutdown();
  _PoolingLayerParams_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingConstant_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingReflection_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingReplication_default_instance_.Shutdown();
  _PaddingLayerParams_default_instance_.Shutdown();
  _ConcatLayerParams_default_instance_.Shutdown();
  _LRNLayerParams_default_instance_.Shutdown();
  _SoftmaxLayerParams_default_instance_.Shutdown();
  _SplitLayerParams_default_instance_.Shutdown();
  _AddLayerParams_default_instance_.Shutdown();
  _MultiplyLayerParams_default_instance_.Shutdown();
  _UnaryFunctionLayerParams_default_instance_.Shutdown();
  _UpsampleLayerParams_default_instance_.Shutdown();
  _ResizeBilinearLayerParams_default_instance_.Shutdown();
  _CropResizeLayerParams_default_instance_.Shutdown();
  _BiasLayerParams_default_instance_.Shutdown();
  _ScaleLayerParams_default_instance_.Shutdown();
  _LoadConstantLayerParams_default_instance_.Shutdown();
  _L2NormalizeLayerParams_default_instance_.Shutdown();
  _FlattenLayerParams_default_instance_.Shutdown();
  _ReshapeLayerParams_default_instance_.Shutdown();
  _PermuteLayerParams_default_instance_.Shutdown();
  _ReorganizeDataLayerParams_default_instance_.Shutdown();
  _SliceLayerParams_default_instance_.Shutdown();
  _ReduceLayerParams_default_instance_.Shutdown();
  _CropLayerParams_default_instance_.Shutdown();
  _AverageLayerParams_default_instance_.Shutdown();
  _MaxLayerParams_default_instance_.Shutdown();
  _MinLayerParams_default_instance_.Shutdown();
  _DotProductLayerParams_default_instance_.Shutdown();
  _MeanVarianceNormalizeLayerParams_default_instance_.Shutdown();
  _SequenceRepeatLayerParams_default_instance_.Shutdown();
  _SimpleRecurrentLayerParams_default_instance_.Shutdown();
  _GRULayerParams_default_instance_.Shutdown();
  _LSTMParams_default_instance_.Shutdown();
  _LSTMWeightParams_default_instance_.Shutdown();
  _UniDirectionalLSTMLayerParams_default_instance_.Shutdown();
  _BiDirectionalLSTMLayerParams_default_instance_.Shutdown();
  _CustomLayerParams_CustomLayerParamValue_default_instance_.Shutdown();
  _CustomLayerParams_default_instance_.Shutdown();
  _TransposeLayerParams_default_instance_.Shutdown();
  _BatchedMatMulLayerParams_default_instance_.Shutdown();
  _ConcatNDLayerParams_default_instance_.Shutdown();
  _SoftmaxNDLayerParams_default_instance_.Shutdown();
  _ReverseLayerParams_default_instance_.Shutdown();
  _ReverseSeqLayerParams_default_instance_.Shutdown();
  _LoadConstantNDLayerParams_default_instance_.Shutdown();
  _FillLikeLayerParams_default_instance_.Shutdown();
  _FillStaticLayerParams_default_instance_.Shutdown();
  _FillDynamicLayerParams_default_instance_.Shutdown();
  _WhereBroadcastableLayerParams_default_instance_.Shutdown();
  _SinLayerParams_default_instance_.Shutdown();
  _CosLayerParams_default_instance_.Shutdown();
  _TanLayerParams_default_instance_.Shutdown();
  _AsinLayerParams_default_instance_.Shutdown();
  _AcosLayerParams_default_instance_.Shutdown();
  _AtanLayerParams_default_instance_.Shutdown();
  _SinhLayerParams_default_instance_.Shutdown();
  _CoshLayerParams_default_instance_.Shutdown();
  _TanhLayerParams_default_instance_.Shutdown();
  _AsinhLayerParams_default_instance_.Shutdown();
  _AcoshLayerParams_default_instance_.Shutdown();
  _AtanhLayerParams_default_instance_.Shutdown();
  _PowBroadcastableLayerParams_default_instance_.Shutdown();
  _Exp2LayerParams_default_instance_.Shutdown();
  _WhereNonZeroLayerParams_default_instance_.Shutdown();
  _MatrixBandPartLayerParams_default_instance_.Shutdown();
  _UpperTriangularLayerParams_default_instance_.Shutdown();
  _LowerTriangularLayerParams_default_instance_.Shutdown();
  _BroadcastToLikeLayerParams_default_instance_.Shutdown();
  _BroadcastToStaticLayerParams_default_instance_.Shutdown();
  _BroadcastToDynamicLayerParams_default_instance_.Shutdown();
  _AddBroadcastableLayerParams_default_instance_.Shutdown();
  _MaxBroadcastableLayerParams_default_instance_.Shutdown();
  _MinBroadcastableLayerParams_default_instance_.Shutdown();
  _ModBroadcastableLayerParams_default_instance_.Shutdown();
  _FloorDivBroadcastableLayerParams_default_instance_.Shutdown();
  _SubtractBroadcastableLayerParams_default_instance_.Shutdown();
  _MultiplyBroadcastableLayerParams_default_instance_.Shutdown();
  _DivideBroadcastableLayerParams_default_instance_.Shutdown();
  _GatherLayerParams_default_instance_.Shutdown();
  _ScatterLayerParams_default_instance_.Shutdown();
  _GatherNDLayerParams_default_instance_.Shutdown();
  _ScatterNDLayerParams_default_instance_.Shutdown();
  _GatherAlongAxisLayerParams_default_instance_.Shutdown();
  _ScatterAlongAxisLayerParams_default_instance_.Shutdown();
  _StackLayerParams_default_instance_.Shutdown();
  _RankPreservingReshapeLayerParams_default_instance_.Shutdown();
  _ConstantPaddingLayerParams_default_instance_.Shutdown();
  _RandomNormalLikeLayerParams_default_instance_.Shutdown();
  _RandomNormalStaticLayerParams_default_instance_.Shutdown();
  _RandomNormalDynamicLayerParams_default_instance_.Shutdown();
  _RandomUniformLikeLayerParams_default_instance_.Shutdown();
  _RandomUniformStaticLayerParams_default_instance_.Shutdown();
  _RandomUniformDynamicLayerParams_default_instance_.Shutdown();
  _RandomBernoulliLikeLayerParams_default_instance_.Shutdown();
  _RandomBernoulliStaticLayerParams_default_instance_.Shutdown();
  _RandomBernoulliDynamicLayerParams_default_instance_.Shutdown();
  _CategoricalDistributionLayerParams_default_instance_.Shutdown();
  _ReduceL1LayerParams_default_instance_.Shutdown();
  _ReduceL2LayerParams_default_instance_.Shutdown();
  _ReduceMaxLayerParams_default_instance_.Shutdown();
  _ReduceMinLayerParams_default_instance_.Shutdown();
  _ReduceSumLayerParams_default_instance_.Shutdown();
  _ReduceProdLayerParams_default_instance_.Shutdown();
  _ReduceMeanLayerParams_default_instance_.Shutdown();
  _ReduceLogSumLayerParams_default_instance_.Shutdown();
  _ReduceSumSquareLayerParams_default_instance_.Shutdown();
  _ReduceLogSumExpLayerParams_default_instance_.Shutdown();
  _ExpandDimsLayerParams_default_instance_.Shutdown();
  _FlattenTo2DLayerParams_default_instance_.Shutdown();
  _ReshapeStaticLayerParams_default_instance_.Shutdown();
  _ReshapeLikeLayerParams_default_instance_.Shutdown();
  _ReshapeDynamicLayerParams_default_instance_.Shutdown();
  _SqueezeLayerParams_default_instance_.Shutdown();
  _TopKLayerParams_default_instance_.Shutdown();
  _ArgMaxLayerParams_default_instance_.Shutdown();
  _ArgMinLayerParams_default_instance_.Shutdown();
  _SplitNDLayerParams_default_instance_.Shutdown();
  _CeilLayerParams_default_instance_.Shutdown();
  _RoundLayerParams_default_instance_.Shutdown();
  _FloorLayerParams_default_instance_.Shutdown();
  _SignLayerParams_default_instance_.Shutdown();
  _ClipLayerParams_default_instance_.Shutdown();
  _SliceStaticLayerParams_default_instance_.Shutdown();
  _SliceDynamicLayerParams_default_instance_.Shutdown();
  _TileLayerParams_default_instance_.Shutdown();
  _GetShapeLayerParams_default_instance_.Shutdown();
  _ErfLayerParams_default_instance_.Shutdown();
  _GeluLayerParams_default_instance_.Shutdown();
  _RangeStaticLayerParams_default_instance_.Shutdown();
  _RangeDynamicLayerParams_default_instance_.Shutdown();
  _SlidingWindowsLayerParams_default_instance_.Shutdown();
  _LayerNormalizationLayerParams_default_instance_.Shutdown();
  _NonMaximumSuppressionLayerParams_default_instance_.Shutdown();
  _NeuralNetworkClassifier_default_instance_.Shutdown();
  _NeuralNetworkRegressor_default_instance_.Shutdown();
  _NetworkUpdateParameters_default_instance_.Shutdown();
  _LossLayer_default_instance_.Shutdown();
  _CategoricalCrossEntropyLossLayer_default_instance_.Shutdown();
  _MeanSquaredErrorLossLayer_default_instance_.Shutdown();
  _Optimizer_default_instance_.Shutdown();
  _SGDOptimizer_default_instance_.Shutdown();
  _AdamOptimizer_default_instance_.Shutdown();
}

void TableStruct::InitDefaultsImpl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::google::protobuf::internal::InitProtobufDefaults();
  ::CoreML::Specification::protobuf_DataStructures_2eproto::InitDefaults();
  ::CoreML::Specification::protobuf_Parameters_2eproto::InitDefaults();
  _NeuralNetwork_default_instance_.DefaultConstruct();
  _NeuralNetworkImageScaler_default_instance_.DefaultConstruct();
  _NeuralNetworkMeanImage_default_instance_.DefaultConstruct();
  _NeuralNetworkPreprocessing_default_instance_.DefaultConstruct();
  _ActivationReLU_default_instance_.DefaultConstruct();
  _ActivationLeakyReLU_default_instance_.DefaultConstruct();
  _ActivationTanh_default_instance_.DefaultConstruct();
  _ActivationScaledTanh_default_instance_.DefaultConstruct();
  _ActivationSigmoid_default_instance_.DefaultConstruct();
  _ActivationLinear_default_instance_.DefaultConstruct();
  _ActivationSigmoidHard_default_instance_.DefaultConstruct();
  _ActivationPReLU_default_instance_.DefaultConstruct();
  _ActivationELU_default_instance_.DefaultConstruct();
  _ActivationThresholdedReLU_default_instance_.DefaultConstruct();
  _ActivationSoftsign_default_instance_.DefaultConstruct();
  _ActivationSoftplus_default_instance_.DefaultConstruct();
  _ActivationParametricSoftplus_default_instance_.DefaultConstruct();
  _ActivationParams_default_instance_.DefaultConstruct();
  _Tensor_default_instance_.DefaultConstruct();
  _NeuralNetworkLayer_default_instance_.DefaultConstruct();
  _BranchLayerParams_default_instance_.DefaultConstruct();
  _LoopLayerParams_default_instance_.DefaultConstruct();
  _LoopBreakLayerParams_default_instance_.DefaultConstruct();
  _LoopContinueLayerParams_default_instance_.DefaultConstruct();
  _CopyLayerParams_default_instance_.DefaultConstruct();
  _GreaterThanLayerParams_default_instance_.DefaultConstruct();
  _GreaterEqualLayerParams_default_instance_.DefaultConstruct();
  _LessThanLayerParams_default_instance_.DefaultConstruct();
  _LessEqualLayerParams_default_instance_.DefaultConstruct();
  _EqualLayerParams_default_instance_.DefaultConstruct();
  _NotEqualLayerParams_default_instance_.DefaultConstruct();
  _LogicalAndLayerParams_default_instance_.DefaultConstruct();
  _LogicalOrLayerParams_default_instance_.DefaultConstruct();
  _LogicalXorLayerParams_default_instance_.DefaultConstruct();
  _LogicalNotLayerParams_default_instance_.DefaultConstruct();
  _BorderAmounts_EdgeSizes_default_instance_.DefaultConstruct();
  _BorderAmounts_default_instance_.DefaultConstruct();
  _ValidPadding_default_instance_.DefaultConstruct();
  _SamePadding_default_instance_.DefaultConstruct();
  _SamplingMode_default_instance_.DefaultConstruct();
  _BoxCoordinatesMode_default_instance_.DefaultConstruct();
  _WeightParams_default_instance_.DefaultConstruct();
  _QuantizationParams_default_instance_.DefaultConstruct();
  _LinearQuantizationParams_default_instance_.DefaultConstruct();
  _LookUpTableQuantizationParams_default_instance_.DefaultConstruct();
  _ConvolutionLayerParams_default_instance_.DefaultConstruct();
  _InnerProductLayerParams_default_instance_.DefaultConstruct();
  _EmbeddingLayerParams_default_instance_.DefaultConstruct();
  _EmbeddingNDLayerParams_default_instance_.DefaultConstruct();
  _BatchnormLayerParams_default_instance_.DefaultConstruct();
  _PoolingLayerParams_ValidCompletePadding_default_instance_.DefaultConstruct();
  _PoolingLayerParams_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingConstant_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingReflection_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingReplication_default_instance_.DefaultConstruct();
  _PaddingLayerParams_default_instance_.DefaultConstruct();
  _ConcatLayerParams_default_instance_.DefaultConstruct();
  _LRNLayerParams_default_instance_.DefaultConstruct();
  _SoftmaxLayerParams_default_instance_.DefaultConstruct();
  _SplitLayerParams_default_instance_.DefaultConstruct();
  _AddLayerParams_default_instance_.DefaultConstruct();
  _MultiplyLayerParams_default_instance_.DefaultConstruct();
  _UnaryFunctionLayerParams_default_instance_.DefaultConstruct();
  _UpsampleLayerParams_default_instance_.DefaultConstruct();
  _ResizeBilinearLayerParams_default_instance_.DefaultConstruct();
  _CropResizeLayerParams_default_instance_.DefaultConstruct();
  _BiasLayerParams_default_instance_.DefaultConstruct();
  _ScaleLayerParams_default_instance_.DefaultConstruct();
  _LoadConstantLayerParams_default_instance_.DefaultConstruct();
  _L2NormalizeLayerParams_default_instance_.DefaultConstruct();
  _FlattenLayerParams_default_instance_.DefaultConstruct();
  _ReshapeLayerParams_default_instance_.DefaultConstruct();
  _PermuteLayerParams_default_instance_.DefaultConstruct();
  _ReorganizeDataLayerParams_default_instance_.DefaultConstruct();
  _SliceLayerParams_default_instance_.DefaultConstruct();
  _ReduceLayerParams_default_instance_.DefaultConstruct();
  _CropLayerParams_default_instance_.DefaultConstruct();
  _AverageLayerParams_default_instance_.DefaultConstruct();
  _MaxLayerParams_default_instance_.DefaultConstruct();
  _MinLayerParams_default_instance_.DefaultConstruct();
  _DotProductLayerParams_default_instance_.DefaultConstruct();
  _MeanVarianceNormalizeLayerParams_default_instance_.DefaultConstruct();
  _SequenceRepeatLayerParams_default_instance_.DefaultConstruct();
  _SimpleRecurrentLayerParams_default_instance_.DefaultConstruct();
  _GRULayerParams_default_instance_.DefaultConstruct();
  _LSTMParams_default_instance_.DefaultConstruct();
  _LSTMWeightParams_default_instance_.DefaultConstruct();
  _UniDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  _BiDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  _CustomLayerParams_CustomLayerParamValue_default_instance_.DefaultConstruct();
  _CustomLayerParams_ParametersEntry_default_instance_.DefaultConstruct();
  _CustomLayerParams_default_instance_.DefaultConstruct();
  _TransposeLayerParams_default_instance_.DefaultConstruct();
  _BatchedMatMulLayerParams_default_instance_.DefaultConstruct();
  _ConcatNDLayerParams_default_instance_.DefaultConstruct();
  _SoftmaxNDLayerParams_default_instance_.DefaultConstruct();
  _ReverseLayerParams_default_instance_.DefaultConstruct();
  _ReverseSeqLayerParams_default_instance_.DefaultConstruct();
  _LoadConstantNDLayerParams_default_instance_.DefaultConstruct();
  _FillLikeLayerParams_default_instance_.DefaultConstruct();
  _FillStaticLayerParams_default_instance_.DefaultConstruct();
  _FillDynamicLayerParams_default_instance_.DefaultConstruct();
  _WhereBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _SinLayerParams_default_instance_.DefaultConstruct();
  _CosLayerParams_default_instance_.DefaultConstruct();
  _TanLayerParams_default_instance_.DefaultConstruct();
  _AsinLayerParams_default_instance_.DefaultConstruct();
  _AcosLayerParams_default_instance_.DefaultConstruct();
  _AtanLayerParams_default_instance_.DefaultConstruct();
  _SinhLayerParams_default_instance_.DefaultConstruct();
  _CoshLayerParams_default_instance_.DefaultConstruct();
  _TanhLayerParams_default_instance_.DefaultConstruct();
  _AsinhLayerParams_default_instance_.DefaultConstruct();
  _AcoshLayerParams_default_instance_.DefaultConstruct();
  _AtanhLayerParams_default_instance_.DefaultConstruct();
  _PowBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _Exp2LayerParams_default_instance_.DefaultConstruct();
  _WhereNonZeroLayerParams_default_instance_.DefaultConstruct();
  _MatrixBandPartLayerParams_default_instance_.DefaultConstruct();
  _UpperTriangularLayerParams_default_instance_.DefaultConstruct();
  _LowerTriangularLayerParams_default_instance_.DefaultConstruct();
  _BroadcastToLikeLayerParams_default_instance_.DefaultConstruct();
  _BroadcastToStaticLayerParams_default_instance_.DefaultConstruct();
  _BroadcastToDynamicLayerParams_default_instance_.DefaultConstruct();
  _AddBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _MaxBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _MinBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _ModBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _FloorDivBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _SubtractBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _MultiplyBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _DivideBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _GatherLayerParams_default_instance_.DefaultConstruct();
  _ScatterLayerParams_default_instance_.DefaultConstruct();
  _GatherNDLayerParams_default_instance_.DefaultConstruct();
  _ScatterNDLayerParams_default_instance_.DefaultConstruct();
  _GatherAlongAxisLayerParams_default_instance_.DefaultConstruct();
  _ScatterAlongAxisLayerParams_default_instance_.DefaultConstruct();
  _StackLayerParams_default_instance_.DefaultConstruct();
  _RankPreservingReshapeLayerParams_default_instance_.DefaultConstruct();
  _ConstantPaddingLayerParams_default_instance_.DefaultConstruct();
  _RandomNormalLikeLayerParams_default_instance_.DefaultConstruct();
  _RandomNormalStaticLayerParams_default_instance_.DefaultConstruct();
  _RandomNormalDynamicLayerParams_default_instance_.DefaultConstruct();
  _RandomUniformLikeLayerParams_default_instance_.DefaultConstruct();
  _RandomUniformStaticLayerParams_default_instance_.DefaultConstruct();
  _RandomUniformDynamicLayerParams_default_instance_.DefaultConstruct();
  _RandomBernoulliLikeLayerParams_default_instance_.DefaultConstruct();
  _RandomBernoulliStaticLayerParams_default_instance_.DefaultConstruct();
  _RandomBernoulliDynamicLayerParams_default_instance_.DefaultConstruct();
  _CategoricalDistributionLayerParams_default_instance_.DefaultConstruct();
  _ReduceL1LayerParams_default_instance_.DefaultConstruct();
  _ReduceL2LayerParams_default_instance_.DefaultConstruct();
  _ReduceMaxLayerParams_default_instance_.DefaultConstruct();
  _ReduceMinLayerParams_default_instance_.DefaultConstruct();
  _ReduceSumLayerParams_default_instance_.DefaultConstruct();
  _ReduceProdLayerParams_default_instance_.DefaultConstruct();
  _ReduceMeanLayerParams_default_instance_.DefaultConstruct();
  _ReduceLogSumLayerParams_default_instance_.DefaultConstruct();
  _ReduceSumSquareLayerParams_default_instance_.DefaultConstruct();
  _ReduceLogSumExpLayerParams_default_instance_.DefaultConstruct();
  _ExpandDimsLayerParams_default_instance_.DefaultConstruct();
  _FlattenTo2DLayerParams_default_instance_.DefaultConstruct();
  _ReshapeStaticLayerParams_default_instance_.DefaultConstruct();
  _ReshapeLikeLayerParams_default_instance_.DefaultConstruct();
  _ReshapeDynamicLayerParams_default_instance_.DefaultConstruct();
  _SqueezeLayerParams_default_instance_.DefaultConstruct();
  _TopKLayerParams_default_instance_.DefaultConstruct();
  _ArgMaxLayerParams_default_instance_.DefaultConstruct();
  _ArgMinLayerParams_default_instance_.DefaultConstruct();
  _SplitNDLayerParams_default_instance_.DefaultConstruct();
  _CeilLayerParams_default_instance_.DefaultConstruct();
  _RoundLayerParams_default_instance_.DefaultConstruct();
  _FloorLayerParams_default_instance_.DefaultConstruct();
  _SignLayerParams_default_instance_.DefaultConstruct();
  _ClipLayerParams_default_instance_.DefaultConstruct();
  _SliceStaticLayerParams_default_instance_.DefaultConstruct();
  _SliceDynamicLayerParams_default_instance_.DefaultConstruct();
  _TileLayerParams_default_instance_.DefaultConstruct();
  _GetShapeLayerParams_default_instance_.DefaultConstruct();
  _ErfLayerParams_default_instance_.DefaultConstruct();
  _GeluLayerParams_default_instance_.DefaultConstruct();
  _RangeStaticLayerParams_default_instance_.DefaultConstruct();
  _RangeDynamicLayerParams_default_instance_.DefaultConstruct();
  _SlidingWindowsLayerParams_default_instance_.DefaultConstruct();
  _LayerNormalizationLayerParams_default_instance_.DefaultConstruct();
  _NonMaximumSuppressionLayerParams_default_instance_.DefaultConstruct();
  _NeuralNetworkClassifier_default_instance_.DefaultConstruct();
  _NeuralNetworkRegressor_default_instance_.DefaultConstruct();
  _NetworkUpdateParameters_default_instance_.DefaultConstruct();
  _LossLayer_default_instance_.DefaultConstruct();
  _CategoricalCrossEntropyLossLayer_default_instance_.DefaultConstruct();
  _MeanSquaredErrorLossLayer_default_instance_.DefaultConstruct();
  _Optimizer_default_instance_.DefaultConstruct();
  _SGDOptimizer_default_instance_.DefaultConstruct();
  _AdamOptimizer_default_instance_.DefaultConstruct();
  _NeuralNetwork_default_instance_.get_mutable()->updateparams_ = const_cast< ::CoreML::Specification::NetworkUpdateParameters*>(
      ::CoreML::Specification::NetworkUpdateParameters::internal_default_instance());
  _ActivationPReLU_default_instance_.get_mutable()->alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ActivationParametricSoftplus_default_instance_.get_mutable()->alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ActivationParametricSoftplus_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BranchLayerParams_default_instance_.get_mutable()->ifbranch_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _BranchLayerParams_default_instance_.get_mutable()->elsebranch_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _LoopLayerParams_default_instance_.get_mutable()->conditionnetwork_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _LoopLayerParams_default_instance_.get_mutable()->bodynetwork_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _ValidPadding_default_instance_.get_mutable()->paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _WeightParams_default_instance_.get_mutable()->quantization_ = const_cast< ::CoreML::Specification::QuantizationParams*>(
      ::CoreML::Specification::QuantizationParams::internal_default_instance());
  _ConvolutionLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ConvolutionLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _InnerProductLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _InnerProductLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingNDLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingNDLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->gamma_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->mean_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->variance_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _PaddingLayerParams_default_instance_.get_mutable()->paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _ResizeBilinearLayerParams_default_instance_.get_mutable()->mode_ = const_cast< ::CoreML::Specification::SamplingMode*>(
      ::CoreML::Specification::SamplingMode::internal_default_instance());
  _CropResizeLayerParams_default_instance_.get_mutable()->mode_ = const_cast< ::CoreML::Specification::SamplingMode*>(
      ::CoreML::Specification::SamplingMode::internal_default_instance());
  _CropResizeLayerParams_default_instance_.get_mutable()->boxindicesmode_ = const_cast< ::CoreML::Specification::BoxCoordinatesMode*>(
      ::CoreML::Specification::BoxCoordinatesMode::internal_default_instance());
  _BiasLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ScaleLayerParams_default_instance_.get_mutable()->scale_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ScaleLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LoadConstantLayerParams_default_instance_.get_mutable()->data_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _CropLayerParams_default_instance_.get_mutable()->cropamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->activation_ = const_cast< ::CoreML::Specification::ActivationParams*>(
      ::CoreML::Specification::ActivationParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->weightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->recursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->biasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputrecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputbiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  _UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->weightparams_ = const_cast< ::CoreML::Specification::LSTMWeightParams*>(
      ::CoreML::Specification::LSTMWeightParams::internal_default_instance());
  _BiDirectionalLSTMLayerParams_default_instance_.get_mutable()->params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  _CustomLayerParams_ParametersEntry_default_instance_.get_mutable()->set_default_instance(_CustomLayerParams_ParametersEntry_default_instance_.get_mutable());
  _CustomLayerParams_ParametersEntry_default_instance_.get_mutable()->InitAsDefaultInstance();
  _BatchedMatMulLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchedMatMulLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LoadConstantNDLayerParams_default_instance_.get_mutable()->data_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LayerNormalizationLayerParams_default_instance_.get_mutable()->gamma_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LayerNormalizationLayerParams_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _NeuralNetworkClassifier_default_instance_.get_mutable()->updateparams_ = const_cast< ::CoreML::Specification::NetworkUpdateParameters*>(
      ::CoreML::Specification::NetworkUpdateParameters::internal_default_instance());
  _NeuralNetworkRegressor_default_instance_.get_mutable()->updateparams_ = const_cast< ::CoreML::Specification::NetworkUpdateParameters*>(
      ::CoreML::Specification::NetworkUpdateParameters::internal_default_instance());
  _NetworkUpdateParameters_default_instance_.get_mutable()->optimizer_ = const_cast< ::CoreML::Specification::Optimizer*>(
      ::CoreML::Specification::Optimizer::internal_default_instance());
  _NetworkUpdateParameters_default_instance_.get_mutable()->epochs_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _NetworkUpdateParameters_default_instance_.get_mutable()->shuffle_ = const_cast< ::CoreML::Specification::BoolParameter*>(
      ::CoreML::Specification::BoolParameter::internal_default_instance());
  _NetworkUpdateParameters_default_instance_.get_mutable()->seed_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _SGDOptimizer_default_instance_.get_mutable()->learningrate_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _SGDOptimizer_default_instance_.get_mutable()->minibatchsize_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _SGDOptimizer_default_instance_.get_mutable()->momentum_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->learningrate_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->minibatchsize_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->beta1_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->beta2_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->eps_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
}

void InitDefaults() {
  static GOOGLE_PROTOBUF_DECLARE_ONCE(once);
  ::google::protobuf::GoogleOnceInit(&once, &TableStruct::InitDefaultsImpl);
}
void AddDescriptorsImpl() {
  InitDefaults();
  ::CoreML::Specification::protobuf_DataStructures_2eproto::AddDescriptors();
  ::CoreML::Specification::protobuf_Parameters_2eproto::AddDescriptors();
  ::google::protobuf::internal::OnShutdown(&TableStruct::Shutdown);
}

void AddDescriptors() {
  static GOOGLE_PROTOBUF_DECLARE_ONCE(once);
  ::google::protobuf::GoogleOnceInit(&once, &AddDescriptorsImpl);
}
#ifdef GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer {
  StaticDescriptorInitializer() {
    AddDescriptors();
  }
} static_descriptor_initializer;
#endif  // GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER

}  // namespace protobuf_NeuralNetwork_2eproto

bool SamePadding_SamePaddingMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SamePadding_SamePaddingMode SamePadding::BOTTOM_RIGHT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::TOP_LEFT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MIN;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MAX;
const int SamePadding::SamePaddingMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool SamplingMode_Method_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SamplingMode_Method SamplingMode::STRICT_ALIGN_ENDPOINTS_MODE;
const SamplingMode_Method SamplingMode::ALIGN_ENDPOINTS_MODE;
const SamplingMode_Method SamplingMode::UPSAMPLE_MODE;
const SamplingMode_Method SamplingMode::ROI_ALIGN_MODE;
const SamplingMode_Method SamplingMode::Method_MIN;
const SamplingMode_Method SamplingMode::Method_MAX;
const int SamplingMode::Method_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool BoxCoordinatesMode_Coordinates_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CORNERS_HEIGHT_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CORNERS_WIDTH_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CENTER_SIZE_HEIGHT_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CENTER_SIZE_WIDTH_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::Coordinates_MIN;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::Coordinates_MAX;
const int BoxCoordinatesMode::Coordinates_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool PoolingLayerParams_PoolingType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const PoolingLayerParams_PoolingType PoolingLayerParams::MAX;
const PoolingLayerParams_PoolingType PoolingLayerParams::AVERAGE;
const PoolingLayerParams_PoolingType PoolingLayerParams::L2;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MIN;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MAX;
const int PoolingLayerParams::PoolingType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool UnaryFunctionLayerParams_Operation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::SQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::RSQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::INVERSE;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::POWER;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::EXP;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::LOG;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::ABS;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::THRESHOLD;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MIN;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MAX;
const int UnaryFunctionLayerParams::Operation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool UpsampleLayerParams_InterpolationMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::NN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::BILINEAR;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::InterpolationMode_MIN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::InterpolationMode_MAX;
const int UpsampleLayerParams::InterpolationMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool FlattenLayerParams_FlattenOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_FIRST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_LAST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MIN;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MAX;
const int FlattenLayerParams::FlattenOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReshapeLayerParams_ReshapeOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_FIRST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_LAST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MIN;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MAX;
const int ReshapeLayerParams::ReshapeOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReorganizeDataLayerParams_ReorganizationType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::SPACE_TO_DEPTH;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::DEPTH_TO_SPACE;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::ReorganizationType_MIN;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::ReorganizationType_MAX;
const int ReorganizeDataLayerParams::ReorganizationType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool SliceLayerParams_SliceAxis_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SliceLayerParams_SliceAxis SliceLayerParams::CHANNEL_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::HEIGHT_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::WIDTH_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::SliceAxis_MIN;
const SliceLayerParams_SliceAxis SliceLayerParams::SliceAxis_MAX;
const int SliceLayerParams::SliceAxis_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReduceLayerParams_ReduceOperation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
    case 8:
    case 9:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::AVG;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::PROD;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::LOGSUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUMSQUARE;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L1;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L2;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::MAX;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ARGMAX;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MAX;
const int ReduceLayerParams::ReduceOperation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReduceLayerParams_ReduceAxis_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceAxis ReduceLayerParams::CHW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::HW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::C;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::H;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::W;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::ReduceAxis_MIN;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::ReduceAxis_MAX;
const int ReduceLayerParams::ReduceAxis_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool GeluLayerParams_GeluMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const GeluLayerParams_GeluMode GeluLayerParams::EXACT;
const GeluLayerParams_GeluMode GeluLayerParams::TANH_APPROXIMATION;
const GeluLayerParams_GeluMode GeluLayerParams::SIGMOID_APPROXIMATION;
const GeluLayerParams_GeluMode GeluLayerParams::GeluMode_MIN;
const GeluLayerParams_GeluMode GeluLayerParams::GeluMode_MAX;
const int GeluLayerParams::GeluMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool NeuralNetworkMultiArrayShapeMapping_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

bool NeuralNetworkImageShapeMapping_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

bool ScatterMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
      return true;
    default:
      return false;
  }
}


// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetwork::kLayersFieldNumber;
const int NeuralNetwork::kPreprocessingFieldNumber;
const int NeuralNetwork::kArrayInputShapeMappingFieldNumber;
const int NeuralNetwork::kImageInputShapeMappingFieldNumber;
const int NeuralNetwork::kUpdateParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetwork::NeuralNetwork()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetwork)
}
NeuralNetwork::NeuralNetwork(const NeuralNetwork& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updateparams()) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters(*from.updateparams_);
  } else {
    updateparams_ = NULL;
  }
  ::memcpy(&arrayinputshapemapping_, &from.arrayinputshapemapping_,
    reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetwork)
}

void NeuralNetwork::SharedCtor() {
  ::memset(&updateparams_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&updateparams_) + sizeof(imageinputshapemapping_));
  _cached_size_ = 0;
}

NeuralNetwork::~NeuralNetwork() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetwork)
  SharedDtor();
}

void NeuralNetwork::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updateparams_;
  }
}

void NeuralNetwork::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetwork& NeuralNetwork::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetwork* NeuralNetwork::New(::google::protobuf::Arena* arena) const {
  NeuralNetwork* n = new NeuralNetwork;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetwork::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetwork)
  layers_.Clear();
  preprocessing_.Clear();
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) {
    delete updateparams_;
  }
  updateparams_ = NULL;
  ::memset(&arrayinputshapemapping_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
}

bool NeuralNetwork::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetwork)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_arrayinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_imageinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updateparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetwork)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetwork)
  return false;
#undef DO_
}

void NeuralNetwork::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetwork)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->arrayinputshapemapping(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      6, this->imageinputshapemapping(), output);
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->updateparams_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetwork)
}

size_t NeuralNetwork::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetwork)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updateparams_);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->arrayinputshapemapping());
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->imageinputshapemapping());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetwork::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetwork*>(&from));
}

void NeuralNetwork::MergeFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetwork)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.has_updateparams()) {
    mutable_updateparams()->::CoreML::Specification::NetworkUpdateParameters::MergeFrom(from.updateparams());
  }
  if (from.arrayinputshapemapping() != 0) {
    set_arrayinputshapemapping(from.arrayinputshapemapping());
  }
  if (from.imageinputshapemapping() != 0) {
    set_imageinputshapemapping(from.imageinputshapemapping());
  }
}

void NeuralNetwork::CopyFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetwork)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetwork::IsInitialized() const {
  return true;
}

void NeuralNetwork::Swap(NeuralNetwork* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetwork::InternalSwap(NeuralNetwork* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  std::swap(updateparams_, other->updateparams_);
  std::swap(arrayinputshapemapping_, other->arrayinputshapemapping_);
  std::swap(imageinputshapemapping_, other->imageinputshapemapping_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetwork::GetTypeName() const {
  return "CoreML.Specification.NeuralNetwork";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetwork::layers_size() const {
  return layers_.size();
}
void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetwork::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
void NeuralNetwork::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetwork::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
void NeuralNetwork::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
void NeuralNetwork::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetwork::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
void NeuralNetwork::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
bool NeuralNetwork::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
void NeuralNetwork::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetwork::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.updateParams)
  return updateparams_;
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetwork.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
void NeuralNetwork::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetwork.updateParams)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkImageScaler::kChannelScaleFieldNumber;
const int NeuralNetworkImageScaler::kBlueBiasFieldNumber;
const int NeuralNetworkImageScaler::kGreenBiasFieldNumber;
const int NeuralNetworkImageScaler::kRedBiasFieldNumber;
const int NeuralNetworkImageScaler::kGrayBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkImageScaler::NeuralNetworkImageScaler()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkImageScaler)
}
NeuralNetworkImageScaler::NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&graybias_, &from.graybias_,
    reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkImageScaler)
}

void NeuralNetworkImageScaler::SharedCtor() {
  ::memset(&graybias_, 0, reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
  _cached_size_ = 0;
}

NeuralNetworkImageScaler::~NeuralNetworkImageScaler() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkImageScaler)
  SharedDtor();
}

void NeuralNetworkImageScaler::SharedDtor() {
}

void NeuralNetworkImageScaler::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkImageScaler& NeuralNetworkImageScaler::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkImageScaler* NeuralNetworkImageScaler::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkImageScaler* n = new NeuralNetworkImageScaler;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkImageScaler::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkImageScaler)
  ::memset(&graybias_, 0, reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
}

bool NeuralNetworkImageScaler::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkImageScaler)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float channelScale = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(85u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &channelscale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float blueBias = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(165u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &bluebias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float greenBias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(173u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &greenbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float redBias = 22;
      case 22: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(181u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &redbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float grayBias = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(245u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &graybias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkImageScaler)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkImageScaler)
  return false;
#undef DO_
}

void NeuralNetworkImageScaler::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkImageScaler)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float channelScale = 10;
  if (this->channelscale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->channelscale(), output);
  }

  // float blueBias = 20;
  if (this->bluebias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(20, this->bluebias(), output);
  }

  // float greenBias = 21;
  if (this->greenbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(21, this->greenbias(), output);
  }

  // float redBias = 22;
  if (this->redbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(22, this->redbias(), output);
  }

  // float grayBias = 30;
  if (this->graybias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(30, this->graybias(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkImageScaler)
}

size_t NeuralNetworkImageScaler::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkImageScaler)
  size_t total_size = 0;

  // float grayBias = 30;
  if (this->graybias() != 0) {
    total_size += 2 + 4;
  }

  // float channelScale = 10;
  if (this->channelscale() != 0) {
    total_size += 1 + 4;
  }

  // float blueBias = 20;
  if (this->bluebias() != 0) {
    total_size += 2 + 4;
  }

  // float greenBias = 21;
  if (this->greenbias() != 0) {
    total_size += 2 + 4;
  }

  // float redBias = 22;
  if (this->redbias() != 0) {
    total_size += 2 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkImageScaler::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkImageScaler*>(&from));
}

void NeuralNetworkImageScaler::MergeFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.graybias() != 0) {
    set_graybias(from.graybias());
  }
  if (from.channelscale() != 0) {
    set_channelscale(from.channelscale());
  }
  if (from.bluebias() != 0) {
    set_bluebias(from.bluebias());
  }
  if (from.greenbias() != 0) {
    set_greenbias(from.greenbias());
  }
  if (from.redbias() != 0) {
    set_redbias(from.redbias());
  }
}

void NeuralNetworkImageScaler::CopyFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkImageScaler::IsInitialized() const {
  return true;
}

void NeuralNetworkImageScaler::Swap(NeuralNetworkImageScaler* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkImageScaler::InternalSwap(NeuralNetworkImageScaler* other) {
  std::swap(graybias_, other->graybias_);
  std::swap(channelscale_, other->channelscale_);
  std::swap(bluebias_, other->bluebias_);
  std::swap(greenbias_, other->greenbias_);
  std::swap(redbias_, other->redbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkImageScaler::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkImageScaler";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkImageScaler

// float channelScale = 10;
void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return channelscale_;
}
void NeuralNetworkImageScaler::set_channelscale(float value) {
  
  channelscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// float blueBias = 20;
void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return bluebias_;
}
void NeuralNetworkImageScaler::set_bluebias(float value) {
  
  bluebias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// float greenBias = 21;
void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return greenbias_;
}
void NeuralNetworkImageScaler::set_greenbias(float value) {
  
  greenbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// float redBias = 22;
void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return redbias_;
}
void NeuralNetworkImageScaler::set_redbias(float value) {
  
  redbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// float grayBias = 30;
void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return graybias_;
}
void NeuralNetworkImageScaler::set_graybias(float value) {
  
  graybias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkMeanImage::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkMeanImage::NeuralNetworkMeanImage()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkMeanImage)
}
NeuralNetworkMeanImage::NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      meanimage_(from.meanimage_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkMeanImage)
}

void NeuralNetworkMeanImage::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetworkMeanImage::~NeuralNetworkMeanImage() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkMeanImage)
  SharedDtor();
}

void NeuralNetworkMeanImage::SharedDtor() {
}

void NeuralNetworkMeanImage::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkMeanImage& NeuralNetworkMeanImage::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkMeanImage* NeuralNetworkMeanImage::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkMeanImage* n = new NeuralNetworkMeanImage;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkMeanImage::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkMeanImage)
  meanimage_.Clear();
}

bool NeuralNetworkMeanImage::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkMeanImage)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float meanImage = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_meanimage())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_meanimage())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkMeanImage)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkMeanImage)
  return false;
#undef DO_
}

void NeuralNetworkMeanImage::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkMeanImage)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float meanImage = 1;
  if (this->meanimage_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_meanimage_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->meanimage().data(), this->meanimage_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkMeanImage)
}

size_t NeuralNetworkMeanImage::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkMeanImage)
  size_t total_size = 0;

  // repeated float meanImage = 1;
  {
    unsigned int count = this->meanimage_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _meanimage_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkMeanImage::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkMeanImage*>(&from));
}

void NeuralNetworkMeanImage::MergeFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  meanimage_.MergeFrom(from.meanimage_);
}

void NeuralNetworkMeanImage::CopyFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkMeanImage::IsInitialized() const {
  return true;
}

void NeuralNetworkMeanImage::Swap(NeuralNetworkMeanImage* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkMeanImage::InternalSwap(NeuralNetworkMeanImage* other) {
  meanimage_.InternalSwap(&other->meanimage_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkMeanImage::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkMeanImage";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkMeanImage

// repeated float meanImage = 1;
int NeuralNetworkMeanImage::meanimage_size() const {
  return meanimage_.size();
}
void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_.Get(index);
}
void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
void NeuralNetworkMeanImage::add_meanimage(float value) {
  meanimage_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
const ::google::protobuf::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_;
}
::google::protobuf::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return &meanimage_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkPreprocessing::kFeatureNameFieldNumber;
const int NeuralNetworkPreprocessing::kScalerFieldNumber;
const int NeuralNetworkPreprocessing::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkPreprocessing::NeuralNetworkPreprocessing()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}
NeuralNetworkPreprocessing::NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.featurename().size() > 0) {
    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
  clear_has_preprocessor();
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}

void NeuralNetworkPreprocessing::SharedCtor() {
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_preprocessor();
  _cached_size_ = 0;
}

NeuralNetworkPreprocessing::~NeuralNetworkPreprocessing() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkPreprocessing)
  SharedDtor();
}

void NeuralNetworkPreprocessing::SharedDtor() {
  featurename_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_preprocessor()) {
    clear_preprocessor();
  }
}

void NeuralNetworkPreprocessing::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkPreprocessing& NeuralNetworkPreprocessing::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkPreprocessing* NeuralNetworkPreprocessing::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkPreprocessing* n = new NeuralNetworkPreprocessing;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkPreprocessing::clear_preprocessor() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  switch (preprocessor_case()) {
    case kScaler: {
      delete preprocessor_.scaler_;
      break;
    }
    case kMeanImage: {
      delete preprocessor_.meanimage_;
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}


void NeuralNetworkPreprocessing::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_preprocessor();
}

bool NeuralNetworkPreprocessing::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkPreprocessing)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string featureName = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_featurename()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->featurename().data(), this->featurename().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkPreprocessing.featureName"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaler()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_meanimage()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkPreprocessing)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkPreprocessing)
  return false;
#undef DO_
}

void NeuralNetworkPreprocessing::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkPreprocessing)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string featureName = 1;
  if (this->featurename().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->featurename().data(), this->featurename().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkPreprocessing.featureName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->featurename(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  if (has_scaler()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *preprocessor_.scaler_, output);
  }

  // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  if (has_meanimage()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *preprocessor_.meanimage_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkPreprocessing)
}

size_t NeuralNetworkPreprocessing::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkPreprocessing)
  size_t total_size = 0;

  // string featureName = 1;
  if (this->featurename().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->featurename());
  }

  switch (preprocessor_case()) {
    // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
    case kScaler: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.scaler_);
      break;
    }
    // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
    case kMeanImage: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.meanimage_);
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkPreprocessing::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkPreprocessing*>(&from));
}

void NeuralNetworkPreprocessing::MergeFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.featurename().size() > 0) {

    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkPreprocessing::CopyFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkPreprocessing::IsInitialized() const {
  return true;
}

void NeuralNetworkPreprocessing::Swap(NeuralNetworkPreprocessing* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkPreprocessing::InternalSwap(NeuralNetworkPreprocessing* other) {
  featurename_.Swap(&other->featurename_);
  std::swap(preprocessor_, other->preprocessor_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkPreprocessing::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkPreprocessing";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkPreprocessing

// string featureName = 1;
void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.GetNoArena();
}
void NeuralNetworkPreprocessing::set_featurename(const ::std::string& value) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#if LANG_CXX11
void NeuralNetworkPreprocessing::set_featurename(::std::string&& value) {
  
  featurename_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#endif
void NeuralNetworkPreprocessing::set_featurename(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
void NeuralNetworkPreprocessing::set_featurename(const char* value, size_t size) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
::std::string* NeuralNetworkPreprocessing::mutable_featurename() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  
  return featurename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkPreprocessing::set_allocated_featurename(::std::string* featurename) {
  if (featurename != NULL) {
    
  } else {
    
  }
  featurename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), featurename);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
bool NeuralNetworkPreprocessing::has_scaler() const {
  return preprocessor_case() == kScaler;
}
void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
void NeuralNetworkPreprocessing::clear_scaler() {
  if (has_scaler()) {
    delete preprocessor_.scaler_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return has_scaler()
      ? *preprocessor_.scaler_
      : ::CoreML::Specification::NeuralNetworkImageScaler::default_instance();
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  if (!has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = new ::CoreML::Specification::NeuralNetworkImageScaler;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return preprocessor_.scaler_;
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}

// .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
bool NeuralNetworkPreprocessing::has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
void NeuralNetworkPreprocessing::clear_meanimage() {
  if (has_meanimage()) {
    delete preprocessor_.meanimage_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return has_meanimage()
      ? *preprocessor_.meanimage_
      : ::CoreML::Specification::NeuralNetworkMeanImage::default_instance();
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  if (!has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = new ::CoreML::Specification::NeuralNetworkMeanImage;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return preprocessor_.meanimage_;
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}

bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationReLU::ActivationReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationReLU)
}
ActivationReLU::ActivationReLU(const ActivationReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationReLU)
}

void ActivationReLU::SharedCtor() {
  _cached_size_ = 0;
}

ActivationReLU::~ActivationReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationReLU)
  SharedDtor();
}

void ActivationReLU::SharedDtor() {
}

void ActivationReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationReLU& ActivationReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationReLU* ActivationReLU::New(::google::protobuf::Arena* arena) const {
  ActivationReLU* n = new ActivationReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationReLU)
}

bool ActivationReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationReLU)
  return false;
#undef DO_
}

void ActivationReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationReLU)
}

size_t ActivationReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationReLU)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationReLU*>(&from));
}

void ActivationReLU::MergeFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationReLU::CopyFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationReLU::IsInitialized() const {
  return true;
}

void ActivationReLU::Swap(ActivationReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationReLU::InternalSwap(ActivationReLU* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationReLU

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLeakyReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLeakyReLU::ActivationLeakyReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLeakyReLU)
}
ActivationLeakyReLU::ActivationLeakyReLU(const ActivationLeakyReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLeakyReLU)
}

void ActivationLeakyReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationLeakyReLU::~ActivationLeakyReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLeakyReLU)
  SharedDtor();
}

void ActivationLeakyReLU::SharedDtor() {
}

void ActivationLeakyReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLeakyReLU& ActivationLeakyReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationLeakyReLU* ActivationLeakyReLU::New(::google::protobuf::Arena* arena) const {
  ActivationLeakyReLU* n = new ActivationLeakyReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLeakyReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLeakyReLU)
  alpha_ = 0;
}

bool ActivationLeakyReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLeakyReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLeakyReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLeakyReLU)
  return false;
#undef DO_
}

void ActivationLeakyReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLeakyReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLeakyReLU)
}

size_t ActivationLeakyReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLeakyReLU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLeakyReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLeakyReLU*>(&from));
}

void ActivationLeakyReLU::MergeFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLeakyReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationLeakyReLU::CopyFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLeakyReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationLeakyReLU::IsInitialized() const {
  return true;
}

void ActivationLeakyReLU::Swap(ActivationLeakyReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLeakyReLU::InternalSwap(ActivationLeakyReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLeakyReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationLeakyReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLeakyReLU

// float alpha = 1;
void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return alpha_;
}
void ActivationLeakyReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationTanh::ActivationTanh()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationTanh)
}
ActivationTanh::ActivationTanh(const ActivationTanh& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationTanh)
}

void ActivationTanh::SharedCtor() {
  _cached_size_ = 0;
}

ActivationTanh::~ActivationTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationTanh)
  SharedDtor();
}

void ActivationTanh::SharedDtor() {
}

void ActivationTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationTanh& ActivationTanh::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationTanh* ActivationTanh::New(::google::protobuf::Arena* arena) const {
  ActivationTanh* n = new ActivationTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationTanh)
}

bool ActivationTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationTanh)
  return false;
#undef DO_
}

void ActivationTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationTanh)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationTanh)
}

size_t ActivationTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationTanh)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationTanh*>(&from));
}

void ActivationTanh::MergeFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationTanh)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationTanh::CopyFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationTanh)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationTanh::IsInitialized() const {
  return true;
}

void ActivationTanh::Swap(ActivationTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationTanh::InternalSwap(ActivationTanh* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationTanh

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationScaledTanh::kAlphaFieldNumber;
const int ActivationScaledTanh::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationScaledTanh::ActivationScaledTanh()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationScaledTanh)
}
ActivationScaledTanh::ActivationScaledTanh(const ActivationScaledTanh& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationScaledTanh)
}

void ActivationScaledTanh::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationScaledTanh::~ActivationScaledTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationScaledTanh)
  SharedDtor();
}

void ActivationScaledTanh::SharedDtor() {
}

void ActivationScaledTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationScaledTanh& ActivationScaledTanh::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationScaledTanh* ActivationScaledTanh::New(::google::protobuf::Arena* arena) const {
  ActivationScaledTanh* n = new ActivationScaledTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationScaledTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationScaledTanh)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationScaledTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationScaledTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationScaledTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationScaledTanh)
  return false;
#undef DO_
}

void ActivationScaledTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationScaledTanh)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationScaledTanh)
}

size_t ActivationScaledTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationScaledTanh)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationScaledTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationScaledTanh*>(&from));
}

void ActivationScaledTanh::MergeFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationScaledTanh)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationScaledTanh::CopyFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationScaledTanh)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationScaledTanh::IsInitialized() const {
  return true;
}

void ActivationScaledTanh::Swap(ActivationScaledTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationScaledTanh::InternalSwap(ActivationScaledTanh* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationScaledTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationScaledTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationScaledTanh

// float alpha = 1;
void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return alpha_;
}
void ActivationScaledTanh::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// float beta = 2;
void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return beta_;
}
void ActivationScaledTanh::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoid::ActivationSigmoid()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoid)
}
ActivationSigmoid::ActivationSigmoid(const ActivationSigmoid& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoid)
}

void ActivationSigmoid::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSigmoid::~ActivationSigmoid() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoid)
  SharedDtor();
}

void ActivationSigmoid::SharedDtor() {
}

void ActivationSigmoid::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoid& ActivationSigmoid::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSigmoid* ActivationSigmoid::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoid* n = new ActivationSigmoid;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoid::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoid)
}

bool ActivationSigmoid::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoid)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoid)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoid)
  return false;
#undef DO_
}

void ActivationSigmoid::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoid)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoid)
}

size_t ActivationSigmoid::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoid)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoid::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoid*>(&from));
}

void ActivationSigmoid::MergeFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoid)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSigmoid::CopyFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoid)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSigmoid::IsInitialized() const {
  return true;
}

void ActivationSigmoid::Swap(ActivationSigmoid* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoid::InternalSwap(ActivationSigmoid* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoid::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoid";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoid

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLinear::kAlphaFieldNumber;
const int ActivationLinear::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLinear::ActivationLinear()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLinear)
}
ActivationLinear::ActivationLinear(const ActivationLinear& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLinear)
}

void ActivationLinear::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationLinear::~ActivationLinear() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLinear)
  SharedDtor();
}

void ActivationLinear::SharedDtor() {
}

void ActivationLinear::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLinear& ActivationLinear::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationLinear* ActivationLinear::New(::google::protobuf::Arena* arena) const {
  ActivationLinear* n = new ActivationLinear;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLinear::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLinear)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationLinear::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLinear)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLinear)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLinear)
  return false;
#undef DO_
}

void ActivationLinear::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLinear)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLinear)
}

size_t ActivationLinear::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLinear)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLinear::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLinear*>(&from));
}

void ActivationLinear::MergeFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLinear)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationLinear::CopyFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLinear)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationLinear::IsInitialized() const {
  return true;
}

void ActivationLinear::Swap(ActivationLinear* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLinear::InternalSwap(ActivationLinear* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLinear::GetTypeName() const {
  return "CoreML.Specification.ActivationLinear";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLinear

// float alpha = 1;
void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return alpha_;
}
void ActivationLinear::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// float beta = 2;
void ActivationLinear::clear_beta() {
  beta_ = 0;
}
float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return beta_;
}
void ActivationLinear::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationSigmoidHard::kAlphaFieldNumber;
const int ActivationSigmoidHard::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoidHard::ActivationSigmoidHard()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoidHard)
}
ActivationSigmoidHard::ActivationSigmoidHard(const ActivationSigmoidHard& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoidHard)
}

void ActivationSigmoidHard::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationSigmoidHard::~ActivationSigmoidHard() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoidHard)
  SharedDtor();
}

void ActivationSigmoidHard::SharedDtor() {
}

void ActivationSigmoidHard::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoidHard& ActivationSigmoidHard::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSigmoidHard* ActivationSigmoidHard::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoidHard* n = new ActivationSigmoidHard;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoidHard::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoidHard)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationSigmoidHard::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoidHard)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoidHard)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoidHard)
  return false;
#undef DO_
}

void ActivationSigmoidHard::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoidHard)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoidHard)
}

size_t ActivationSigmoidHard::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoidHard)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoidHard::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoidHard*>(&from));
}

void ActivationSigmoidHard::MergeFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoidHard)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationSigmoidHard::CopyFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoidHard)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSigmoidHard::IsInitialized() const {
  return true;
}

void ActivationSigmoidHard::Swap(ActivationSigmoidHard* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoidHard::InternalSwap(ActivationSigmoidHard* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoidHard::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoidHard";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoidHard

// float alpha = 1;
void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return alpha_;
}
void ActivationSigmoidHard::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// float beta = 2;
void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return beta_;
}
void ActivationSigmoidHard::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationPReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationPReLU::ActivationPReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationPReLU)
}
ActivationPReLU::ActivationPReLU(const ActivationPReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_alpha()) {
    alpha_ = new ::CoreML::Specification::WeightParams(*from.alpha_);
  } else {
    alpha_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationPReLU)
}

void ActivationPReLU::SharedCtor() {
  alpha_ = NULL;
  _cached_size_ = 0;
}

ActivationPReLU::~ActivationPReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationPReLU)
  SharedDtor();
}

void ActivationPReLU::SharedDtor() {
  if (this != internal_default_instance()) {
    delete alpha_;
  }
}

void ActivationPReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationPReLU& ActivationPReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationPReLU* ActivationPReLU::New(::google::protobuf::Arena* arena) const {
  ActivationPReLU* n = new ActivationPReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationPReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationPReLU)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) {
    delete alpha_;
  }
  alpha_ = NULL;
}

bool ActivationPReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationPReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationPReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationPReLU)
  return false;
#undef DO_
}

void ActivationPReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationPReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationPReLU)
}

size_t ActivationPReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationPReLU)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationPReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationPReLU*>(&from));
}

void ActivationPReLU::MergeFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationPReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
}

void ActivationPReLU::CopyFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationPReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationPReLU::IsInitialized() const {
  return true;
}

void ActivationPReLU::Swap(ActivationPReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationPReLU::InternalSwap(ActivationPReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationPReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationPReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationPReLU

// .CoreML.Specification.WeightParams alpha = 1;
bool ActivationPReLU::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationPReLU::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationELU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationELU::ActivationELU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationELU)
}
ActivationELU::ActivationELU(const ActivationELU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationELU)
}

void ActivationELU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationELU::~ActivationELU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationELU)
  SharedDtor();
}

void ActivationELU::SharedDtor() {
}

void ActivationELU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationELU& ActivationELU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationELU* ActivationELU::New(::google::protobuf::Arena* arena) const {
  ActivationELU* n = new ActivationELU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationELU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationELU)
  alpha_ = 0;
}

bool ActivationELU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationELU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationELU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationELU)
  return false;
#undef DO_
}

void ActivationELU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationELU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationELU)
}

size_t ActivationELU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationELU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationELU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationELU*>(&from));
}

void ActivationELU::MergeFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationELU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationELU::CopyFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationELU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationELU::IsInitialized() const {
  return true;
}

void ActivationELU::Swap(ActivationELU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationELU::InternalSwap(ActivationELU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationELU::GetTypeName() const {
  return "CoreML.Specification.ActivationELU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationELU

// float alpha = 1;
void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return alpha_;
}
void ActivationELU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationThresholdedReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationThresholdedReLU::ActivationThresholdedReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationThresholdedReLU)
}
ActivationThresholdedReLU::ActivationThresholdedReLU(const ActivationThresholdedReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationThresholdedReLU)
}

void ActivationThresholdedReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationThresholdedReLU::~ActivationThresholdedReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationThresholdedReLU)
  SharedDtor();
}

void ActivationThresholdedReLU::SharedDtor() {
}

void ActivationThresholdedReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationThresholdedReLU& ActivationThresholdedReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationThresholdedReLU* ActivationThresholdedReLU::New(::google::protobuf::Arena* arena) const {
  ActivationThresholdedReLU* n = new ActivationThresholdedReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationThresholdedReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationThresholdedReLU)
  alpha_ = 0;
}

bool ActivationThresholdedReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationThresholdedReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationThresholdedReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationThresholdedReLU)
  return false;
#undef DO_
}

void ActivationThresholdedReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationThresholdedReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationThresholdedReLU)
}

size_t ActivationThresholdedReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationThresholdedReLU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationThresholdedReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationThresholdedReLU*>(&from));
}

void ActivationThresholdedReLU::MergeFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationThresholdedReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationThresholdedReLU::CopyFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationThresholdedReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationThresholdedReLU::IsInitialized() const {
  return true;
}

void ActivationThresholdedReLU::Swap(ActivationThresholdedReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationThresholdedReLU::InternalSwap(ActivationThresholdedReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationThresholdedReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationThresholdedReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationThresholdedReLU

// float alpha = 1;
void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return alpha_;
}
void ActivationThresholdedReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftsign::ActivationSoftsign()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftsign)
}
ActivationSoftsign::ActivationSoftsign(const ActivationSoftsign& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftsign)
}

void ActivationSoftsign::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftsign::~ActivationSoftsign() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftsign)
  SharedDtor();
}

void ActivationSoftsign::SharedDtor() {
}

void ActivationSoftsign::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftsign& ActivationSoftsign::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSoftsign* ActivationSoftsign::New(::google::protobuf::Arena* arena) const {
  ActivationSoftsign* n = new ActivationSoftsign;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftsign::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftsign)
}

bool ActivationSoftsign::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftsign)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftsign)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftsign)
  return false;
#undef DO_
}

void ActivationSoftsign::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftsign)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftsign)
}

size_t ActivationSoftsign::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftsign)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftsign::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftsign*>(&from));
}

void ActivationSoftsign::MergeFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftsign)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSoftsign::CopyFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftsign)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSoftsign::IsInitialized() const {
  return true;
}

void ActivationSoftsign::Swap(ActivationSoftsign* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftsign::InternalSwap(ActivationSoftsign* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftsign::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftsign";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftsign

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftplus::ActivationSoftplus()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftplus)
}
ActivationSoftplus::ActivationSoftplus(const ActivationSoftplus& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftplus)
}

void ActivationSoftplus::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftplus::~ActivationSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftplus)
  SharedDtor();
}

void ActivationSoftplus::SharedDtor() {
}

void ActivationSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftplus& ActivationSoftplus::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSoftplus* ActivationSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationSoftplus* n = new ActivationSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftplus)
}

bool ActivationSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftplus)
  return false;
#undef DO_
}

void ActivationSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftplus)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftplus)
}

size_t ActivationSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftplus)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftplus*>(&from));
}

void ActivationSoftplus::MergeFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftplus)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSoftplus::CopyFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftplus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSoftplus::IsInitialized() const {
  return true;
}

void ActivationSoftplus::Swap(ActivationSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftplus::InternalSwap(ActivationSoftplus* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftplus

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParametricSoftplus::kAlphaFieldNumber;
const int ActivationParametricSoftplus::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParametricSoftplus::ActivationParametricSoftplus()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParametricSoftplus)
}
ActivationParametricSoftplus::ActivationParametricSoftplus(const ActivationParametricSoftplus& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_alpha()) {
    alpha_ = new ::CoreML::Specification::WeightParams(*from.alpha_);
  } else {
    alpha_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParametricSoftplus)
}

void ActivationParametricSoftplus::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationParametricSoftplus::~ActivationParametricSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParametricSoftplus)
  SharedDtor();
}

void ActivationParametricSoftplus::SharedDtor() {
  if (this != internal_default_instance()) {
    delete alpha_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
}

void ActivationParametricSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParametricSoftplus& ActivationParametricSoftplus::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationParametricSoftplus* ActivationParametricSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationParametricSoftplus* n = new ActivationParametricSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParametricSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParametricSoftplus)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) {
    delete alpha_;
  }
  alpha_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
}

bool ActivationParametricSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParametricSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParametricSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParametricSoftplus)
  return false;
#undef DO_
}

void ActivationParametricSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParametricSoftplus)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->beta_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParametricSoftplus)
}

size_t ActivationParametricSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParametricSoftplus)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  // .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParametricSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParametricSoftplus*>(&from));
}

void ActivationParametricSoftplus::MergeFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParametricSoftplus)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
}

void ActivationParametricSoftplus::CopyFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParametricSoftplus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationParametricSoftplus::IsInitialized() const {
  return true;
}

void ActivationParametricSoftplus::Swap(ActivationParametricSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParametricSoftplus::InternalSwap(ActivationParametricSoftplus* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParametricSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationParametricSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParametricSoftplus

// .CoreML.Specification.WeightParams alpha = 1;
bool ActivationParametricSoftplus::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// .CoreML.Specification.WeightParams beta = 2;
bool ActivationParametricSoftplus::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParams::kLinearFieldNumber;
const int ActivationParams::kReLUFieldNumber;
const int ActivationParams::kLeakyReLUFieldNumber;
const int ActivationParams::kThresholdedReLUFieldNumber;
const int ActivationParams::kPReLUFieldNumber;
const int ActivationParams::kTanhFieldNumber;
const int ActivationParams::kScaledTanhFieldNumber;
const int ActivationParams::kSigmoidFieldNumber;
const int ActivationParams::kSigmoidHardFieldNumber;
const int ActivationParams::kELUFieldNumber;
const int ActivationParams::kSoftsignFieldNumber;
const int ActivationParams::kSoftplusFieldNumber;
const int ActivationParams::kParametricSoftplusFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParams::ActivationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParams)
}
ActivationParams::ActivationParams(const ActivationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_NonlinearityType();
  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParams)
}

void ActivationParams::SharedCtor() {
  clear_has_NonlinearityType();
  _cached_size_ = 0;
}

ActivationParams::~ActivationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParams)
  SharedDtor();
}

void ActivationParams::SharedDtor() {
  if (has_NonlinearityType()) {
    clear_NonlinearityType();
  }
}

void ActivationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParams& ActivationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationParams* ActivationParams::New(::google::protobuf::Arena* arena) const {
  ActivationParams* n = new ActivationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParams::clear_NonlinearityType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ActivationParams)
  switch (NonlinearityType_case()) {
    case kLinear: {
      delete NonlinearityType_.linear_;
      break;
    }
    case kReLU: {
      delete NonlinearityType_.relu_;
      break;
    }
    case kLeakyReLU: {
      delete NonlinearityType_.leakyrelu_;
      break;
    }
    case kThresholdedReLU: {
      delete NonlinearityType_.thresholdedrelu_;
      break;
    }
    case kPReLU: {
      delete NonlinearityType_.prelu_;
      break;
    }
    case kTanh: {
      delete NonlinearityType_.tanh_;
      break;
    }
    case kScaledTanh: {
      delete NonlinearityType_.scaledtanh_;
      break;
    }
    case kSigmoid: {
      delete NonlinearityType_.sigmoid_;
      break;
    }
    case kSigmoidHard: {
      delete NonlinearityType_.sigmoidhard_;
      break;
    }
    case kELU: {
      delete NonlinearityType_.elu_;
      break;
    }
    case kSoftsign: {
      delete NonlinearityType_.softsign_;
      break;
    }
    case kSoftplus: {
      delete NonlinearityType_.softplus_;
      break;
    }
    case kParametricSoftplus: {
      delete NonlinearityType_.parametricsoftplus_;
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}


void ActivationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParams)
  clear_NonlinearityType();
}

bool ActivationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ActivationLinear linear = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linear()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationReLU ReLU = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_relu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_leakyrelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_thresholdedrelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationPReLU PReLU = 25;
      case 25: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_prelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationTanh tanh = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaledtanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
      case 41: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(330u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoidhard()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationELU ELU = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_elu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSoftsign softsign = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softsign()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSoftplus softplus = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softplus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
      case 71: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(570u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_parametricsoftplus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParams)
  return false;
#undef DO_
}

void ActivationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ActivationLinear linear = 5;
  if (has_linear()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *NonlinearityType_.linear_, output);
  }

  // .CoreML.Specification.ActivationReLU ReLU = 10;
  if (has_relu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *NonlinearityType_.relu_, output);
  }

  // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  if (has_leakyrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *NonlinearityType_.leakyrelu_, output);
  }

  // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  if (has_thresholdedrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *NonlinearityType_.thresholdedrelu_, output);
  }

  // .CoreML.Specification.ActivationPReLU PReLU = 25;
  if (has_prelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      25, *NonlinearityType_.prelu_, output);
  }

  // .CoreML.Specification.ActivationTanh tanh = 30;
  if (has_tanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *NonlinearityType_.tanh_, output);
  }

  // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  if (has_scaledtanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *NonlinearityType_.scaledtanh_, output);
  }

  // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  if (has_sigmoid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *NonlinearityType_.sigmoid_, output);
  }

  // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  if (has_sigmoidhard()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *NonlinearityType_.sigmoidhard_, output);
  }

  // .CoreML.Specification.ActivationELU ELU = 50;
  if (has_elu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *NonlinearityType_.elu_, output);
  }

  // .CoreML.Specification.ActivationSoftsign softsign = 60;
  if (has_softsign()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *NonlinearityType_.softsign_, output);
  }

  // .CoreML.Specification.ActivationSoftplus softplus = 70;
  if (has_softplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *NonlinearityType_.softplus_, output);
  }

  // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  if (has_parametricsoftplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *NonlinearityType_.parametricsoftplus_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParams)
}

size_t ActivationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParams)
  size_t total_size = 0;

  switch (NonlinearityType_case()) {
    // .CoreML.Specification.ActivationLinear linear = 5;
    case kLinear: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.linear_);
      break;
    }
    // .CoreML.Specification.ActivationReLU ReLU = 10;
    case kReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.relu_);
      break;
    }
    // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
    case kLeakyReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.leakyrelu_);
      break;
    }
    // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
    case kThresholdedReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.thresholdedrelu_);
      break;
    }
    // .CoreML.Specification.ActivationPReLU PReLU = 25;
    case kPReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.prelu_);
      break;
    }
    // .CoreML.Specification.ActivationTanh tanh = 30;
    case kTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.tanh_);
      break;
    }
    // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
    case kScaledTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.scaledtanh_);
      break;
    }
    // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
    case kSigmoid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoid_);
      break;
    }
    // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
    case kSigmoidHard: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoidhard_);
      break;
    }
    // .CoreML.Specification.ActivationELU ELU = 50;
    case kELU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.elu_);
      break;
    }
    // .CoreML.Specification.ActivationSoftsign softsign = 60;
    case kSoftsign: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softsign_);
      break;
    }
    // .CoreML.Specification.ActivationSoftplus softplus = 70;
    case kSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softplus_);
      break;
    }
    // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
    case kParametricSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.parametricsoftplus_);
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParams*>(&from));
}

void ActivationParams::MergeFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
}

void ActivationParams::CopyFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationParams::IsInitialized() const {
  return true;
}

void ActivationParams::Swap(ActivationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParams::InternalSwap(ActivationParams* other) {
  std::swap(NonlinearityType_, other->NonlinearityType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParams::GetTypeName() const {
  return "CoreML.Specification.ActivationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParams

// .CoreML.Specification.ActivationLinear linear = 5;
bool ActivationParams::has_linear() const {
  return NonlinearityType_case() == kLinear;
}
void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
void ActivationParams::clear_linear() {
  if (has_linear()) {
    delete NonlinearityType_.linear_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return has_linear()
      ? *NonlinearityType_.linear_
      : ::CoreML::Specification::ActivationLinear::default_instance();
}
::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  if (!has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = new ::CoreML::Specification::ActivationLinear;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return NonlinearityType_.linear_;
}
::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.linear)
}

// .CoreML.Specification.ActivationReLU ReLU = 10;
bool ActivationParams::has_relu() const {
  return NonlinearityType_case() == kReLU;
}
void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
void ActivationParams::clear_relu() {
  if (has_relu()) {
    delete NonlinearityType_.relu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return has_relu()
      ? *NonlinearityType_.relu_
      : ::CoreML::Specification::ActivationReLU::default_instance();
}
::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  if (!has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = new ::CoreML::Specification::ActivationReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return NonlinearityType_.relu_;
}
::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}

// .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
bool ActivationParams::has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
void ActivationParams::clear_leakyrelu() {
  if (has_leakyrelu()) {
    delete NonlinearityType_.leakyrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : ::CoreML::Specification::ActivationLeakyReLU::default_instance();
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  if (!has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = new ::CoreML::Specification::ActivationLeakyReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return NonlinearityType_.leakyrelu_;
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}

// .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
bool ActivationParams::has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
void ActivationParams::clear_thresholdedrelu() {
  if (has_thresholdedrelu()) {
    delete NonlinearityType_.thresholdedrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : ::CoreML::Specification::ActivationThresholdedReLU::default_instance();
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  if (!has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = new ::CoreML::Specification::ActivationThresholdedReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return NonlinearityType_.thresholdedrelu_;
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}

// .CoreML.Specification.ActivationPReLU PReLU = 25;
bool ActivationParams::has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
void ActivationParams::clear_prelu() {
  if (has_prelu()) {
    delete NonlinearityType_.prelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return has_prelu()
      ? *NonlinearityType_.prelu_
      : ::CoreML::Specification::ActivationPReLU::default_instance();
}
::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  if (!has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = new ::CoreML::Specification::ActivationPReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return NonlinearityType_.prelu_;
}
::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}

// .CoreML.Specification.ActivationTanh tanh = 30;
bool ActivationParams::has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
void ActivationParams::clear_tanh() {
  if (has_tanh()) {
    delete NonlinearityType_.tanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return has_tanh()
      ? *NonlinearityType_.tanh_
      : ::CoreML::Specification::ActivationTanh::default_instance();
}
::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  if (!has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = new ::CoreML::Specification::ActivationTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return NonlinearityType_.tanh_;
}
::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.tanh)
}

// .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
bool ActivationParams::has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
void ActivationParams::clear_scaledtanh() {
  if (has_scaledtanh()) {
    delete NonlinearityType_.scaledtanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : ::CoreML::Specification::ActivationScaledTanh::default_instance();
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  if (!has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = new ::CoreML::Specification::ActivationScaledTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return NonlinearityType_.scaledtanh_;
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}

// .CoreML.Specification.ActivationSigmoid sigmoid = 40;
bool ActivationParams::has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
void ActivationParams::clear_sigmoid() {
  if (has_sigmoid()) {
    delete NonlinearityType_.sigmoid_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : ::CoreML::Specification::ActivationSigmoid::default_instance();
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  if (!has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = new ::CoreML::Specification::ActivationSigmoid;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return NonlinearityType_.sigmoid_;
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}

// .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
bool ActivationParams::has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
void ActivationParams::clear_sigmoidhard() {
  if (has_sigmoidhard()) {
    delete NonlinearityType_.sigmoidhard_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : ::CoreML::Specification::ActivationSigmoidHard::default_instance();
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  if (!has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = new ::CoreML::Specification::ActivationSigmoidHard;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return NonlinearityType_.sigmoidhard_;
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}

// .CoreML.Specification.ActivationELU ELU = 50;
bool ActivationParams::has_elu() const {
  return NonlinearityType_case() == kELU;
}
void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
void ActivationParams::clear_elu() {
  if (has_elu()) {
    delete NonlinearityType_.elu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return has_elu()
      ? *NonlinearityType_.elu_
      : ::CoreML::Specification::ActivationELU::default_instance();
}
::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  if (!has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = new ::CoreML::Specification::ActivationELU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return NonlinearityType_.elu_;
}
::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ELU)
}

// .CoreML.Specification.ActivationSoftsign softsign = 60;
bool ActivationParams::has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
void ActivationParams::clear_softsign() {
  if (has_softsign()) {
    delete NonlinearityType_.softsign_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return has_softsign()
      ? *NonlinearityType_.softsign_
      : ::CoreML::Specification::ActivationSoftsign::default_instance();
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  if (!has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = new ::CoreML::Specification::ActivationSoftsign;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return NonlinearityType_.softsign_;
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softsign)
}

// .CoreML.Specification.ActivationSoftplus softplus = 70;
bool ActivationParams::has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
void ActivationParams::clear_softplus() {
  if (has_softplus()) {
    delete NonlinearityType_.softplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return has_softplus()
      ? *NonlinearityType_.softplus_
      : ::CoreML::Specification::ActivationSoftplus::default_instance();
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  if (!has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = new ::CoreML::Specification::ActivationSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return NonlinearityType_.softplus_;
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softplus)
}

// .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
bool ActivationParams::has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
void ActivationParams::clear_parametricsoftplus() {
  if (has_parametricsoftplus()) {
    delete NonlinearityType_.parametricsoftplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : ::CoreML::Specification::ActivationParametricSoftplus::default_instance();
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  if (!has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = new ::CoreML::Specification::ActivationParametricSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return NonlinearityType_.parametricsoftplus_;
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}

bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int Tensor::kRankFieldNumber;
const int Tensor::kDimValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Tensor::Tensor()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.Tensor)
}
Tensor::Tensor(const Tensor& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      dimvalue_(from.dimvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  rank_ = from.rank_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.Tensor)
}

void Tensor::SharedCtor() {
  rank_ = 0u;
  _cached_size_ = 0;
}

Tensor::~Tensor() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.Tensor)
  SharedDtor();
}

void Tensor::SharedDtor() {
}

void Tensor::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const Tensor& Tensor::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

Tensor* Tensor::New(::google::protobuf::Arena* arena) const {
  Tensor* n = new Tensor;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void Tensor::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.Tensor)
  dimvalue_.Clear();
  rank_ = 0u;
}

bool Tensor::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.Tensor)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint32 rank = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint32, ::google::protobuf::internal::WireFormatLite::TYPE_UINT32>(
                 input, &rank_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 dimValue = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_dimvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 18u, input, this->mutable_dimvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.Tensor)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.Tensor)
  return false;
#undef DO_
}

void Tensor::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.Tensor)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint32 rank = 1;
  if (this->rank() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt32(1, this->rank(), output);
  }

  // repeated int64 dimValue = 2;
  if (this->dimvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_dimvalue_cached_byte_size_);
  }
  for (int i = 0, n = this->dimvalue_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->dimvalue(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.Tensor)
}

size_t Tensor::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.Tensor)
  size_t total_size = 0;

  // repeated int64 dimValue = 2;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->dimvalue_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _dimvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // uint32 rank = 1;
  if (this->rank() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt32Size(
        this->rank());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Tensor::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const Tensor*>(&from));
}

void Tensor::MergeFrom(const Tensor& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.Tensor)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  dimvalue_.MergeFrom(from.dimvalue_);
  if (from.rank() != 0) {
    set_rank(from.rank());
  }
}

void Tensor::CopyFrom(const Tensor& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.Tensor)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Tensor::IsInitialized() const {
  return true;
}

void Tensor::Swap(Tensor* other) {
  if (other == this) return;
  InternalSwap(other);
}
void Tensor::InternalSwap(Tensor* other) {
  dimvalue_.InternalSwap(&other->dimvalue_);
  std::swap(rank_, other->rank_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string Tensor::GetTypeName() const {
  return "CoreML.Specification.Tensor";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Tensor

// uint32 rank = 1;
void Tensor::clear_rank() {
  rank_ = 0u;
}
::google::protobuf::uint32 Tensor::rank() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.rank)
  return rank_;
}
void Tensor::set_rank(::google::protobuf::uint32 value) {
  
  rank_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.rank)
}

// repeated int64 dimValue = 2;
int Tensor::dimvalue_size() const {
  return dimvalue_.size();
}
void Tensor::clear_dimvalue() {
  dimvalue_.Clear();
}
::google::protobuf::int64 Tensor::dimvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.dimValue)
  return dimvalue_.Get(index);
}
void Tensor::set_dimvalue(int index, ::google::protobuf::int64 value) {
  dimvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.dimValue)
}
void Tensor::add_dimvalue(::google::protobuf::int64 value) {
  dimvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.Tensor.dimValue)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
Tensor::dimvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.Tensor.dimValue)
  return dimvalue_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
Tensor::mutable_dimvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.Tensor.dimValue)
  return &dimvalue_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkLayer::kNameFieldNumber;
const int NeuralNetworkLayer::kInputFieldNumber;
const int NeuralNetworkLayer::kOutputFieldNumber;
const int NeuralNetworkLayer::kInputTensorFieldNumber;
const int NeuralNetworkLayer::kOutputTensorFieldNumber;
const int NeuralNetworkLayer::kIsUpdatableFieldNumber;
const int NeuralNetworkLayer::kConvolutionFieldNumber;
const int NeuralNetworkLayer::kPoolingFieldNumber;
const int NeuralNetworkLayer::kActivationFieldNumber;
const int NeuralNetworkLayer::kInnerProductFieldNumber;
const int NeuralNetworkLayer::kEmbeddingFieldNumber;
const int NeuralNetworkLayer::kBatchnormFieldNumber;
const int NeuralNetworkLayer::kMvnFieldNumber;
const int NeuralNetworkLayer::kL2NormalizeFieldNumber;
const int NeuralNetworkLayer::kSoftmaxFieldNumber;
const int NeuralNetworkLayer::kLrnFieldNumber;
const int NeuralNetworkLayer::kCropFieldNumber;
const int NeuralNetworkLayer::kPaddingFieldNumber;
const int NeuralNetworkLayer::kUpsampleFieldNumber;
const int NeuralNetworkLayer::kResizeBilinearFieldNumber;
const int NeuralNetworkLayer::kCropResizeFieldNumber;
const int NeuralNetworkLayer::kUnaryFieldNumber;
const int NeuralNetworkLayer::kAddFieldNumber;
const int NeuralNetworkLayer::kMultiplyFieldNumber;
const int NeuralNetworkLayer::kAverageFieldNumber;
const int NeuralNetworkLayer::kScaleFieldNumber;
const int NeuralNetworkLayer::kBiasFieldNumber;
const int NeuralNetworkLayer::kMaxFieldNumber;
const int NeuralNetworkLayer::kMinFieldNumber;
const int NeuralNetworkLayer::kDotFieldNumber;
const int NeuralNetworkLayer::kReduceFieldNumber;
const int NeuralNetworkLayer::kLoadConstantFieldNumber;
const int NeuralNetworkLayer::kReshapeFieldNumber;
const int NeuralNetworkLayer::kFlattenFieldNumber;
const int NeuralNetworkLayer::kPermuteFieldNumber;
const int NeuralNetworkLayer::kConcatFieldNumber;
const int NeuralNetworkLayer::kSplitFieldNumber;
const int NeuralNetworkLayer::kSequenceRepeatFieldNumber;
const int NeuralNetworkLayer::kReorganizeDataFieldNumber;
const int NeuralNetworkLayer::kSliceFieldNumber;
const int NeuralNetworkLayer::kSimpleRecurrentFieldNumber;
const int NeuralNetworkLayer::kGruFieldNumber;
const int NeuralNetworkLayer::kUniDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kBiDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kCustomFieldNumber;
const int NeuralNetworkLayer::kCopyFieldNumber;
const int NeuralNetworkLayer::kBranchFieldNumber;
const int NeuralNetworkLayer::kLoopFieldNumber;
const int NeuralNetworkLayer::kLoopBreakFieldNumber;
const int NeuralNetworkLayer::kLoopContinueFieldNumber;
const int NeuralNetworkLayer::kRangeStaticFieldNumber;
const int NeuralNetworkLayer::kRangeDynamicFieldNumber;
const int NeuralNetworkLayer::kClipFieldNumber;
const int NeuralNetworkLayer::kCeilFieldNumber;
const int NeuralNetworkLayer::kFloorFieldNumber;
const int NeuralNetworkLayer::kSignFieldNumber;
const int NeuralNetworkLayer::kRoundFieldNumber;
const int NeuralNetworkLayer::kExp2FieldNumber;
const int NeuralNetworkLayer::kSinFieldNumber;
const int NeuralNetworkLayer::kCosFieldNumber;
const int NeuralNetworkLayer::kTanFieldNumber;
const int NeuralNetworkLayer::kAsinFieldNumber;
const int NeuralNetworkLayer::kAcosFieldNumber;
const int NeuralNetworkLayer::kAtanFieldNumber;
const int NeuralNetworkLayer::kSinhFieldNumber;
const int NeuralNetworkLayer::kCoshFieldNumber;
const int NeuralNetworkLayer::kTanhFieldNumber;
const int NeuralNetworkLayer::kAsinhFieldNumber;
const int NeuralNetworkLayer::kAcoshFieldNumber;
const int NeuralNetworkLayer::kAtanhFieldNumber;
const int NeuralNetworkLayer::kErfFieldNumber;
const int NeuralNetworkLayer::kGeluFieldNumber;
const int NeuralNetworkLayer::kEqualFieldNumber;
const int NeuralNetworkLayer::kNotEqualFieldNumber;
const int NeuralNetworkLayer::kLessThanFieldNumber;
const int NeuralNetworkLayer::kLessEqualFieldNumber;
const int NeuralNetworkLayer::kGreaterThanFieldNumber;
const int NeuralNetworkLayer::kGreaterEqualFieldNumber;
const int NeuralNetworkLayer::kLogicalOrFieldNumber;
const int NeuralNetworkLayer::kLogicalXorFieldNumber;
const int NeuralNetworkLayer::kLogicalNotFieldNumber;
const int NeuralNetworkLayer::kLogicalAndFieldNumber;
const int NeuralNetworkLayer::kModBroadcastableFieldNumber;
const int NeuralNetworkLayer::kMinBroadcastableFieldNumber;
const int NeuralNetworkLayer::kMaxBroadcastableFieldNumber;
const int NeuralNetworkLayer::kAddBroadcastableFieldNumber;
const int NeuralNetworkLayer::kPowBroadcastableFieldNumber;
const int NeuralNetworkLayer::kDivideBroadcastableFieldNumber;
const int NeuralNetworkLayer::kFloorDivBroadcastableFieldNumber;
const int NeuralNetworkLayer::kMultiplyBroadcastableFieldNumber;
const int NeuralNetworkLayer::kSubtractBroadcastableFieldNumber;
const int NeuralNetworkLayer::kTileFieldNumber;
const int NeuralNetworkLayer::kStackFieldNumber;
const int NeuralNetworkLayer::kGatherFieldNumber;
const int NeuralNetworkLayer::kScatterFieldNumber;
const int NeuralNetworkLayer::kGatherNDFieldNumber;
const int NeuralNetworkLayer::kScatterNDFieldNumber;
const int NeuralNetworkLayer::kSoftmaxNDFieldNumber;
const int NeuralNetworkLayer::kGatherAlongAxisFieldNumber;
const int NeuralNetworkLayer::kScatterAlongAxisFieldNumber;
const int NeuralNetworkLayer::kReverseFieldNumber;
const int NeuralNetworkLayer::kReverseSeqFieldNumber;
const int NeuralNetworkLayer::kSplitNDFieldNumber;
const int NeuralNetworkLayer::kConcatNDFieldNumber;
const int NeuralNetworkLayer::kTransposeFieldNumber;
const int NeuralNetworkLayer::kSliceStaticFieldNumber;
const int NeuralNetworkLayer::kSliceDynamicFieldNumber;
const int NeuralNetworkLayer::kSlidingWindowsFieldNumber;
const int NeuralNetworkLayer::kTopKFieldNumber;
const int NeuralNetworkLayer::kArgMinFieldNumber;
const int NeuralNetworkLayer::kArgMaxFieldNumber;
const int NeuralNetworkLayer::kEmbeddingNDFieldNumber;
const int NeuralNetworkLayer::kBatchedMatmulFieldNumber;
const int NeuralNetworkLayer::kGetShapeFieldNumber;
const int NeuralNetworkLayer::kLoadConstantNDFieldNumber;
const int NeuralNetworkLayer::kFillLikeFieldNumber;
const int NeuralNetworkLayer::kFillStaticFieldNumber;
const int NeuralNetworkLayer::kFillDynamicFieldNumber;
const int NeuralNetworkLayer::kBroadcastToLikeFieldNumber;
const int NeuralNetworkLayer::kBroadcastToStaticFieldNumber;
const int NeuralNetworkLayer::kBroadcastToDynamicFieldNumber;
const int NeuralNetworkLayer::kSqueezeFieldNumber;
const int NeuralNetworkLayer::kExpandDimsFieldNumber;
const int NeuralNetworkLayer::kFlattenTo2DFieldNumber;
const int NeuralNetworkLayer::kReshapeLikeFieldNumber;
const int NeuralNetworkLayer::kReshapeStaticFieldNumber;
const int NeuralNetworkLayer::kReshapeDynamicFieldNumber;
const int NeuralNetworkLayer::kRankPreservingReshapeFieldNumber;
const int NeuralNetworkLayer::kConstantPadFieldNumber;
const int NeuralNetworkLayer::kRandomNormalLikeFieldNumber;
const int NeuralNetworkLayer::kRandomNormalStaticFieldNumber;
const int NeuralNetworkLayer::kRandomNormalDynamicFieldNumber;
const int NeuralNetworkLayer::kRandomUniformLikeFieldNumber;
const int NeuralNetworkLayer::kRandomUniformStaticFieldNumber;
const int NeuralNetworkLayer::kRandomUniformDynamicFieldNumber;
const int NeuralNetworkLayer::kRandomBernoulliLikeFieldNumber;
const int NeuralNetworkLayer::kRandomBernoulliStaticFieldNumber;
const int NeuralNetworkLayer::kRandomBernoulliDynamicFieldNumber;
const int NeuralNetworkLayer::kCategoricalDistributionFieldNumber;
const int NeuralNetworkLayer::kReduceL1FieldNumber;
const int NeuralNetworkLayer::kReduceL2FieldNumber;
const int NeuralNetworkLayer::kReduceMaxFieldNumber;
const int NeuralNetworkLayer::kReduceMinFieldNumber;
const int NeuralNetworkLayer::kReduceSumFieldNumber;
const int NeuralNetworkLayer::kReduceProdFieldNumber;
const int NeuralNetworkLayer::kReduceMeanFieldNumber;
const int NeuralNetworkLayer::kReduceLogSumFieldNumber;
const int NeuralNetworkLayer::kReduceSumSquareFieldNumber;
const int NeuralNetworkLayer::kReduceLogSumExpFieldNumber;
const int NeuralNetworkLayer::kWhereNonZeroFieldNumber;
const int NeuralNetworkLayer::kMatrixBandPartFieldNumber;
const int NeuralNetworkLayer::kLowerTriangularFieldNumber;
const int NeuralNetworkLayer::kUpperTriangularFieldNumber;
const int NeuralNetworkLayer::kWhereBroadcastableFieldNumber;
const int NeuralNetworkLayer::kLayerNormalizationFieldNumber;
const int NeuralNetworkLayer::kNonMaximumSuppressionFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkLayer::NeuralNetworkLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkLayer)
}
NeuralNetworkLayer::NeuralNetworkLayer(const NeuralNetworkLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      input_(from.input_),
      output_(from.output_),
      inputtensor_(from.inputtensor_),
      outputtensor_(from.outputtensor_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.name().size() > 0) {
    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  isupdatable_ = from.isupdatable_;
  clear_has_layer();
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kResizeBilinear: {
      mutable_resizebilinear()->::CoreML::Specification::ResizeBilinearLayerParams::MergeFrom(from.resizebilinear());
      break;
    }
    case kCropResize: {
      mutable_cropresize()->::CoreML::Specification::CropResizeLayerParams::MergeFrom(from.cropresize());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kReorganizeData: {
      mutable_reorganizedata()->::CoreML::Specification::ReorganizeDataLayerParams::MergeFrom(from.reorganizedata());
      break;
    }
    case kSlice: {
      mutable_slice()->::CoreML::Specification::SliceLayerParams::MergeFrom(from.slice());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case kCustom: {
      mutable_custom()->::CoreML::Specification::CustomLayerParams::MergeFrom(from.custom());
      break;
    }
    case kCopy: {
      mutable_copy()->::CoreML::Specification::CopyLayerParams::MergeFrom(from.copy());
      break;
    }
    case kBranch: {
      mutable_branch()->::CoreML::Specification::BranchLayerParams::MergeFrom(from.branch());
      break;
    }
    case kLoop: {
      mutable_loop()->::CoreML::Specification::LoopLayerParams::MergeFrom(from.loop());
      break;
    }
    case kLoopBreak: {
      mutable_loopbreak()->::CoreML::Specification::LoopBreakLayerParams::MergeFrom(from.loopbreak());
      break;
    }
    case kLoopContinue: {
      mutable_loopcontinue()->::CoreML::Specification::LoopContinueLayerParams::MergeFrom(from.loopcontinue());
      break;
    }
    case kRangeStatic: {
      mutable_rangestatic()->::CoreML::Specification::RangeStaticLayerParams::MergeFrom(from.rangestatic());
      break;
    }
    case kRangeDynamic: {
      mutable_rangedynamic()->::CoreML::Specification::RangeDynamicLayerParams::MergeFrom(from.rangedynamic());
      break;
    }
    case kClip: {
      mutable_clip()->::CoreML::Specification::ClipLayerParams::MergeFrom(from.clip());
      break;
    }
    case kCeil: {
      mutable_ceil()->::CoreML::Specification::CeilLayerParams::MergeFrom(from.ceil());
      break;
    }
    case kFloor: {
      mutable_floor()->::CoreML::Specification::FloorLayerParams::MergeFrom(from.floor());
      break;
    }
    case kSign: {
      mutable_sign()->::CoreML::Specification::SignLayerParams::MergeFrom(from.sign());
      break;
    }
    case kRound: {
      mutable_round()->::CoreML::Specification::RoundLayerParams::MergeFrom(from.round());
      break;
    }
    case kExp2: {
      mutable_exp2()->::CoreML::Specification::Exp2LayerParams::MergeFrom(from.exp2());
      break;
    }
    case kSin: {
      mutable_sin()->::CoreML::Specification::SinLayerParams::MergeFrom(from.sin());
      break;
    }
    case kCos: {
      mutable_cos()->::CoreML::Specification::CosLayerParams::MergeFrom(from.cos());
      break;
    }
    case kTan: {
      mutable_tan()->::CoreML::Specification::TanLayerParams::MergeFrom(from.tan());
      break;
    }
    case kAsin: {
      mutable_asin()->::CoreML::Specification::AsinLayerParams::MergeFrom(from.asin());
      break;
    }
    case kAcos: {
      mutable_acos()->::CoreML::Specification::AcosLayerParams::MergeFrom(from.acos());
      break;
    }
    case kAtan: {
      mutable_atan()->::CoreML::Specification::AtanLayerParams::MergeFrom(from.atan());
      break;
    }
    case kSinh: {
      mutable_sinh()->::CoreML::Specification::SinhLayerParams::MergeFrom(from.sinh());
      break;
    }
    case kCosh: {
      mutable_cosh()->::CoreML::Specification::CoshLayerParams::MergeFrom(from.cosh());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::TanhLayerParams::MergeFrom(from.tanh());
      break;
    }
    case kAsinh: {
      mutable_asinh()->::CoreML::Specification::AsinhLayerParams::MergeFrom(from.asinh());
      break;
    }
    case kAcosh: {
      mutable_acosh()->::CoreML::Specification::AcoshLayerParams::MergeFrom(from.acosh());
      break;
    }
    case kAtanh: {
      mutable_atanh()->::CoreML::Specification::AtanhLayerParams::MergeFrom(from.atanh());
      break;
    }
    case kErf: {
      mutable_erf()->::CoreML::Specification::ErfLayerParams::MergeFrom(from.erf());
      break;
    }
    case kGelu: {
      mutable_gelu()->::CoreML::Specification::GeluLayerParams::MergeFrom(from.gelu());
      break;
    }
    case kEqual: {
      mutable_equal()->::CoreML::Specification::EqualLayerParams::MergeFrom(from.equal());
      break;
    }
    case kNotEqual: {
      mutable_notequal()->::CoreML::Specification::NotEqualLayerParams::MergeFrom(from.notequal());
      break;
    }
    case kLessThan: {
      mutable_lessthan()->::CoreML::Specification::LessThanLayerParams::MergeFrom(from.lessthan());
      break;
    }
    case kLessEqual: {
      mutable_lessequal()->::CoreML::Specification::LessEqualLayerParams::MergeFrom(from.lessequal());
      break;
    }
    case kGreaterThan: {
      mutable_greaterthan()->::CoreML::Specification::GreaterThanLayerParams::MergeFrom(from.greaterthan());
      break;
    }
    case kGreaterEqual: {
      mutable_greaterequal()->::CoreML::Specification::GreaterEqualLayerParams::MergeFrom(from.greaterequal());
      break;
    }
    case kLogicalOr: {
      mutable_logicalor()->::CoreML::Specification::LogicalOrLayerParams::MergeFrom(from.logicalor());
      break;
    }
    case kLogicalXor: {
      mutable_logicalxor()->::CoreML::Specification::LogicalXorLayerParams::MergeFrom(from.logicalxor());
      break;
    }
    case kLogicalNot: {
      mutable_logicalnot()->::CoreML::Specification::LogicalNotLayerParams::MergeFrom(from.logicalnot());
      break;
    }
    case kLogicalAnd: {
      mutable_logicaland()->::CoreML::Specification::LogicalAndLayerParams::MergeFrom(from.logicaland());
      break;
    }
    case kModBroadcastable: {
      mutable_modbroadcastable()->::CoreML::Specification::ModBroadcastableLayerParams::MergeFrom(from.modbroadcastable());
      break;
    }
    case kMinBroadcastable: {
      mutable_minbroadcastable()->::CoreML::Specification::MinBroadcastableLayerParams::MergeFrom(from.minbroadcastable());
      break;
    }
    case kMaxBroadcastable: {
      mutable_maxbroadcastable()->::CoreML::Specification::MaxBroadcastableLayerParams::MergeFrom(from.maxbroadcastable());
      break;
    }
    case kAddBroadcastable: {
      mutable_addbroadcastable()->::CoreML::Specification::AddBroadcastableLayerParams::MergeFrom(from.addbroadcastable());
      break;
    }
    case kPowBroadcastable: {
      mutable_powbroadcastable()->::CoreML::Specification::PowBroadcastableLayerParams::MergeFrom(from.powbroadcastable());
      break;
    }
    case kDivideBroadcastable: {
      mutable_dividebroadcastable()->::CoreML::Specification::DivideBroadcastableLayerParams::MergeFrom(from.dividebroadcastable());
      break;
    }
    case kFloorDivBroadcastable: {
      mutable_floordivbroadcastable()->::CoreML::Specification::FloorDivBroadcastableLayerParams::MergeFrom(from.floordivbroadcastable());
      break;
    }
    case kMultiplyBroadcastable: {
      mutable_multiplybroadcastable()->::CoreML::Specification::MultiplyBroadcastableLayerParams::MergeFrom(from.multiplybroadcastable());
      break;
    }
    case kSubtractBroadcastable: {
      mutable_subtractbroadcastable()->::CoreML::Specification::SubtractBroadcastableLayerParams::MergeFrom(from.subtractbroadcastable());
      break;
    }
    case kTile: {
      mutable_tile()->::CoreML::Specification::TileLayerParams::MergeFrom(from.tile());
      break;
    }
    case kStack: {
      mutable_stack()->::CoreML::Specification::StackLayerParams::MergeFrom(from.stack());
      break;
    }
    case kGather: {
      mutable_gather()->::CoreML::Specification::GatherLayerParams::MergeFrom(from.gather());
      break;
    }
    case kScatter: {
      mutable_scatter()->::CoreML::Specification::ScatterLayerParams::MergeFrom(from.scatter());
      break;
    }
    case kGatherND: {
      mutable_gathernd()->::CoreML::Specification::GatherNDLayerParams::MergeFrom(from.gathernd());
      break;
    }
    case kScatterND: {
      mutable_scatternd()->::CoreML::Specification::ScatterNDLayerParams::MergeFrom(from.scatternd());
      break;
    }
    case kSoftmaxND: {
      mutable_softmaxnd()->::CoreML::Specification::SoftmaxNDLayerParams::MergeFrom(from.softmaxnd());
      break;
    }
    case kGatherAlongAxis: {
      mutable_gatheralongaxis()->::CoreML::Specification::GatherAlongAxisLayerParams::MergeFrom(from.gatheralongaxis());
      break;
    }
    case kScatterAlongAxis: {
      mutable_scatteralongaxis()->::CoreML::Specification::ScatterAlongAxisLayerParams::MergeFrom(from.scatteralongaxis());
      break;
    }
    case kReverse: {
      mutable_reverse()->::CoreML::Specification::ReverseLayerParams::MergeFrom(from.reverse());
      break;
    }
    case kReverseSeq: {
      mutable_reverseseq()->::CoreML::Specification::ReverseSeqLayerParams::MergeFrom(from.reverseseq());
      break;
    }
    case kSplitND: {
      mutable_splitnd()->::CoreML::Specification::SplitNDLayerParams::MergeFrom(from.splitnd());
      break;
    }
    case kConcatND: {
      mutable_concatnd()->::CoreML::Specification::ConcatNDLayerParams::MergeFrom(from.concatnd());
      break;
    }
    case kTranspose: {
      mutable_transpose()->::CoreML::Specification::TransposeLayerParams::MergeFrom(from.transpose());
      break;
    }
    case kSliceStatic: {
      mutable_slicestatic()->::CoreML::Specification::SliceStaticLayerParams::MergeFrom(from.slicestatic());
      break;
    }
    case kSliceDynamic: {
      mutable_slicedynamic()->::CoreML::Specification::SliceDynamicLayerParams::MergeFrom(from.slicedynamic());
      break;
    }
    case kSlidingWindows: {
      mutable_slidingwindows()->::CoreML::Specification::SlidingWindowsLayerParams::MergeFrom(from.slidingwindows());
      break;
    }
    case kTopK: {
      mutable_topk()->::CoreML::Specification::TopKLayerParams::MergeFrom(from.topk());
      break;
    }
    case kArgMin: {
      mutable_argmin()->::CoreML::Specification::ArgMinLayerParams::MergeFrom(from.argmin());
      break;
    }
    case kArgMax: {
      mutable_argmax()->::CoreML::Specification::ArgMaxLayerParams::MergeFrom(from.argmax());
      break;
    }
    case kEmbeddingND: {
      mutable_embeddingnd()->::CoreML::Specification::EmbeddingNDLayerParams::MergeFrom(from.embeddingnd());
      break;
    }
    case kBatchedMatmul: {
      mutable_batchedmatmul()->::CoreML::Specification::BatchedMatMulLayerParams::MergeFrom(from.batchedmatmul());
      break;
    }
    case kGetShape: {
      mutable_getshape()->::CoreML::Specification::GetShapeLayerParams::MergeFrom(from.getshape());
      break;
    }
    case kLoadConstantND: {
      mutable_loadconstantnd()->::CoreML::Specification::LoadConstantNDLayerParams::MergeFrom(from.loadconstantnd());
      break;
    }
    case kFillLike: {
      mutable_filllike()->::CoreML::Specification::FillLikeLayerParams::MergeFrom(from.filllike());
      break;
    }
    case kFillStatic: {
      mutable_fillstatic()->::CoreML::Specification::FillStaticLayerParams::MergeFrom(from.fillstatic());
      break;
    }
    case kFillDynamic: {
      mutable_filldynamic()->::CoreML::Specification::FillDynamicLayerParams::MergeFrom(from.filldynamic());
      break;
    }
    case kBroadcastToLike: {
      mutable_broadcasttolike()->::CoreML::Specification::BroadcastToLikeLayerParams::MergeFrom(from.broadcasttolike());
      break;
    }
    case kBroadcastToStatic: {
      mutable_broadcasttostatic()->::CoreML::Specification::BroadcastToStaticLayerParams::MergeFrom(from.broadcasttostatic());
      break;
    }
    case kBroadcastToDynamic: {
      mutable_broadcasttodynamic()->::CoreML::Specification::BroadcastToDynamicLayerParams::MergeFrom(from.broadcasttodynamic());
      break;
    }
    case kSqueeze: {
      mutable_squeeze()->::CoreML::Specification::SqueezeLayerParams::MergeFrom(from.squeeze());
      break;
    }
    case kExpandDims: {
      mutable_expanddims()->::CoreML::Specification::ExpandDimsLayerParams::MergeFrom(from.expanddims());
      break;
    }
    case kFlattenTo2D: {
      mutable_flattento2d()->::CoreML::Specification::FlattenTo2DLayerParams::MergeFrom(from.flattento2d());
      break;
    }
    case kReshapeLike: {
      mutable_reshapelike()->::CoreML::Specification::ReshapeLikeLayerParams::MergeFrom(from.reshapelike());
      break;
    }
    case kReshapeStatic: {
      mutable_reshapestatic()->::CoreML::Specification::ReshapeStaticLayerParams::MergeFrom(from.reshapestatic());
      break;
    }
    case kReshapeDynamic: {
      mutable_reshapedynamic()->::CoreML::Specification::ReshapeDynamicLayerParams::MergeFrom(from.reshapedynamic());
      break;
    }
    case kRankPreservingReshape: {
      mutable_rankpreservingreshape()->::CoreML::Specification::RankPreservingReshapeLayerParams::MergeFrom(from.rankpreservingreshape());
      break;
    }
    case kConstantPad: {
      mutable_constantpad()->::CoreML::Specification::ConstantPaddingLayerParams::MergeFrom(from.constantpad());
      break;
    }
    case kRandomNormalLike: {
      mutable_randomnormallike()->::CoreML::Specification::RandomNormalLikeLayerParams::MergeFrom(from.randomnormallike());
      break;
    }
    case kRandomNormalStatic: {
      mutable_randomnormalstatic()->::CoreML::Specification::RandomNormalStaticLayerParams::MergeFrom(from.randomnormalstatic());
      break;
    }
    case kRandomNormalDynamic: {
      mutable_randomnormaldynamic()->::CoreML::Specification::RandomNormalDynamicLayerParams::MergeFrom(from.randomnormaldynamic());
      break;
    }
    case kRandomUniformLike: {
      mutable_randomuniformlike()->::CoreML::Specification::RandomUniformLikeLayerParams::MergeFrom(from.randomuniformlike());
      break;
    }
    case kRandomUniformStatic: {
      mutable_randomuniformstatic()->::CoreML::Specification::RandomUniformStaticLayerParams::MergeFrom(from.randomuniformstatic());
      break;
    }
    case kRandomUniformDynamic: {
      mutable_randomuniformdynamic()->::CoreML::Specification::RandomUniformDynamicLayerParams::MergeFrom(from.randomuniformdynamic());
      break;
    }
    case kRandomBernoulliLike: {
      mutable_randombernoullilike()->::CoreML::Specification::RandomBernoulliLikeLayerParams::MergeFrom(from.randombernoullilike());
      break;
    }
    case kRandomBernoulliStatic: {
      mutable_randombernoullistatic()->::CoreML::Specification::RandomBernoulliStaticLayerParams::MergeFrom(from.randombernoullistatic());
      break;
    }
    case kRandomBernoulliDynamic: {
      mutable_randombernoullidynamic()->::CoreML::Specification::RandomBernoulliDynamicLayerParams::MergeFrom(from.randombernoullidynamic());
      break;
    }
    case kCategoricalDistribution: {
      mutable_categoricaldistribution()->::CoreML::Specification::CategoricalDistributionLayerParams::MergeFrom(from.categoricaldistribution());
      break;
    }
    case kReduceL1: {
      mutable_reducel1()->::CoreML::Specification::ReduceL1LayerParams::MergeFrom(from.reducel1());
      break;
    }
    case kReduceL2: {
      mutable_reducel2()->::CoreML::Specification::ReduceL2LayerParams::MergeFrom(from.reducel2());
      break;
    }
    case kReduceMax: {
      mutable_reducemax()->::CoreML::Specification::ReduceMaxLayerParams::MergeFrom(from.reducemax());
      break;
    }
    case kReduceMin: {
      mutable_reducemin()->::CoreML::Specification::ReduceMinLayerParams::MergeFrom(from.reducemin());
      break;
    }
    case kReduceSum: {
      mutable_reducesum()->::CoreML::Specification::ReduceSumLayerParams::MergeFrom(from.reducesum());
      break;
    }
    case kReduceProd: {
      mutable_reduceprod()->::CoreML::Specification::ReduceProdLayerParams::MergeFrom(from.reduceprod());
      break;
    }
    case kReduceMean: {
      mutable_reducemean()->::CoreML::Specification::ReduceMeanLayerParams::MergeFrom(from.reducemean());
      break;
    }
    case kReduceLogSum: {
      mutable_reducelogsum()->::CoreML::Specification::ReduceLogSumLayerParams::MergeFrom(from.reducelogsum());
      break;
    }
    case kReduceSumSquare: {
      mutable_reducesumsquare()->::CoreML::Specification::ReduceSumSquareLayerParams::MergeFrom(from.reducesumsquare());
      break;
    }
    case kReduceLogSumExp: {
      mutable_reducelogsumexp()->::CoreML::Specification::ReduceLogSumExpLayerParams::MergeFrom(from.reducelogsumexp());
      break;
    }
    case kWhereNonZero: {
      mutable_wherenonzero()->::CoreML::Specification::WhereNonZeroLayerParams::MergeFrom(from.wherenonzero());
      break;
    }
    case kMatrixBandPart: {
      mutable_matrixbandpart()->::CoreML::Specification::MatrixBandPartLayerParams::MergeFrom(from.matrixbandpart());
      break;
    }
    case kLowerTriangular: {
      mutable_lowertriangular()->::CoreML::Specification::LowerTriangularLayerParams::MergeFrom(from.lowertriangular());
      break;
    }
    case kUpperTriangular: {
      mutable_uppertriangular()->::CoreML::Specification::UpperTriangularLayerParams::MergeFrom(from.uppertriangular());
      break;
    }
    case kWhereBroadcastable: {
      mutable_wherebroadcastable()->::CoreML::Specification::WhereBroadcastableLayerParams::MergeFrom(from.wherebroadcastable());
      break;
    }
    case kLayerNormalization: {
      mutable_layernormalization()->::CoreML::Specification::LayerNormalizationLayerParams::MergeFrom(from.layernormalization());
      break;
    }
    case kNonMaximumSuppression: {
      mutable_nonmaximumsuppression()->::CoreML::Specification::NonMaximumSuppressionLayerParams::MergeFrom(from.nonmaximumsuppression());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkLayer)
}

void NeuralNetworkLayer::SharedCtor() {
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  isupdatable_ = false;
  clear_has_layer();
  _cached_size_ = 0;
}

NeuralNetworkLayer::~NeuralNetworkLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkLayer)
  SharedDtor();
}

void NeuralNetworkLayer::SharedDtor() {
  name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_layer()) {
    clear_layer();
  }
}

void NeuralNetworkLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkLayer& NeuralNetworkLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkLayer* NeuralNetworkLayer::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkLayer* n = new NeuralNetworkLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkLayer::clear_layer() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkLayer)
  switch (layer_case()) {
    case kConvolution: {
      delete layer_.convolution_;
      break;
    }
    case kPooling: {
      delete layer_.pooling_;
      break;
    }
    case kActivation: {
      delete layer_.activation_;
      break;
    }
    case kInnerProduct: {
      delete layer_.innerproduct_;
      break;
    }
    case kEmbedding: {
      delete layer_.embedding_;
      break;
    }
    case kBatchnorm: {
      delete layer_.batchnorm_;
      break;
    }
    case kMvn: {
      delete layer_.mvn_;
      break;
    }
    case kL2Normalize: {
      delete layer_.l2normalize_;
      break;
    }
    case kSoftmax: {
      delete layer_.softmax_;
      break;
    }
    case kLrn: {
      delete layer_.lrn_;
      break;
    }
    case kCrop: {
      delete layer_.crop_;
      break;
    }
    case kPadding: {
      delete layer_.padding_;
      break;
    }
    case kUpsample: {
      delete layer_.upsample_;
      break;
    }
    case kResizeBilinear: {
      delete layer_.resizebilinear_;
      break;
    }
    case kCropResize: {
      delete layer_.cropresize_;
      break;
    }
    case kUnary: {
      delete layer_.unary_;
      break;
    }
    case kAdd: {
      delete layer_.add_;
      break;
    }
    case kMultiply: {
      delete layer_.multiply_;
      break;
    }
    case kAverage: {
      delete layer_.average_;
      break;
    }
    case kScale: {
      delete layer_.scale_;
      break;
    }
    case kBias: {
      delete layer_.bias_;
      break;
    }
    case kMax: {
      delete layer_.max_;
      break;
    }
    case kMin: {
      delete layer_.min_;
      break;
    }
    case kDot: {
      delete layer_.dot_;
      break;
    }
    case kReduce: {
      delete layer_.reduce_;
      break;
    }
    case kLoadConstant: {
      delete layer_.loadconstant_;
      break;
    }
    case kReshape: {
      delete layer_.reshape_;
      break;
    }
    case kFlatten: {
      delete layer_.flatten_;
      break;
    }
    case kPermute: {
      delete layer_.permute_;
      break;
    }
    case kConcat: {
      delete layer_.concat_;
      break;
    }
    case kSplit: {
      delete layer_.split_;
      break;
    }
    case kSequenceRepeat: {
      delete layer_.sequencerepeat_;
      break;
    }
    case kReorganizeData: {
      delete layer_.reorganizedata_;
      break;
    }
    case kSlice: {
      delete layer_.slice_;
      break;
    }
    case kSimpleRecurrent: {
      delete layer_.simplerecurrent_;
      break;
    }
    case kGru: {
      delete layer_.gru_;
      break;
    }
    case kUniDirectionalLSTM: {
      delete layer_.unidirectionallstm_;
      break;
    }
    case kBiDirectionalLSTM: {
      delete layer_.bidirectionallstm_;
      break;
    }
    case kCustom: {
      delete layer_.custom_;
      break;
    }
    case kCopy: {
      delete layer_.copy_;
      break;
    }
    case kBranch: {
      delete layer_.branch_;
      break;
    }
    case kLoop: {
      delete layer_.loop_;
      break;
    }
    case kLoopBreak: {
      delete layer_.loopbreak_;
      break;
    }
    case kLoopContinue: {
      delete layer_.loopcontinue_;
      break;
    }
    case kRangeStatic: {
      delete layer_.rangestatic_;
      break;
    }
    case kRangeDynamic: {
      delete layer_.rangedynamic_;
      break;
    }
    case kClip: {
      delete layer_.clip_;
      break;
    }
    case kCeil: {
      delete layer_.ceil_;
      break;
    }
    case kFloor: {
      delete layer_.floor_;
      break;
    }
    case kSign: {
      delete layer_.sign_;
      break;
    }
    case kRound: {
      delete layer_.round_;
      break;
    }
    case kExp2: {
      delete layer_.exp2_;
      break;
    }
    case kSin: {
      delete layer_.sin_;
      break;
    }
    case kCos: {
      delete layer_.cos_;
      break;
    }
    case kTan: {
      delete layer_.tan_;
      break;
    }
    case kAsin: {
      delete layer_.asin_;
      break;
    }
    case kAcos: {
      delete layer_.acos_;
      break;
    }
    case kAtan: {
      delete layer_.atan_;
      break;
    }
    case kSinh: {
      delete layer_.sinh_;
      break;
    }
    case kCosh: {
      delete layer_.cosh_;
      break;
    }
    case kTanh: {
      delete layer_.tanh_;
      break;
    }
    case kAsinh: {
      delete layer_.asinh_;
      break;
    }
    case kAcosh: {
      delete layer_.acosh_;
      break;
    }
    case kAtanh: {
      delete layer_.atanh_;
      break;
    }
    case kErf: {
      delete layer_.erf_;
      break;
    }
    case kGelu: {
      delete layer_.gelu_;
      break;
    }
    case kEqual: {
      delete layer_.equal_;
      break;
    }
    case kNotEqual: {
      delete layer_.notequal_;
      break;
    }
    case kLessThan: {
      delete layer_.lessthan_;
      break;
    }
    case kLessEqual: {
      delete layer_.lessequal_;
      break;
    }
    case kGreaterThan: {
      delete layer_.greaterthan_;
      break;
    }
    case kGreaterEqual: {
      delete layer_.greaterequal_;
      break;
    }
    case kLogicalOr: {
      delete layer_.logicalor_;
      break;
    }
    case kLogicalXor: {
      delete layer_.logicalxor_;
      break;
    }
    case kLogicalNot: {
      delete layer_.logicalnot_;
      break;
    }
    case kLogicalAnd: {
      delete layer_.logicaland_;
      break;
    }
    case kModBroadcastable: {
      delete layer_.modbroadcastable_;
      break;
    }
    case kMinBroadcastable: {
      delete layer_.minbroadcastable_;
      break;
    }
    case kMaxBroadcastable: {
      delete layer_.maxbroadcastable_;
      break;
    }
    case kAddBroadcastable: {
      delete layer_.addbroadcastable_;
      break;
    }
    case kPowBroadcastable: {
      delete layer_.powbroadcastable_;
      break;
    }
    case kDivideBroadcastable: {
      delete layer_.dividebroadcastable_;
      break;
    }
    case kFloorDivBroadcastable: {
      delete layer_.floordivbroadcastable_;
      break;
    }
    case kMultiplyBroadcastable: {
      delete layer_.multiplybroadcastable_;
      break;
    }
    case kSubtractBroadcastable: {
      delete layer_.subtractbroadcastable_;
      break;
    }
    case kTile: {
      delete layer_.tile_;
      break;
    }
    case kStack: {
      delete layer_.stack_;
      break;
    }
    case kGather: {
      delete layer_.gather_;
      break;
    }
    case kScatter: {
      delete layer_.scatter_;
      break;
    }
    case kGatherND: {
      delete layer_.gathernd_;
      break;
    }
    case kScatterND: {
      delete layer_.scatternd_;
      break;
    }
    case kSoftmaxND: {
      delete layer_.softmaxnd_;
      break;
    }
    case kGatherAlongAxis: {
      delete layer_.gatheralongaxis_;
      break;
    }
    case kScatterAlongAxis: {
      delete layer_.scatteralongaxis_;
      break;
    }
    case kReverse: {
      delete layer_.reverse_;
      break;
    }
    case kReverseSeq: {
      delete layer_.reverseseq_;
      break;
    }
    case kSplitND: {
      delete layer_.splitnd_;
      break;
    }
    case kConcatND: {
      delete layer_.concatnd_;
      break;
    }
    case kTranspose: {
      delete layer_.transpose_;
      break;
    }
    case kSliceStatic: {
      delete layer_.slicestatic_;
      break;
    }
    case kSliceDynamic: {
      delete layer_.slicedynamic_;
      break;
    }
    case kSlidingWindows: {
      delete layer_.slidingwindows_;
      break;
    }
    case kTopK: {
      delete layer_.topk_;
      break;
    }
    case kArgMin: {
      delete layer_.argmin_;
      break;
    }
    case kArgMax: {
      delete layer_.argmax_;
      break;
    }
    case kEmbeddingND: {
      delete layer_.embeddingnd_;
      break;
    }
    case kBatchedMatmul: {
      delete layer_.batchedmatmul_;
      break;
    }
    case kGetShape: {
      delete layer_.getshape_;
      break;
    }
    case kLoadConstantND: {
      delete layer_.loadconstantnd_;
      break;
    }
    case kFillLike: {
      delete layer_.filllike_;
      break;
    }
    case kFillStatic: {
      delete layer_.fillstatic_;
      break;
    }
    case kFillDynamic: {
      delete layer_.filldynamic_;
      break;
    }
    case kBroadcastToLike: {
      delete layer_.broadcasttolike_;
      break;
    }
    case kBroadcastToStatic: {
      delete layer_.broadcasttostatic_;
      break;
    }
    case kBroadcastToDynamic: {
      delete layer_.broadcasttodynamic_;
      break;
    }
    case kSqueeze: {
      delete layer_.squeeze_;
      break;
    }
    case kExpandDims: {
      delete layer_.expanddims_;
      break;
    }
    case kFlattenTo2D: {
      delete layer_.flattento2d_;
      break;
    }
    case kReshapeLike: {
      delete layer_.reshapelike_;
      break;
    }
    case kReshapeStatic: {
      delete layer_.reshapestatic_;
      break;
    }
    case kReshapeDynamic: {
      delete layer_.reshapedynamic_;
      break;
    }
    case kRankPreservingReshape: {
      delete layer_.rankpreservingreshape_;
      break;
    }
    case kConstantPad: {
      delete layer_.constantpad_;
      break;
    }
    case kRandomNormalLike: {
      delete layer_.randomnormallike_;
      break;
    }
    case kRandomNormalStatic: {
      delete layer_.randomnormalstatic_;
      break;
    }
    case kRandomNormalDynamic: {
      delete layer_.randomnormaldynamic_;
      break;
    }
    case kRandomUniformLike: {
      delete layer_.randomuniformlike_;
      break;
    }
    case kRandomUniformStatic: {
      delete layer_.randomuniformstatic_;
      break;
    }
    case kRandomUniformDynamic: {
      delete layer_.randomuniformdynamic_;
      break;
    }
    case kRandomBernoulliLike: {
      delete layer_.randombernoullilike_;
      break;
    }
    case kRandomBernoulliStatic: {
      delete layer_.randombernoullistatic_;
      break;
    }
    case kRandomBernoulliDynamic: {
      delete layer_.randombernoullidynamic_;
      break;
    }
    case kCategoricalDistribution: {
      delete layer_.categoricaldistribution_;
      break;
    }
    case kReduceL1: {
      delete layer_.reducel1_;
      break;
    }
    case kReduceL2: {
      delete layer_.reducel2_;
      break;
    }
    case kReduceMax: {
      delete layer_.reducemax_;
      break;
    }
    case kReduceMin: {
      delete layer_.reducemin_;
      break;
    }
    case kReduceSum: {
      delete layer_.reducesum_;
      break;
    }
    case kReduceProd: {
      delete layer_.reduceprod_;
      break;
    }
    case kReduceMean: {
      delete layer_.reducemean_;
      break;
    }
    case kReduceLogSum: {
      delete layer_.reducelogsum_;
      break;
    }
    case kReduceSumSquare: {
      delete layer_.reducesumsquare_;
      break;
    }
    case kReduceLogSumExp: {
      delete layer_.reducelogsumexp_;
      break;
    }
    case kWhereNonZero: {
      delete layer_.wherenonzero_;
      break;
    }
    case kMatrixBandPart: {
      delete layer_.matrixbandpart_;
      break;
    }
    case kLowerTriangular: {
      delete layer_.lowertriangular_;
      break;
    }
    case kUpperTriangular: {
      delete layer_.uppertriangular_;
      break;
    }
    case kWhereBroadcastable: {
      delete layer_.wherebroadcastable_;
      break;
    }
    case kLayerNormalization: {
      delete layer_.layernormalization_;
      break;
    }
    case kNonMaximumSuppression: {
      delete layer_.nonmaximumsuppression_;
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LAYER_NOT_SET;
}


void NeuralNetworkLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkLayer)
  input_.Clear();
  output_.Clear();
  inputtensor_.Clear();
  outputtensor_.Clear();
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  isupdatable_ = false;
  clear_layer();
}

bool NeuralNetworkLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string name = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->name().data(), this->name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.name"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string input = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input(this->input_size() - 1).data(),
            this->input(this->input_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string output = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_output()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->output(this->output_size() - 1).data(),
            this->output(this->output_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.output"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.Tensor inputTensor = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_inputtensor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.Tensor outputTensor = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_outputtensor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isUpdatable = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isupdatable_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_convolution()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PoolingLayerParams pooling = 120;
      case 120: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_pooling()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParams activation = 130;
      case 130: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
      case 140: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_innerproduct()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
      case 150: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_embedding()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
      case 160: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_batchnorm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
      case 165: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mvn()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
      case 170: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_l2normalize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
      case 175: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softmax()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LRNLayerParams lrn = 180;
      case 180: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lrn()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CropLayerParams crop = 190;
      case 190: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_crop()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams padding = 200;
      case 200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_padding()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpsampleLayerParams upsample = 210;
      case 210: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_upsample()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
      case 211: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1690u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resizebilinear()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
      case 212: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1698u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cropresize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
      case 220: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unary()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AddLayerParams add = 230;
      case 230: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_add()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MultiplyLayerParams multiply = 231;
      case 231: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1850u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multiply()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AverageLayerParams average = 240;
      case 240: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_average()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScaleLayerParams scale = 245;
      case 245: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BiasLayerParams bias = 250;
      case 250: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MaxLayerParams max = 260;
      case 260: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_max()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MinLayerParams min = 261;
      case 261: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2090u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_min()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DotProductLayerParams dot = 270;
      case 270: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_dot()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLayerParams reduce = 280;
      case 280: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reduce()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
      case 290: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loadconstant()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLayerParams reshape = 300;
      case 300: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FlattenLayerParams flatten = 301;
      case 301: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_flatten()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PermuteLayerParams permute = 310;
      case 310: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_permute()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConcatLayerParams concat = 320;
      case 320: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_concat()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SplitLayerParams split = 330;
      case 330: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_split()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
      case 340: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sequencerepeat()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
      case 345: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reorganizedata()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceLayerParams slice = 350;
      case 350: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slice()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
      case 400: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_simplerecurrent()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GRULayerParams gru = 410;
      case 410: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gru()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
      case 420: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
      case 430: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CustomLayerParams custom = 500;
      case 500: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_custom()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CopyLayerParams copy = 600;
      case 600: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_copy()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BranchLayerParams branch = 605;
      case 605: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_branch()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoopLayerParams loop = 615;
      case 615: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loop()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
      case 620: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loopbreak()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
      case 625: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loopcontinue()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
      case 635: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_rangestatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
      case 640: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_rangedynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ClipLayerParams clip = 660;
      case 660: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_clip()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CeilLayerParams ceil = 665;
      case 665: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_ceil()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FloorLayerParams floor = 670;
      case 670: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_floor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SignLayerParams sign = 680;
      case 680: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sign()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RoundLayerParams round = 685;
      case 685: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_round()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Exp2LayerParams exp2 = 700;
      case 700: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_exp2()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SinLayerParams sin = 710;
      case 710: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sin()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CosLayerParams cos = 715;
      case 715: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cos()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TanLayerParams tan = 720;
      case 720: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tan()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AsinLayerParams asin = 730;
      case 730: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_asin()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AcosLayerParams acos = 735;
      case 735: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5882u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_acos()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AtanLayerParams atan = 740;
      case 740: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_atan()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SinhLayerParams sinh = 750;
      case 750: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sinh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CoshLayerParams cosh = 755;
      case 755: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cosh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TanhLayerParams tanh = 760;
      case 760: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AsinhLayerParams asinh = 770;
      case 770: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_asinh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AcoshLayerParams acosh = 775;
      case 775: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_acosh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AtanhLayerParams atanh = 780;
      case 780: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_atanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ErfLayerParams erf = 790;
      case 790: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_erf()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GeluLayerParams gelu = 795;
      case 795: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EqualLayerParams equal = 815;
      case 815: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_equal()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
      case 820: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_notequal()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LessThanLayerParams lessThan = 825;
      case 825: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lessthan()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
      case 827: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6618u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lessequal()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
      case 830: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_greaterthan()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
      case 832: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6658u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_greaterequal()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
      case 840: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicalor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
      case 845: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicalxor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
      case 850: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicalnot()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
      case 855: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicaland()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
      case 865: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_modbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
      case 870: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_minbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
      case 875: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_maxbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
      case 880: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_addbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
      case 885: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_powbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
      case 890: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_dividebroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
      case 895: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_floordivbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
      case 900: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multiplybroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
      case 905: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_subtractbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TileLayerParams tile = 920;
      case 920: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tile()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.StackLayerParams stack = 925;
      case 925: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_stack()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GatherLayerParams gather = 930;
      case 930: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gather()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScatterLayerParams scatter = 935;
      case 935: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scatter()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GatherNDLayerParams gatherND = 940;
      case 940: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gathernd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
      case 945: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scatternd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
      case 950: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softmaxnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
      case 952: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7618u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gatheralongaxis()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
      case 954: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7634u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scatteralongaxis()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReverseLayerParams reverse = 960;
      case 960: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reverse()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
      case 965: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reverseseq()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SplitNDLayerParams splitND = 975;
      case 975: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_splitnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
      case 980: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_concatnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TransposeLayerParams transpose = 985;
      case 985: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7882u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_transpose()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
      case 995: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slicestatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
      case 1000: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slicedynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
      case 1005: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slidingwindows()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TopKLayerParams topK = 1015;
      case 1015: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_topk()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ArgMinLayerParams argMin = 1020;
      case 1020: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_argmin()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
      case 1025: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_argmax()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
      case 1040: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_embeddingnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
      case 1045: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_batchedmatmul()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
      case 1065: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_getshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
      case 1070: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loadconstantnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
      case 1080: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_filllike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
      case 1085: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_fillstatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
      case 1090: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_filldynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
      case 1100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_broadcasttolike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
      case 1105: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_broadcasttostatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
      case 1110: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8882u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_broadcasttodynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
      case 1120: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_squeeze()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
      case 1125: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_expanddims()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
      case 1130: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_flattento2d()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
      case 1135: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshapelike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
      case 1140: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshapestatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
      case 1145: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshapedynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
      case 1150: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_rankpreservingreshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
      case 1155: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_constantpad()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
      case 1170: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randomnormallike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
      case 1175: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randomnormalstatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
      case 1180: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randomnormaldynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
      case 1190: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randomuniformlike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
      case 1195: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randomuniformstatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
      case 1200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randomuniformdynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
      case 1210: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randombernoullilike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
      case 1215: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randombernoullistatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
      case 1220: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_randombernoullidynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
      case 1230: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_categoricaldistribution()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
      case 1250: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducel1()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
      case 1255: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducel2()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
      case 1260: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducemax()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
      case 1265: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducemin()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
      case 1270: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducesum()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
      case 1275: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reduceprod()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
      case 1280: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducemean()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
      case 1285: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducelogsum()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
      case 1290: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducesumsquare()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
      case 1295: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reducelogsumexp()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
      case 1313: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10506u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_wherenonzero()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
      case 1315: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_matrixbandpart()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
      case 1320: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lowertriangular()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
      case 1325: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_uppertriangular()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
      case 1330: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_wherebroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
      case 1350: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_layernormalization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
      case 1400: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(11202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_nonmaximumsuppression()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkLayer)
  return false;
#undef DO_
}

void NeuralNetworkLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string name = 1;
  if (this->name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->name().data(), this->name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->name(), output);
  }

  // repeated string input = 2;
  for (int i = 0, n = this->input_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input(i).data(), this->input(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      2, this->input(i), output);
  }

  // repeated string output = 3;
  for (int i = 0, n = this->output_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output(i).data(), this->output(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.output");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      3, this->output(i), output);
  }

  // repeated .CoreML.Specification.Tensor inputTensor = 4;
  for (unsigned int i = 0, n = this->inputtensor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, this->inputtensor(i), output);
  }

  // repeated .CoreML.Specification.Tensor outputTensor = 5;
  for (unsigned int i = 0, n = this->outputtensor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, this->outputtensor(i), output);
  }

  // bool isUpdatable = 10;
  if (this->isupdatable() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->isupdatable(), output);
  }

  // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  if (has_convolution()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *layer_.convolution_, output);
  }

  // .CoreML.Specification.PoolingLayerParams pooling = 120;
  if (has_pooling()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      120, *layer_.pooling_, output);
  }

  // .CoreML.Specification.ActivationParams activation = 130;
  if (has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      130, *layer_.activation_, output);
  }

  // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  if (has_innerproduct()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      140, *layer_.innerproduct_, output);
  }

  // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  if (has_embedding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      150, *layer_.embedding_, output);
  }

  // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  if (has_batchnorm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      160, *layer_.batchnorm_, output);
  }

  // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  if (has_mvn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      165, *layer_.mvn_, output);
  }

  // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  if (has_l2normalize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      170, *layer_.l2normalize_, output);
  }

  // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  if (has_softmax()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      175, *layer_.softmax_, output);
  }

  // .CoreML.Specification.LRNLayerParams lrn = 180;
  if (has_lrn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      180, *layer_.lrn_, output);
  }

  // .CoreML.Specification.CropLayerParams crop = 190;
  if (has_crop()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      190, *layer_.crop_, output);
  }

  // .CoreML.Specification.PaddingLayerParams padding = 200;
  if (has_padding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      200, *layer_.padding_, output);
  }

  // .CoreML.Specification.UpsampleLayerParams upsample = 210;
  if (has_upsample()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      210, *layer_.upsample_, output);
  }

  // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
  if (has_resizebilinear()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      211, *layer_.resizebilinear_, output);
  }

  // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
  if (has_cropresize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      212, *layer_.cropresize_, output);
  }

  // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  if (has_unary()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      220, *layer_.unary_, output);
  }

  // .CoreML.Specification.AddLayerParams add = 230;
  if (has_add()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      230, *layer_.add_, output);
  }

  // .CoreML.Specification.MultiplyLayerParams multiply = 231;
  if (has_multiply()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      231, *layer_.multiply_, output);
  }

  // .CoreML.Specification.AverageLayerParams average = 240;
  if (has_average()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      240, *layer_.average_, output);
  }

  // .CoreML.Specification.ScaleLayerParams scale = 245;
  if (has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      245, *layer_.scale_, output);
  }

  // .CoreML.Specification.BiasLayerParams bias = 250;
  if (has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      250, *layer_.bias_, output);
  }

  // .CoreML.Specification.MaxLayerParams max = 260;
  if (has_max()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      260, *layer_.max_, output);
  }

  // .CoreML.Specification.MinLayerParams min = 261;
  if (has_min()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      261, *layer_.min_, output);
  }

  // .CoreML.Specification.DotProductLayerParams dot = 270;
  if (has_dot()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      270, *layer_.dot_, output);
  }

  // .CoreML.Specification.ReduceLayerParams reduce = 280;
  if (has_reduce()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      280, *layer_.reduce_, output);
  }

  // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  if (has_loadconstant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      290, *layer_.loadconstant_, output);
  }

  // .CoreML.Specification.ReshapeLayerParams reshape = 300;
  if (has_reshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      300, *layer_.reshape_, output);
  }

  // .CoreML.Specification.FlattenLayerParams flatten = 301;
  if (has_flatten()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      301, *layer_.flatten_, output);
  }

  // .CoreML.Specification.PermuteLayerParams permute = 310;
  if (has_permute()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      310, *layer_.permute_, output);
  }

  // .CoreML.Specification.ConcatLayerParams concat = 320;
  if (has_concat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      320, *layer_.concat_, output);
  }

  // .CoreML.Specification.SplitLayerParams split = 330;
  if (has_split()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      330, *layer_.split_, output);
  }

  // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  if (has_sequencerepeat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      340, *layer_.sequencerepeat_, output);
  }

  // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
  if (has_reorganizedata()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      345, *layer_.reorganizedata_, output);
  }

  // .CoreML.Specification.SliceLayerParams slice = 350;
  if (has_slice()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      350, *layer_.slice_, output);
  }

  // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  if (has_simplerecurrent()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      400, *layer_.simplerecurrent_, output);
  }

  // .CoreML.Specification.GRULayerParams gru = 410;
  if (has_gru()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      410, *layer_.gru_, output);
  }

  // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  if (has_unidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      420, *layer_.unidirectionallstm_, output);
  }

  // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  if (has_bidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      430, *layer_.bidirectionallstm_, output);
  }

  // .CoreML.Specification.CustomLayerParams custom = 500;
  if (has_custom()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      500, *layer_.custom_, output);
  }

  // .CoreML.Specification.CopyLayerParams copy = 600;
  if (has_copy()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      600, *layer_.copy_, output);
  }

  // .CoreML.Specification.BranchLayerParams branch = 605;
  if (has_branch()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      605, *layer_.branch_, output);
  }

  // .CoreML.Specification.LoopLayerParams loop = 615;
  if (has_loop()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      615, *layer_.loop_, output);
  }

  // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
  if (has_loopbreak()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      620, *layer_.loopbreak_, output);
  }

  // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
  if (has_loopcontinue()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      625, *layer_.loopcontinue_, output);
  }

  // .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
  if (has_rangestatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      635, *layer_.rangestatic_, output);
  }

  // .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
  if (has_rangedynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      640, *layer_.rangedynamic_, output);
  }

  // .CoreML.Specification.ClipLayerParams clip = 660;
  if (has_clip()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      660, *layer_.clip_, output);
  }

  // .CoreML.Specification.CeilLayerParams ceil = 665;
  if (has_ceil()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      665, *layer_.ceil_, output);
  }

  // .CoreML.Specification.FloorLayerParams floor = 670;
  if (has_floor()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      670, *layer_.floor_, output);
  }

  // .CoreML.Specification.SignLayerParams sign = 680;
  if (has_sign()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      680, *layer_.sign_, output);
  }

  // .CoreML.Specification.RoundLayerParams round = 685;
  if (has_round()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      685, *layer_.round_, output);
  }

  // .CoreML.Specification.Exp2LayerParams exp2 = 700;
  if (has_exp2()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      700, *layer_.exp2_, output);
  }

  // .CoreML.Specification.SinLayerParams sin = 710;
  if (has_sin()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      710, *layer_.sin_, output);
  }

  // .CoreML.Specification.CosLayerParams cos = 715;
  if (has_cos()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      715, *layer_.cos_, output);
  }

  // .CoreML.Specification.TanLayerParams tan = 720;
  if (has_tan()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      720, *layer_.tan_, output);
  }

  // .CoreML.Specification.AsinLayerParams asin = 730;
  if (has_asin()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      730, *layer_.asin_, output);
  }

  // .CoreML.Specification.AcosLayerParams acos = 735;
  if (has_acos()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      735, *layer_.acos_, output);
  }

  // .CoreML.Specification.AtanLayerParams atan = 740;
  if (has_atan()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      740, *layer_.atan_, output);
  }

  // .CoreML.Specification.SinhLayerParams sinh = 750;
  if (has_sinh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      750, *layer_.sinh_, output);
  }

  // .CoreML.Specification.CoshLayerParams cosh = 755;
  if (has_cosh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      755, *layer_.cosh_, output);
  }

  // .CoreML.Specification.TanhLayerParams tanh = 760;
  if (has_tanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      760, *layer_.tanh_, output);
  }

  // .CoreML.Specification.AsinhLayerParams asinh = 770;
  if (has_asinh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      770, *layer_.asinh_, output);
  }

  // .CoreML.Specification.AcoshLayerParams acosh = 775;
  if (has_acosh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      775, *layer_.acosh_, output);
  }

  // .CoreML.Specification.AtanhLayerParams atanh = 780;
  if (has_atanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      780, *layer_.atanh_, output);
  }

  // .CoreML.Specification.ErfLayerParams erf = 790;
  if (has_erf()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      790, *layer_.erf_, output);
  }

  // .CoreML.Specification.GeluLayerParams gelu = 795;
  if (has_gelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      795, *layer_.gelu_, output);
  }

  // .CoreML.Specification.EqualLayerParams equal = 815;
  if (has_equal()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      815, *layer_.equal_, output);
  }

  // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
  if (has_notequal()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      820, *layer_.notequal_, output);
  }

  // .CoreML.Specification.LessThanLayerParams lessThan = 825;
  if (has_lessthan()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      825, *layer_.lessthan_, output);
  }

  // .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
  if (has_lessequal()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      827, *layer_.lessequal_, output);
  }

  // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
  if (has_greaterthan()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      830, *layer_.greaterthan_, output);
  }

  // .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
  if (has_greaterequal()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      832, *layer_.greaterequal_, output);
  }

  // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
  if (has_logicalor()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      840, *layer_.logicalor_, output);
  }

  // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
  if (has_logicalxor()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      845, *layer_.logicalxor_, output);
  }

  // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
  if (has_logicalnot()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      850, *layer_.logicalnot_, output);
  }

  // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
  if (has_logicaland()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      855, *layer_.logicaland_, output);
  }

  // .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
  if (has_modbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      865, *layer_.modbroadcastable_, output);
  }

  // .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
  if (has_minbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      870, *layer_.minbroadcastable_, output);
  }

  // .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
  if (has_maxbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      875, *layer_.maxbroadcastable_, output);
  }

  // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
  if (has_addbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      880, *layer_.addbroadcastable_, output);
  }

  // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
  if (has_powbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      885, *layer_.powbroadcastable_, output);
  }

  // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
  if (has_dividebroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      890, *layer_.dividebroadcastable_, output);
  }

  // .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
  if (has_floordivbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      895, *layer_.floordivbroadcastable_, output);
  }

  // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
  if (has_multiplybroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      900, *layer_.multiplybroadcastable_, output);
  }

  // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
  if (has_subtractbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      905, *layer_.subtractbroadcastable_, output);
  }

  // .CoreML.Specification.TileLayerParams tile = 920;
  if (has_tile()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      920, *layer_.tile_, output);
  }

  // .CoreML.Specification.StackLayerParams stack = 925;
  if (has_stack()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      925, *layer_.stack_, output);
  }

  // .CoreML.Specification.GatherLayerParams gather = 930;
  if (has_gather()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      930, *layer_.gather_, output);
  }

  // .CoreML.Specification.ScatterLayerParams scatter = 935;
  if (has_scatter()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      935, *layer_.scatter_, output);
  }

  // .CoreML.Specification.GatherNDLayerParams gatherND = 940;
  if (has_gathernd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      940, *layer_.gathernd_, output);
  }

  // .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
  if (has_scatternd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      945, *layer_.scatternd_, output);
  }

  // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
  if (has_softmaxnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      950, *layer_.softmaxnd_, output);
  }

  // .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
  if (has_gatheralongaxis()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      952, *layer_.gatheralongaxis_, output);
  }

  // .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
  if (has_scatteralongaxis()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      954, *layer_.scatteralongaxis_, output);
  }

  // .CoreML.Specification.ReverseLayerParams reverse = 960;
  if (has_reverse()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      960, *layer_.reverse_, output);
  }

  // .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
  if (has_reverseseq()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      965, *layer_.reverseseq_, output);
  }

  // .CoreML.Specification.SplitNDLayerParams splitND = 975;
  if (has_splitnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      975, *layer_.splitnd_, output);
  }

  // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
  if (has_concatnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      980, *layer_.concatnd_, output);
  }

  // .CoreML.Specification.TransposeLayerParams transpose = 985;
  if (has_transpose()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      985, *layer_.transpose_, output);
  }

  // .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
  if (has_slicestatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      995, *layer_.slicestatic_, output);
  }

  // .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
  if (has_slicedynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1000, *layer_.slicedynamic_, output);
  }

  // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
  if (has_slidingwindows()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1005, *layer_.slidingwindows_, output);
  }

  // .CoreML.Specification.TopKLayerParams topK = 1015;
  if (has_topk()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1015, *layer_.topk_, output);
  }

  // .CoreML.Specification.ArgMinLayerParams argMin = 1020;
  if (has_argmin()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1020, *layer_.argmin_, output);
  }

  // .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
  if (has_argmax()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1025, *layer_.argmax_, output);
  }

  // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
  if (has_embeddingnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1040, *layer_.embeddingnd_, output);
  }

  // .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
  if (has_batchedmatmul()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1045, *layer_.batchedmatmul_, output);
  }

  // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
  if (has_getshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1065, *layer_.getshape_, output);
  }

  // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
  if (has_loadconstantnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1070, *layer_.loadconstantnd_, output);
  }

  // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
  if (has_filllike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1080, *layer_.filllike_, output);
  }

  // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
  if (has_fillstatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1085, *layer_.fillstatic_, output);
  }

  // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
  if (has_filldynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1090, *layer_.filldynamic_, output);
  }

  // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
  if (has_broadcasttolike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1100, *layer_.broadcasttolike_, output);
  }

  // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
  if (has_broadcasttostatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1105, *layer_.broadcasttostatic_, output);
  }

  // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
  if (has_broadcasttodynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1110, *layer_.broadcasttodynamic_, output);
  }

  // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
  if (has_squeeze()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1120, *layer_.squeeze_, output);
  }

  // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
  if (has_expanddims()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1125, *layer_.expanddims_, output);
  }

  // .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
  if (has_flattento2d()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1130, *layer_.flattento2d_, output);
  }

  // .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
  if (has_reshapelike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1135, *layer_.reshapelike_, output);
  }

  // .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
  if (has_reshapestatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1140, *layer_.reshapestatic_, output);
  }

  // .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
  if (has_reshapedynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1145, *layer_.reshapedynamic_, output);
  }

  // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
  if (has_rankpreservingreshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1150, *layer_.rankpreservingreshape_, output);
  }

  // .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
  if (has_constantpad()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1155, *layer_.constantpad_, output);
  }

  // .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
  if (has_randomnormallike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1170, *layer_.randomnormallike_, output);
  }

  // .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
  if (has_randomnormalstatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1175, *layer_.randomnormalstatic_, output);
  }

  // .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
  if (has_randomnormaldynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1180, *layer_.randomnormaldynamic_, output);
  }

  // .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
  if (has_randomuniformlike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1190, *layer_.randomuniformlike_, output);
  }

  // .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
  if (has_randomuniformstatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1195, *layer_.randomuniformstatic_, output);
  }

  // .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
  if (has_randomuniformdynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1200, *layer_.randomuniformdynamic_, output);
  }

  // .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
  if (has_randombernoullilike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1210, *layer_.randombernoullilike_, output);
  }

  // .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
  if (has_randombernoullistatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1215, *layer_.randombernoullistatic_, output);
  }

  // .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
  if (has_randombernoullidynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1220, *layer_.randombernoullidynamic_, output);
  }

  // .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
  if (has_categoricaldistribution()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1230, *layer_.categoricaldistribution_, output);
  }

  // .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
  if (has_reducel1()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1250, *layer_.reducel1_, output);
  }

  // .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
  if (has_reducel2()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1255, *layer_.reducel2_, output);
  }

  // .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
  if (has_reducemax()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1260, *layer_.reducemax_, output);
  }

  // .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
  if (has_reducemin()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1265, *layer_.reducemin_, output);
  }

  // .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
  if (has_reducesum()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1270, *layer_.reducesum_, output);
  }

  // .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
  if (has_reduceprod()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1275, *layer_.reduceprod_, output);
  }

  // .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
  if (has_reducemean()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1280, *layer_.reducemean_, output);
  }

  // .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
  if (has_reducelogsum()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1285, *layer_.reducelogsum_, output);
  }

  // .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
  if (has_reducesumsquare()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1290, *layer_.reducesumsquare_, output);
  }

  // .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
  if (has_reducelogsumexp()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1295, *layer_.reducelogsumexp_, output);
  }

  // .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
  if (has_wherenonzero()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1313, *layer_.wherenonzero_, output);
  }

  // .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
  if (has_matrixbandpart()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1315, *layer_.matrixbandpart_, output);
  }

  // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
  if (has_lowertriangular()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1320, *layer_.lowertriangular_, output);
  }

  // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
  if (has_uppertriangular()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1325, *layer_.uppertriangular_, output);
  }

  // .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
  if (has_wherebroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1330, *layer_.wherebroadcastable_, output);
  }

  // .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
  if (has_layernormalization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1350, *layer_.layernormalization_, output);
  }

  // .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
  if (has_nonmaximumsuppression()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1400, *layer_.nonmaximumsuppression_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkLayer)
}

size_t NeuralNetworkLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkLayer)
  size_t total_size = 0;

  // repeated string input = 2;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->input_size());
  for (int i = 0, n = this->input_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->input(i));
  }

  // repeated string output = 3;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->output_size());
  for (int i = 0, n = this->output_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->output(i));
  }

  // repeated .CoreML.Specification.Tensor inputTensor = 4;
  {
    unsigned int count = this->inputtensor_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->inputtensor(i));
    }
  }

  // repeated .CoreML.Specification.Tensor outputTensor = 5;
  {
    unsigned int count = this->outputtensor_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->outputtensor(i));
    }
  }

  // string name = 1;
  if (this->name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->name());
  }

  // bool isUpdatable = 10;
  if (this->isupdatable() != 0) {
    total_size += 1 + 1;
  }

  switch (layer_case()) {
    // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
    case kConvolution: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.convolution_);
      break;
    }
    // .CoreML.Specification.PoolingLayerParams pooling = 120;
    case kPooling: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.pooling_);
      break;
    }
    // .CoreML.Specification.ActivationParams activation = 130;
    case kActivation: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.activation_);
      break;
    }
    // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
    case kInnerProduct: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.innerproduct_);
      break;
    }
    // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
    case kEmbedding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.embedding_);
      break;
    }
    // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
    case kBatchnorm: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.batchnorm_);
      break;
    }
    // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
    case kMvn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.mvn_);
      break;
    }
    // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
    case kL2Normalize: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.l2normalize_);
      break;
    }
    // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
    case kSoftmax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.softmax_);
      break;
    }
    // .CoreML.Specification.LRNLayerParams lrn = 180;
    case kLrn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lrn_);
      break;
    }
    // .CoreML.Specification.CropLayerParams crop = 190;
    case kCrop: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.crop_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams padding = 200;
    case kPadding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.padding_);
      break;
    }
    // .CoreML.Specification.UpsampleLayerParams upsample = 210;
    case kUpsample: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.upsample_);
      break;
    }
    // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
    case kResizeBilinear: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.resizebilinear_);
      break;
    }
    // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
    case kCropResize: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.cropresize_);
      break;
    }
    // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
    case kUnary: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unary_);
      break;
    }
    // .CoreML.Specification.AddLayerParams add = 230;
    case kAdd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.add_);
      break;
    }
    // .CoreML.Specification.MultiplyLayerParams multiply = 231;
    case kMultiply: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.multiply_);
      break;
    }
    // .CoreML.Specification.AverageLayerParams average = 240;
    case kAverage: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.average_);
      break;
    }
    // .CoreML.Specification.ScaleLayerParams scale = 245;
    case kScale: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scale_);
      break;
    }
    // .CoreML.Specification.BiasLayerParams bias = 250;
    case kBias: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bias_);
      break;
    }
    // .CoreML.Specification.MaxLayerParams max = 260;
    case kMax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.max_);
      break;
    }
    // .CoreML.Specification.MinLayerParams min = 261;
    case kMin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.min_);
      break;
    }
    // .CoreML.Specification.DotProductLayerParams dot = 270;
    case kDot: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.dot_);
      break;
    }
    // .CoreML.Specification.ReduceLayerParams reduce = 280;
    case kReduce: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reduce_);
      break;
    }
    // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
    case kLoadConstant: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loadconstant_);
      break;
    }
    // .CoreML.Specification.ReshapeLayerParams reshape = 300;
    case kReshape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshape_);
      break;
    }
    // .CoreML.Specification.FlattenLayerParams flatten = 301;
    case kFlatten: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.flatten_);
      break;
    }
    // .CoreML.Specification.PermuteLayerParams permute = 310;
    case kPermute: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.permute_);
      break;
    }
    // .CoreML.Specification.ConcatLayerParams concat = 320;
    case kConcat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.concat_);
      break;
    }
    // .CoreML.Specification.SplitLayerParams split = 330;
    case kSplit: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.split_);
      break;
    }
    // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
    case kSequenceRepeat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sequencerepeat_);
      break;
    }
    // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
    case kReorganizeData: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reorganizedata_);
      break;
    }
    // .CoreML.Specification.SliceLayerParams slice = 350;
    case kSlice: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slice_);
      break;
    }
    // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
    case kSimpleRecurrent: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.simplerecurrent_);
      break;
    }
    // .CoreML.Specification.GRULayerParams gru = 410;
    case kGru: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gru_);
      break;
    }
    // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
    case kUniDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unidirectionallstm_);
      break;
    }
    // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
    case kBiDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bidirectionallstm_);
      break;
    }
    // .CoreML.Specification.CustomLayerParams custom = 500;
    case kCustom: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.custom_);
      break;
    }
    // .CoreML.Specification.CopyLayerParams copy = 600;
    case kCopy: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.copy_);
      break;
    }
    // .CoreML.Specification.BranchLayerParams branch = 605;
    case kBranch: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.branch_);
      break;
    }
    // .CoreML.Specification.LoopLayerParams loop = 615;
    case kLoop: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loop_);
      break;
    }
    // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
    case kLoopBreak: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loopbreak_);
      break;
    }
    // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
    case kLoopContinue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loopcontinue_);
      break;
    }
    // .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
    case kRangeStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.rangestatic_);
      break;
    }
    // .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
    case kRangeDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.rangedynamic_);
      break;
    }
    // .CoreML.Specification.ClipLayerParams clip = 660;
    case kClip: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.clip_);
      break;
    }
    // .CoreML.Specification.CeilLayerParams ceil = 665;
    case kCeil: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.ceil_);
      break;
    }
    // .CoreML.Specification.FloorLayerParams floor = 670;
    case kFloor: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.floor_);
      break;
    }
    // .CoreML.Specification.SignLayerParams sign = 680;
    case kSign: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sign_);
      break;
    }
    // .CoreML.Specification.RoundLayerParams round = 685;
    case kRound: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.round_);
      break;
    }
    // .CoreML.Specification.Exp2LayerParams exp2 = 700;
    case kExp2: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.exp2_);
      break;
    }
    // .CoreML.Specification.SinLayerParams sin = 710;
    case kSin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sin_);
      break;
    }
    // .CoreML.Specification.CosLayerParams cos = 715;
    case kCos: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.cos_);
      break;
    }
    // .CoreML.Specification.TanLayerParams tan = 720;
    case kTan: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.tan_);
      break;
    }
    // .CoreML.Specification.AsinLayerParams asin = 730;
    case kAsin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.asin_);
      break;
    }
    // .CoreML.Specification.AcosLayerParams acos = 735;
    case kAcos: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.acos_);
      break;
    }
    // .CoreML.Specification.AtanLayerParams atan = 740;
    case kAtan: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.atan_);
      break;
    }
    // .CoreML.Specification.SinhLayerParams sinh = 750;
    case kSinh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sinh_);
      break;
    }
    // .CoreML.Specification.CoshLayerParams cosh = 755;
    case kCosh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.cosh_);
      break;
    }
    // .CoreML.Specification.TanhLayerParams tanh = 760;
    case kTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.tanh_);
      break;
    }
    // .CoreML.Specification.AsinhLayerParams asinh = 770;
    case kAsinh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.asinh_);
      break;
    }
    // .CoreML.Specification.AcoshLayerParams acosh = 775;
    case kAcosh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.acosh_);
      break;
    }
    // .CoreML.Specification.AtanhLayerParams atanh = 780;
    case kAtanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.atanh_);
      break;
    }
    // .CoreML.Specification.ErfLayerParams erf = 790;
    case kErf: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.erf_);
      break;
    }
    // .CoreML.Specification.GeluLayerParams gelu = 795;
    case kGelu: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gelu_);
      break;
    }
    // .CoreML.Specification.EqualLayerParams equal = 815;
    case kEqual: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.equal_);
      break;
    }
    // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
    case kNotEqual: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.notequal_);
      break;
    }
    // .CoreML.Specification.LessThanLayerParams lessThan = 825;
    case kLessThan: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lessthan_);
      break;
    }
    // .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
    case kLessEqual: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lessequal_);
      break;
    }
    // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
    case kGreaterThan: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.greaterthan_);
      break;
    }
    // .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
    case kGreaterEqual: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.greaterequal_);
      break;
    }
    // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
    case kLogicalOr: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicalor_);
      break;
    }
    // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
    case kLogicalXor: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicalxor_);
      break;
    }
    // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
    case kLogicalNot: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicalnot_);
      break;
    }
    // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
    case kLogicalAnd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicaland_);
      break;
    }
    // .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
    case kModBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.modbroadcastable_);
      break;
    }
    // .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
    case kMinBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.minbroadcastable_);
      break;
    }
    // .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
    case kMaxBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.maxbroadcastable_);
      break;
    }
    // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
    case kAddBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.addbroadcastable_);
      break;
    }
    // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
    case kPowBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.powbroadcastable_);
      break;
    }
    // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
    case kDivideBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.dividebroadcastable_);
      break;
    }
    // .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
    case kFloorDivBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.floordivbroadcastable_);
      break;
    }
    // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
    case kMultiplyBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.multiplybroadcastable_);
      break;
    }
    // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
    case kSubtractBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.subtractbroadcastable_);
      break;
    }
    // .CoreML.Specification.TileLayerParams tile = 920;
    case kTile: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.tile_);
      break;
    }
    // .CoreML.Specification.StackLayerParams stack = 925;
    case kStack: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.stack_);
      break;
    }
    // .CoreML.Specification.GatherLayerParams gather = 930;
    case kGather: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gather_);
      break;
    }
    // .CoreML.Specification.ScatterLayerParams scatter = 935;
    case kScatter: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scatter_);
      break;
    }
    // .CoreML.Specification.GatherNDLayerParams gatherND = 940;
    case kGatherND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gathernd_);
      break;
    }
    // .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
    case kScatterND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scatternd_);
      break;
    }
    // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
    case kSoftmaxND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.softmaxnd_);
      break;
    }
    // .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
    case kGatherAlongAxis: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gatheralongaxis_);
      break;
    }
    // .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
    case kScatterAlongAxis: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scatteralongaxis_);
      break;
    }
    // .CoreML.Specification.ReverseLayerParams reverse = 960;
    case kReverse: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reverse_);
      break;
    }
    // .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
    case kReverseSeq: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reverseseq_);
      break;
    }
    // .CoreML.Specification.SplitNDLayerParams splitND = 975;
    case kSplitND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.splitnd_);
      break;
    }
    // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
    case kConcatND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.concatnd_);
      break;
    }
    // .CoreML.Specification.TransposeLayerParams transpose = 985;
    case kTranspose: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.transpose_);
      break;
    }
    // .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
    case kSliceStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slicestatic_);
      break;
    }
    // .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
    case kSliceDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slicedynamic_);
      break;
    }
    // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
    case kSlidingWindows: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slidingwindows_);
      break;
    }
    // .CoreML.Specification.TopKLayerParams topK = 1015;
    case kTopK: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.topk_);
      break;
    }
    // .CoreML.Specification.ArgMinLayerParams argMin = 1020;
    case kArgMin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.argmin_);
      break;
    }
    // .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
    case kArgMax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.argmax_);
      break;
    }
    // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
    case kEmbeddingND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.embeddingnd_);
      break;
    }
    // .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
    case kBatchedMatmul: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.batchedmatmul_);
      break;
    }
    // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
    case kGetShape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.getshape_);
      break;
    }
    // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
    case kLoadConstantND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loadconstantnd_);
      break;
    }
    // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
    case kFillLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.filllike_);
      break;
    }
    // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
    case kFillStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.fillstatic_);
      break;
    }
    // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
    case kFillDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.filldynamic_);
      break;
    }
    // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
    case kBroadcastToLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.broadcasttolike_);
      break;
    }
    // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
    case kBroadcastToStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.broadcasttostatic_);
      break;
    }
    // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
    case kBroadcastToDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.broadcasttodynamic_);
      break;
    }
    // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
    case kSqueeze: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.squeeze_);
      break;
    }
    // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
    case kExpandDims: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.expanddims_);
      break;
    }
    // .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
    case kFlattenTo2D: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.flattento2d_);
      break;
    }
    // .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
    case kReshapeLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshapelike_);
      break;
    }
    // .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
    case kReshapeStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshapestatic_);
      break;
    }
    // .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
    case kReshapeDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshapedynamic_);
      break;
    }
    // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
    case kRankPreservingReshape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.rankpreservingreshape_);
      break;
    }
    // .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
    case kConstantPad: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.constantpad_);
      break;
    }
    // .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
    case kRandomNormalLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randomnormallike_);
      break;
    }
    // .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
    case kRandomNormalStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randomnormalstatic_);
      break;
    }
    // .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
    case kRandomNormalDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randomnormaldynamic_);
      break;
    }
    // .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
    case kRandomUniformLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randomuniformlike_);
      break;
    }
    // .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
    case kRandomUniformStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randomuniformstatic_);
      break;
    }
    // .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
    case kRandomUniformDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randomuniformdynamic_);
      break;
    }
    // .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
    case kRandomBernoulliLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randombernoullilike_);
      break;
    }
    // .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
    case kRandomBernoulliStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randombernoullistatic_);
      break;
    }
    // .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
    case kRandomBernoulliDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.randombernoullidynamic_);
      break;
    }
    // .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
    case kCategoricalDistribution: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.categoricaldistribution_);
      break;
    }
    // .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
    case kReduceL1: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducel1_);
      break;
    }
    // .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
    case kReduceL2: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducel2_);
      break;
    }
    // .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
    case kReduceMax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducemax_);
      break;
    }
    // .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
    case kReduceMin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducemin_);
      break;
    }
    // .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
    case kReduceSum: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducesum_);
      break;
    }
    // .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
    case kReduceProd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reduceprod_);
      break;
    }
    // .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
    case kReduceMean: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducemean_);
      break;
    }
    // .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
    case kReduceLogSum: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducelogsum_);
      break;
    }
    // .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
    case kReduceSumSquare: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducesumsquare_);
      break;
    }
    // .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
    case kReduceLogSumExp: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reducelogsumexp_);
      break;
    }
    // .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
    case kWhereNonZero: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.wherenonzero_);
      break;
    }
    // .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
    case kMatrixBandPart: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.matrixbandpart_);
      break;
    }
    // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
    case kLowerTriangular: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lowertriangular_);
      break;
    }
    // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
    case kUpperTriangular: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.uppertriangular_);
      break;
    }
    // .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
    case kWhereBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.wherebroadcastable_);
      break;
    }
    // .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
    case kLayerNormalization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.layernormalization_);
      break;
    }
    // .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
    case kNonMaximumSuppression: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.nonmaximumsuppression_);
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkLayer*>(&from));
}

void NeuralNetworkLayer::MergeFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  input_.MergeFrom(from.input_);
  output_.MergeFrom(from.output_);
  inputtensor_.MergeFrom(from.inputtensor_);
  outputtensor_.MergeFrom(from.outputtensor_);
  if (from.name().size() > 0) {

    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  if (from.isupdatable() != 0) {
    set_isupdatable(from.isupdatable());
  }
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kResizeBilinear: {
      mutable_resizebilinear()->::CoreML::Specification::ResizeBilinearLayerParams::MergeFrom(from.resizebilinear());
      break;
    }
    case kCropResize: {
      mutable_cropresize()->::CoreML::Specification::CropResizeLayerParams::MergeFrom(from.cropresize());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kReorganizeData: {
      mutable_reorganizedata()->::CoreML::Specification::ReorganizeDataLayerParams::MergeFrom(from.reorganizedata());
      break;
    }
    case kSlice: {
      mutable_slice()->::CoreML::Specification::SliceLayerParams::MergeFrom(from.slice());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case kCustom: {
      mutable_custom()->::CoreML::Specification::CustomLayerParams::MergeFrom(from.custom());
      break;
    }
    case kCopy: {
      mutable_copy()->::CoreML::Specification::CopyLayerParams::MergeFrom(from.copy());
      break;
    }
    case kBranch: {
      mutable_branch()->::CoreML::Specification::BranchLayerParams::MergeFrom(from.branch());
      break;
    }
    case kLoop: {
      mutable_loop()->::CoreML::Specification::LoopLayerParams::MergeFrom(from.loop());
      break;
    }
    case kLoopBreak: {
      mutable_loopbreak()->::CoreML::Specification::LoopBreakLayerParams::MergeFrom(from.loopbreak());
      break;
    }
    case kLoopContinue: {
      mutable_loopcontinue()->::CoreML::Specification::LoopContinueLayerParams::MergeFrom(from.loopcontinue());
      break;
    }
    case kRangeStatic: {
      mutable_rangestatic()->::CoreML::Specification::RangeStaticLayerParams::MergeFrom(from.rangestatic());
      break;
    }
    case kRangeDynamic: {
      mutable_rangedynamic()->::CoreML::Specification::RangeDynamicLayerParams::MergeFrom(from.rangedynamic());
      break;
    }
    case kClip: {
      mutable_clip()->::CoreML::Specification::ClipLayerParams::MergeFrom(from.clip());
      break;
    }
    case kCeil: {
      mutable_ceil()->::CoreML::Specification::CeilLayerParams::MergeFrom(from.ceil());
      break;
    }
    case kFloor: {
      mutable_floor()->::CoreML::Specification::FloorLayerParams::MergeFrom(from.floor());
      break;
    }
    case kSign: {
      mutable_sign()->::CoreML::Specification::SignLayerParams::MergeFrom(from.sign());
      break;
    }
    case kRound: {
      mutable_round()->::CoreML::Specification::RoundLayerParams::MergeFrom(from.round());
      break;
    }
    case kExp2: {
      mutable_exp2()->::CoreML::Specification::Exp2LayerParams::MergeFrom(from.exp2());
      break;
    }
    case kSin: {
      mutable_sin()->::CoreML::Specification::SinLayerParams::MergeFrom(from.sin());
      break;
    }
    case kCos: {
      mutable_cos()->::CoreML::Specification::CosLayerParams::MergeFrom(from.cos());
      break;
    }
    case kTan: {
      mutable_tan()->::CoreML::Specification::TanLayerParams::MergeFrom(from.tan());
      break;
    }
    case kAsin: {
      mutable_asin()->::CoreML::Specification::AsinLayerParams::MergeFrom(from.asin());
      break;
    }
    case kAcos: {
      mutable_acos()->::CoreML::Specification::AcosLayerParams::MergeFrom(from.acos());
      break;
    }
    case kAtan: {
      mutable_atan()->::CoreML::Specification::AtanLayerParams::MergeFrom(from.atan());
      break;
    }
    case kSinh: {
      mutable_sinh()->::CoreML::Specification::SinhLayerParams::MergeFrom(from.sinh());
      break;
    }
    case kCosh: {
      mutable_cosh()->::CoreML::Specification::CoshLayerParams::MergeFrom(from.cosh());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::TanhLayerParams::MergeFrom(from.tanh());
      break;
    }
    case kAsinh: {
      mutable_asinh()->::CoreML::Specification::AsinhLayerParams::MergeFrom(from.asinh());
      break;
    }
    case kAcosh: {
      mutable_acosh()->::CoreML::Specification::AcoshLayerParams::MergeFrom(from.acosh());
      break;
    }
    case kAtanh: {
      mutable_atanh()->::CoreML::Specification::AtanhLayerParams::MergeFrom(from.atanh());
      break;
    }
    case kErf: {
      mutable_erf()->::CoreML::Specification::ErfLayerParams::MergeFrom(from.erf());
      break;
    }
    case kGelu: {
      mutable_gelu()->::CoreML::Specification::GeluLayerParams::MergeFrom(from.gelu());
      break;
    }
    case kEqual: {
      mutable_equal()->::CoreML::Specification::EqualLayerParams::MergeFrom(from.equal());
      break;
    }
    case kNotEqual: {
      mutable_notequal()->::CoreML::Specification::NotEqualLayerParams::MergeFrom(from.notequal());
      break;
    }
    case kLessThan: {
      mutable_lessthan()->::CoreML::Specification::LessThanLayerParams::MergeFrom(from.lessthan());
      break;
    }
    case kLessEqual: {
      mutable_lessequal()->::CoreML::Specification::LessEqualLayerParams::MergeFrom(from.lessequal());
      break;
    }
    case kGreaterThan: {
      mutable_greaterthan()->::CoreML::Specification::GreaterThanLayerParams::MergeFrom(from.greaterthan());
      break;
    }
    case kGreaterEqual: {
      mutable_greaterequal()->::CoreML::Specification::GreaterEqualLayerParams::MergeFrom(from.greaterequal());
      break;
    }
    case kLogicalOr: {
      mutable_logicalor()->::CoreML::Specification::LogicalOrLayerParams::MergeFrom(from.logicalor());
      break;
    }
    case kLogicalXor: {
      mutable_logicalxor()->::CoreML::Specification::LogicalXorLayerParams::MergeFrom(from.logicalxor());
      break;
    }
    case kLogicalNot: {
      mutable_logicalnot()->::CoreML::Specification::LogicalNotLayerParams::MergeFrom(from.logicalnot());
      break;
    }
    case kLogicalAnd: {
      mutable_logicaland()->::CoreML::Specification::LogicalAndLayerParams::MergeFrom(from.logicaland());
      break;
    }
    case kModBroadcastable: {
      mutable_modbroadcastable()->::CoreML::Specification::ModBroadcastableLayerParams::MergeFrom(from.modbroadcastable());
      break;
    }
    case kMinBroadcastable: {
      mutable_minbroadcastable()->::CoreML::Specification::MinBroadcastableLayerParams::MergeFrom(from.minbroadcastable());
      break;
    }
    case kMaxBroadcastable: {
      mutable_maxbroadcastable()->::CoreML::Specification::MaxBroadcastableLayerParams::MergeFrom(from.maxbroadcastable());
      break;
    }
    case kAddBroadcastable: {
      mutable_addbroadcastable()->::CoreML::Specification::AddBroadcastableLayerParams::MergeFrom(from.addbroadcastable());
      break;
    }
    case kPowBroadcastable: {
      mutable_powbroadcastable()->::CoreML::Specification::PowBroadcastableLayerParams::MergeFrom(from.powbroadcastable());
      break;
    }
    case kDivideBroadcastable: {
      mutable_dividebroadcastable()->::CoreML::Specification::DivideBroadcastableLayerParams::MergeFrom(from.dividebroadcastable());
      break;
    }
    case kFloorDivBroadcastable: {
      mutable_floordivbroadcastable()->::CoreML::Specification::FloorDivBroadcastableLayerParams::MergeFrom(from.floordivbroadcastable());
      break;
    }
    case kMultiplyBroadcastable: {
      mutable_multiplybroadcastable()->::CoreML::Specification::MultiplyBroadcastableLayerParams::MergeFrom(from.multiplybroadcastable());
      break;
    }
    case kSubtractBroadcastable: {
      mutable_subtractbroadcastable()->::CoreML::Specification::SubtractBroadcastableLayerParams::MergeFrom(from.subtractbroadcastable());
      break;
    }
    case kTile: {
      mutable_tile()->::CoreML::Specification::TileLayerParams::MergeFrom(from.tile());
      break;
    }
    case kStack: {
      mutable_stack()->::CoreML::Specification::StackLayerParams::MergeFrom(from.stack());
      break;
    }
    case kGather: {
      mutable_gather()->::CoreML::Specification::GatherLayerParams::MergeFrom(from.gather());
      break;
    }
    case kScatter: {
      mutable_scatter()->::CoreML::Specification::ScatterLayerParams::MergeFrom(from.scatter());
      break;
    }
    case kGatherND: {
      mutable_gathernd()->::CoreML::Specification::GatherNDLayerParams::MergeFrom(from.gathernd());
      break;
    }
    case kScatterND: {
      mutable_scatternd()->::CoreML::Specification::ScatterNDLayerParams::MergeFrom(from.scatternd());
      break;
    }
    case kSoftmaxND: {
      mutable_softmaxnd()->::CoreML::Specification::SoftmaxNDLayerParams::MergeFrom(from.softmaxnd());
      break;
    }
    case kGatherAlongAxis: {
      mutable_gatheralongaxis()->::CoreML::Specification::GatherAlongAxisLayerParams::MergeFrom(from.gatheralongaxis());
      break;
    }
    case kScatterAlongAxis: {
      mutable_scatteralongaxis()->::CoreML::Specification::ScatterAlongAxisLayerParams::MergeFrom(from.scatteralongaxis());
      break;
    }
    case kReverse: {
      mutable_reverse()->::CoreML::Specification::ReverseLayerParams::MergeFrom(from.reverse());
      break;
    }
    case kReverseSeq: {
      mutable_reverseseq()->::CoreML::Specification::ReverseSeqLayerParams::MergeFrom(from.reverseseq());
      break;
    }
    case kSplitND: {
      mutable_splitnd()->::CoreML::Specification::SplitNDLayerParams::MergeFrom(from.splitnd());
      break;
    }
    case kConcatND: {
      mutable_concatnd()->::CoreML::Specification::ConcatNDLayerParams::MergeFrom(from.concatnd());
      break;
    }
    case kTranspose: {
      mutable_transpose()->::CoreML::Specification::TransposeLayerParams::MergeFrom(from.transpose());
      break;
    }
    case kSliceStatic: {
      mutable_slicestatic()->::CoreML::Specification::SliceStaticLayerParams::MergeFrom(from.slicestatic());
      break;
    }
    case kSliceDynamic: {
      mutable_slicedynamic()->::CoreML::Specification::SliceDynamicLayerParams::MergeFrom(from.slicedynamic());
      break;
    }
    case kSlidingWindows: {
      mutable_slidingwindows()->::CoreML::Specification::SlidingWindowsLayerParams::MergeFrom(from.slidingwindows());
      break;
    }
    case kTopK: {
      mutable_topk()->::CoreML::Specification::TopKLayerParams::MergeFrom(from.topk());
      break;
    }
    case kArgMin: {
      mutable_argmin()->::CoreML::Specification::ArgMinLayerParams::MergeFrom(from.argmin());
      break;
    }
    case kArgMax: {
      mutable_argmax()->::CoreML::Specification::ArgMaxLayerParams::MergeFrom(from.argmax());
      break;
    }
    case kEmbeddingND: {
      mutable_embeddingnd()->::CoreML::Specification::EmbeddingNDLayerParams::MergeFrom(from.embeddingnd());
      break;
    }
    case kBatchedMatmul: {
      mutable_batchedmatmul()->::CoreML::Specification::BatchedMatMulLayerParams::MergeFrom(from.batchedmatmul());
      break;
    }
    case kGetShape: {
      mutable_getshape()->::CoreML::Specification::GetShapeLayerParams::MergeFrom(from.getshape());
      break;
    }
    case kLoadConstantND: {
      mutable_loadconstantnd()->::CoreML::Specification::LoadConstantNDLayerParams::MergeFrom(from.loadconstantnd());
      break;
    }
    case kFillLike: {
      mutable_filllike()->::CoreML::Specification::FillLikeLayerParams::MergeFrom(from.filllike());
      break;
    }
    case kFillStatic: {
      mutable_fillstatic()->::CoreML::Specification::FillStaticLayerParams::MergeFrom(from.fillstatic());
      break;
    }
    case kFillDynamic: {
      mutable_filldynamic()->::CoreML::Specification::FillDynamicLayerParams::MergeFrom(from.filldynamic());
      break;
    }
    case kBroadcastToLike: {
      mutable_broadcasttolike()->::CoreML::Specification::BroadcastToLikeLayerParams::MergeFrom(from.broadcasttolike());
      break;
    }
    case kBroadcastToStatic: {
      mutable_broadcasttostatic()->::CoreML::Specification::BroadcastToStaticLayerParams::MergeFrom(from.broadcasttostatic());
      break;
    }
    case kBroadcastToDynamic: {
      mutable_broadcasttodynamic()->::CoreML::Specification::BroadcastToDynamicLayerParams::MergeFrom(from.broadcasttodynamic());
      break;
    }
    case kSqueeze: {
      mutable_squeeze()->::CoreML::Specification::SqueezeLayerParams::MergeFrom(from.squeeze());
      break;
    }
    case kExpandDims: {
      mutable_expanddims()->::CoreML::Specification::ExpandDimsLayerParams::MergeFrom(from.expanddims());
      break;
    }
    case kFlattenTo2D: {
      mutable_flattento2d()->::CoreML::Specification::FlattenTo2DLayerParams::MergeFrom(from.flattento2d());
      break;
    }
    case kReshapeLike: {
      mutable_reshapelike()->::CoreML::Specification::ReshapeLikeLayerParams::MergeFrom(from.reshapelike());
      break;
    }
    case kReshapeStatic: {
      mutable_reshapestatic()->::CoreML::Specification::ReshapeStaticLayerParams::MergeFrom(from.reshapestatic());
      break;
    }
    case kReshapeDynamic: {
      mutable_reshapedynamic()->::CoreML::Specification::ReshapeDynamicLayerParams::MergeFrom(from.reshapedynamic());
      break;
    }
    case kRankPreservingReshape: {
      mutable_rankpreservingreshape()->::CoreML::Specification::RankPreservingReshapeLayerParams::MergeFrom(from.rankpreservingreshape());
      break;
    }
    case kConstantPad: {
      mutable_constantpad()->::CoreML::Specification::ConstantPaddingLayerParams::MergeFrom(from.constantpad());
      break;
    }
    case kRandomNormalLike: {
      mutable_randomnormallike()->::CoreML::Specification::RandomNormalLikeLayerParams::MergeFrom(from.randomnormallike());
      break;
    }
    case kRandomNormalStatic: {
      mutable_randomnormalstatic()->::CoreML::Specification::RandomNormalStaticLayerParams::MergeFrom(from.randomnormalstatic());
      break;
    }
    case kRandomNormalDynamic: {
      mutable_randomnormaldynamic()->::CoreML::Specification::RandomNormalDynamicLayerParams::MergeFrom(from.randomnormaldynamic());
      break;
    }
    case kRandomUniformLike: {
      mutable_randomuniformlike()->::CoreML::Specification::RandomUniformLikeLayerParams::MergeFrom(from.randomuniformlike());
      break;
    }
    case kRandomUniformStatic: {
      mutable_randomuniformstatic()->::CoreML::Specification::RandomUniformStaticLayerParams::MergeFrom(from.randomuniformstatic());
      break;
    }
    case kRandomUniformDynamic: {
      mutable_randomuniformdynamic()->::CoreML::Specification::RandomUniformDynamicLayerParams::MergeFrom(from.randomuniformdynamic());
      break;
    }
    case kRandomBernoulliLike: {
      mutable_randombernoullilike()->::CoreML::Specification::RandomBernoulliLikeLayerParams::MergeFrom(from.randombernoullilike());
      break;
    }
    case kRandomBernoulliStatic: {
      mutable_randombernoullistatic()->::CoreML::Specification::RandomBernoulliStaticLayerParams::MergeFrom(from.randombernoullistatic());
      break;
    }
    case kRandomBernoulliDynamic: {
      mutable_randombernoullidynamic()->::CoreML::Specification::RandomBernoulliDynamicLayerParams::MergeFrom(from.randombernoullidynamic());
      break;
    }
    case kCategoricalDistribution: {
      mutable_categoricaldistribution()->::CoreML::Specification::CategoricalDistributionLayerParams::MergeFrom(from.categoricaldistribution());
      break;
    }
    case kReduceL1: {
      mutable_reducel1()->::CoreML::Specification::ReduceL1LayerParams::MergeFrom(from.reducel1());
      break;
    }
    case kReduceL2: {
      mutable_reducel2()->::CoreML::Specification::ReduceL2LayerParams::MergeFrom(from.reducel2());
      break;
    }
    case kReduceMax: {
      mutable_reducemax()->::CoreML::Specification::ReduceMaxLayerParams::MergeFrom(from.reducemax());
      break;
    }
    case kReduceMin: {
      mutable_reducemin()->::CoreML::Specification::ReduceMinLayerParams::MergeFrom(from.reducemin());
      break;
    }
    case kReduceSum: {
      mutable_reducesum()->::CoreML::Specification::ReduceSumLayerParams::MergeFrom(from.reducesum());
      break;
    }
    case kReduceProd: {
      mutable_reduceprod()->::CoreML::Specification::ReduceProdLayerParams::MergeFrom(from.reduceprod());
      break;
    }
    case kReduceMean: {
      mutable_reducemean()->::CoreML::Specification::ReduceMeanLayerParams::MergeFrom(from.reducemean());
      break;
    }
    case kReduceLogSum: {
      mutable_reducelogsum()->::CoreML::Specification::ReduceLogSumLayerParams::MergeFrom(from.reducelogsum());
      break;
    }
    case kReduceSumSquare: {
      mutable_reducesumsquare()->::CoreML::Specification::ReduceSumSquareLayerParams::MergeFrom(from.reducesumsquare());
      break;
    }
    case kReduceLogSumExp: {
      mutable_reducelogsumexp()->::CoreML::Specification::ReduceLogSumExpLayerParams::MergeFrom(from.reducelogsumexp());
      break;
    }
    case kWhereNonZero: {
      mutable_wherenonzero()->::CoreML::Specification::WhereNonZeroLayerParams::MergeFrom(from.wherenonzero());
      break;
    }
    case kMatrixBandPart: {
      mutable_matrixbandpart()->::CoreML::Specification::MatrixBandPartLayerParams::MergeFrom(from.matrixbandpart());
      break;
    }
    case kLowerTriangular: {
      mutable_lowertriangular()->::CoreML::Specification::LowerTriangularLayerParams::MergeFrom(from.lowertriangular());
      break;
    }
    case kUpperTriangular: {
      mutable_uppertriangular()->::CoreML::Specification::UpperTriangularLayerParams::MergeFrom(from.uppertriangular());
      break;
    }
    case kWhereBroadcastable: {
      mutable_wherebroadcastable()->::CoreML::Specification::WhereBroadcastableLayerParams::MergeFrom(from.wherebroadcastable());
      break;
    }
    case kLayerNormalization: {
      mutable_layernormalization()->::CoreML::Specification::LayerNormalizationLayerParams::MergeFrom(from.layernormalization());
      break;
    }
    case kNonMaximumSuppression: {
      mutable_nonmaximumsuppression()->::CoreML::Specification::NonMaximumSuppressionLayerParams::MergeFrom(from.nonmaximumsuppression());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkLayer::CopyFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkLayer::IsInitialized() const {
  return true;
}

void NeuralNetworkLayer::Swap(NeuralNetworkLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkLayer::InternalSwap(NeuralNetworkLayer* other) {
  input_.InternalSwap(&other->input_);
  output_.InternalSwap(&other->output_);
  inputtensor_.InternalSwap(&other->inputtensor_);
  outputtensor_.InternalSwap(&other->outputtensor_);
  name_.Swap(&other->name_);
  std::swap(isupdatable_, other->isupdatable_);
  std::swap(layer_, other->layer_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkLayer::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkLayer

// string name = 1;
void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.GetNoArena();
}
void NeuralNetworkLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
#if LANG_CXX11
void NeuralNetworkLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkLayer.name)
}
#endif
void NeuralNetworkLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.name)
}
void NeuralNetworkLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.name)
}
::std::string* NeuralNetworkLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
int NeuralNetworkLayer::input_size() const {
  return input_.size();
}
void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
const ::std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
void NeuralNetworkLayer::set_input(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(value);
}
#if LANG_CXX11
void NeuralNetworkLayer::set_input(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(std::move(value));
}
#endif
void NeuralNetworkLayer::set_input(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
::std::string* NeuralNetworkLayer::add_input() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Add();
}
void NeuralNetworkLayer::add_input(const ::std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#if LANG_CXX11
void NeuralNetworkLayer::add_input(::std::string&& value) {
  input_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#endif
void NeuralNetworkLayer::add_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
int NeuralNetworkLayer::output_size() const {
  return output_.size();
}
void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
const ::std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
void NeuralNetworkLayer::set_output(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(value);
}
#if LANG_CXX11
void NeuralNetworkLayer::set_output(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(std::move(value));
}
#endif
void NeuralNetworkLayer::set_output(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
::std::string* NeuralNetworkLayer::add_output() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Add();
}
void NeuralNetworkLayer::add_output(const ::std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#if LANG_CXX11
void NeuralNetworkLayer::add_output(::std::string&& value) {
  output_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#endif
void NeuralNetworkLayer::add_output(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// repeated .CoreML.Specification.Tensor inputTensor = 4;
int NeuralNetworkLayer::inputtensor_size() const {
  return inputtensor_.size();
}
void NeuralNetworkLayer::clear_inputtensor() {
  inputtensor_.Clear();
}
const ::CoreML::Specification::Tensor& NeuralNetworkLayer::inputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Get(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_inputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Mutable(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::add_inputtensor() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_inputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return &inputtensor_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::inputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_;
}

// repeated .CoreML.Specification.Tensor outputTensor = 5;
int NeuralNetworkLayer::outputtensor_size() const {
  return outputtensor_.size();
}
void NeuralNetworkLayer::clear_outputtensor() {
  outputtensor_.Clear();
}
const ::CoreML::Specification::Tensor& NeuralNetworkLayer::outputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Get(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_outputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Mutable(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::add_outputtensor() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_outputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return &outputtensor_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::outputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_;
}

// bool isUpdatable = 10;
void NeuralNetworkLayer::clear_isupdatable() {
  isupdatable_ = false;
}
bool NeuralNetworkLayer::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
  return isupdatable_;
}
void NeuralNetworkLayer::set_isupdatable(bool value) {
  
  isupdatable_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
}

// .CoreML.Specification.ConvolutionLayerParams convolution = 100;
bool NeuralNetworkLayer::has_convolution() const {
  return layer_case() == kConvolution;
}
void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
void NeuralNetworkLayer::clear_convolution() {
  if (has_convolution()) {
    delete layer_.convolution_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return has_convolution()
      ? *layer_.convolution_
      : ::CoreML::Specification::ConvolutionLayerParams::default_instance();
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  if (!has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = new ::CoreML::Specification::ConvolutionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return layer_.convolution_;
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}

// .CoreML.Specification.PoolingLayerParams pooling = 120;
bool NeuralNetworkLayer::has_pooling() const {
  return layer_case() == kPooling;
}
void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
void NeuralNetworkLayer::clear_pooling() {
  if (has_pooling()) {
    delete layer_.pooling_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return has_pooling()
      ? *layer_.pooling_
      : ::CoreML::Specification::PoolingLayerParams::default_instance();
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  if (!has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = new ::CoreML::Specification::PoolingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return layer_.pooling_;
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}

// .CoreML.Specification.ActivationParams activation = 130;
bool NeuralNetworkLayer::has_activation() const {
  return layer_case() == kActivation;
}
void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
void NeuralNetworkLayer::clear_activation() {
  if (has_activation()) {
    delete layer_.activation_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return has_activation()
      ? *layer_.activation_
      : ::CoreML::Specification::ActivationParams::default_instance();
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  if (!has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return layer_.activation_;
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}

// .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
bool NeuralNetworkLayer::has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
void NeuralNetworkLayer::clear_innerproduct() {
  if (has_innerproduct()) {
    delete layer_.innerproduct_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return has_innerproduct()
      ? *layer_.innerproduct_
      : ::CoreML::Specification::InnerProductLayerParams::default_instance();
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  if (!has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = new ::CoreML::Specification::InnerProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return layer_.innerproduct_;
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}

// .CoreML.Specification.EmbeddingLayerParams embedding = 150;
bool NeuralNetworkLayer::has_embedding() const {
  return layer_case() == kEmbedding;
}
void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
void NeuralNetworkLayer::clear_embedding() {
  if (has_embedding()) {
    delete layer_.embedding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return has_embedding()
      ? *layer_.embedding_
      : ::CoreML::Specification::EmbeddingLayerParams::default_instance();
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  if (!has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = new ::CoreML::Specification::EmbeddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return layer_.embedding_;
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}

// .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
bool NeuralNetworkLayer::has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
void NeuralNetworkLayer::clear_batchnorm() {
  if (has_batchnorm()) {
    delete layer_.batchnorm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return has_batchnorm()
      ? *layer_.batchnorm_
      : ::CoreML::Specification::BatchnormLayerParams::default_instance();
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  if (!has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = new ::CoreML::Specification::BatchnormLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return layer_.batchnorm_;
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}

// .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
bool NeuralNetworkLayer::has_mvn() const {
  return layer_case() == kMvn;
}
void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
void NeuralNetworkLayer::clear_mvn() {
  if (has_mvn()) {
    delete layer_.mvn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return has_mvn()
      ? *layer_.mvn_
      : ::CoreML::Specification::MeanVarianceNormalizeLayerParams::default_instance();
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  if (!has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = new ::CoreML::Specification::MeanVarianceNormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return layer_.mvn_;
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}

// .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
bool NeuralNetworkLayer::has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
void NeuralNetworkLayer::clear_l2normalize() {
  if (has_l2normalize()) {
    delete layer_.l2normalize_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return has_l2normalize()
      ? *layer_.l2normalize_
      : ::CoreML::Specification::L2NormalizeLayerParams::default_instance();
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  if (!has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = new ::CoreML::Specification::L2NormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return layer_.l2normalize_;
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}

// .CoreML.Specification.SoftmaxLayerParams softmax = 175;
bool NeuralNetworkLayer::has_softmax() const {
  return layer_case() == kSoftmax;
}
void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
void NeuralNetworkLayer::clear_softmax() {
  if (has_softmax()) {
    delete layer_.softmax_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return has_softmax()
      ? *layer_.softmax_
      : ::CoreML::Specification::SoftmaxLayerParams::default_instance();
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  if (!has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = new ::CoreML::Specification::SoftmaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return layer_.softmax_;
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}

// .CoreML.Specification.LRNLayerParams lrn = 180;
bool NeuralNetworkLayer::has_lrn() const {
  return layer_case() == kLrn;
}
void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
void NeuralNetworkLayer::clear_lrn() {
  if (has_lrn()) {
    delete layer_.lrn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return has_lrn()
      ? *layer_.lrn_
      : ::CoreML::Specification::LRNLayerParams::default_instance();
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  if (!has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = new ::CoreML::Specification::LRNLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return layer_.lrn_;
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}

// .CoreML.Specification.CropLayerParams crop = 190;
bool NeuralNetworkLayer::has_crop() const {
  return layer_case() == kCrop;
}
void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
void NeuralNetworkLayer::clear_crop() {
  if (has_crop()) {
    delete layer_.crop_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return has_crop()
      ? *layer_.crop_
      : ::CoreML::Specification::CropLayerParams::default_instance();
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  if (!has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = new ::CoreML::Specification::CropLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return layer_.crop_;
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}

// .CoreML.Specification.PaddingLayerParams padding = 200;
bool NeuralNetworkLayer::has_padding() const {
  return layer_case() == kPadding;
}
void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
void NeuralNetworkLayer::clear_padding() {
  if (has_padding()) {
    delete layer_.padding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return has_padding()
      ? *layer_.padding_
      : ::CoreML::Specification::PaddingLayerParams::default_instance();
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  if (!has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = new ::CoreML::Specification::PaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return layer_.padding_;
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}

// .CoreML.Specification.UpsampleLayerParams upsample = 210;
bool NeuralNetworkLayer::has_upsample() const {
  return layer_case() == kUpsample;
}
void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
void NeuralNetworkLayer::clear_upsample() {
  if (has_upsample()) {
    delete layer_.upsample_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return has_upsample()
      ? *layer_.upsample_
      : ::CoreML::Specification::UpsampleLayerParams::default_instance();
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  if (!has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = new ::CoreML::Specification::UpsampleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return layer_.upsample_;
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}

// .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
bool NeuralNetworkLayer::has_resizebilinear() const {
  return layer_case() == kResizeBilinear;
}
void NeuralNetworkLayer::set_has_resizebilinear() {
  _oneof_case_[0] = kResizeBilinear;
}
void NeuralNetworkLayer::clear_resizebilinear() {
  if (has_resizebilinear()) {
    delete layer_.resizebilinear_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ResizeBilinearLayerParams& NeuralNetworkLayer::resizebilinear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return has_resizebilinear()
      ? *layer_.resizebilinear_
      : ::CoreML::Specification::ResizeBilinearLayerParams::default_instance();
}
::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::mutable_resizebilinear() {
  if (!has_resizebilinear()) {
    clear_layer();
    set_has_resizebilinear();
    layer_.resizebilinear_ = new ::CoreML::Specification::ResizeBilinearLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return layer_.resizebilinear_;
}
::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::release_resizebilinear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  if (has_resizebilinear()) {
    clear_has_layer();
    ::CoreML::Specification::ResizeBilinearLayerParams* temp = layer_.resizebilinear_;
    layer_.resizebilinear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_resizebilinear(::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear) {
  clear_layer();
  if (resizebilinear) {
    set_has_resizebilinear();
    layer_.resizebilinear_ = resizebilinear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
}

// .CoreML.Specification.CropResizeLayerParams cropResize = 212;
bool NeuralNetworkLayer::has_cropresize() const {
  return layer_case() == kCropResize;
}
void NeuralNetworkLayer::set_has_cropresize() {
  _oneof_case_[0] = kCropResize;
}
void NeuralNetworkLayer::clear_cropresize() {
  if (has_cropresize()) {
    delete layer_.cropresize_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CropResizeLayerParams& NeuralNetworkLayer::cropresize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return has_cropresize()
      ? *layer_.cropresize_
      : ::CoreML::Specification::CropResizeLayerParams::default_instance();
}
::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::mutable_cropresize() {
  if (!has_cropresize()) {
    clear_layer();
    set_has_cropresize();
    layer_.cropresize_ = new ::CoreML::Specification::CropResizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return layer_.cropresize_;
}
::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::release_cropresize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cropResize)
  if (has_cropresize()) {
    clear_has_layer();
    ::CoreML::Specification::CropResizeLayerParams* temp = layer_.cropresize_;
    layer_.cropresize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_cropresize(::CoreML::Specification::CropResizeLayerParams* cropresize) {
  clear_layer();
  if (cropresize) {
    set_has_cropresize();
    layer_.cropresize_ = cropresize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cropResize)
}

// .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
bool NeuralNetworkLayer::has_unary() const {
  return layer_case() == kUnary;
}
void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
void NeuralNetworkLayer::clear_unary() {
  if (has_unary()) {
    delete layer_.unary_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return has_unary()
      ? *layer_.unary_
      : ::CoreML::Specification::UnaryFunctionLayerParams::default_instance();
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  if (!has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = new ::CoreML::Specification::UnaryFunctionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return layer_.unary_;
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}

// .CoreML.Specification.AddLayerParams add = 230;
bool NeuralNetworkLayer::has_add() const {
  return layer_case() == kAdd;
}
void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
void NeuralNetworkLayer::clear_add() {
  if (has_add()) {
    delete layer_.add_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return has_add()
      ? *layer_.add_
      : ::CoreML::Specification::AddLayerParams::default_instance();
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  if (!has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = new ::CoreML::Specification::AddLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return layer_.add_;
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}

// .CoreML.Specification.MultiplyLayerParams multiply = 231;
bool NeuralNetworkLayer::has_multiply() const {
  return layer_case() == kMultiply;
}
void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
void NeuralNetworkLayer::clear_multiply() {
  if (has_multiply()) {
    delete layer_.multiply_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return has_multiply()
      ? *layer_.multiply_
      : ::CoreML::Specification::MultiplyLayerParams::default_instance();
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  if (!has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = new ::CoreML::Specification::MultiplyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return layer_.multiply_;
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}

// .CoreML.Specification.AverageLayerParams average = 240;
bool NeuralNetworkLayer::has_average() const {
  return layer_case() == kAverage;
}
void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
void NeuralNetworkLayer::clear_average() {
  if (has_average()) {
    delete layer_.average_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return has_average()
      ? *layer_.average_
      : ::CoreML::Specification::AverageLayerParams::default_instance();
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  if (!has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = new ::CoreML::Specification::AverageLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return layer_.average_;
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}

// .CoreML.Specification.ScaleLayerParams scale = 245;
bool NeuralNetworkLayer::has_scale() const {
  return layer_case() == kScale;
}
void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
void NeuralNetworkLayer::clear_scale() {
  if (has_scale()) {
    delete layer_.scale_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return has_scale()
      ? *layer_.scale_
      : ::CoreML::Specification::ScaleLayerParams::default_instance();
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  if (!has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = new ::CoreML::Specification::ScaleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return layer_.scale_;
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}

// .CoreML.Specification.BiasLayerParams bias = 250;
bool NeuralNetworkLayer::has_bias() const {
  return layer_case() == kBias;
}
void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
void NeuralNetworkLayer::clear_bias() {
  if (has_bias()) {
    delete layer_.bias_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return has_bias()
      ? *layer_.bias_
      : ::CoreML::Specification::BiasLayerParams::default_instance();
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  if (!has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = new ::CoreML::Specification::BiasLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return layer_.bias_;
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}

// .CoreML.Specification.MaxLayerParams max = 260;
bool NeuralNetworkLayer::has_max() const {
  return layer_case() == kMax;
}
void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
void NeuralNetworkLayer::clear_max() {
  if (has_max()) {
    delete layer_.max_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return has_max()
      ? *layer_.max_
      : ::CoreML::Specification::MaxLayerParams::default_instance();
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  if (!has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = new ::CoreML::Specification::MaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return layer_.max_;
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}

// .CoreML.Specification.MinLayerParams min = 261;
bool NeuralNetworkLayer::has_min() const {
  return layer_case() == kMin;
}
void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
void NeuralNetworkLayer::clear_min() {
  if (has_min()) {
    delete layer_.min_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return has_min()
      ? *layer_.min_
      : ::CoreML::Specification::MinLayerParams::default_instance();
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  if (!has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = new ::CoreML::Specification::MinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return layer_.min_;
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}

// .CoreML.Specification.DotProductLayerParams dot = 270;
bool NeuralNetworkLayer::has_dot() const {
  return layer_case() == kDot;
}
void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
void NeuralNetworkLayer::clear_dot() {
  if (has_dot()) {
    delete layer_.dot_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return has_dot()
      ? *layer_.dot_
      : ::CoreML::Specification::DotProductLayerParams::default_instance();
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  if (!has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = new ::CoreML::Specification::DotProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return layer_.dot_;
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}

// .CoreML.Specification.ReduceLayerParams reduce = 280;
bool NeuralNetworkLayer::has_reduce() const {
  return layer_case() == kReduce;
}
void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
void NeuralNetworkLayer::clear_reduce() {
  if (has_reduce()) {
    delete layer_.reduce_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return has_reduce()
      ? *layer_.reduce_
      : ::CoreML::Specification::ReduceLayerParams::default_instance();
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  if (!has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = new ::CoreML::Specification::ReduceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return layer_.reduce_;
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}

// .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
bool NeuralNetworkLayer::has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
void NeuralNetworkLayer::clear_loadconstant() {
  if (has_loadconstant()) {
    delete layer_.loadconstant_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return has_loadconstant()
      ? *layer_.loadconstant_
      : ::CoreML::Specification::LoadConstantLayerParams::default_instance();
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  if (!has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = new ::CoreML::Specification::LoadConstantLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return layer_.loadconstant_;
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}

// .CoreML.Specification.ReshapeLayerParams reshape = 300;
bool NeuralNetworkLayer::has_reshape() const {
  return layer_case() == kReshape;
}
void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
void NeuralNetworkLayer::clear_reshape() {
  if (has_reshape()) {
    delete layer_.reshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return has_reshape()
      ? *layer_.reshape_
      : ::CoreML::Specification::ReshapeLayerParams::default_instance();
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  if (!has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = new ::CoreML::Specification::ReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return layer_.reshape_;
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}

// .CoreML.Specification.FlattenLayerParams flatten = 301;
bool NeuralNetworkLayer::has_flatten() const {
  return layer_case() == kFlatten;
}
void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
void NeuralNetworkLayer::clear_flatten() {
  if (has_flatten()) {
    delete layer_.flatten_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return has_flatten()
      ? *layer_.flatten_
      : ::CoreML::Specification::FlattenLayerParams::default_instance();
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  if (!has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = new ::CoreML::Specification::FlattenLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return layer_.flatten_;
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}

// .CoreML.Specification.PermuteLayerParams permute = 310;
bool NeuralNetworkLayer::has_permute() const {
  return layer_case() == kPermute;
}
void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
void NeuralNetworkLayer::clear_permute() {
  if (has_permute()) {
    delete layer_.permute_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return has_permute()
      ? *layer_.permute_
      : ::CoreML::Specification::PermuteLayerParams::default_instance();
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  if (!has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = new ::CoreML::Specification::PermuteLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return layer_.permute_;
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}

// .CoreML.Specification.ConcatLayerParams concat = 320;
bool NeuralNetworkLayer::has_concat() const {
  return layer_case() == kConcat;
}
void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
void NeuralNetworkLayer::clear_concat() {
  if (has_concat()) {
    delete layer_.concat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return has_concat()
      ? *layer_.concat_
      : ::CoreML::Specification::ConcatLayerParams::default_instance();
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  if (!has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = new ::CoreML::Specification::ConcatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return layer_.concat_;
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}

// .CoreML.Specification.SplitLayerParams split = 330;
bool NeuralNetworkLayer::has_split() const {
  return layer_case() == kSplit;
}
void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
void NeuralNetworkLayer::clear_split() {
  if (has_split()) {
    delete layer_.split_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return has_split()
      ? *layer_.split_
      : ::CoreML::Specification::SplitLayerParams::default_instance();
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  if (!has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = new ::CoreML::Specification::SplitLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return layer_.split_;
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}

// .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
bool NeuralNetworkLayer::has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
void NeuralNetworkLayer::clear_sequencerepeat() {
  if (has_sequencerepeat()) {
    delete layer_.sequencerepeat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : ::CoreML::Specification::SequenceRepeatLayerParams::default_instance();
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  if (!has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = new ::CoreML::Specification::SequenceRepeatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return layer_.sequencerepeat_;
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}

// .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
bool NeuralNetworkLayer::has_reorganizedata() const {
  return layer_case() == kReorganizeData;
}
void NeuralNetworkLayer::set_has_reorganizedata() {
  _oneof_case_[0] = kReorganizeData;
}
void NeuralNetworkLayer::clear_reorganizedata() {
  if (has_reorganizedata()) {
    delete layer_.reorganizedata_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReorganizeDataLayerParams& NeuralNetworkLayer::reorganizedata() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return has_reorganizedata()
      ? *layer_.reorganizedata_
      : ::CoreML::Specification::ReorganizeDataLayerParams::default_instance();
}
::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::mutable_reorganizedata() {
  if (!has_reorganizedata()) {
    clear_layer();
    set_has_reorganizedata();
    layer_.reorganizedata_ = new ::CoreML::Specification::ReorganizeDataLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return layer_.reorganizedata_;
}
::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::release_reorganizedata() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  if (has_reorganizedata()) {
    clear_has_layer();
    ::CoreML::Specification::ReorganizeDataLayerParams* temp = layer_.reorganizedata_;
    layer_.reorganizedata_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata) {
  clear_layer();
  if (reorganizedata) {
    set_has_reorganizedata();
    layer_.reorganizedata_ = reorganizedata;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
}

// .CoreML.Specification.SliceLayerParams slice = 350;
bool NeuralNetworkLayer::has_slice() const {
  return layer_case() == kSlice;
}
void NeuralNetworkLayer::set_has_slice() {
  _oneof_case_[0] = kSlice;
}
void NeuralNetworkLayer::clear_slice() {
  if (has_slice()) {
    delete layer_.slice_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SliceLayerParams& NeuralNetworkLayer::slice() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slice)
  return has_slice()
      ? *layer_.slice_
      : ::CoreML::Specification::SliceLayerParams::default_instance();
}
::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::mutable_slice() {
  if (!has_slice()) {
    clear_layer();
    set_has_slice();
    layer_.slice_ = new ::CoreML::Specification::SliceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slice)
  return layer_.slice_;
}
::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::release_slice() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slice)
  if (has_slice()) {
    clear_has_layer();
    ::CoreML::Specification::SliceLayerParams* temp = layer_.slice_;
    layer_.slice_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice) {
  clear_layer();
  if (slice) {
    set_has_slice();
    layer_.slice_ = slice;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slice)
}

// .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
bool NeuralNetworkLayer::has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
void NeuralNetworkLayer::clear_simplerecurrent() {
  if (has_simplerecurrent()) {
    delete layer_.simplerecurrent_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : ::CoreML::Specification::SimpleRecurrentLayerParams::default_instance();
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  if (!has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = new ::CoreML::Specification::SimpleRecurrentLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return layer_.simplerecurrent_;
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}

// .CoreML.Specification.GRULayerParams gru = 410;
bool NeuralNetworkLayer::has_gru() const {
  return layer_case() == kGru;
}
void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
void NeuralNetworkLayer::clear_gru() {
  if (has_gru()) {
    delete layer_.gru_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return has_gru()
      ? *layer_.gru_
      : ::CoreML::Specification::GRULayerParams::default_instance();
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  if (!has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = new ::CoreML::Specification::GRULayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return layer_.gru_;
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}

// .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (has_unidirectionallstm()) {
    delete layer_.unidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : ::CoreML::Specification::UniDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  if (!has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = new ::CoreML::Specification::UniDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return layer_.unidirectionallstm_;
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}

// .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (has_bidirectionallstm()) {
    delete layer_.bidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : ::CoreML::Specification::BiDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  if (!has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = new ::CoreML::Specification::BiDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return layer_.bidirectionallstm_;
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}

// .CoreML.Specification.CustomLayerParams custom = 500;
bool NeuralNetworkLayer::has_custom() const {
  return layer_case() == kCustom;
}
void NeuralNetworkLayer::set_has_custom() {
  _oneof_case_[0] = kCustom;
}
void NeuralNetworkLayer::clear_custom() {
  if (has_custom()) {
    delete layer_.custom_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CustomLayerParams& NeuralNetworkLayer::custom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.custom)
  return has_custom()
      ? *layer_.custom_
      : ::CoreML::Specification::CustomLayerParams::default_instance();
}
::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::mutable_custom() {
  if (!has_custom()) {
    clear_layer();
    set_has_custom();
    layer_.custom_ = new ::CoreML::Specification::CustomLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.custom)
  return layer_.custom_;
}
::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::release_custom() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.custom)
  if (has_custom()) {
    clear_has_layer();
    ::CoreML::Specification::CustomLayerParams* temp = layer_.custom_;
    layer_.custom_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom) {
  clear_layer();
  if (custom) {
    set_has_custom();
    layer_.custom_ = custom;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.custom)
}

// .CoreML.Specification.CopyLayerParams copy = 600;
bool NeuralNetworkLayer::has_copy() const {
  return layer_case() == kCopy;
}
void NeuralNetworkLayer::set_has_copy() {
  _oneof_case_[0] = kCopy;
}
void NeuralNetworkLayer::clear_copy() {
  if (has_copy()) {
    delete layer_.copy_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CopyLayerParams& NeuralNetworkLayer::copy() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.copy)
  return has_copy()
      ? *layer_.copy_
      : ::CoreML::Specification::CopyLayerParams::default_instance();
}
::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::mutable_copy() {
  if (!has_copy()) {
    clear_layer();
    set_has_copy();
    layer_.copy_ = new ::CoreML::Specification::CopyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.copy)
  return layer_.copy_;
}
::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::release_copy() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.copy)
  if (has_copy()) {
    clear_has_layer();
    ::CoreML::Specification::CopyLayerParams* temp = layer_.copy_;
    layer_.copy_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_copy(::CoreML::Specification::CopyLayerParams* copy) {
  clear_layer();
  if (copy) {
    set_has_copy();
    layer_.copy_ = copy;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.copy)
}

// .CoreML.Specification.BranchLayerParams branch = 605;
bool NeuralNetworkLayer::has_branch() const {
  return layer_case() == kBranch;
}
void NeuralNetworkLayer::set_has_branch() {
  _oneof_case_[0] = kBranch;
}
void NeuralNetworkLayer::clear_branch() {
  if (has_branch()) {
    delete layer_.branch_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BranchLayerParams& NeuralNetworkLayer::branch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.branch)
  return has_branch()
      ? *layer_.branch_
      : ::CoreML::Specification::BranchLayerParams::default_instance();
}
::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::mutable_branch() {
  if (!has_branch()) {
    clear_layer();
    set_has_branch();
    layer_.branch_ = new ::CoreML::Specification::BranchLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.branch)
  return layer_.branch_;
}
::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::release_branch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.branch)
  if (has_branch()) {
    clear_has_layer();
    ::CoreML::Specification::BranchLayerParams* temp = layer_.branch_;
    layer_.branch_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_branch(::CoreML::Specification::BranchLayerParams* branch) {
  clear_layer();
  if (branch) {
    set_has_branch();
    layer_.branch_ = branch;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.branch)
}

// .CoreML.Specification.LoopLayerParams loop = 615;
bool NeuralNetworkLayer::has_loop() const {
  return layer_case() == kLoop;
}
void NeuralNetworkLayer::set_has_loop() {
  _oneof_case_[0] = kLoop;
}
void NeuralNetworkLayer::clear_loop() {
  if (has_loop()) {
    delete layer_.loop_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoopLayerParams& NeuralNetworkLayer::loop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loop)
  return has_loop()
      ? *layer_.loop_
      : ::CoreML::Specification::LoopLayerParams::default_instance();
}
::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::mutable_loop() {
  if (!has_loop()) {
    clear_layer();
    set_has_loop();
    layer_.loop_ = new ::CoreML::Specification::LoopLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loop)
  return layer_.loop_;
}
::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::release_loop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loop)
  if (has_loop()) {
    clear_has_layer();
    ::CoreML::Specification::LoopLayerParams* temp = layer_.loop_;
    layer_.loop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loop(::CoreML::Specification::LoopLayerParams* loop) {
  clear_layer();
  if (loop) {
    set_has_loop();
    layer_.loop_ = loop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loop)
}

// .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
bool NeuralNetworkLayer::has_loopbreak() const {
  return layer_case() == kLoopBreak;
}
void NeuralNetworkLayer::set_has_loopbreak() {
  _oneof_case_[0] = kLoopBreak;
}
void NeuralNetworkLayer::clear_loopbreak() {
  if (has_loopbreak()) {
    delete layer_.loopbreak_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoopBreakLayerParams& NeuralNetworkLayer::loopbreak() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return has_loopbreak()
      ? *layer_.loopbreak_
      : ::CoreML::Specification::LoopBreakLayerParams::default_instance();
}
::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::mutable_loopbreak() {
  if (!has_loopbreak()) {
    clear_layer();
    set_has_loopbreak();
    layer_.loopbreak_ = new ::CoreML::Specification::LoopBreakLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return layer_.loopbreak_;
}
::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::release_loopbreak() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  if (has_loopbreak()) {
    clear_has_layer();
    ::CoreML::Specification::LoopBreakLayerParams* temp = layer_.loopbreak_;
    layer_.loopbreak_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loopbreak(::CoreML::Specification::LoopBreakLayerParams* loopbreak) {
  clear_layer();
  if (loopbreak) {
    set_has_loopbreak();
    layer_.loopbreak_ = loopbreak;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopBreak)
}

// .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
bool NeuralNetworkLayer::has_loopcontinue() const {
  return layer_case() == kLoopContinue;
}
void NeuralNetworkLayer::set_has_loopcontinue() {
  _oneof_case_[0] = kLoopContinue;
}
void NeuralNetworkLayer::clear_loopcontinue() {
  if (has_loopcontinue()) {
    delete layer_.loopcontinue_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoopContinueLayerParams& NeuralNetworkLayer::loopcontinue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return has_loopcontinue()
      ? *layer_.loopcontinue_
      : ::CoreML::Specification::LoopContinueLayerParams::default_instance();
}
::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::mutable_loopcontinue() {
  if (!has_loopcontinue()) {
    clear_layer();
    set_has_loopcontinue();
    layer_.loopcontinue_ = new ::CoreML::Specification::LoopContinueLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return layer_.loopcontinue_;
}
::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::release_loopcontinue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  if (has_loopcontinue()) {
    clear_has_layer();
    ::CoreML::Specification::LoopContinueLayerParams* temp = layer_.loopcontinue_;
    layer_.loopcontinue_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loopcontinue(::CoreML::Specification::LoopContinueLayerParams* loopcontinue) {
  clear_layer();
  if (loopcontinue) {
    set_has_loopcontinue();
    layer_.loopcontinue_ = loopcontinue;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopContinue)
}

// .CoreML.Specification.RangeStaticLayerParams rangeStatic = 635;
bool NeuralNetworkLayer::has_rangestatic() const {
  return layer_case() == kRangeStatic;
}
void NeuralNetworkLayer::set_has_rangestatic() {
  _oneof_case_[0] = kRangeStatic;
}
void NeuralNetworkLayer::clear_rangestatic() {
  if (has_rangestatic()) {
    delete layer_.rangestatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RangeStaticLayerParams& NeuralNetworkLayer::rangestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  return has_rangestatic()
      ? *layer_.rangestatic_
      : ::CoreML::Specification::RangeStaticLayerParams::default_instance();
}
::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::mutable_rangestatic() {
  if (!has_rangestatic()) {
    clear_layer();
    set_has_rangestatic();
    layer_.rangestatic_ = new ::CoreML::Specification::RangeStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  return layer_.rangestatic_;
}
::CoreML::Specification::RangeStaticLayerParams* NeuralNetworkLayer::release_rangestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
  if (has_rangestatic()) {
    clear_has_layer();
    ::CoreML::Specification::RangeStaticLayerParams* temp = layer_.rangestatic_;
    layer_.rangestatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_rangestatic(::CoreML::Specification::RangeStaticLayerParams* rangestatic) {
  clear_layer();
  if (rangestatic) {
    set_has_rangestatic();
    layer_.rangestatic_ = rangestatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rangeStatic)
}

// .CoreML.Specification.RangeDynamicLayerParams rangeDynamic = 640;
bool NeuralNetworkLayer::has_rangedynamic() const {
  return layer_case() == kRangeDynamic;
}
void NeuralNetworkLayer::set_has_rangedynamic() {
  _oneof_case_[0] = kRangeDynamic;
}
void NeuralNetworkLayer::clear_rangedynamic() {
  if (has_rangedynamic()) {
    delete layer_.rangedynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RangeDynamicLayerParams& NeuralNetworkLayer::rangedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  return has_rangedynamic()
      ? *layer_.rangedynamic_
      : ::CoreML::Specification::RangeDynamicLayerParams::default_instance();
}
::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::mutable_rangedynamic() {
  if (!has_rangedynamic()) {
    clear_layer();
    set_has_rangedynamic();
    layer_.rangedynamic_ = new ::CoreML::Specification::RangeDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  return layer_.rangedynamic_;
}
::CoreML::Specification::RangeDynamicLayerParams* NeuralNetworkLayer::release_rangedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
  if (has_rangedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RangeDynamicLayerParams* temp = layer_.rangedynamic_;
    layer_.rangedynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_rangedynamic(::CoreML::Specification::RangeDynamicLayerParams* rangedynamic) {
  clear_layer();
  if (rangedynamic) {
    set_has_rangedynamic();
    layer_.rangedynamic_ = rangedynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rangeDynamic)
}

// .CoreML.Specification.ClipLayerParams clip = 660;
bool NeuralNetworkLayer::has_clip() const {
  return layer_case() == kClip;
}
void NeuralNetworkLayer::set_has_clip() {
  _oneof_case_[0] = kClip;
}
void NeuralNetworkLayer::clear_clip() {
  if (has_clip()) {
    delete layer_.clip_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ClipLayerParams& NeuralNetworkLayer::clip() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.clip)
  return has_clip()
      ? *layer_.clip_
      : ::CoreML::Specification::ClipLayerParams::default_instance();
}
::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::mutable_clip() {
  if (!has_clip()) {
    clear_layer();
    set_has_clip();
    layer_.clip_ = new ::CoreML::Specification::ClipLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.clip)
  return layer_.clip_;
}
::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::release_clip() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.clip)
  if (has_clip()) {
    clear_has_layer();
    ::CoreML::Specification::ClipLayerParams* temp = layer_.clip_;
    layer_.clip_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_clip(::CoreML::Specification::ClipLayerParams* clip) {
  clear_layer();
  if (clip) {
    set_has_clip();
    layer_.clip_ = clip;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.clip)
}

// .CoreML.Specification.CeilLayerParams ceil = 665;
bool NeuralNetworkLayer::has_ceil() const {
  return layer_case() == kCeil;
}
void NeuralNetworkLayer::set_has_ceil() {
  _oneof_case_[0] = kCeil;
}
void NeuralNetworkLayer::clear_ceil() {
  if (has_ceil()) {
    delete layer_.ceil_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CeilLayerParams& NeuralNetworkLayer::ceil() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.ceil)
  return has_ceil()
      ? *layer_.ceil_
      : ::CoreML::Specification::CeilLayerParams::default_instance();
}
::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::mutable_ceil() {
  if (!has_ceil()) {
    clear_layer();
    set_has_ceil();
    layer_.ceil_ = new ::CoreML::Specification::CeilLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.ceil)
  return layer_.ceil_;
}
::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::release_ceil() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.ceil)
  if (has_ceil()) {
    clear_has_layer();
    ::CoreML::Specification::CeilLayerParams* temp = layer_.ceil_;
    layer_.ceil_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_ceil(::CoreML::Specification::CeilLayerParams* ceil) {
  clear_layer();
  if (ceil) {
    set_has_ceil();
    layer_.ceil_ = ceil;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.ceil)
}

// .CoreML.Specification.FloorLayerParams floor = 670;
bool NeuralNetworkLayer::has_floor() const {
  return layer_case() == kFloor;
}
void NeuralNetworkLayer::set_has_floor() {
  _oneof_case_[0] = kFloor;
}
void NeuralNetworkLayer::clear_floor() {
  if (has_floor()) {
    delete layer_.floor_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FloorLayerParams& NeuralNetworkLayer::floor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floor)
  return has_floor()
      ? *layer_.floor_
      : ::CoreML::Specification::FloorLayerParams::default_instance();
}
::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::mutable_floor() {
  if (!has_floor()) {
    clear_layer();
    set_has_floor();
    layer_.floor_ = new ::CoreML::Specification::FloorLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floor)
  return layer_.floor_;
}
::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::release_floor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floor)
  if (has_floor()) {
    clear_has_layer();
    ::CoreML::Specification::FloorLayerParams* temp = layer_.floor_;
    layer_.floor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_floor(::CoreML::Specification::FloorLayerParams* floor) {
  clear_layer();
  if (floor) {
    set_has_floor();
    layer_.floor_ = floor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.floor)
}

// .CoreML.Specification.SignLayerParams sign = 680;
bool NeuralNetworkLayer::has_sign() const {
  return layer_case() == kSign;
}
void NeuralNetworkLayer::set_has_sign() {
  _oneof_case_[0] = kSign;
}
void NeuralNetworkLayer::clear_sign() {
  if (has_sign()) {
    delete layer_.sign_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SignLayerParams& NeuralNetworkLayer::sign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sign)
  return has_sign()
      ? *layer_.sign_
      : ::CoreML::Specification::SignLayerParams::default_instance();
}
::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::mutable_sign() {
  if (!has_sign()) {
    clear_layer();
    set_has_sign();
    layer_.sign_ = new ::CoreML::Specification::SignLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sign)
  return layer_.sign_;
}
::CoreML::Specification::SignLayerParams* NeuralNetworkLayer::release_sign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sign)
  if (has_sign()) {
    clear_has_layer();
    ::CoreML::Specification::SignLayerParams* temp = layer_.sign_;
    layer_.sign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sign(::CoreML::Specification::SignLayerParams* sign) {
  clear_layer();
  if (sign) {
    set_has_sign();
    layer_.sign_ = sign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sign)
}

// .CoreML.Specification.RoundLayerParams round = 685;
bool NeuralNetworkLayer::has_round() const {
  return layer_case() == kRound;
}
void NeuralNetworkLayer::set_has_round() {
  _oneof_case_[0] = kRound;
}
void NeuralNetworkLayer::clear_round() {
  if (has_round()) {
    delete layer_.round_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RoundLayerParams& NeuralNetworkLayer::round() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.round)
  return has_round()
      ? *layer_.round_
      : ::CoreML::Specification::RoundLayerParams::default_instance();
}
::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::mutable_round() {
  if (!has_round()) {
    clear_layer();
    set_has_round();
    layer_.round_ = new ::CoreML::Specification::RoundLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.round)
  return layer_.round_;
}
::CoreML::Specification::RoundLayerParams* NeuralNetworkLayer::release_round() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.round)
  if (has_round()) {
    clear_has_layer();
    ::CoreML::Specification::RoundLayerParams* temp = layer_.round_;
    layer_.round_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_round(::CoreML::Specification::RoundLayerParams* round) {
  clear_layer();
  if (round) {
    set_has_round();
    layer_.round_ = round;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.round)
}

// .CoreML.Specification.Exp2LayerParams exp2 = 700;
bool NeuralNetworkLayer::has_exp2() const {
  return layer_case() == kExp2;
}
void NeuralNetworkLayer::set_has_exp2() {
  _oneof_case_[0] = kExp2;
}
void NeuralNetworkLayer::clear_exp2() {
  if (has_exp2()) {
    delete layer_.exp2_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::Exp2LayerParams& NeuralNetworkLayer::exp2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.exp2)
  return has_exp2()
      ? *layer_.exp2_
      : ::CoreML::Specification::Exp2LayerParams::default_instance();
}
::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::mutable_exp2() {
  if (!has_exp2()) {
    clear_layer();
    set_has_exp2();
    layer_.exp2_ = new ::CoreML::Specification::Exp2LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.exp2)
  return layer_.exp2_;
}
::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::release_exp2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.exp2)
  if (has_exp2()) {
    clear_has_layer();
    ::CoreML::Specification::Exp2LayerParams* temp = layer_.exp2_;
    layer_.exp2_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_exp2(::CoreML::Specification::Exp2LayerParams* exp2) {
  clear_layer();
  if (exp2) {
    set_has_exp2();
    layer_.exp2_ = exp2;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.exp2)
}

// .CoreML.Specification.SinLayerParams sin = 710;
bool NeuralNetworkLayer::has_sin() const {
  return layer_case() == kSin;
}
void NeuralNetworkLayer::set_has_sin() {
  _oneof_case_[0] = kSin;
}
void NeuralNetworkLayer::clear_sin() {
  if (has_sin()) {
    delete layer_.sin_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SinLayerParams& NeuralNetworkLayer::sin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sin)
  return has_sin()
      ? *layer_.sin_
      : ::CoreML::Specification::SinLayerParams::default_instance();
}
::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::mutable_sin() {
  if (!has_sin()) {
    clear_layer();
    set_has_sin();
    layer_.sin_ = new ::CoreML::Specification::SinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sin)
  return layer_.sin_;
}
::CoreML::Specification::SinLayerParams* NeuralNetworkLayer::release_sin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sin)
  if (has_sin()) {
    clear_has_layer();
    ::CoreML::Specification::SinLayerParams* temp = layer_.sin_;
    layer_.sin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sin(::CoreML::Specification::SinLayerParams* sin) {
  clear_layer();
  if (sin) {
    set_has_sin();
    layer_.sin_ = sin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sin)
}

// .CoreML.Specification.CosLayerParams cos = 715;
bool NeuralNetworkLayer::has_cos() const {
  return layer_case() == kCos;
}
void NeuralNetworkLayer::set_has_cos() {
  _oneof_case_[0] = kCos;
}
void NeuralNetworkLayer::clear_cos() {
  if (has_cos()) {
    delete layer_.cos_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CosLayerParams& NeuralNetworkLayer::cos() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cos)
  return has_cos()
      ? *layer_.cos_
      : ::CoreML::Specification::CosLayerParams::default_instance();
}
::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::mutable_cos() {
  if (!has_cos()) {
    clear_layer();
    set_has_cos();
    layer_.cos_ = new ::CoreML::Specification::CosLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cos)
  return layer_.cos_;
}
::CoreML::Specification::CosLayerParams* NeuralNetworkLayer::release_cos() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cos)
  if (has_cos()) {
    clear_has_layer();
    ::CoreML::Specification::CosLayerParams* temp = layer_.cos_;
    layer_.cos_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_cos(::CoreML::Specification::CosLayerParams* cos) {
  clear_layer();
  if (cos) {
    set_has_cos();
    layer_.cos_ = cos;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cos)
}

// .CoreML.Specification.TanLayerParams tan = 720;
bool NeuralNetworkLayer::has_tan() const {
  return layer_case() == kTan;
}
void NeuralNetworkLayer::set_has_tan() {
  _oneof_case_[0] = kTan;
}
void NeuralNetworkLayer::clear_tan() {
  if (has_tan()) {
    delete layer_.tan_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TanLayerParams& NeuralNetworkLayer::tan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tan)
  return has_tan()
      ? *layer_.tan_
      : ::CoreML::Specification::TanLayerParams::default_instance();
}
::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::mutable_tan() {
  if (!has_tan()) {
    clear_layer();
    set_has_tan();
    layer_.tan_ = new ::CoreML::Specification::TanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tan)
  return layer_.tan_;
}
::CoreML::Specification::TanLayerParams* NeuralNetworkLayer::release_tan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tan)
  if (has_tan()) {
    clear_has_layer();
    ::CoreML::Specification::TanLayerParams* temp = layer_.tan_;
    layer_.tan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_tan(::CoreML::Specification::TanLayerParams* tan) {
  clear_layer();
  if (tan) {
    set_has_tan();
    layer_.tan_ = tan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tan)
}

// .CoreML.Specification.AsinLayerParams asin = 730;
bool NeuralNetworkLayer::has_asin() const {
  return layer_case() == kAsin;
}
void NeuralNetworkLayer::set_has_asin() {
  _oneof_case_[0] = kAsin;
}
void NeuralNetworkLayer::clear_asin() {
  if (has_asin()) {
    delete layer_.asin_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AsinLayerParams& NeuralNetworkLayer::asin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.asin)
  return has_asin()
      ? *layer_.asin_
      : ::CoreML::Specification::AsinLayerParams::default_instance();
}
::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::mutable_asin() {
  if (!has_asin()) {
    clear_layer();
    set_has_asin();
    layer_.asin_ = new ::CoreML::Specification::AsinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.asin)
  return layer_.asin_;
}
::CoreML::Specification::AsinLayerParams* NeuralNetworkLayer::release_asin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.asin)
  if (has_asin()) {
    clear_has_layer();
    ::CoreML::Specification::AsinLayerParams* temp = layer_.asin_;
    layer_.asin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_asin(::CoreML::Specification::AsinLayerParams* asin) {
  clear_layer();
  if (asin) {
    set_has_asin();
    layer_.asin_ = asin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.asin)
}

// .CoreML.Specification.AcosLayerParams acos = 735;
bool NeuralNetworkLayer::has_acos() const {
  return layer_case() == kAcos;
}
void NeuralNetworkLayer::set_has_acos() {
  _oneof_case_[0] = kAcos;
}
void NeuralNetworkLayer::clear_acos() {
  if (has_acos()) {
    delete layer_.acos_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AcosLayerParams& NeuralNetworkLayer::acos() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.acos)
  return has_acos()
      ? *layer_.acos_
      : ::CoreML::Specification::AcosLayerParams::default_instance();
}
::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::mutable_acos() {
  if (!has_acos()) {
    clear_layer();
    set_has_acos();
    layer_.acos_ = new ::CoreML::Specification::AcosLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.acos)
  return layer_.acos_;
}
::CoreML::Specification::AcosLayerParams* NeuralNetworkLayer::release_acos() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.acos)
  if (has_acos()) {
    clear_has_layer();
    ::CoreML::Specification::AcosLayerParams* temp = layer_.acos_;
    layer_.acos_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_acos(::CoreML::Specification::AcosLayerParams* acos) {
  clear_layer();
  if (acos) {
    set_has_acos();
    layer_.acos_ = acos;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.acos)
}

// .CoreML.Specification.AtanLayerParams atan = 740;
bool NeuralNetworkLayer::has_atan() const {
  return layer_case() == kAtan;
}
void NeuralNetworkLayer::set_has_atan() {
  _oneof_case_[0] = kAtan;
}
void NeuralNetworkLayer::clear_atan() {
  if (has_atan()) {
    delete layer_.atan_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AtanLayerParams& NeuralNetworkLayer::atan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.atan)
  return has_atan()
      ? *layer_.atan_
      : ::CoreML::Specification::AtanLayerParams::default_instance();
}
::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::mutable_atan() {
  if (!has_atan()) {
    clear_layer();
    set_has_atan();
    layer_.atan_ = new ::CoreML::Specification::AtanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.atan)
  return layer_.atan_;
}
::CoreML::Specification::AtanLayerParams* NeuralNetworkLayer::release_atan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.atan)
  if (has_atan()) {
    clear_has_layer();
    ::CoreML::Specification::AtanLayerParams* temp = layer_.atan_;
    layer_.atan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_atan(::CoreML::Specification::AtanLayerParams* atan) {
  clear_layer();
  if (atan) {
    set_has_atan();
    layer_.atan_ = atan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.atan)
}

// .CoreML.Specification.SinhLayerParams sinh = 750;
bool NeuralNetworkLayer::has_sinh() const {
  return layer_case() == kSinh;
}
void NeuralNetworkLayer::set_has_sinh() {
  _oneof_case_[0] = kSinh;
}
void NeuralNetworkLayer::clear_sinh() {
  if (has_sinh()) {
    delete layer_.sinh_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SinhLayerParams& NeuralNetworkLayer::sinh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sinh)
  return has_sinh()
      ? *layer_.sinh_
      : ::CoreML::Specification::SinhLayerParams::default_instance();
}
::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::mutable_sinh() {
  if (!has_sinh()) {
    clear_layer();
    set_has_sinh();
    layer_.sinh_ = new ::CoreML::Specification::SinhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sinh)
  return layer_.sinh_;
}
::CoreML::Specification::SinhLayerParams* NeuralNetworkLayer::release_sinh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sinh)
  if (has_sinh()) {
    clear_has_layer();
    ::CoreML::Specification::SinhLayerParams* temp = layer_.sinh_;
    layer_.sinh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sinh(::CoreML::Specification::SinhLayerParams* sinh) {
  clear_layer();
  if (sinh) {
    set_has_sinh();
    layer_.sinh_ = sinh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sinh)
}

// .CoreML.Specification.CoshLayerParams cosh = 755;
bool NeuralNetworkLayer::has_cosh() const {
  return layer_case() == kCosh;
}
void NeuralNetworkLayer::set_has_cosh() {
  _oneof_case_[0] = kCosh;
}
void NeuralNetworkLayer::clear_cosh() {
  if (has_cosh()) {
    delete layer_.cosh_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CoshLayerParams& NeuralNetworkLayer::cosh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cosh)
  return has_cosh()
      ? *layer_.cosh_
      : ::CoreML::Specification::CoshLayerParams::default_instance();
}
::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::mutable_cosh() {
  if (!has_cosh()) {
    clear_layer();
    set_has_cosh();
    layer_.cosh_ = new ::CoreML::Specification::CoshLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cosh)
  return layer_.cosh_;
}
::CoreML::Specification::CoshLayerParams* NeuralNetworkLayer::release_cosh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cosh)
  if (has_cosh()) {
    clear_has_layer();
    ::CoreML::Specification::CoshLayerParams* temp = layer_.cosh_;
    layer_.cosh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_cosh(::CoreML::Specification::CoshLayerParams* cosh) {
  clear_layer();
  if (cosh) {
    set_has_cosh();
    layer_.cosh_ = cosh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cosh)
}

// .CoreML.Specification.TanhLayerParams tanh = 760;
bool NeuralNetworkLayer::has_tanh() const {
  return layer_case() == kTanh;
}
void NeuralNetworkLayer::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
void NeuralNetworkLayer::clear_tanh() {
  if (has_tanh()) {
    delete layer_.tanh_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TanhLayerParams& NeuralNetworkLayer::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tanh)
  return has_tanh()
      ? *layer_.tanh_
      : ::CoreML::Specification::TanhLayerParams::default_instance();
}
::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::mutable_tanh() {
  if (!has_tanh()) {
    clear_layer();
    set_has_tanh();
    layer_.tanh_ = new ::CoreML::Specification::TanhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tanh)
  return layer_.tanh_;
}
::CoreML::Specification::TanhLayerParams* NeuralNetworkLayer::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tanh)
  if (has_tanh()) {
    clear_has_layer();
    ::CoreML::Specification::TanhLayerParams* temp = layer_.tanh_;
    layer_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_tanh(::CoreML::Specification::TanhLayerParams* tanh) {
  clear_layer();
  if (tanh) {
    set_has_tanh();
    layer_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tanh)
}

// .CoreML.Specification.AsinhLayerParams asinh = 770;
bool NeuralNetworkLayer::has_asinh() const {
  return layer_case() == kAsinh;
}
void NeuralNetworkLayer::set_has_asinh() {
  _oneof_case_[0] = kAsinh;
}
void NeuralNetworkLayer::clear_asinh() {
  if (has_asinh()) {
    delete layer_.asinh_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AsinhLayerParams& NeuralNetworkLayer::asinh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.asinh)
  return has_asinh()
      ? *layer_.asinh_
      : ::CoreML::Specification::AsinhLayerParams::default_instance();
}
::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::mutable_asinh() {
  if (!has_asinh()) {
    clear_layer();
    set_has_asinh();
    layer_.asinh_ = new ::CoreML::Specification::AsinhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.asinh)
  return layer_.asinh_;
}
::CoreML::Specification::AsinhLayerParams* NeuralNetworkLayer::release_asinh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.asinh)
  if (has_asinh()) {
    clear_has_layer();
    ::CoreML::Specification::AsinhLayerParams* temp = layer_.asinh_;
    layer_.asinh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_asinh(::CoreML::Specification::AsinhLayerParams* asinh) {
  clear_layer();
  if (asinh) {
    set_has_asinh();
    layer_.asinh_ = asinh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.asinh)
}

// .CoreML.Specification.AcoshLayerParams acosh = 775;
bool NeuralNetworkLayer::has_acosh() const {
  return layer_case() == kAcosh;
}
void NeuralNetworkLayer::set_has_acosh() {
  _oneof_case_[0] = kAcosh;
}
void NeuralNetworkLayer::clear_acosh() {
  if (has_acosh()) {
    delete layer_.acosh_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AcoshLayerParams& NeuralNetworkLayer::acosh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.acosh)
  return has_acosh()
      ? *layer_.acosh_
      : ::CoreML::Specification::AcoshLayerParams::default_instance();
}
::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::mutable_acosh() {
  if (!has_acosh()) {
    clear_layer();
    set_has_acosh();
    layer_.acosh_ = new ::CoreML::Specification::AcoshLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.acosh)
  return layer_.acosh_;
}
::CoreML::Specification::AcoshLayerParams* NeuralNetworkLayer::release_acosh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.acosh)
  if (has_acosh()) {
    clear_has_layer();
    ::CoreML::Specification::AcoshLayerParams* temp = layer_.acosh_;
    layer_.acosh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_acosh(::CoreML::Specification::AcoshLayerParams* acosh) {
  clear_layer();
  if (acosh) {
    set_has_acosh();
    layer_.acosh_ = acosh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.acosh)
}

// .CoreML.Specification.AtanhLayerParams atanh = 780;
bool NeuralNetworkLayer::has_atanh() const {
  return layer_case() == kAtanh;
}
void NeuralNetworkLayer::set_has_atanh() {
  _oneof_case_[0] = kAtanh;
}
void NeuralNetworkLayer::clear_atanh() {
  if (has_atanh()) {
    delete layer_.atanh_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AtanhLayerParams& NeuralNetworkLayer::atanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.atanh)
  return has_atanh()
      ? *layer_.atanh_
      : ::CoreML::Specification::AtanhLayerParams::default_instance();
}
::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::mutable_atanh() {
  if (!has_atanh()) {
    clear_layer();
    set_has_atanh();
    layer_.atanh_ = new ::CoreML::Specification::AtanhLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.atanh)
  return layer_.atanh_;
}
::CoreML::Specification::AtanhLayerParams* NeuralNetworkLayer::release_atanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.atanh)
  if (has_atanh()) {
    clear_has_layer();
    ::CoreML::Specification::AtanhLayerParams* temp = layer_.atanh_;
    layer_.atanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_atanh(::CoreML::Specification::AtanhLayerParams* atanh) {
  clear_layer();
  if (atanh) {
    set_has_atanh();
    layer_.atanh_ = atanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.atanh)
}

// .CoreML.Specification.ErfLayerParams erf = 790;
bool NeuralNetworkLayer::has_erf() const {
  return layer_case() == kErf;
}
void NeuralNetworkLayer::set_has_erf() {
  _oneof_case_[0] = kErf;
}
void NeuralNetworkLayer::clear_erf() {
  if (has_erf()) {
    delete layer_.erf_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ErfLayerParams& NeuralNetworkLayer::erf() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.erf)
  return has_erf()
      ? *layer_.erf_
      : ::CoreML::Specification::ErfLayerParams::default_instance();
}
::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::mutable_erf() {
  if (!has_erf()) {
    clear_layer();
    set_has_erf();
    layer_.erf_ = new ::CoreML::Specification::ErfLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.erf)
  return layer_.erf_;
}
::CoreML::Specification::ErfLayerParams* NeuralNetworkLayer::release_erf() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.erf)
  if (has_erf()) {
    clear_has_layer();
    ::CoreML::Specification::ErfLayerParams* temp = layer_.erf_;
    layer_.erf_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_erf(::CoreML::Specification::ErfLayerParams* erf) {
  clear_layer();
  if (erf) {
    set_has_erf();
    layer_.erf_ = erf;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.erf)
}

// .CoreML.Specification.GeluLayerParams gelu = 795;
bool NeuralNetworkLayer::has_gelu() const {
  return layer_case() == kGelu;
}
void NeuralNetworkLayer::set_has_gelu() {
  _oneof_case_[0] = kGelu;
}
void NeuralNetworkLayer::clear_gelu() {
  if (has_gelu()) {
    delete layer_.gelu_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GeluLayerParams& NeuralNetworkLayer::gelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gelu)
  return has_gelu()
      ? *layer_.gelu_
      : ::CoreML::Specification::GeluLayerParams::default_instance();
}
::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::mutable_gelu() {
  if (!has_gelu()) {
    clear_layer();
    set_has_gelu();
    layer_.gelu_ = new ::CoreML::Specification::GeluLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gelu)
  return layer_.gelu_;
}
::CoreML::Specification::GeluLayerParams* NeuralNetworkLayer::release_gelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gelu)
  if (has_gelu()) {
    clear_has_layer();
    ::CoreML::Specification::GeluLayerParams* temp = layer_.gelu_;
    layer_.gelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gelu(::CoreML::Specification::GeluLayerParams* gelu) {
  clear_layer();
  if (gelu) {
    set_has_gelu();
    layer_.gelu_ = gelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gelu)
}

// .CoreML.Specification.EqualLayerParams equal = 815;
bool NeuralNetworkLayer::has_equal() const {
  return layer_case() == kEqual;
}
void NeuralNetworkLayer::set_has_equal() {
  _oneof_case_[0] = kEqual;
}
void NeuralNetworkLayer::clear_equal() {
  if (has_equal()) {
    delete layer_.equal_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EqualLayerParams& NeuralNetworkLayer::equal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.equal)
  return has_equal()
      ? *layer_.equal_
      : ::CoreML::Specification::EqualLayerParams::default_instance();
}
::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::mutable_equal() {
  if (!has_equal()) {
    clear_layer();
    set_has_equal();
    layer_.equal_ = new ::CoreML::Specification::EqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.equal)
  return layer_.equal_;
}
::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::release_equal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.equal)
  if (has_equal()) {
    clear_has_layer();
    ::CoreML::Specification::EqualLayerParams* temp = layer_.equal_;
    layer_.equal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_equal(::CoreML::Specification::EqualLayerParams* equal) {
  clear_layer();
  if (equal) {
    set_has_equal();
    layer_.equal_ = equal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.equal)
}

// .CoreML.Specification.NotEqualLayerParams notEqual = 820;
bool NeuralNetworkLayer::has_notequal() const {
  return layer_case() == kNotEqual;
}
void NeuralNetworkLayer::set_has_notequal() {
  _oneof_case_[0] = kNotEqual;
}
void NeuralNetworkLayer::clear_notequal() {
  if (has_notequal()) {
    delete layer_.notequal_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::NotEqualLayerParams& NeuralNetworkLayer::notequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return has_notequal()
      ? *layer_.notequal_
      : ::CoreML::Specification::NotEqualLayerParams::default_instance();
}
::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::mutable_notequal() {
  if (!has_notequal()) {
    clear_layer();
    set_has_notequal();
    layer_.notequal_ = new ::CoreML::Specification::NotEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return layer_.notequal_;
}
::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::release_notequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.notEqual)
  if (has_notequal()) {
    clear_has_layer();
    ::CoreML::Specification::NotEqualLayerParams* temp = layer_.notequal_;
    layer_.notequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_notequal(::CoreML::Specification::NotEqualLayerParams* notequal) {
  clear_layer();
  if (notequal) {
    set_has_notequal();
    layer_.notequal_ = notequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.notEqual)
}

// .CoreML.Specification.LessThanLayerParams lessThan = 825;
bool NeuralNetworkLayer::has_lessthan() const {
  return layer_case() == kLessThan;
}
void NeuralNetworkLayer::set_has_lessthan() {
  _oneof_case_[0] = kLessThan;
}
void NeuralNetworkLayer::clear_lessthan() {
  if (has_lessthan()) {
    delete layer_.lessthan_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LessThanLayerParams& NeuralNetworkLayer::lessthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return has_lessthan()
      ? *layer_.lessthan_
      : ::CoreML::Specification::LessThanLayerParams::default_instance();
}
::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::mutable_lessthan() {
  if (!has_lessthan()) {
    clear_layer();
    set_has_lessthan();
    layer_.lessthan_ = new ::CoreML::Specification::LessThanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return layer_.lessthan_;
}
::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::release_lessthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessThan)
  if (has_lessthan()) {
    clear_has_layer();
    ::CoreML::Specification::LessThanLayerParams* temp = layer_.lessthan_;
    layer_.lessthan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lessthan(::CoreML::Specification::LessThanLayerParams* lessthan) {
  clear_layer();
  if (lessthan) {
    set_has_lessthan();
    layer_.lessthan_ = lessthan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessThan)
}

// .CoreML.Specification.LessEqualLayerParams lessEqual = 827;
bool NeuralNetworkLayer::has_lessequal() const {
  return layer_case() == kLessEqual;
}
void NeuralNetworkLayer::set_has_lessequal() {
  _oneof_case_[0] = kLessEqual;
}
void NeuralNetworkLayer::clear_lessequal() {
  if (has_lessequal()) {
    delete layer_.lessequal_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LessEqualLayerParams& NeuralNetworkLayer::lessequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  return has_lessequal()
      ? *layer_.lessequal_
      : ::CoreML::Specification::LessEqualLayerParams::default_instance();
}
::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::mutable_lessequal() {
  if (!has_lessequal()) {
    clear_layer();
    set_has_lessequal();
    layer_.lessequal_ = new ::CoreML::Specification::LessEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  return layer_.lessequal_;
}
::CoreML::Specification::LessEqualLayerParams* NeuralNetworkLayer::release_lessequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessEqual)
  if (has_lessequal()) {
    clear_has_layer();
    ::CoreML::Specification::LessEqualLayerParams* temp = layer_.lessequal_;
    layer_.lessequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lessequal(::CoreML::Specification::LessEqualLayerParams* lessequal) {
  clear_layer();
  if (lessequal) {
    set_has_lessequal();
    layer_.lessequal_ = lessequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessEqual)
}

// .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
bool NeuralNetworkLayer::has_greaterthan() const {
  return layer_case() == kGreaterThan;
}
void NeuralNetworkLayer::set_has_greaterthan() {
  _oneof_case_[0] = kGreaterThan;
}
void NeuralNetworkLayer::clear_greaterthan() {
  if (has_greaterthan()) {
    delete layer_.greaterthan_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GreaterThanLayerParams& NeuralNetworkLayer::greaterthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return has_greaterthan()
      ? *layer_.greaterthan_
      : ::CoreML::Specification::GreaterThanLayerParams::default_instance();
}
::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::mutable_greaterthan() {
  if (!has_greaterthan()) {
    clear_layer();
    set_has_greaterthan();
    layer_.greaterthan_ = new ::CoreML::Specification::GreaterThanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return layer_.greaterthan_;
}
::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::release_greaterthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  if (has_greaterthan()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterThanLayerParams* temp = layer_.greaterthan_;
    layer_.greaterthan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_greaterthan(::CoreML::Specification::GreaterThanLayerParams* greaterthan) {
  clear_layer();
  if (greaterthan) {
    set_has_greaterthan();
    layer_.greaterthan_ = greaterthan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterThan)
}

// .CoreML.Specification.GreaterEqualLayerParams greaterEqual = 832;
bool NeuralNetworkLayer::has_greaterequal() const {
  return layer_case() == kGreaterEqual;
}
void NeuralNetworkLayer::set_has_greaterequal() {
  _oneof_case_[0] = kGreaterEqual;
}
void NeuralNetworkLayer::clear_greaterequal() {
  if (has_greaterequal()) {
    delete layer_.greaterequal_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GreaterEqualLayerParams& NeuralNetworkLayer::greaterequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  return has_greaterequal()
      ? *layer_.greaterequal_
      : ::CoreML::Specification::GreaterEqualLayerParams::default_instance();
}
::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::mutable_greaterequal() {
  if (!has_greaterequal()) {
    clear_layer();
    set_has_greaterequal();
    layer_.greaterequal_ = new ::CoreML::Specification::GreaterEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  return layer_.greaterequal_;
}
::CoreML::Specification::GreaterEqualLayerParams* NeuralNetworkLayer::release_greaterequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
  if (has_greaterequal()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterEqualLayerParams* temp = layer_.greaterequal_;
    layer_.greaterequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_greaterequal(::CoreML::Specification::GreaterEqualLayerParams* greaterequal) {
  clear_layer();
  if (greaterequal) {
    set_has_greaterequal();
    layer_.greaterequal_ = greaterequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterEqual)
}

// .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
bool NeuralNetworkLayer::has_logicalor() const {
  return layer_case() == kLogicalOr;
}
void NeuralNetworkLayer::set_has_logicalor() {
  _oneof_case_[0] = kLogicalOr;
}
void NeuralNetworkLayer::clear_logicalor() {
  if (has_logicalor()) {
    delete layer_.logicalor_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalOrLayerParams& NeuralNetworkLayer::logicalor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return has_logicalor()
      ? *layer_.logicalor_
      : ::CoreML::Specification::LogicalOrLayerParams::default_instance();
}
::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::mutable_logicalor() {
  if (!has_logicalor()) {
    clear_layer();
    set_has_logicalor();
    layer_.logicalor_ = new ::CoreML::Specification::LogicalOrLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return layer_.logicalor_;
}
::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::release_logicalor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  if (has_logicalor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalOrLayerParams* temp = layer_.logicalor_;
    layer_.logicalor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicalor(::CoreML::Specification::LogicalOrLayerParams* logicalor) {
  clear_layer();
  if (logicalor) {
    set_has_logicalor();
    layer_.logicalor_ = logicalor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalOr)
}

// .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
bool NeuralNetworkLayer::has_logicalxor() const {
  return layer_case() == kLogicalXor;
}
void NeuralNetworkLayer::set_has_logicalxor() {
  _oneof_case_[0] = kLogicalXor;
}
void NeuralNetworkLayer::clear_logicalxor() {
  if (has_logicalxor()) {
    delete layer_.logicalxor_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalXorLayerParams& NeuralNetworkLayer::logicalxor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return has_logicalxor()
      ? *layer_.logicalxor_
      : ::CoreML::Specification::LogicalXorLayerParams::default_instance();
}
::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::mutable_logicalxor() {
  if (!has_logicalxor()) {
    clear_layer();
    set_has_logicalxor();
    layer_.logicalxor_ = new ::CoreML::Specification::LogicalXorLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return layer_.logicalxor_;
}
::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::release_logicalxor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  if (has_logicalxor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalXorLayerParams* temp = layer_.logicalxor_;
    layer_.logicalxor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicalxor(::CoreML::Specification::LogicalXorLayerParams* logicalxor) {
  clear_layer();
  if (logicalxor) {
    set_has_logicalxor();
    layer_.logicalxor_ = logicalxor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalXor)
}

// .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
bool NeuralNetworkLayer::has_logicalnot() const {
  return layer_case() == kLogicalNot;
}
void NeuralNetworkLayer::set_has_logicalnot() {
  _oneof_case_[0] = kLogicalNot;
}
void NeuralNetworkLayer::clear_logicalnot() {
  if (has_logicalnot()) {
    delete layer_.logicalnot_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalNotLayerParams& NeuralNetworkLayer::logicalnot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return has_logicalnot()
      ? *layer_.logicalnot_
      : ::CoreML::Specification::LogicalNotLayerParams::default_instance();
}
::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::mutable_logicalnot() {
  if (!has_logicalnot()) {
    clear_layer();
    set_has_logicalnot();
    layer_.logicalnot_ = new ::CoreML::Specification::LogicalNotLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return layer_.logicalnot_;
}
::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::release_logicalnot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  if (has_logicalnot()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalNotLayerParams* temp = layer_.logicalnot_;
    layer_.logicalnot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicalnot(::CoreML::Specification::LogicalNotLayerParams* logicalnot) {
  clear_layer();
  if (logicalnot) {
    set_has_logicalnot();
    layer_.logicalnot_ = logicalnot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalNot)
}

// .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
bool NeuralNetworkLayer::has_logicaland() const {
  return layer_case() == kLogicalAnd;
}
void NeuralNetworkLayer::set_has_logicaland() {
  _oneof_case_[0] = kLogicalAnd;
}
void NeuralNetworkLayer::clear_logicaland() {
  if (has_logicaland()) {
    delete layer_.logicaland_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalAndLayerParams& NeuralNetworkLayer::logicaland() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return has_logicaland()
      ? *layer_.logicaland_
      : ::CoreML::Specification::LogicalAndLayerParams::default_instance();
}
::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::mutable_logicaland() {
  if (!has_logicaland()) {
    clear_layer();
    set_has_logicaland();
    layer_.logicaland_ = new ::CoreML::Specification::LogicalAndLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return layer_.logicaland_;
}
::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::release_logicaland() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  if (has_logicaland()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalAndLayerParams* temp = layer_.logicaland_;
    layer_.logicaland_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicaland(::CoreML::Specification::LogicalAndLayerParams* logicaland) {
  clear_layer();
  if (logicaland) {
    set_has_logicaland();
    layer_.logicaland_ = logicaland;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
}

// .CoreML.Specification.ModBroadcastableLayerParams modBroadcastable = 865;
bool NeuralNetworkLayer::has_modbroadcastable() const {
  return layer_case() == kModBroadcastable;
}
void NeuralNetworkLayer::set_has_modbroadcastable() {
  _oneof_case_[0] = kModBroadcastable;
}
void NeuralNetworkLayer::clear_modbroadcastable() {
  if (has_modbroadcastable()) {
    delete layer_.modbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ModBroadcastableLayerParams& NeuralNetworkLayer::modbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  return has_modbroadcastable()
      ? *layer_.modbroadcastable_
      : ::CoreML::Specification::ModBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::mutable_modbroadcastable() {
  if (!has_modbroadcastable()) {
    clear_layer();
    set_has_modbroadcastable();
    layer_.modbroadcastable_ = new ::CoreML::Specification::ModBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  return layer_.modbroadcastable_;
}
::CoreML::Specification::ModBroadcastableLayerParams* NeuralNetworkLayer::release_modbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
  if (has_modbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::ModBroadcastableLayerParams* temp = layer_.modbroadcastable_;
    layer_.modbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_modbroadcastable(::CoreML::Specification::ModBroadcastableLayerParams* modbroadcastable) {
  clear_layer();
  if (modbroadcastable) {
    set_has_modbroadcastable();
    layer_.modbroadcastable_ = modbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.modBroadcastable)
}

// .CoreML.Specification.MinBroadcastableLayerParams minBroadcastable = 870;
bool NeuralNetworkLayer::has_minbroadcastable() const {
  return layer_case() == kMinBroadcastable;
}
void NeuralNetworkLayer::set_has_minbroadcastable() {
  _oneof_case_[0] = kMinBroadcastable;
}
void NeuralNetworkLayer::clear_minbroadcastable() {
  if (has_minbroadcastable()) {
    delete layer_.minbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MinBroadcastableLayerParams& NeuralNetworkLayer::minbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  return has_minbroadcastable()
      ? *layer_.minbroadcastable_
      : ::CoreML::Specification::MinBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::mutable_minbroadcastable() {
  if (!has_minbroadcastable()) {
    clear_layer();
    set_has_minbroadcastable();
    layer_.minbroadcastable_ = new ::CoreML::Specification::MinBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  return layer_.minbroadcastable_;
}
::CoreML::Specification::MinBroadcastableLayerParams* NeuralNetworkLayer::release_minbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
  if (has_minbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MinBroadcastableLayerParams* temp = layer_.minbroadcastable_;
    layer_.minbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_minbroadcastable(::CoreML::Specification::MinBroadcastableLayerParams* minbroadcastable) {
  clear_layer();
  if (minbroadcastable) {
    set_has_minbroadcastable();
    layer_.minbroadcastable_ = minbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.minBroadcastable)
}

// .CoreML.Specification.MaxBroadcastableLayerParams maxBroadcastable = 875;
bool NeuralNetworkLayer::has_maxbroadcastable() const {
  return layer_case() == kMaxBroadcastable;
}
void NeuralNetworkLayer::set_has_maxbroadcastable() {
  _oneof_case_[0] = kMaxBroadcastable;
}
void NeuralNetworkLayer::clear_maxbroadcastable() {
  if (has_maxbroadcastable()) {
    delete layer_.maxbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MaxBroadcastableLayerParams& NeuralNetworkLayer::maxbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  return has_maxbroadcastable()
      ? *layer_.maxbroadcastable_
      : ::CoreML::Specification::MaxBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::mutable_maxbroadcastable() {
  if (!has_maxbroadcastable()) {
    clear_layer();
    set_has_maxbroadcastable();
    layer_.maxbroadcastable_ = new ::CoreML::Specification::MaxBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  return layer_.maxbroadcastable_;
}
::CoreML::Specification::MaxBroadcastableLayerParams* NeuralNetworkLayer::release_maxbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
  if (has_maxbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MaxBroadcastableLayerParams* temp = layer_.maxbroadcastable_;
    layer_.maxbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_maxbroadcastable(::CoreML::Specification::MaxBroadcastableLayerParams* maxbroadcastable) {
  clear_layer();
  if (maxbroadcastable) {
    set_has_maxbroadcastable();
    layer_.maxbroadcastable_ = maxbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.maxBroadcastable)
}

// .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
bool NeuralNetworkLayer::has_addbroadcastable() const {
  return layer_case() == kAddBroadcastable;
}
void NeuralNetworkLayer::set_has_addbroadcastable() {
  _oneof_case_[0] = kAddBroadcastable;
}
void NeuralNetworkLayer::clear_addbroadcastable() {
  if (has_addbroadcastable()) {
    delete layer_.addbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AddBroadcastableLayerParams& NeuralNetworkLayer::addbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return has_addbroadcastable()
      ? *layer_.addbroadcastable_
      : ::CoreML::Specification::AddBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::mutable_addbroadcastable() {
  if (!has_addbroadcastable()) {
    clear_layer();
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = new ::CoreML::Specification::AddBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return layer_.addbroadcastable_;
}
::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::release_addbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  if (has_addbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::AddBroadcastableLayerParams* temp = layer_.addbroadcastable_;
    layer_.addbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_addbroadcastable(::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable) {
  clear_layer();
  if (addbroadcastable) {
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = addbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
}

// .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
bool NeuralNetworkLayer::has_powbroadcastable() const {
  return layer_case() == kPowBroadcastable;
}
void NeuralNetworkLayer::set_has_powbroadcastable() {
  _oneof_case_[0] = kPowBroadcastable;
}
void NeuralNetworkLayer::clear_powbroadcastable() {
  if (has_powbroadcastable()) {
    delete layer_.powbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PowBroadcastableLayerParams& NeuralNetworkLayer::powbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return has_powbroadcastable()
      ? *layer_.powbroadcastable_
      : ::CoreML::Specification::PowBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::mutable_powbroadcastable() {
  if (!has_powbroadcastable()) {
    clear_layer();
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = new ::CoreML::Specification::PowBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return layer_.powbroadcastable_;
}
::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::release_powbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  if (has_powbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::PowBroadcastableLayerParams* temp = layer_.powbroadcastable_;
    layer_.powbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_powbroadcastable(::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable) {
  clear_layer();
  if (powbroadcastable) {
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = powbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
}

// .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
bool NeuralNetworkLayer::has_dividebroadcastable() const {
  return layer_case() == kDivideBroadcastable;
}
void NeuralNetworkLayer::set_has_dividebroadcastable() {
  _oneof_case_[0] = kDivideBroadcastable;
}
void NeuralNetworkLayer::clear_dividebroadcastable() {
  if (has_dividebroadcastable()) {
    delete layer_.dividebroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::DivideBroadcastableLayerParams& NeuralNetworkLayer::dividebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return has_dividebroadcastable()
      ? *layer_.dividebroadcastable_
      : ::CoreML::Specification::DivideBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::mutable_dividebroadcastable() {
  if (!has_dividebroadcastable()) {
    clear_layer();
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = new ::CoreML::Specification::DivideBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return layer_.dividebroadcastable_;
}
::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::release_dividebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  if (has_dividebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::DivideBroadcastableLayerParams* temp = layer_.dividebroadcastable_;
    layer_.dividebroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_dividebroadcastable(::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable) {
  clear_layer();
  if (dividebroadcastable) {
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = dividebroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
}

// .CoreML.Specification.FloorDivBroadcastableLayerParams floorDivBroadcastable = 895;
bool NeuralNetworkLayer::has_floordivbroadcastable() const {
  return layer_case() == kFloorDivBroadcastable;
}
void NeuralNetworkLayer::set_has_floordivbroadcastable() {
  _oneof_case_[0] = kFloorDivBroadcastable;
}
void NeuralNetworkLayer::clear_floordivbroadcastable() {
  if (has_floordivbroadcastable()) {
    delete layer_.floordivbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FloorDivBroadcastableLayerParams& NeuralNetworkLayer::floordivbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  return has_floordivbroadcastable()
      ? *layer_.floordivbroadcastable_
      : ::CoreML::Specification::FloorDivBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::mutable_floordivbroadcastable() {
  if (!has_floordivbroadcastable()) {
    clear_layer();
    set_has_floordivbroadcastable();
    layer_.floordivbroadcastable_ = new ::CoreML::Specification::FloorDivBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  return layer_.floordivbroadcastable_;
}
::CoreML::Specification::FloorDivBroadcastableLayerParams* NeuralNetworkLayer::release_floordivbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
  if (has_floordivbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::FloorDivBroadcastableLayerParams* temp = layer_.floordivbroadcastable_;
    layer_.floordivbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_floordivbroadcastable(::CoreML::Specification::FloorDivBroadcastableLayerParams* floordivbroadcastable) {
  clear_layer();
  if (floordivbroadcastable) {
    set_has_floordivbroadcastable();
    layer_.floordivbroadcastable_ = floordivbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.floorDivBroadcastable)
}

// .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
bool NeuralNetworkLayer::has_multiplybroadcastable() const {
  return layer_case() == kMultiplyBroadcastable;
}
void NeuralNetworkLayer::set_has_multiplybroadcastable() {
  _oneof_case_[0] = kMultiplyBroadcastable;
}
void NeuralNetworkLayer::clear_multiplybroadcastable() {
  if (has_multiplybroadcastable()) {
    delete layer_.multiplybroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MultiplyBroadcastableLayerParams& NeuralNetworkLayer::multiplybroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return has_multiplybroadcastable()
      ? *layer_.multiplybroadcastable_
      : ::CoreML::Specification::MultiplyBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::mutable_multiplybroadcastable() {
  if (!has_multiplybroadcastable()) {
    clear_layer();
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = new ::CoreML::Specification::MultiplyBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return layer_.multiplybroadcastable_;
}
::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::release_multiplybroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  if (has_multiplybroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyBroadcastableLayerParams* temp = layer_.multiplybroadcastable_;
    layer_.multiplybroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_multiplybroadcastable(::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable) {
  clear_layer();
  if (multiplybroadcastable) {
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = multiplybroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
}

// .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
bool NeuralNetworkLayer::has_subtractbroadcastable() const {
  return layer_case() == kSubtractBroadcastable;
}
void NeuralNetworkLayer::set_has_subtractbroadcastable() {
  _oneof_case_[0] = kSubtractBroadcastable;
}
void NeuralNetworkLayer::clear_subtractbroadcastable() {
  if (has_subtractbroadcastable()) {
    delete layer_.subtractbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SubtractBroadcastableLayerParams& NeuralNetworkLayer::subtractbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return has_subtractbroadcastable()
      ? *layer_.subtractbroadcastable_
      : ::CoreML::Specification::SubtractBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::mutable_subtractbroadcastable() {
  if (!has_subtractbroadcastable()) {
    clear_layer();
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = new ::CoreML::Specification::SubtractBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return layer_.subtractbroadcastable_;
}
::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::release_subtractbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  if (has_subtractbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::SubtractBroadcastableLayerParams* temp = layer_.subtractbroadcastable_;
    layer_.subtractbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_subtractbroadcastable(::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable) {
  clear_layer();
  if (subtractbroadcastable) {
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = subtractbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
}

// .CoreML.Specification.TileLayerParams tile = 920;
bool NeuralNetworkLayer::has_tile() const {
  return layer_case() == kTile;
}
void NeuralNetworkLayer::set_has_tile() {
  _oneof_case_[0] = kTile;
}
void NeuralNetworkLayer::clear_tile() {
  if (has_tile()) {
    delete layer_.tile_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TileLayerParams& NeuralNetworkLayer::tile() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tile)
  return has_tile()
      ? *layer_.tile_
      : ::CoreML::Specification::TileLayerParams::default_instance();
}
::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::mutable_tile() {
  if (!has_tile()) {
    clear_layer();
    set_has_tile();
    layer_.tile_ = new ::CoreML::Specification::TileLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tile)
  return layer_.tile_;
}
::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::release_tile() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tile)
  if (has_tile()) {
    clear_has_layer();
    ::CoreML::Specification::TileLayerParams* temp = layer_.tile_;
    layer_.tile_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_tile(::CoreML::Specification::TileLayerParams* tile) {
  clear_layer();
  if (tile) {
    set_has_tile();
    layer_.tile_ = tile;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tile)
}

// .CoreML.Specification.StackLayerParams stack = 925;
bool NeuralNetworkLayer::has_stack() const {
  return layer_case() == kStack;
}
void NeuralNetworkLayer::set_has_stack() {
  _oneof_case_[0] = kStack;
}
void NeuralNetworkLayer::clear_stack() {
  if (has_stack()) {
    delete layer_.stack_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::StackLayerParams& NeuralNetworkLayer::stack() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.stack)
  return has_stack()
      ? *layer_.stack_
      : ::CoreML::Specification::StackLayerParams::default_instance();
}
::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::mutable_stack() {
  if (!has_stack()) {
    clear_layer();
    set_has_stack();
    layer_.stack_ = new ::CoreML::Specification::StackLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.stack)
  return layer_.stack_;
}
::CoreML::Specification::StackLayerParams* NeuralNetworkLayer::release_stack() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.stack)
  if (has_stack()) {
    clear_has_layer();
    ::CoreML::Specification::StackLayerParams* temp = layer_.stack_;
    layer_.stack_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_stack(::CoreML::Specification::StackLayerParams* stack) {
  clear_layer();
  if (stack) {
    set_has_stack();
    layer_.stack_ = stack;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.stack)
}

// .CoreML.Specification.GatherLayerParams gather = 930;
bool NeuralNetworkLayer::has_gather() const {
  return layer_case() == kGather;
}
void NeuralNetworkLayer::set_has_gather() {
  _oneof_case_[0] = kGather;
}
void NeuralNetworkLayer::clear_gather() {
  if (has_gather()) {
    delete layer_.gather_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GatherLayerParams& NeuralNetworkLayer::gather() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gather)
  return has_gather()
      ? *layer_.gather_
      : ::CoreML::Specification::GatherLayerParams::default_instance();
}
::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::mutable_gather() {
  if (!has_gather()) {
    clear_layer();
    set_has_gather();
    layer_.gather_ = new ::CoreML::Specification::GatherLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gather)
  return layer_.gather_;
}
::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::release_gather() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gather)
  if (has_gather()) {
    clear_has_layer();
    ::CoreML::Specification::GatherLayerParams* temp = layer_.gather_;
    layer_.gather_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gather(::CoreML::Specification::GatherLayerParams* gather) {
  clear_layer();
  if (gather) {
    set_has_gather();
    layer_.gather_ = gather;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gather)
}

// .CoreML.Specification.ScatterLayerParams scatter = 935;
bool NeuralNetworkLayer::has_scatter() const {
  return layer_case() == kScatter;
}
void NeuralNetworkLayer::set_has_scatter() {
  _oneof_case_[0] = kScatter;
}
void NeuralNetworkLayer::clear_scatter() {
  if (has_scatter()) {
    delete layer_.scatter_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScatterLayerParams& NeuralNetworkLayer::scatter() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatter)
  return has_scatter()
      ? *layer_.scatter_
      : ::CoreML::Specification::ScatterLayerParams::default_instance();
}
::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::mutable_scatter() {
  if (!has_scatter()) {
    clear_layer();
    set_has_scatter();
    layer_.scatter_ = new ::CoreML::Specification::ScatterLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatter)
  return layer_.scatter_;
}
::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::release_scatter() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatter)
  if (has_scatter()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterLayerParams* temp = layer_.scatter_;
    layer_.scatter_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scatter(::CoreML::Specification::ScatterLayerParams* scatter) {
  clear_layer();
  if (scatter) {
    set_has_scatter();
    layer_.scatter_ = scatter;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatter)
}

// .CoreML.Specification.GatherNDLayerParams gatherND = 940;
bool NeuralNetworkLayer::has_gathernd() const {
  return layer_case() == kGatherND;
}
void NeuralNetworkLayer::set_has_gathernd() {
  _oneof_case_[0] = kGatherND;
}
void NeuralNetworkLayer::clear_gathernd() {
  if (has_gathernd()) {
    delete layer_.gathernd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GatherNDLayerParams& NeuralNetworkLayer::gathernd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gatherND)
  return has_gathernd()
      ? *layer_.gathernd_
      : ::CoreML::Specification::GatherNDLayerParams::default_instance();
}
::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::mutable_gathernd() {
  if (!has_gathernd()) {
    clear_layer();
    set_has_gathernd();
    layer_.gathernd_ = new ::CoreML::Specification::GatherNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gatherND)
  return layer_.gathernd_;
}
::CoreML::Specification::GatherNDLayerParams* NeuralNetworkLayer::release_gathernd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gatherND)
  if (has_gathernd()) {
    clear_has_layer();
    ::CoreML::Specification::GatherNDLayerParams* temp = layer_.gathernd_;
    layer_.gathernd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gathernd(::CoreML::Specification::GatherNDLayerParams* gathernd) {
  clear_layer();
  if (gathernd) {
    set_has_gathernd();
    layer_.gathernd_ = gathernd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gatherND)
}

// .CoreML.Specification.ScatterNDLayerParams scatterND = 945;
bool NeuralNetworkLayer::has_scatternd() const {
  return layer_case() == kScatterND;
}
void NeuralNetworkLayer::set_has_scatternd() {
  _oneof_case_[0] = kScatterND;
}
void NeuralNetworkLayer::clear_scatternd() {
  if (has_scatternd()) {
    delete layer_.scatternd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScatterNDLayerParams& NeuralNetworkLayer::scatternd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatterND)
  return has_scatternd()
      ? *layer_.scatternd_
      : ::CoreML::Specification::ScatterNDLayerParams::default_instance();
}
::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::mutable_scatternd() {
  if (!has_scatternd()) {
    clear_layer();
    set_has_scatternd();
    layer_.scatternd_ = new ::CoreML::Specification::ScatterNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatterND)
  return layer_.scatternd_;
}
::CoreML::Specification::ScatterNDLayerParams* NeuralNetworkLayer::release_scatternd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatterND)
  if (has_scatternd()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterNDLayerParams* temp = layer_.scatternd_;
    layer_.scatternd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scatternd(::CoreML::Specification::ScatterNDLayerParams* scatternd) {
  clear_layer();
  if (scatternd) {
    set_has_scatternd();
    layer_.scatternd_ = scatternd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatterND)
}

// .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
bool NeuralNetworkLayer::has_softmaxnd() const {
  return layer_case() == kSoftmaxND;
}
void NeuralNetworkLayer::set_has_softmaxnd() {
  _oneof_case_[0] = kSoftmaxND;
}
void NeuralNetworkLayer::clear_softmaxnd() {
  if (has_softmaxnd()) {
    delete layer_.softmaxnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SoftmaxNDLayerParams& NeuralNetworkLayer::softmaxnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return has_softmaxnd()
      ? *layer_.softmaxnd_
      : ::CoreML::Specification::SoftmaxNDLayerParams::default_instance();
}
::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::mutable_softmaxnd() {
  if (!has_softmaxnd()) {
    clear_layer();
    set_has_softmaxnd();
    layer_.softmaxnd_ = new ::CoreML::Specification::SoftmaxNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return layer_.softmaxnd_;
}
::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::release_softmaxnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  if (has_softmaxnd()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxNDLayerParams* temp = layer_.softmaxnd_;
    layer_.softmaxnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_softmaxnd(::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd) {
  clear_layer();
  if (softmaxnd) {
    set_has_softmaxnd();
    layer_.softmaxnd_ = softmaxnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmaxND)
}

// .CoreML.Specification.GatherAlongAxisLayerParams gatherAlongAxis = 952;
bool NeuralNetworkLayer::has_gatheralongaxis() const {
  return layer_case() == kGatherAlongAxis;
}
void NeuralNetworkLayer::set_has_gatheralongaxis() {
  _oneof_case_[0] = kGatherAlongAxis;
}
void NeuralNetworkLayer::clear_gatheralongaxis() {
  if (has_gatheralongaxis()) {
    delete layer_.gatheralongaxis_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GatherAlongAxisLayerParams& NeuralNetworkLayer::gatheralongaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  return has_gatheralongaxis()
      ? *layer_.gatheralongaxis_
      : ::CoreML::Specification::GatherAlongAxisLayerParams::default_instance();
}
::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::mutable_gatheralongaxis() {
  if (!has_gatheralongaxis()) {
    clear_layer();
    set_has_gatheralongaxis();
    layer_.gatheralongaxis_ = new ::CoreML::Specification::GatherAlongAxisLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  return layer_.gatheralongaxis_;
}
::CoreML::Specification::GatherAlongAxisLayerParams* NeuralNetworkLayer::release_gatheralongaxis() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
  if (has_gatheralongaxis()) {
    clear_has_layer();
    ::CoreML::Specification::GatherAlongAxisLayerParams* temp = layer_.gatheralongaxis_;
    layer_.gatheralongaxis_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gatheralongaxis(::CoreML::Specification::GatherAlongAxisLayerParams* gatheralongaxis) {
  clear_layer();
  if (gatheralongaxis) {
    set_has_gatheralongaxis();
    layer_.gatheralongaxis_ = gatheralongaxis;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gatherAlongAxis)
}

// .CoreML.Specification.ScatterAlongAxisLayerParams scatterAlongAxis = 954;
bool NeuralNetworkLayer::has_scatteralongaxis() const {
  return layer_case() == kScatterAlongAxis;
}
void NeuralNetworkLayer::set_has_scatteralongaxis() {
  _oneof_case_[0] = kScatterAlongAxis;
}
void NeuralNetworkLayer::clear_scatteralongaxis() {
  if (has_scatteralongaxis()) {
    delete layer_.scatteralongaxis_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScatterAlongAxisLayerParams& NeuralNetworkLayer::scatteralongaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  return has_scatteralongaxis()
      ? *layer_.scatteralongaxis_
      : ::CoreML::Specification::ScatterAlongAxisLayerParams::default_instance();
}
::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::mutable_scatteralongaxis() {
  if (!has_scatteralongaxis()) {
    clear_layer();
    set_has_scatteralongaxis();
    layer_.scatteralongaxis_ = new ::CoreML::Specification::ScatterAlongAxisLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  return layer_.scatteralongaxis_;
}
::CoreML::Specification::ScatterAlongAxisLayerParams* NeuralNetworkLayer::release_scatteralongaxis() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
  if (has_scatteralongaxis()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterAlongAxisLayerParams* temp = layer_.scatteralongaxis_;
    layer_.scatteralongaxis_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scatteralongaxis(::CoreML::Specification::ScatterAlongAxisLayerParams* scatteralongaxis) {
  clear_layer();
  if (scatteralongaxis) {
    set_has_scatteralongaxis();
    layer_.scatteralongaxis_ = scatteralongaxis;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatterAlongAxis)
}

// .CoreML.Specification.ReverseLayerParams reverse = 960;
bool NeuralNetworkLayer::has_reverse() const {
  return layer_case() == kReverse;
}
void NeuralNetworkLayer::set_has_reverse() {
  _oneof_case_[0] = kReverse;
}
void NeuralNetworkLayer::clear_reverse() {
  if (has_reverse()) {
    delete layer_.reverse_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReverseLayerParams& NeuralNetworkLayer::reverse() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reverse)
  return has_reverse()
      ? *layer_.reverse_
      : ::CoreML::Specification::ReverseLayerParams::default_instance();
}
::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::mutable_reverse() {
  if (!has_reverse()) {
    clear_layer();
    set_has_reverse();
    layer_.reverse_ = new ::CoreML::Specification::ReverseLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reverse)
  return layer_.reverse_;
}
::CoreML::Specification::ReverseLayerParams* NeuralNetworkLayer::release_reverse() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reverse)
  if (has_reverse()) {
    clear_has_layer();
    ::CoreML::Specification::ReverseLayerParams* temp = layer_.reverse_;
    layer_.reverse_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reverse(::CoreML::Specification::ReverseLayerParams* reverse) {
  clear_layer();
  if (reverse) {
    set_has_reverse();
    layer_.reverse_ = reverse;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reverse)
}

// .CoreML.Specification.ReverseSeqLayerParams reverseSeq = 965;
bool NeuralNetworkLayer::has_reverseseq() const {
  return layer_case() == kReverseSeq;
}
void NeuralNetworkLayer::set_has_reverseseq() {
  _oneof_case_[0] = kReverseSeq;
}
void NeuralNetworkLayer::clear_reverseseq() {
  if (has_reverseseq()) {
    delete layer_.reverseseq_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReverseSeqLayerParams& NeuralNetworkLayer::reverseseq() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  return has_reverseseq()
      ? *layer_.reverseseq_
      : ::CoreML::Specification::ReverseSeqLayerParams::default_instance();
}
::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::mutable_reverseseq() {
  if (!has_reverseseq()) {
    clear_layer();
    set_has_reverseseq();
    layer_.reverseseq_ = new ::CoreML::Specification::ReverseSeqLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  return layer_.reverseseq_;
}
::CoreML::Specification::ReverseSeqLayerParams* NeuralNetworkLayer::release_reverseseq() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
  if (has_reverseseq()) {
    clear_has_layer();
    ::CoreML::Specification::ReverseSeqLayerParams* temp = layer_.reverseseq_;
    layer_.reverseseq_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reverseseq(::CoreML::Specification::ReverseSeqLayerParams* reverseseq) {
  clear_layer();
  if (reverseseq) {
    set_has_reverseseq();
    layer_.reverseseq_ = reverseseq;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reverseSeq)
}

// .CoreML.Specification.SplitNDLayerParams splitND = 975;
bool NeuralNetworkLayer::has_splitnd() const {
  return layer_case() == kSplitND;
}
void NeuralNetworkLayer::set_has_splitnd() {
  _oneof_case_[0] = kSplitND;
}
void NeuralNetworkLayer::clear_splitnd() {
  if (has_splitnd()) {
    delete layer_.splitnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SplitNDLayerParams& NeuralNetworkLayer::splitnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.splitND)
  return has_splitnd()
      ? *layer_.splitnd_
      : ::CoreML::Specification::SplitNDLayerParams::default_instance();
}
::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::mutable_splitnd() {
  if (!has_splitnd()) {
    clear_layer();
    set_has_splitnd();
    layer_.splitnd_ = new ::CoreML::Specification::SplitNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.splitND)
  return layer_.splitnd_;
}
::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::release_splitnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.splitND)
  if (has_splitnd()) {
    clear_has_layer();
    ::CoreML::Specification::SplitNDLayerParams* temp = layer_.splitnd_;
    layer_.splitnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_splitnd(::CoreML::Specification::SplitNDLayerParams* splitnd) {
  clear_layer();
  if (splitnd) {
    set_has_splitnd();
    layer_.splitnd_ = splitnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.splitND)
}

// .CoreML.Specification.ConcatNDLayerParams concatND = 980;
bool NeuralNetworkLayer::has_concatnd() const {
  return layer_case() == kConcatND;
}
void NeuralNetworkLayer::set_has_concatnd() {
  _oneof_case_[0] = kConcatND;
}
void NeuralNetworkLayer::clear_concatnd() {
  if (has_concatnd()) {
    delete layer_.concatnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConcatNDLayerParams& NeuralNetworkLayer::concatnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concatND)
  return has_concatnd()
      ? *layer_.concatnd_
      : ::CoreML::Specification::ConcatNDLayerParams::default_instance();
}
::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::mutable_concatnd() {
  if (!has_concatnd()) {
    clear_layer();
    set_has_concatnd();
    layer_.concatnd_ = new ::CoreML::Specification::ConcatNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concatND)
  return layer_.concatnd_;
}
::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::release_concatnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concatND)
  if (has_concatnd()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatNDLayerParams* temp = layer_.concatnd_;
    layer_.concatnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_concatnd(::CoreML::Specification::ConcatNDLayerParams* concatnd) {
  clear_layer();
  if (concatnd) {
    set_has_concatnd();
    layer_.concatnd_ = concatnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concatND)
}

// .CoreML.Specification.TransposeLayerParams transpose = 985;
bool NeuralNetworkLayer::has_transpose() const {
  return layer_case() == kTranspose;
}
void NeuralNetworkLayer::set_has_transpose() {
  _oneof_case_[0] = kTranspose;
}
void NeuralNetworkLayer::clear_transpose() {
  if (has_transpose()) {
    delete layer_.transpose_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TransposeLayerParams& NeuralNetworkLayer::transpose() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.transpose)
  return has_transpose()
      ? *layer_.transpose_
      : ::CoreML::Specification::TransposeLayerParams::default_instance();
}
::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::mutable_transpose() {
  if (!has_transpose()) {
    clear_layer();
    set_has_transpose();
    layer_.transpose_ = new ::CoreML::Specification::TransposeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.transpose)
  return layer_.transpose_;
}
::CoreML::Specification::TransposeLayerParams* NeuralNetworkLayer::release_transpose() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.transpose)
  if (has_transpose()) {
    clear_has_layer();
    ::CoreML::Specification::TransposeLayerParams* temp = layer_.transpose_;
    layer_.transpose_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_transpose(::CoreML::Specification::TransposeLayerParams* transpose) {
  clear_layer();
  if (transpose) {
    set_has_transpose();
    layer_.transpose_ = transpose;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.transpose)
}

// .CoreML.Specification.SliceStaticLayerParams sliceStatic = 995;
bool NeuralNetworkLayer::has_slicestatic() const {
  return layer_case() == kSliceStatic;
}
void NeuralNetworkLayer::set_has_slicestatic() {
  _oneof_case_[0] = kSliceStatic;
}
void NeuralNetworkLayer::clear_slicestatic() {
  if (has_slicestatic()) {
    delete layer_.slicestatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SliceStaticLayerParams& NeuralNetworkLayer::slicestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  return has_slicestatic()
      ? *layer_.slicestatic_
      : ::CoreML::Specification::SliceStaticLayerParams::default_instance();
}
::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::mutable_slicestatic() {
  if (!has_slicestatic()) {
    clear_layer();
    set_has_slicestatic();
    layer_.slicestatic_ = new ::CoreML::Specification::SliceStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  return layer_.slicestatic_;
}
::CoreML::Specification::SliceStaticLayerParams* NeuralNetworkLayer::release_slicestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
  if (has_slicestatic()) {
    clear_has_layer();
    ::CoreML::Specification::SliceStaticLayerParams* temp = layer_.slicestatic_;
    layer_.slicestatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slicestatic(::CoreML::Specification::SliceStaticLayerParams* slicestatic) {
  clear_layer();
  if (slicestatic) {
    set_has_slicestatic();
    layer_.slicestatic_ = slicestatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceStatic)
}

// .CoreML.Specification.SliceDynamicLayerParams sliceDynamic = 1000;
bool NeuralNetworkLayer::has_slicedynamic() const {
  return layer_case() == kSliceDynamic;
}
void NeuralNetworkLayer::set_has_slicedynamic() {
  _oneof_case_[0] = kSliceDynamic;
}
void NeuralNetworkLayer::clear_slicedynamic() {
  if (has_slicedynamic()) {
    delete layer_.slicedynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SliceDynamicLayerParams& NeuralNetworkLayer::slicedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  return has_slicedynamic()
      ? *layer_.slicedynamic_
      : ::CoreML::Specification::SliceDynamicLayerParams::default_instance();
}
::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::mutable_slicedynamic() {
  if (!has_slicedynamic()) {
    clear_layer();
    set_has_slicedynamic();
    layer_.slicedynamic_ = new ::CoreML::Specification::SliceDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  return layer_.slicedynamic_;
}
::CoreML::Specification::SliceDynamicLayerParams* NeuralNetworkLayer::release_slicedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
  if (has_slicedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::SliceDynamicLayerParams* temp = layer_.slicedynamic_;
    layer_.slicedynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slicedynamic(::CoreML::Specification::SliceDynamicLayerParams* slicedynamic) {
  clear_layer();
  if (slicedynamic) {
    set_has_slicedynamic();
    layer_.slicedynamic_ = slicedynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceDynamic)
}

// .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
bool NeuralNetworkLayer::has_slidingwindows() const {
  return layer_case() == kSlidingWindows;
}
void NeuralNetworkLayer::set_has_slidingwindows() {
  _oneof_case_[0] = kSlidingWindows;
}
void NeuralNetworkLayer::clear_slidingwindows() {
  if (has_slidingwindows()) {
    delete layer_.slidingwindows_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SlidingWindowsLayerParams& NeuralNetworkLayer::slidingwindows() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return has_slidingwindows()
      ? *layer_.slidingwindows_
      : ::CoreML::Specification::SlidingWindowsLayerParams::default_instance();
}
::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::mutable_slidingwindows() {
  if (!has_slidingwindows()) {
    clear_layer();
    set_has_slidingwindows();
    layer_.slidingwindows_ = new ::CoreML::Specification::SlidingWindowsLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return layer_.slidingwindows_;
}
::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::release_slidingwindows() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  if (has_slidingwindows()) {
    clear_has_layer();
    ::CoreML::Specification::SlidingWindowsLayerParams* temp = layer_.slidingwindows_;
    layer_.slidingwindows_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slidingwindows(::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows) {
  clear_layer();
  if (slidingwindows) {
    set_has_slidingwindows();
    layer_.slidingwindows_ = slidingwindows;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
}

// .CoreML.Specification.TopKLayerParams topK = 1015;
bool NeuralNetworkLayer::has_topk() const {
  return layer_case() == kTopK;
}
void NeuralNetworkLayer::set_has_topk() {
  _oneof_case_[0] = kTopK;
}
void NeuralNetworkLayer::clear_topk() {
  if (has_topk()) {
    delete layer_.topk_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TopKLayerParams& NeuralNetworkLayer::topk() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.topK)
  return has_topk()
      ? *layer_.topk_
      : ::CoreML::Specification::TopKLayerParams::default_instance();
}
::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::mutable_topk() {
  if (!has_topk()) {
    clear_layer();
    set_has_topk();
    layer_.topk_ = new ::CoreML::Specification::TopKLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.topK)
  return layer_.topk_;
}
::CoreML::Specification::TopKLayerParams* NeuralNetworkLayer::release_topk() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.topK)
  if (has_topk()) {
    clear_has_layer();
    ::CoreML::Specification::TopKLayerParams* temp = layer_.topk_;
    layer_.topk_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_topk(::CoreML::Specification::TopKLayerParams* topk) {
  clear_layer();
  if (topk) {
    set_has_topk();
    layer_.topk_ = topk;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.topK)
}

// .CoreML.Specification.ArgMinLayerParams argMin = 1020;
bool NeuralNetworkLayer::has_argmin() const {
  return layer_case() == kArgMin;
}
void NeuralNetworkLayer::set_has_argmin() {
  _oneof_case_[0] = kArgMin;
}
void NeuralNetworkLayer::clear_argmin() {
  if (has_argmin()) {
    delete layer_.argmin_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ArgMinLayerParams& NeuralNetworkLayer::argmin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argMin)
  return has_argmin()
      ? *layer_.argmin_
      : ::CoreML::Specification::ArgMinLayerParams::default_instance();
}
::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::mutable_argmin() {
  if (!has_argmin()) {
    clear_layer();
    set_has_argmin();
    layer_.argmin_ = new ::CoreML::Specification::ArgMinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argMin)
  return layer_.argmin_;
}
::CoreML::Specification::ArgMinLayerParams* NeuralNetworkLayer::release_argmin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argMin)
  if (has_argmin()) {
    clear_has_layer();
    ::CoreML::Specification::ArgMinLayerParams* temp = layer_.argmin_;
    layer_.argmin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_argmin(::CoreML::Specification::ArgMinLayerParams* argmin) {
  clear_layer();
  if (argmin) {
    set_has_argmin();
    layer_.argmin_ = argmin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.argMin)
}

// .CoreML.Specification.ArgMaxLayerParams argMax = 1025;
bool NeuralNetworkLayer::has_argmax() const {
  return layer_case() == kArgMax;
}
void NeuralNetworkLayer::set_has_argmax() {
  _oneof_case_[0] = kArgMax;
}
void NeuralNetworkLayer::clear_argmax() {
  if (has_argmax()) {
    delete layer_.argmax_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ArgMaxLayerParams& NeuralNetworkLayer::argmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.argMax)
  return has_argmax()
      ? *layer_.argmax_
      : ::CoreML::Specification::ArgMaxLayerParams::default_instance();
}
::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::mutable_argmax() {
  if (!has_argmax()) {
    clear_layer();
    set_has_argmax();
    layer_.argmax_ = new ::CoreML::Specification::ArgMaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.argMax)
  return layer_.argmax_;
}
::CoreML::Specification::ArgMaxLayerParams* NeuralNetworkLayer::release_argmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.argMax)
  if (has_argmax()) {
    clear_has_layer();
    ::CoreML::Specification::ArgMaxLayerParams* temp = layer_.argmax_;
    layer_.argmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_argmax(::CoreML::Specification::ArgMaxLayerParams* argmax) {
  clear_layer();
  if (argmax) {
    set_has_argmax();
    layer_.argmax_ = argmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.argMax)
}

// .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
bool NeuralNetworkLayer::has_embeddingnd() const {
  return layer_case() == kEmbeddingND;
}
void NeuralNetworkLayer::set_has_embeddingnd() {
  _oneof_case_[0] = kEmbeddingND;
}
void NeuralNetworkLayer::clear_embeddingnd() {
  if (has_embeddingnd()) {
    delete layer_.embeddingnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EmbeddingNDLayerParams& NeuralNetworkLayer::embeddingnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return has_embeddingnd()
      ? *layer_.embeddingnd_
      : ::CoreML::Specification::EmbeddingNDLayerParams::default_instance();
}
::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::mutable_embeddingnd() {
  if (!has_embeddingnd()) {
    clear_layer();
    set_has_embeddingnd();
    layer_.embeddingnd_ = new ::CoreML::Specification::EmbeddingNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return layer_.embeddingnd_;
}
::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::release_embeddingnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  if (has_embeddingnd()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingNDLayerParams* temp = layer_.embeddingnd_;
    layer_.embeddingnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_embeddingnd(::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd) {
  clear_layer();
  if (embeddingnd) {
    set_has_embeddingnd();
    layer_.embeddingnd_ = embeddingnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embeddingND)
}

// .CoreML.Specification.BatchedMatMulLayerParams batchedMatmul = 1045;
bool NeuralNetworkLayer::has_batchedmatmul() const {
  return layer_case() == kBatchedMatmul;
}
void NeuralNetworkLayer::set_has_batchedmatmul() {
  _oneof_case_[0] = kBatchedMatmul;
}
void NeuralNetworkLayer::clear_batchedmatmul() {
  if (has_batchedmatmul()) {
    delete layer_.batchedmatmul_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BatchedMatMulLayerParams& NeuralNetworkLayer::batchedmatmul() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return has_batchedmatmul()
      ? *layer_.batchedmatmul_
      : ::CoreML::Specification::BatchedMatMulLayerParams::default_instance();
}
::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::mutable_batchedmatmul() {
  if (!has_batchedmatmul()) {
    clear_layer();
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = new ::CoreML::Specification::BatchedMatMulLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return layer_.batchedmatmul_;
}
::CoreML::Specification::BatchedMatMulLayerParams* NeuralNetworkLayer::release_batchedmatmul() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  if (has_batchedmatmul()) {
    clear_has_layer();
    ::CoreML::Specification::BatchedMatMulLayerParams* temp = layer_.batchedmatmul_;
    layer_.batchedmatmul_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_batchedmatmul(::CoreML::Specification::BatchedMatMulLayerParams* batchedmatmul) {
  clear_layer();
  if (batchedmatmul) {
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = batchedmatmul;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
}

// .CoreML.Specification.GetShapeLayerParams getShape = 1065;
bool NeuralNetworkLayer::has_getshape() const {
  return layer_case() == kGetShape;
}
void NeuralNetworkLayer::set_has_getshape() {
  _oneof_case_[0] = kGetShape;
}
void NeuralNetworkLayer::clear_getshape() {
  if (has_getshape()) {
    delete layer_.getshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GetShapeLayerParams& NeuralNetworkLayer::getshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.getShape)
  return has_getshape()
      ? *layer_.getshape_
      : ::CoreML::Specification::GetShapeLayerParams::default_instance();
}
::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::mutable_getshape() {
  if (!has_getshape()) {
    clear_layer();
    set_has_getshape();
    layer_.getshape_ = new ::CoreML::Specification::GetShapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.getShape)
  return layer_.getshape_;
}
::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::release_getshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.getShape)
  if (has_getshape()) {
    clear_has_layer();
    ::CoreML::Specification::GetShapeLayerParams* temp = layer_.getshape_;
    layer_.getshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_getshape(::CoreML::Specification::GetShapeLayerParams* getshape) {
  clear_layer();
  if (getshape) {
    set_has_getshape();
    layer_.getshape_ = getshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.getShape)
}

// .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
bool NeuralNetworkLayer::has_loadconstantnd() const {
  return layer_case() == kLoadConstantND;
}
void NeuralNetworkLayer::set_has_loadconstantnd() {
  _oneof_case_[0] = kLoadConstantND;
}
void NeuralNetworkLayer::clear_loadconstantnd() {
  if (has_loadconstantnd()) {
    delete layer_.loadconstantnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoadConstantNDLayerParams& NeuralNetworkLayer::loadconstantnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return has_loadconstantnd()
      ? *layer_.loadconstantnd_
      : ::CoreML::Specification::LoadConstantNDLayerParams::default_instance();
}
::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::mutable_loadconstantnd() {
  if (!has_loadconstantnd()) {
    clear_layer();
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = new ::CoreML::Specification::LoadConstantNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return layer_.loadconstantnd_;
}
::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::release_loadconstantnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  if (has_loadconstantnd()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantNDLayerParams* temp = layer_.loadconstantnd_;
    layer_.loadconstantnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loadconstantnd(::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd) {
  clear_layer();
  if (loadconstantnd) {
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = loadconstantnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
}

// .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
bool NeuralNetworkLayer::has_filllike() const {
  return layer_case() == kFillLike;
}
void NeuralNetworkLayer::set_has_filllike() {
  _oneof_case_[0] = kFillLike;
}
void NeuralNetworkLayer::clear_filllike() {
  if (has_filllike()) {
    delete layer_.filllike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FillLikeLayerParams& NeuralNetworkLayer::filllike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return has_filllike()
      ? *layer_.filllike_
      : ::CoreML::Specification::FillLikeLayerParams::default_instance();
}
::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::mutable_filllike() {
  if (!has_filllike()) {
    clear_layer();
    set_has_filllike();
    layer_.filllike_ = new ::CoreML::Specification::FillLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return layer_.filllike_;
}
::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::release_filllike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillLike)
  if (has_filllike()) {
    clear_has_layer();
    ::CoreML::Specification::FillLikeLayerParams* temp = layer_.filllike_;
    layer_.filllike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_filllike(::CoreML::Specification::FillLikeLayerParams* filllike) {
  clear_layer();
  if (filllike) {
    set_has_filllike();
    layer_.filllike_ = filllike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillLike)
}

// .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
bool NeuralNetworkLayer::has_fillstatic() const {
  return layer_case() == kFillStatic;
}
void NeuralNetworkLayer::set_has_fillstatic() {
  _oneof_case_[0] = kFillStatic;
}
void NeuralNetworkLayer::clear_fillstatic() {
  if (has_fillstatic()) {
    delete layer_.fillstatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FillStaticLayerParams& NeuralNetworkLayer::fillstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return has_fillstatic()
      ? *layer_.fillstatic_
      : ::CoreML::Specification::FillStaticLayerParams::default_instance();
}
::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::mutable_fillstatic() {
  if (!has_fillstatic()) {
    clear_layer();
    set_has_fillstatic();
    layer_.fillstatic_ = new ::CoreML::Specification::FillStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return layer_.fillstatic_;
}
::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::release_fillstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  if (has_fillstatic()) {
    clear_has_layer();
    ::CoreML::Specification::FillStaticLayerParams* temp = layer_.fillstatic_;
    layer_.fillstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_fillstatic(::CoreML::Specification::FillStaticLayerParams* fillstatic) {
  clear_layer();
  if (fillstatic) {
    set_has_fillstatic();
    layer_.fillstatic_ = fillstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillStatic)
}

// .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
bool NeuralNetworkLayer::has_filldynamic() const {
  return layer_case() == kFillDynamic;
}
void NeuralNetworkLayer::set_has_filldynamic() {
  _oneof_case_[0] = kFillDynamic;
}
void NeuralNetworkLayer::clear_filldynamic() {
  if (has_filldynamic()) {
    delete layer_.filldynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FillDynamicLayerParams& NeuralNetworkLayer::filldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return has_filldynamic()
      ? *layer_.filldynamic_
      : ::CoreML::Specification::FillDynamicLayerParams::default_instance();
}
::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::mutable_filldynamic() {
  if (!has_filldynamic()) {
    clear_layer();
    set_has_filldynamic();
    layer_.filldynamic_ = new ::CoreML::Specification::FillDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return layer_.filldynamic_;
}
::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::release_filldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  if (has_filldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::FillDynamicLayerParams* temp = layer_.filldynamic_;
    layer_.filldynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_filldynamic(::CoreML::Specification::FillDynamicLayerParams* filldynamic) {
  clear_layer();
  if (filldynamic) {
    set_has_filldynamic();
    layer_.filldynamic_ = filldynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
}

// .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
bool NeuralNetworkLayer::has_broadcasttolike() const {
  return layer_case() == kBroadcastToLike;
}
void NeuralNetworkLayer::set_has_broadcasttolike() {
  _oneof_case_[0] = kBroadcastToLike;
}
void NeuralNetworkLayer::clear_broadcasttolike() {
  if (has_broadcasttolike()) {
    delete layer_.broadcasttolike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BroadcastToLikeLayerParams& NeuralNetworkLayer::broadcasttolike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return has_broadcasttolike()
      ? *layer_.broadcasttolike_
      : ::CoreML::Specification::BroadcastToLikeLayerParams::default_instance();
}
::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::mutable_broadcasttolike() {
  if (!has_broadcasttolike()) {
    clear_layer();
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = new ::CoreML::Specification::BroadcastToLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return layer_.broadcasttolike_;
}
::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::release_broadcasttolike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  if (has_broadcasttolike()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToLikeLayerParams* temp = layer_.broadcasttolike_;
    layer_.broadcasttolike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_broadcasttolike(::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike) {
  clear_layer();
  if (broadcasttolike) {
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = broadcasttolike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
}

// .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
bool NeuralNetworkLayer::has_broadcasttostatic() const {
  return layer_case() == kBroadcastToStatic;
}
void NeuralNetworkLayer::set_has_broadcasttostatic() {
  _oneof_case_[0] = kBroadcastToStatic;
}
void NeuralNetworkLayer::clear_broadcasttostatic() {
  if (has_broadcasttostatic()) {
    delete layer_.broadcasttostatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BroadcastToStaticLayerParams& NeuralNetworkLayer::broadcasttostatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return has_broadcasttostatic()
      ? *layer_.broadcasttostatic_
      : ::CoreML::Specification::BroadcastToStaticLayerParams::default_instance();
}
::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::mutable_broadcasttostatic() {
  if (!has_broadcasttostatic()) {
    clear_layer();
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = new ::CoreML::Specification::BroadcastToStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return layer_.broadcasttostatic_;
}
::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::release_broadcasttostatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  if (has_broadcasttostatic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToStaticLayerParams* temp = layer_.broadcasttostatic_;
    layer_.broadcasttostatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_broadcasttostatic(::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic) {
  clear_layer();
  if (broadcasttostatic) {
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = broadcasttostatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
}

// .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
bool NeuralNetworkLayer::has_broadcasttodynamic() const {
  return layer_case() == kBroadcastToDynamic;
}
void NeuralNetworkLayer::set_has_broadcasttodynamic() {
  _oneof_case_[0] = kBroadcastToDynamic;
}
void NeuralNetworkLayer::clear_broadcasttodynamic() {
  if (has_broadcasttodynamic()) {
    delete layer_.broadcasttodynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BroadcastToDynamicLayerParams& NeuralNetworkLayer::broadcasttodynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return has_broadcasttodynamic()
      ? *layer_.broadcasttodynamic_
      : ::CoreML::Specification::BroadcastToDynamicLayerParams::default_instance();
}
::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::mutable_broadcasttodynamic() {
  if (!has_broadcasttodynamic()) {
    clear_layer();
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = new ::CoreML::Specification::BroadcastToDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return layer_.broadcasttodynamic_;
}
::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::release_broadcasttodynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  if (has_broadcasttodynamic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToDynamicLayerParams* temp = layer_.broadcasttodynamic_;
    layer_.broadcasttodynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_broadcasttodynamic(::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic) {
  clear_layer();
  if (broadcasttodynamic) {
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = broadcasttodynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
}

// .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
bool NeuralNetworkLayer::has_squeeze() const {
  return layer_case() == kSqueeze;
}
void NeuralNetworkLayer::set_has_squeeze() {
  _oneof_case_[0] = kSqueeze;
}
void NeuralNetworkLayer::clear_squeeze() {
  if (has_squeeze()) {
    delete layer_.squeeze_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SqueezeLayerParams& NeuralNetworkLayer::squeeze() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return has_squeeze()
      ? *layer_.squeeze_
      : ::CoreML::Specification::SqueezeLayerParams::default_instance();
}
::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::mutable_squeeze() {
  if (!has_squeeze()) {
    clear_layer();
    set_has_squeeze();
    layer_.squeeze_ = new ::CoreML::Specification::SqueezeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return layer_.squeeze_;
}
::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::release_squeeze() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.squeeze)
  if (has_squeeze()) {
    clear_has_layer();
    ::CoreML::Specification::SqueezeLayerParams* temp = layer_.squeeze_;
    layer_.squeeze_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_squeeze(::CoreML::Specification::SqueezeLayerParams* squeeze) {
  clear_layer();
  if (squeeze) {
    set_has_squeeze();
    layer_.squeeze_ = squeeze;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.squeeze)
}

// .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
bool NeuralNetworkLayer::has_expanddims() const {
  return layer_case() == kExpandDims;
}
void NeuralNetworkLayer::set_has_expanddims() {
  _oneof_case_[0] = kExpandDims;
}
void NeuralNetworkLayer::clear_expanddims() {
  if (has_expanddims()) {
    delete layer_.expanddims_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ExpandDimsLayerParams& NeuralNetworkLayer::expanddims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return has_expanddims()
      ? *layer_.expanddims_
      : ::CoreML::Specification::ExpandDimsLayerParams::default_instance();
}
::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::mutable_expanddims() {
  if (!has_expanddims()) {
    clear_layer();
    set_has_expanddims();
    layer_.expanddims_ = new ::CoreML::Specification::ExpandDimsLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return layer_.expanddims_;
}
::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::release_expanddims() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.expandDims)
  if (has_expanddims()) {
    clear_has_layer();
    ::CoreML::Specification::ExpandDimsLayerParams* temp = layer_.expanddims_;
    layer_.expanddims_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_expanddims(::CoreML::Specification::ExpandDimsLayerParams* expanddims) {
  clear_layer();
  if (expanddims) {
    set_has_expanddims();
    layer_.expanddims_ = expanddims;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.expandDims)
}

// .CoreML.Specification.FlattenTo2DLayerParams flattenTo2D = 1130;
bool NeuralNetworkLayer::has_flattento2d() const {
  return layer_case() == kFlattenTo2D;
}
void NeuralNetworkLayer::set_has_flattento2d() {
  _oneof_case_[0] = kFlattenTo2D;
}
void NeuralNetworkLayer::clear_flattento2d() {
  if (has_flattento2d()) {
    delete layer_.flattento2d_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FlattenTo2DLayerParams& NeuralNetworkLayer::flattento2d() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  return has_flattento2d()
      ? *layer_.flattento2d_
      : ::CoreML::Specification::FlattenTo2DLayerParams::default_instance();
}
::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::mutable_flattento2d() {
  if (!has_flattento2d()) {
    clear_layer();
    set_has_flattento2d();
    layer_.flattento2d_ = new ::CoreML::Specification::FlattenTo2DLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  return layer_.flattento2d_;
}
::CoreML::Specification::FlattenTo2DLayerParams* NeuralNetworkLayer::release_flattento2d() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
  if (has_flattento2d()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenTo2DLayerParams* temp = layer_.flattento2d_;
    layer_.flattento2d_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_flattento2d(::CoreML::Specification::FlattenTo2DLayerParams* flattento2d) {
  clear_layer();
  if (flattento2d) {
    set_has_flattento2d();
    layer_.flattento2d_ = flattento2d;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flattenTo2D)
}

// .CoreML.Specification.ReshapeLikeLayerParams reshapeLike = 1135;
bool NeuralNetworkLayer::has_reshapelike() const {
  return layer_case() == kReshapeLike;
}
void NeuralNetworkLayer::set_has_reshapelike() {
  _oneof_case_[0] = kReshapeLike;
}
void NeuralNetworkLayer::clear_reshapelike() {
  if (has_reshapelike()) {
    delete layer_.reshapelike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeLikeLayerParams& NeuralNetworkLayer::reshapelike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  return has_reshapelike()
      ? *layer_.reshapelike_
      : ::CoreML::Specification::ReshapeLikeLayerParams::default_instance();
}
::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::mutable_reshapelike() {
  if (!has_reshapelike()) {
    clear_layer();
    set_has_reshapelike();
    layer_.reshapelike_ = new ::CoreML::Specification::ReshapeLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  return layer_.reshapelike_;
}
::CoreML::Specification::ReshapeLikeLayerParams* NeuralNetworkLayer::release_reshapelike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
  if (has_reshapelike()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLikeLayerParams* temp = layer_.reshapelike_;
    layer_.reshapelike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshapelike(::CoreML::Specification::ReshapeLikeLayerParams* reshapelike) {
  clear_layer();
  if (reshapelike) {
    set_has_reshapelike();
    layer_.reshapelike_ = reshapelike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeLike)
}

// .CoreML.Specification.ReshapeStaticLayerParams reshapeStatic = 1140;
bool NeuralNetworkLayer::has_reshapestatic() const {
  return layer_case() == kReshapeStatic;
}
void NeuralNetworkLayer::set_has_reshapestatic() {
  _oneof_case_[0] = kReshapeStatic;
}
void NeuralNetworkLayer::clear_reshapestatic() {
  if (has_reshapestatic()) {
    delete layer_.reshapestatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeStaticLayerParams& NeuralNetworkLayer::reshapestatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  return has_reshapestatic()
      ? *layer_.reshapestatic_
      : ::CoreML::Specification::ReshapeStaticLayerParams::default_instance();
}
::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::mutable_reshapestatic() {
  if (!has_reshapestatic()) {
    clear_layer();
    set_has_reshapestatic();
    layer_.reshapestatic_ = new ::CoreML::Specification::ReshapeStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  return layer_.reshapestatic_;
}
::CoreML::Specification::ReshapeStaticLayerParams* NeuralNetworkLayer::release_reshapestatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
  if (has_reshapestatic()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeStaticLayerParams* temp = layer_.reshapestatic_;
    layer_.reshapestatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshapestatic(::CoreML::Specification::ReshapeStaticLayerParams* reshapestatic) {
  clear_layer();
  if (reshapestatic) {
    set_has_reshapestatic();
    layer_.reshapestatic_ = reshapestatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeStatic)
}

// .CoreML.Specification.ReshapeDynamicLayerParams reshapeDynamic = 1145;
bool NeuralNetworkLayer::has_reshapedynamic() const {
  return layer_case() == kReshapeDynamic;
}
void NeuralNetworkLayer::set_has_reshapedynamic() {
  _oneof_case_[0] = kReshapeDynamic;
}
void NeuralNetworkLayer::clear_reshapedynamic() {
  if (has_reshapedynamic()) {
    delete layer_.reshapedynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeDynamicLayerParams& NeuralNetworkLayer::reshapedynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  return has_reshapedynamic()
      ? *layer_.reshapedynamic_
      : ::CoreML::Specification::ReshapeDynamicLayerParams::default_instance();
}
::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::mutable_reshapedynamic() {
  if (!has_reshapedynamic()) {
    clear_layer();
    set_has_reshapedynamic();
    layer_.reshapedynamic_ = new ::CoreML::Specification::ReshapeDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  return layer_.reshapedynamic_;
}
::CoreML::Specification::ReshapeDynamicLayerParams* NeuralNetworkLayer::release_reshapedynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
  if (has_reshapedynamic()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeDynamicLayerParams* temp = layer_.reshapedynamic_;
    layer_.reshapedynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshapedynamic(::CoreML::Specification::ReshapeDynamicLayerParams* reshapedynamic) {
  clear_layer();
  if (reshapedynamic) {
    set_has_reshapedynamic();
    layer_.reshapedynamic_ = reshapedynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshapeDynamic)
}

// .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
bool NeuralNetworkLayer::has_rankpreservingreshape() const {
  return layer_case() == kRankPreservingReshape;
}
void NeuralNetworkLayer::set_has_rankpreservingreshape() {
  _oneof_case_[0] = kRankPreservingReshape;
}
void NeuralNetworkLayer::clear_rankpreservingreshape() {
  if (has_rankpreservingreshape()) {
    delete layer_.rankpreservingreshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RankPreservingReshapeLayerParams& NeuralNetworkLayer::rankpreservingreshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return has_rankpreservingreshape()
      ? *layer_.rankpreservingreshape_
      : ::CoreML::Specification::RankPreservingReshapeLayerParams::default_instance();
}
::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::mutable_rankpreservingreshape() {
  if (!has_rankpreservingreshape()) {
    clear_layer();
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = new ::CoreML::Specification::RankPreservingReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return layer_.rankpreservingreshape_;
}
::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::release_rankpreservingreshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  if (has_rankpreservingreshape()) {
    clear_has_layer();
    ::CoreML::Specification::RankPreservingReshapeLayerParams* temp = layer_.rankpreservingreshape_;
    layer_.rankpreservingreshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_rankpreservingreshape(::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape) {
  clear_layer();
  if (rankpreservingreshape) {
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = rankpreservingreshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
}

// .CoreML.Specification.ConstantPaddingLayerParams constantPad = 1155;
bool NeuralNetworkLayer::has_constantpad() const {
  return layer_case() == kConstantPad;
}
void NeuralNetworkLayer::set_has_constantpad() {
  _oneof_case_[0] = kConstantPad;
}
void NeuralNetworkLayer::clear_constantpad() {
  if (has_constantpad()) {
    delete layer_.constantpad_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConstantPaddingLayerParams& NeuralNetworkLayer::constantpad() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.constantPad)
  return has_constantpad()
      ? *layer_.constantpad_
      : ::CoreML::Specification::ConstantPaddingLayerParams::default_instance();
}
::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::mutable_constantpad() {
  if (!has_constantpad()) {
    clear_layer();
    set_has_constantpad();
    layer_.constantpad_ = new ::CoreML::Specification::ConstantPaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.constantPad)
  return layer_.constantpad_;
}
::CoreML::Specification::ConstantPaddingLayerParams* NeuralNetworkLayer::release_constantpad() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.constantPad)
  if (has_constantpad()) {
    clear_has_layer();
    ::CoreML::Specification::ConstantPaddingLayerParams* temp = layer_.constantpad_;
    layer_.constantpad_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_constantpad(::CoreML::Specification::ConstantPaddingLayerParams* constantpad) {
  clear_layer();
  if (constantpad) {
    set_has_constantpad();
    layer_.constantpad_ = constantpad;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.constantPad)
}

// .CoreML.Specification.RandomNormalLikeLayerParams randomNormalLike = 1170;
bool NeuralNetworkLayer::has_randomnormallike() const {
  return layer_case() == kRandomNormalLike;
}
void NeuralNetworkLayer::set_has_randomnormallike() {
  _oneof_case_[0] = kRandomNormalLike;
}
void NeuralNetworkLayer::clear_randomnormallike() {
  if (has_randomnormallike()) {
    delete layer_.randomnormallike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomNormalLikeLayerParams& NeuralNetworkLayer::randomnormallike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  return has_randomnormallike()
      ? *layer_.randomnormallike_
      : ::CoreML::Specification::RandomNormalLikeLayerParams::default_instance();
}
::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::mutable_randomnormallike() {
  if (!has_randomnormallike()) {
    clear_layer();
    set_has_randomnormallike();
    layer_.randomnormallike_ = new ::CoreML::Specification::RandomNormalLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  return layer_.randomnormallike_;
}
::CoreML::Specification::RandomNormalLikeLayerParams* NeuralNetworkLayer::release_randomnormallike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
  if (has_randomnormallike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalLikeLayerParams* temp = layer_.randomnormallike_;
    layer_.randomnormallike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randomnormallike(::CoreML::Specification::RandomNormalLikeLayerParams* randomnormallike) {
  clear_layer();
  if (randomnormallike) {
    set_has_randomnormallike();
    layer_.randomnormallike_ = randomnormallike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalLike)
}

// .CoreML.Specification.RandomNormalStaticLayerParams randomNormalStatic = 1175;
bool NeuralNetworkLayer::has_randomnormalstatic() const {
  return layer_case() == kRandomNormalStatic;
}
void NeuralNetworkLayer::set_has_randomnormalstatic() {
  _oneof_case_[0] = kRandomNormalStatic;
}
void NeuralNetworkLayer::clear_randomnormalstatic() {
  if (has_randomnormalstatic()) {
    delete layer_.randomnormalstatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomNormalStaticLayerParams& NeuralNetworkLayer::randomnormalstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  return has_randomnormalstatic()
      ? *layer_.randomnormalstatic_
      : ::CoreML::Specification::RandomNormalStaticLayerParams::default_instance();
}
::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::mutable_randomnormalstatic() {
  if (!has_randomnormalstatic()) {
    clear_layer();
    set_has_randomnormalstatic();
    layer_.randomnormalstatic_ = new ::CoreML::Specification::RandomNormalStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  return layer_.randomnormalstatic_;
}
::CoreML::Specification::RandomNormalStaticLayerParams* NeuralNetworkLayer::release_randomnormalstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
  if (has_randomnormalstatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalStaticLayerParams* temp = layer_.randomnormalstatic_;
    layer_.randomnormalstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randomnormalstatic(::CoreML::Specification::RandomNormalStaticLayerParams* randomnormalstatic) {
  clear_layer();
  if (randomnormalstatic) {
    set_has_randomnormalstatic();
    layer_.randomnormalstatic_ = randomnormalstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalStatic)
}

// .CoreML.Specification.RandomNormalDynamicLayerParams randomNormalDynamic = 1180;
bool NeuralNetworkLayer::has_randomnormaldynamic() const {
  return layer_case() == kRandomNormalDynamic;
}
void NeuralNetworkLayer::set_has_randomnormaldynamic() {
  _oneof_case_[0] = kRandomNormalDynamic;
}
void NeuralNetworkLayer::clear_randomnormaldynamic() {
  if (has_randomnormaldynamic()) {
    delete layer_.randomnormaldynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomNormalDynamicLayerParams& NeuralNetworkLayer::randomnormaldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  return has_randomnormaldynamic()
      ? *layer_.randomnormaldynamic_
      : ::CoreML::Specification::RandomNormalDynamicLayerParams::default_instance();
}
::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::mutable_randomnormaldynamic() {
  if (!has_randomnormaldynamic()) {
    clear_layer();
    set_has_randomnormaldynamic();
    layer_.randomnormaldynamic_ = new ::CoreML::Specification::RandomNormalDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  return layer_.randomnormaldynamic_;
}
::CoreML::Specification::RandomNormalDynamicLayerParams* NeuralNetworkLayer::release_randomnormaldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
  if (has_randomnormaldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomNormalDynamicLayerParams* temp = layer_.randomnormaldynamic_;
    layer_.randomnormaldynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randomnormaldynamic(::CoreML::Specification::RandomNormalDynamicLayerParams* randomnormaldynamic) {
  clear_layer();
  if (randomnormaldynamic) {
    set_has_randomnormaldynamic();
    layer_.randomnormaldynamic_ = randomnormaldynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomNormalDynamic)
}

// .CoreML.Specification.RandomUniformLikeLayerParams randomUniformLike = 1190;
bool NeuralNetworkLayer::has_randomuniformlike() const {
  return layer_case() == kRandomUniformLike;
}
void NeuralNetworkLayer::set_has_randomuniformlike() {
  _oneof_case_[0] = kRandomUniformLike;
}
void NeuralNetworkLayer::clear_randomuniformlike() {
  if (has_randomuniformlike()) {
    delete layer_.randomuniformlike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomUniformLikeLayerParams& NeuralNetworkLayer::randomuniformlike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  return has_randomuniformlike()
      ? *layer_.randomuniformlike_
      : ::CoreML::Specification::RandomUniformLikeLayerParams::default_instance();
}
::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::mutable_randomuniformlike() {
  if (!has_randomuniformlike()) {
    clear_layer();
    set_has_randomuniformlike();
    layer_.randomuniformlike_ = new ::CoreML::Specification::RandomUniformLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  return layer_.randomuniformlike_;
}
::CoreML::Specification::RandomUniformLikeLayerParams* NeuralNetworkLayer::release_randomuniformlike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
  if (has_randomuniformlike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformLikeLayerParams* temp = layer_.randomuniformlike_;
    layer_.randomuniformlike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randomuniformlike(::CoreML::Specification::RandomUniformLikeLayerParams* randomuniformlike) {
  clear_layer();
  if (randomuniformlike) {
    set_has_randomuniformlike();
    layer_.randomuniformlike_ = randomuniformlike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformLike)
}

// .CoreML.Specification.RandomUniformStaticLayerParams randomUniformStatic = 1195;
bool NeuralNetworkLayer::has_randomuniformstatic() const {
  return layer_case() == kRandomUniformStatic;
}
void NeuralNetworkLayer::set_has_randomuniformstatic() {
  _oneof_case_[0] = kRandomUniformStatic;
}
void NeuralNetworkLayer::clear_randomuniformstatic() {
  if (has_randomuniformstatic()) {
    delete layer_.randomuniformstatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomUniformStaticLayerParams& NeuralNetworkLayer::randomuniformstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  return has_randomuniformstatic()
      ? *layer_.randomuniformstatic_
      : ::CoreML::Specification::RandomUniformStaticLayerParams::default_instance();
}
::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::mutable_randomuniformstatic() {
  if (!has_randomuniformstatic()) {
    clear_layer();
    set_has_randomuniformstatic();
    layer_.randomuniformstatic_ = new ::CoreML::Specification::RandomUniformStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  return layer_.randomuniformstatic_;
}
::CoreML::Specification::RandomUniformStaticLayerParams* NeuralNetworkLayer::release_randomuniformstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
  if (has_randomuniformstatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformStaticLayerParams* temp = layer_.randomuniformstatic_;
    layer_.randomuniformstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randomuniformstatic(::CoreML::Specification::RandomUniformStaticLayerParams* randomuniformstatic) {
  clear_layer();
  if (randomuniformstatic) {
    set_has_randomuniformstatic();
    layer_.randomuniformstatic_ = randomuniformstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformStatic)
}

// .CoreML.Specification.RandomUniformDynamicLayerParams randomUniformDynamic = 1200;
bool NeuralNetworkLayer::has_randomuniformdynamic() const {
  return layer_case() == kRandomUniformDynamic;
}
void NeuralNetworkLayer::set_has_randomuniformdynamic() {
  _oneof_case_[0] = kRandomUniformDynamic;
}
void NeuralNetworkLayer::clear_randomuniformdynamic() {
  if (has_randomuniformdynamic()) {
    delete layer_.randomuniformdynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomUniformDynamicLayerParams& NeuralNetworkLayer::randomuniformdynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  return has_randomuniformdynamic()
      ? *layer_.randomuniformdynamic_
      : ::CoreML::Specification::RandomUniformDynamicLayerParams::default_instance();
}
::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::mutable_randomuniformdynamic() {
  if (!has_randomuniformdynamic()) {
    clear_layer();
    set_has_randomuniformdynamic();
    layer_.randomuniformdynamic_ = new ::CoreML::Specification::RandomUniformDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  return layer_.randomuniformdynamic_;
}
::CoreML::Specification::RandomUniformDynamicLayerParams* NeuralNetworkLayer::release_randomuniformdynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
  if (has_randomuniformdynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomUniformDynamicLayerParams* temp = layer_.randomuniformdynamic_;
    layer_.randomuniformdynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randomuniformdynamic(::CoreML::Specification::RandomUniformDynamicLayerParams* randomuniformdynamic) {
  clear_layer();
  if (randomuniformdynamic) {
    set_has_randomuniformdynamic();
    layer_.randomuniformdynamic_ = randomuniformdynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomUniformDynamic)
}

// .CoreML.Specification.RandomBernoulliLikeLayerParams randomBernoulliLike = 1210;
bool NeuralNetworkLayer::has_randombernoullilike() const {
  return layer_case() == kRandomBernoulliLike;
}
void NeuralNetworkLayer::set_has_randombernoullilike() {
  _oneof_case_[0] = kRandomBernoulliLike;
}
void NeuralNetworkLayer::clear_randombernoullilike() {
  if (has_randombernoullilike()) {
    delete layer_.randombernoullilike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomBernoulliLikeLayerParams& NeuralNetworkLayer::randombernoullilike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  return has_randombernoullilike()
      ? *layer_.randombernoullilike_
      : ::CoreML::Specification::RandomBernoulliLikeLayerParams::default_instance();
}
::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::mutable_randombernoullilike() {
  if (!has_randombernoullilike()) {
    clear_layer();
    set_has_randombernoullilike();
    layer_.randombernoullilike_ = new ::CoreML::Specification::RandomBernoulliLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  return layer_.randombernoullilike_;
}
::CoreML::Specification::RandomBernoulliLikeLayerParams* NeuralNetworkLayer::release_randombernoullilike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
  if (has_randombernoullilike()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliLikeLayerParams* temp = layer_.randombernoullilike_;
    layer_.randombernoullilike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randombernoullilike(::CoreML::Specification::RandomBernoulliLikeLayerParams* randombernoullilike) {
  clear_layer();
  if (randombernoullilike) {
    set_has_randombernoullilike();
    layer_.randombernoullilike_ = randombernoullilike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliLike)
}

// .CoreML.Specification.RandomBernoulliStaticLayerParams randomBernoulliStatic = 1215;
bool NeuralNetworkLayer::has_randombernoullistatic() const {
  return layer_case() == kRandomBernoulliStatic;
}
void NeuralNetworkLayer::set_has_randombernoullistatic() {
  _oneof_case_[0] = kRandomBernoulliStatic;
}
void NeuralNetworkLayer::clear_randombernoullistatic() {
  if (has_randombernoullistatic()) {
    delete layer_.randombernoullistatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomBernoulliStaticLayerParams& NeuralNetworkLayer::randombernoullistatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  return has_randombernoullistatic()
      ? *layer_.randombernoullistatic_
      : ::CoreML::Specification::RandomBernoulliStaticLayerParams::default_instance();
}
::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::mutable_randombernoullistatic() {
  if (!has_randombernoullistatic()) {
    clear_layer();
    set_has_randombernoullistatic();
    layer_.randombernoullistatic_ = new ::CoreML::Specification::RandomBernoulliStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  return layer_.randombernoullistatic_;
}
::CoreML::Specification::RandomBernoulliStaticLayerParams* NeuralNetworkLayer::release_randombernoullistatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
  if (has_randombernoullistatic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliStaticLayerParams* temp = layer_.randombernoullistatic_;
    layer_.randombernoullistatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randombernoullistatic(::CoreML::Specification::RandomBernoulliStaticLayerParams* randombernoullistatic) {
  clear_layer();
  if (randombernoullistatic) {
    set_has_randombernoullistatic();
    layer_.randombernoullistatic_ = randombernoullistatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliStatic)
}

// .CoreML.Specification.RandomBernoulliDynamicLayerParams randomBernoulliDynamic = 1220;
bool NeuralNetworkLayer::has_randombernoullidynamic() const {
  return layer_case() == kRandomBernoulliDynamic;
}
void NeuralNetworkLayer::set_has_randombernoullidynamic() {
  _oneof_case_[0] = kRandomBernoulliDynamic;
}
void NeuralNetworkLayer::clear_randombernoullidynamic() {
  if (has_randombernoullidynamic()) {
    delete layer_.randombernoullidynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RandomBernoulliDynamicLayerParams& NeuralNetworkLayer::randombernoullidynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  return has_randombernoullidynamic()
      ? *layer_.randombernoullidynamic_
      : ::CoreML::Specification::RandomBernoulliDynamicLayerParams::default_instance();
}
::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::mutable_randombernoullidynamic() {
  if (!has_randombernoullidynamic()) {
    clear_layer();
    set_has_randombernoullidynamic();
    layer_.randombernoullidynamic_ = new ::CoreML::Specification::RandomBernoulliDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  return layer_.randombernoullidynamic_;
}
::CoreML::Specification::RandomBernoulliDynamicLayerParams* NeuralNetworkLayer::release_randombernoullidynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
  if (has_randombernoullidynamic()) {
    clear_has_layer();
    ::CoreML::Specification::RandomBernoulliDynamicLayerParams* temp = layer_.randombernoullidynamic_;
    layer_.randombernoullidynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_randombernoullidynamic(::CoreML::Specification::RandomBernoulliDynamicLayerParams* randombernoullidynamic) {
  clear_layer();
  if (randombernoullidynamic) {
    set_has_randombernoullidynamic();
    layer_.randombernoullidynamic_ = randombernoullidynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.randomBernoulliDynamic)
}

// .CoreML.Specification.CategoricalDistributionLayerParams categoricalDistribution = 1230;
bool NeuralNetworkLayer::has_categoricaldistribution() const {
  return layer_case() == kCategoricalDistribution;
}
void NeuralNetworkLayer::set_has_categoricaldistribution() {
  _oneof_case_[0] = kCategoricalDistribution;
}
void NeuralNetworkLayer::clear_categoricaldistribution() {
  if (has_categoricaldistribution()) {
    delete layer_.categoricaldistribution_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CategoricalDistributionLayerParams& NeuralNetworkLayer::categoricaldistribution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  return has_categoricaldistribution()
      ? *layer_.categoricaldistribution_
      : ::CoreML::Specification::CategoricalDistributionLayerParams::default_instance();
}
::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::mutable_categoricaldistribution() {
  if (!has_categoricaldistribution()) {
    clear_layer();
    set_has_categoricaldistribution();
    layer_.categoricaldistribution_ = new ::CoreML::Specification::CategoricalDistributionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  return layer_.categoricaldistribution_;
}
::CoreML::Specification::CategoricalDistributionLayerParams* NeuralNetworkLayer::release_categoricaldistribution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
  if (has_categoricaldistribution()) {
    clear_has_layer();
    ::CoreML::Specification::CategoricalDistributionLayerParams* temp = layer_.categoricaldistribution_;
    layer_.categoricaldistribution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_categoricaldistribution(::CoreML::Specification::CategoricalDistributionLayerParams* categoricaldistribution) {
  clear_layer();
  if (categoricaldistribution) {
    set_has_categoricaldistribution();
    layer_.categoricaldistribution_ = categoricaldistribution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.categoricalDistribution)
}

// .CoreML.Specification.ReduceL1LayerParams reduceL1 = 1250;
bool NeuralNetworkLayer::has_reducel1() const {
  return layer_case() == kReduceL1;
}
void NeuralNetworkLayer::set_has_reducel1() {
  _oneof_case_[0] = kReduceL1;
}
void NeuralNetworkLayer::clear_reducel1() {
  if (has_reducel1()) {
    delete layer_.reducel1_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceL1LayerParams& NeuralNetworkLayer::reducel1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  return has_reducel1()
      ? *layer_.reducel1_
      : ::CoreML::Specification::ReduceL1LayerParams::default_instance();
}
::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::mutable_reducel1() {
  if (!has_reducel1()) {
    clear_layer();
    set_has_reducel1();
    layer_.reducel1_ = new ::CoreML::Specification::ReduceL1LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  return layer_.reducel1_;
}
::CoreML::Specification::ReduceL1LayerParams* NeuralNetworkLayer::release_reducel1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceL1)
  if (has_reducel1()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceL1LayerParams* temp = layer_.reducel1_;
    layer_.reducel1_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducel1(::CoreML::Specification::ReduceL1LayerParams* reducel1) {
  clear_layer();
  if (reducel1) {
    set_has_reducel1();
    layer_.reducel1_ = reducel1;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceL1)
}

// .CoreML.Specification.ReduceL2LayerParams reduceL2 = 1255;
bool NeuralNetworkLayer::has_reducel2() const {
  return layer_case() == kReduceL2;
}
void NeuralNetworkLayer::set_has_reducel2() {
  _oneof_case_[0] = kReduceL2;
}
void NeuralNetworkLayer::clear_reducel2() {
  if (has_reducel2()) {
    delete layer_.reducel2_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceL2LayerParams& NeuralNetworkLayer::reducel2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  return has_reducel2()
      ? *layer_.reducel2_
      : ::CoreML::Specification::ReduceL2LayerParams::default_instance();
}
::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::mutable_reducel2() {
  if (!has_reducel2()) {
    clear_layer();
    set_has_reducel2();
    layer_.reducel2_ = new ::CoreML::Specification::ReduceL2LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  return layer_.reducel2_;
}
::CoreML::Specification::ReduceL2LayerParams* NeuralNetworkLayer::release_reducel2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceL2)
  if (has_reducel2()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceL2LayerParams* temp = layer_.reducel2_;
    layer_.reducel2_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducel2(::CoreML::Specification::ReduceL2LayerParams* reducel2) {
  clear_layer();
  if (reducel2) {
    set_has_reducel2();
    layer_.reducel2_ = reducel2;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceL2)
}

// .CoreML.Specification.ReduceMaxLayerParams reduceMax = 1260;
bool NeuralNetworkLayer::has_reducemax() const {
  return layer_case() == kReduceMax;
}
void NeuralNetworkLayer::set_has_reducemax() {
  _oneof_case_[0] = kReduceMax;
}
void NeuralNetworkLayer::clear_reducemax() {
  if (has_reducemax()) {
    delete layer_.reducemax_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceMaxLayerParams& NeuralNetworkLayer::reducemax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  return has_reducemax()
      ? *layer_.reducemax_
      : ::CoreML::Specification::ReduceMaxLayerParams::default_instance();
}
::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::mutable_reducemax() {
  if (!has_reducemax()) {
    clear_layer();
    set_has_reducemax();
    layer_.reducemax_ = new ::CoreML::Specification::ReduceMaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  return layer_.reducemax_;
}
::CoreML::Specification::ReduceMaxLayerParams* NeuralNetworkLayer::release_reducemax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMax)
  if (has_reducemax()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMaxLayerParams* temp = layer_.reducemax_;
    layer_.reducemax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducemax(::CoreML::Specification::ReduceMaxLayerParams* reducemax) {
  clear_layer();
  if (reducemax) {
    set_has_reducemax();
    layer_.reducemax_ = reducemax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMax)
}

// .CoreML.Specification.ReduceMinLayerParams reduceMin = 1265;
bool NeuralNetworkLayer::has_reducemin() const {
  return layer_case() == kReduceMin;
}
void NeuralNetworkLayer::set_has_reducemin() {
  _oneof_case_[0] = kReduceMin;
}
void NeuralNetworkLayer::clear_reducemin() {
  if (has_reducemin()) {
    delete layer_.reducemin_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceMinLayerParams& NeuralNetworkLayer::reducemin() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  return has_reducemin()
      ? *layer_.reducemin_
      : ::CoreML::Specification::ReduceMinLayerParams::default_instance();
}
::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::mutable_reducemin() {
  if (!has_reducemin()) {
    clear_layer();
    set_has_reducemin();
    layer_.reducemin_ = new ::CoreML::Specification::ReduceMinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  return layer_.reducemin_;
}
::CoreML::Specification::ReduceMinLayerParams* NeuralNetworkLayer::release_reducemin() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMin)
  if (has_reducemin()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMinLayerParams* temp = layer_.reducemin_;
    layer_.reducemin_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducemin(::CoreML::Specification::ReduceMinLayerParams* reducemin) {
  clear_layer();
  if (reducemin) {
    set_has_reducemin();
    layer_.reducemin_ = reducemin;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMin)
}

// .CoreML.Specification.ReduceSumLayerParams reduceSum = 1270;
bool NeuralNetworkLayer::has_reducesum() const {
  return layer_case() == kReduceSum;
}
void NeuralNetworkLayer::set_has_reducesum() {
  _oneof_case_[0] = kReduceSum;
}
void NeuralNetworkLayer::clear_reducesum() {
  if (has_reducesum()) {
    delete layer_.reducesum_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceSumLayerParams& NeuralNetworkLayer::reducesum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  return has_reducesum()
      ? *layer_.reducesum_
      : ::CoreML::Specification::ReduceSumLayerParams::default_instance();
}
::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::mutable_reducesum() {
  if (!has_reducesum()) {
    clear_layer();
    set_has_reducesum();
    layer_.reducesum_ = new ::CoreML::Specification::ReduceSumLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  return layer_.reducesum_;
}
::CoreML::Specification::ReduceSumLayerParams* NeuralNetworkLayer::release_reducesum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceSum)
  if (has_reducesum()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceSumLayerParams* temp = layer_.reducesum_;
    layer_.reducesum_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducesum(::CoreML::Specification::ReduceSumLayerParams* reducesum) {
  clear_layer();
  if (reducesum) {
    set_has_reducesum();
    layer_.reducesum_ = reducesum;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceSum)
}

// .CoreML.Specification.ReduceProdLayerParams reduceProd = 1275;
bool NeuralNetworkLayer::has_reduceprod() const {
  return layer_case() == kReduceProd;
}
void NeuralNetworkLayer::set_has_reduceprod() {
  _oneof_case_[0] = kReduceProd;
}
void NeuralNetworkLayer::clear_reduceprod() {
  if (has_reduceprod()) {
    delete layer_.reduceprod_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceProdLayerParams& NeuralNetworkLayer::reduceprod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  return has_reduceprod()
      ? *layer_.reduceprod_
      : ::CoreML::Specification::ReduceProdLayerParams::default_instance();
}
::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::mutable_reduceprod() {
  if (!has_reduceprod()) {
    clear_layer();
    set_has_reduceprod();
    layer_.reduceprod_ = new ::CoreML::Specification::ReduceProdLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  return layer_.reduceprod_;
}
::CoreML::Specification::ReduceProdLayerParams* NeuralNetworkLayer::release_reduceprod() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceProd)
  if (has_reduceprod()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceProdLayerParams* temp = layer_.reduceprod_;
    layer_.reduceprod_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reduceprod(::CoreML::Specification::ReduceProdLayerParams* reduceprod) {
  clear_layer();
  if (reduceprod) {
    set_has_reduceprod();
    layer_.reduceprod_ = reduceprod;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceProd)
}

// .CoreML.Specification.ReduceMeanLayerParams reduceMean = 1280;
bool NeuralNetworkLayer::has_reducemean() const {
  return layer_case() == kReduceMean;
}
void NeuralNetworkLayer::set_has_reducemean() {
  _oneof_case_[0] = kReduceMean;
}
void NeuralNetworkLayer::clear_reducemean() {
  if (has_reducemean()) {
    delete layer_.reducemean_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceMeanLayerParams& NeuralNetworkLayer::reducemean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  return has_reducemean()
      ? *layer_.reducemean_
      : ::CoreML::Specification::ReduceMeanLayerParams::default_instance();
}
::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::mutable_reducemean() {
  if (!has_reducemean()) {
    clear_layer();
    set_has_reducemean();
    layer_.reducemean_ = new ::CoreML::Specification::ReduceMeanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  return layer_.reducemean_;
}
::CoreML::Specification::ReduceMeanLayerParams* NeuralNetworkLayer::release_reducemean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceMean)
  if (has_reducemean()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceMeanLayerParams* temp = layer_.reducemean_;
    layer_.reducemean_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducemean(::CoreML::Specification::ReduceMeanLayerParams* reducemean) {
  clear_layer();
  if (reducemean) {
    set_has_reducemean();
    layer_.reducemean_ = reducemean;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceMean)
}

// .CoreML.Specification.ReduceLogSumLayerParams reduceLogSum = 1285;
bool NeuralNetworkLayer::has_reducelogsum() const {
  return layer_case() == kReduceLogSum;
}
void NeuralNetworkLayer::set_has_reducelogsum() {
  _oneof_case_[0] = kReduceLogSum;
}
void NeuralNetworkLayer::clear_reducelogsum() {
  if (has_reducelogsum()) {
    delete layer_.reducelogsum_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceLogSumLayerParams& NeuralNetworkLayer::reducelogsum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  return has_reducelogsum()
      ? *layer_.reducelogsum_
      : ::CoreML::Specification::ReduceLogSumLayerParams::default_instance();
}
::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::mutable_reducelogsum() {
  if (!has_reducelogsum()) {
    clear_layer();
    set_has_reducelogsum();
    layer_.reducelogsum_ = new ::CoreML::Specification::ReduceLogSumLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  return layer_.reducelogsum_;
}
::CoreML::Specification::ReduceLogSumLayerParams* NeuralNetworkLayer::release_reducelogsum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
  if (has_reducelogsum()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLogSumLayerParams* temp = layer_.reducelogsum_;
    layer_.reducelogsum_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducelogsum(::CoreML::Specification::ReduceLogSumLayerParams* reducelogsum) {
  clear_layer();
  if (reducelogsum) {
    set_has_reducelogsum();
    layer_.reducelogsum_ = reducelogsum;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceLogSum)
}

// .CoreML.Specification.ReduceSumSquareLayerParams reduceSumSquare = 1290;
bool NeuralNetworkLayer::has_reducesumsquare() const {
  return layer_case() == kReduceSumSquare;
}
void NeuralNetworkLayer::set_has_reducesumsquare() {
  _oneof_case_[0] = kReduceSumSquare;
}
void NeuralNetworkLayer::clear_reducesumsquare() {
  if (has_reducesumsquare()) {
    delete layer_.reducesumsquare_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceSumSquareLayerParams& NeuralNetworkLayer::reducesumsquare() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  return has_reducesumsquare()
      ? *layer_.reducesumsquare_
      : ::CoreML::Specification::ReduceSumSquareLayerParams::default_instance();
}
::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::mutable_reducesumsquare() {
  if (!has_reducesumsquare()) {
    clear_layer();
    set_has_reducesumsquare();
    layer_.reducesumsquare_ = new ::CoreML::Specification::ReduceSumSquareLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  return layer_.reducesumsquare_;
}
::CoreML::Specification::ReduceSumSquareLayerParams* NeuralNetworkLayer::release_reducesumsquare() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
  if (has_reducesumsquare()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceSumSquareLayerParams* temp = layer_.reducesumsquare_;
    layer_.reducesumsquare_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducesumsquare(::CoreML::Specification::ReduceSumSquareLayerParams* reducesumsquare) {
  clear_layer();
  if (reducesumsquare) {
    set_has_reducesumsquare();
    layer_.reducesumsquare_ = reducesumsquare;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceSumSquare)
}

// .CoreML.Specification.ReduceLogSumExpLayerParams reduceLogSumExp = 1295;
bool NeuralNetworkLayer::has_reducelogsumexp() const {
  return layer_case() == kReduceLogSumExp;
}
void NeuralNetworkLayer::set_has_reducelogsumexp() {
  _oneof_case_[0] = kReduceLogSumExp;
}
void NeuralNetworkLayer::clear_reducelogsumexp() {
  if (has_reducelogsumexp()) {
    delete layer_.reducelogsumexp_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceLogSumExpLayerParams& NeuralNetworkLayer::reducelogsumexp() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  return has_reducelogsumexp()
      ? *layer_.reducelogsumexp_
      : ::CoreML::Specification::ReduceLogSumExpLayerParams::default_instance();
}
::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::mutable_reducelogsumexp() {
  if (!has_reducelogsumexp()) {
    clear_layer();
    set_has_reducelogsumexp();
    layer_.reducelogsumexp_ = new ::CoreML::Specification::ReduceLogSumExpLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  return layer_.reducelogsumexp_;
}
::CoreML::Specification::ReduceLogSumExpLayerParams* NeuralNetworkLayer::release_reducelogsumexp() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
  if (has_reducelogsumexp()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLogSumExpLayerParams* temp = layer_.reducelogsumexp_;
    layer_.reducelogsumexp_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reducelogsumexp(::CoreML::Specification::ReduceLogSumExpLayerParams* reducelogsumexp) {
  clear_layer();
  if (reducelogsumexp) {
    set_has_reducelogsumexp();
    layer_.reducelogsumexp_ = reducelogsumexp;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduceLogSumExp)
}

// .CoreML.Specification.WhereNonZeroLayerParams whereNonZero = 1313;
bool NeuralNetworkLayer::has_wherenonzero() const {
  return layer_case() == kWhereNonZero;
}
void NeuralNetworkLayer::set_has_wherenonzero() {
  _oneof_case_[0] = kWhereNonZero;
}
void NeuralNetworkLayer::clear_wherenonzero() {
  if (has_wherenonzero()) {
    delete layer_.wherenonzero_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::WhereNonZeroLayerParams& NeuralNetworkLayer::wherenonzero() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  return has_wherenonzero()
      ? *layer_.wherenonzero_
      : ::CoreML::Specification::WhereNonZeroLayerParams::default_instance();
}
::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::mutable_wherenonzero() {
  if (!has_wherenonzero()) {
    clear_layer();
    set_has_wherenonzero();
    layer_.wherenonzero_ = new ::CoreML::Specification::WhereNonZeroLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  return layer_.wherenonzero_;
}
::CoreML::Specification::WhereNonZeroLayerParams* NeuralNetworkLayer::release_wherenonzero() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
  if (has_wherenonzero()) {
    clear_has_layer();
    ::CoreML::Specification::WhereNonZeroLayerParams* temp = layer_.wherenonzero_;
    layer_.wherenonzero_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_wherenonzero(::CoreML::Specification::WhereNonZeroLayerParams* wherenonzero) {
  clear_layer();
  if (wherenonzero) {
    set_has_wherenonzero();
    layer_.wherenonzero_ = wherenonzero;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.whereNonZero)
}

// .CoreML.Specification.MatrixBandPartLayerParams matrixBandPart = 1315;
bool NeuralNetworkLayer::has_matrixbandpart() const {
  return layer_case() == kMatrixBandPart;
}
void NeuralNetworkLayer::set_has_matrixbandpart() {
  _oneof_case_[0] = kMatrixBandPart;
}
void NeuralNetworkLayer::clear_matrixbandpart() {
  if (has_matrixbandpart()) {
    delete layer_.matrixbandpart_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MatrixBandPartLayerParams& NeuralNetworkLayer::matrixbandpart() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  return has_matrixbandpart()
      ? *layer_.matrixbandpart_
      : ::CoreML::Specification::MatrixBandPartLayerParams::default_instance();
}
::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::mutable_matrixbandpart() {
  if (!has_matrixbandpart()) {
    clear_layer();
    set_has_matrixbandpart();
    layer_.matrixbandpart_ = new ::CoreML::Specification::MatrixBandPartLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  return layer_.matrixbandpart_;
}
::CoreML::Specification::MatrixBandPartLayerParams* NeuralNetworkLayer::release_matrixbandpart() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
  if (has_matrixbandpart()) {
    clear_has_layer();
    ::CoreML::Specification::MatrixBandPartLayerParams* temp = layer_.matrixbandpart_;
    layer_.matrixbandpart_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_matrixbandpart(::CoreML::Specification::MatrixBandPartLayerParams* matrixbandpart) {
  clear_layer();
  if (matrixbandpart) {
    set_has_matrixbandpart();
    layer_.matrixbandpart_ = matrixbandpart;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.matrixBandPart)
}

// .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
bool NeuralNetworkLayer::has_lowertriangular() const {
  return layer_case() == kLowerTriangular;
}
void NeuralNetworkLayer::set_has_lowertriangular() {
  _oneof_case_[0] = kLowerTriangular;
}
void NeuralNetworkLayer::clear_lowertriangular() {
  if (has_lowertriangular()) {
    delete layer_.lowertriangular_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LowerTriangularLayerParams& NeuralNetworkLayer::lowertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return has_lowertriangular()
      ? *layer_.lowertriangular_
      : ::CoreML::Specification::LowerTriangularLayerParams::default_instance();
}
::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::mutable_lowertriangular() {
  if (!has_lowertriangular()) {
    clear_layer();
    set_has_lowertriangular();
    layer_.lowertriangular_ = new ::CoreML::Specification::LowerTriangularLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return layer_.lowertriangular_;
}
::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::release_lowertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  if (has_lowertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::LowerTriangularLayerParams* temp = layer_.lowertriangular_;
    layer_.lowertriangular_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lowertriangular(::CoreML::Specification::LowerTriangularLayerParams* lowertriangular) {
  clear_layer();
  if (lowertriangular) {
    set_has_lowertriangular();
    layer_.lowertriangular_ = lowertriangular;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
}

// .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
bool NeuralNetworkLayer::has_uppertriangular() const {
  return layer_case() == kUpperTriangular;
}
void NeuralNetworkLayer::set_has_uppertriangular() {
  _oneof_case_[0] = kUpperTriangular;
}
void NeuralNetworkLayer::clear_uppertriangular() {
  if (has_uppertriangular()) {
    delete layer_.uppertriangular_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UpperTriangularLayerParams& NeuralNetworkLayer::uppertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return has_uppertriangular()
      ? *layer_.uppertriangular_
      : ::CoreML::Specification::UpperTriangularLayerParams::default_instance();
}
::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::mutable_uppertriangular() {
  if (!has_uppertriangular()) {
    clear_layer();
    set_has_uppertriangular();
    layer_.uppertriangular_ = new ::CoreML::Specification::UpperTriangularLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return layer_.uppertriangular_;
}
::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::release_uppertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  if (has_uppertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::UpperTriangularLayerParams* temp = layer_.uppertriangular_;
    layer_.uppertriangular_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_uppertriangular(::CoreML::Specification::UpperTriangularLayerParams* uppertriangular) {
  clear_layer();
  if (uppertriangular) {
    set_has_uppertriangular();
    layer_.uppertriangular_ = uppertriangular;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
}

// .CoreML.Specification.WhereBroadcastableLayerParams whereBroadcastable = 1330;
bool NeuralNetworkLayer::has_wherebroadcastable() const {
  return layer_case() == kWhereBroadcastable;
}
void NeuralNetworkLayer::set_has_wherebroadcastable() {
  _oneof_case_[0] = kWhereBroadcastable;
}
void NeuralNetworkLayer::clear_wherebroadcastable() {
  if (has_wherebroadcastable()) {
    delete layer_.wherebroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::WhereBroadcastableLayerParams& NeuralNetworkLayer::wherebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  return has_wherebroadcastable()
      ? *layer_.wherebroadcastable_
      : ::CoreML::Specification::WhereBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::mutable_wherebroadcastable() {
  if (!has_wherebroadcastable()) {
    clear_layer();
    set_has_wherebroadcastable();
    layer_.wherebroadcastable_ = new ::CoreML::Specification::WhereBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  return layer_.wherebroadcastable_;
}
::CoreML::Specification::WhereBroadcastableLayerParams* NeuralNetworkLayer::release_wherebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
  if (has_wherebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::WhereBroadcastableLayerParams* temp = layer_.wherebroadcastable_;
    layer_.wherebroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_wherebroadcastable(::CoreML::Specification::WhereBroadcastableLayerParams* wherebroadcastable) {
  clear_layer();
  if (wherebroadcastable) {
    set_has_wherebroadcastable();
    layer_.wherebroadcastable_ = wherebroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.whereBroadcastable)
}

// .CoreML.Specification.LayerNormalizationLayerParams layerNormalization = 1350;
bool NeuralNetworkLayer::has_layernormalization() const {
  return layer_case() == kLayerNormalization;
}
void NeuralNetworkLayer::set_has_layernormalization() {
  _oneof_case_[0] = kLayerNormalization;
}
void NeuralNetworkLayer::clear_layernormalization() {
  if (has_layernormalization()) {
    delete layer_.layernormalization_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LayerNormalizationLayerParams& NeuralNetworkLayer::layernormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  return has_layernormalization()
      ? *layer_.layernormalization_
      : ::CoreML::Specification::LayerNormalizationLayerParams::default_instance();
}
::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::mutable_layernormalization() {
  if (!has_layernormalization()) {
    clear_layer();
    set_has_layernormalization();
    layer_.layernormalization_ = new ::CoreML::Specification::LayerNormalizationLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  return layer_.layernormalization_;
}
::CoreML::Specification::LayerNormalizationLayerParams* NeuralNetworkLayer::release_layernormalization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
  if (has_layernormalization()) {
    clear_has_layer();
    ::CoreML::Specification::LayerNormalizationLayerParams* temp = layer_.layernormalization_;
    layer_.layernormalization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_layernormalization(::CoreML::Specification::LayerNormalizationLayerParams* layernormalization) {
  clear_layer();
  if (layernormalization) {
    set_has_layernormalization();
    layer_.layernormalization_ = layernormalization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.layerNormalization)
}

// .CoreML.Specification.NonMaximumSuppressionLayerParams NonMaximumSuppression = 1400;
bool NeuralNetworkLayer::has_nonmaximumsuppression() const {
  return layer_case() == kNonMaximumSuppression;
}
void NeuralNetworkLayer::set_has_nonmaximumsuppression() {
  _oneof_case_[0] = kNonMaximumSuppression;
}
void NeuralNetworkLayer::clear_nonmaximumsuppression() {
  if (has_nonmaximumsuppression()) {
    delete layer_.nonmaximumsuppression_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::NonMaximumSuppressionLayerParams& NeuralNetworkLayer::nonmaximumsuppression() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  return has_nonmaximumsuppression()
      ? *layer_.nonmaximumsuppression_
      : ::CoreML::Specification::NonMaximumSuppressionLayerParams::default_instance();
}
::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::mutable_nonmaximumsuppression() {
  if (!has_nonmaximumsuppression()) {
    clear_layer();
    set_has_nonmaximumsuppression();
    layer_.nonmaximumsuppression_ = new ::CoreML::Specification::NonMaximumSuppressionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  return layer_.nonmaximumsuppression_;
}
::CoreML::Specification::NonMaximumSuppressionLayerParams* NeuralNetworkLayer::release_nonmaximumsuppression() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
  if (has_nonmaximumsuppression()) {
    clear_has_layer();
    ::CoreML::Specification::NonMaximumSuppressionLayerParams* temp = layer_.nonmaximumsuppression_;
    layer_.nonmaximumsuppression_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_nonmaximumsuppression(::CoreML::Specification::NonMaximumSuppressionLayerParams* nonmaximumsuppression) {
  clear_layer();
  if (nonmaximumsuppression) {
    set_has_nonmaximumsuppression();
    layer_.nonmaximumsuppression_ = nonmaximumsuppression;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.NonMaximumSuppression)
}

bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BranchLayerParams::kIfBranchFieldNumber;
const int BranchLayerParams::kElseBranchFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BranchLayerParams::BranchLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BranchLayerParams)
}
BranchLayerParams::BranchLayerParams(const BranchLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_ifbranch()) {
    ifbranch_ = new ::CoreML::Specification::NeuralNetwork(*from.ifbranch_);
  } else {
    ifbranch_ = NULL;
  }
  if (from.has_elsebranch()) {
    elsebranch_ = new ::CoreML::Specification::NeuralNetwork(*from.elsebranch_);
  } else {
    elsebranch_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BranchLayerParams)
}

void BranchLayerParams::SharedCtor() {
  ::memset(&ifbranch_, 0, reinterpret_cast<char*>(&elsebranch_) -
    reinterpret_cast<char*>(&ifbranch_) + sizeof(elsebranch_));
  _cached_size_ = 0;
}

BranchLayerParams::~BranchLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BranchLayerParams)
  SharedDtor();
}

void BranchLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete ifbranch_;
  }
  if (this != internal_default_instance()) {
    delete elsebranch_;
  }
}

void BranchLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BranchLayerParams& BranchLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BranchLayerParams* BranchLayerParams::New(::google::protobuf::Arena* arena) const {
  BranchLayerParams* n = new BranchLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BranchLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BranchLayerParams)
  if (GetArenaNoVirtual() == NULL && ifbranch_ != NULL) {
    delete ifbranch_;
  }
  ifbranch_ = NULL;
  if (GetArenaNoVirtual() == NULL && elsebranch_ != NULL) {
    delete elsebranch_;
  }
  elsebranch_ = NULL;
}

bool BranchLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BranchLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.NeuralNetwork ifBranch = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_ifbranch()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetwork elseBranch = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_elsebranch()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BranchLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BranchLayerParams)
  return false;
#undef DO_
}

void BranchLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BranchLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.NeuralNetwork ifBranch = 1;
  if (this->has_ifbranch()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->ifbranch_, output);
  }

  // .CoreML.Specification.NeuralNetwork elseBranch = 2;
  if (this->has_elsebranch()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->elsebranch_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BranchLayerParams)
}

size_t BranchLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BranchLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.NeuralNetwork ifBranch = 1;
  if (this->has_ifbranch()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->ifbranch_);
  }

  // .CoreML.Specification.NeuralNetwork elseBranch = 2;
  if (this->has_elsebranch()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->elsebranch_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BranchLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BranchLayerParams*>(&from));
}

void BranchLayerParams::MergeFrom(const BranchLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BranchLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_ifbranch()) {
    mutable_ifbranch()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.ifbranch());
  }
  if (from.has_elsebranch()) {
    mutable_elsebranch()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.elsebranch());
  }
}

void BranchLayerParams::CopyFrom(const BranchLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BranchLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BranchLayerParams::IsInitialized() const {
  return true;
}

void BranchLayerParams::Swap(BranchLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BranchLayerParams::InternalSwap(BranchLayerParams* other) {
  std::swap(ifbranch_, other->ifbranch_);
  std::swap(elsebranch_, other->elsebranch_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BranchLayerParams::GetTypeName() const {
  return "CoreML.Specification.BranchLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BranchLayerParams

// .CoreML.Specification.NeuralNetwork ifBranch = 1;
bool BranchLayerParams::has_ifbranch() const {
  return this != internal_default_instance() && ifbranch_ != NULL;
}
void BranchLayerParams::clear_ifbranch() {
  if (GetArenaNoVirtual() == NULL && ifbranch_ != NULL) delete ifbranch_;
  ifbranch_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::ifbranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.ifBranch)
  return ifbranch_ != NULL ? *ifbranch_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_ifbranch() {
  
  if (ifbranch_ == NULL) {
    ifbranch_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.ifBranch)
  return ifbranch_;
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_ifbranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.ifBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = ifbranch_;
  ifbranch_ = NULL;
  return temp;
}
void BranchLayerParams::set_allocated_ifbranch(::CoreML::Specification::NeuralNetwork* ifbranch) {
  delete ifbranch_;
  ifbranch_ = ifbranch;
  if (ifbranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.ifBranch)
}

// .CoreML.Specification.NeuralNetwork elseBranch = 2;
bool BranchLayerParams::has_elsebranch() const {
  return this != internal_default_instance() && elsebranch_ != NULL;
}
void BranchLayerParams::clear_elsebranch() {
  if (GetArenaNoVirtual() == NULL && elsebranch_ != NULL) delete elsebranch_;
  elsebranch_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::elsebranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.elseBranch)
  return elsebranch_ != NULL ? *elsebranch_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_elsebranch() {
  
  if (elsebranch_ == NULL) {
    elsebranch_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.elseBranch)
  return elsebranch_;
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_elsebranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.elseBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = elsebranch_;
  elsebranch_ = NULL;
  return temp;
}
void BranchLayerParams::set_allocated_elsebranch(::CoreML::Specification::NeuralNetwork* elsebranch) {
  delete elsebranch_;
  elsebranch_ = elsebranch;
  if (elsebranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.elseBranch)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoopLayerParams::kMaxLoopIterationsFieldNumber;
const int LoopLayerParams::kConditionVarFieldNumber;
const int LoopLayerParams::kConditionNetworkFieldNumber;
const int LoopLayerParams::kBodyNetworkFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoopLayerParams::LoopLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoopLayerParams)
}
LoopLayerParams::LoopLayerParams(const LoopLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  conditionvar_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.conditionvar().size() > 0) {
    conditionvar_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.conditionvar_);
  }
  if (from.has_conditionnetwork()) {
    conditionnetwork_ = new ::CoreML::Specification::NeuralNetwork(*from.conditionnetwork_);
  } else {
    conditionnetwork_ = NULL;
  }
  if (from.has_bodynetwork()) {
    bodynetwork_ = new ::CoreML::Specification::NeuralNetwork(*from.bodynetwork_);
  } else {
    bodynetwork_ = NULL;
  }
  maxloopiterations_ = from.maxloopiterations_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoopLayerParams)
}

void LoopLayerParams::SharedCtor() {
  conditionvar_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&conditionnetwork_, 0, reinterpret_cast<char*>(&maxloopiterations_) -
    reinterpret_cast<char*>(&conditionnetwork_) + sizeof(maxloopiterations_));
  _cached_size_ = 0;
}

LoopLayerParams::~LoopLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoopLayerParams)
  SharedDtor();
}

void LoopLayerParams::SharedDtor() {
  conditionvar_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete conditionnetwork_;
  }
  if (this != internal_default_instance()) {
    delete bodynetwork_;
  }
}

void LoopLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoopLayerParams& LoopLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoopLayerParams* LoopLayerParams::New(::google::protobuf::Arena* arena) const {
  LoopLayerParams* n = new LoopLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoopLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoopLayerParams)
  conditionvar_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && conditionnetwork_ != NULL) {
    delete conditionnetwork_;
  }
  conditionnetwork_ = NULL;
  if (GetArenaNoVirtual() == NULL && bodynetwork_ != NULL) {
    delete bodynetwork_;
  }
  bodynetwork_ = NULL;
  maxloopiterations_ = GOOGLE_ULONGLONG(0);
}

bool LoopLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoopLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 maxLoopIterations = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &maxloopiterations_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string conditionVar = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_conditionvar()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->conditionvar().data(), this->conditionvar().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.LoopLayerParams.conditionVar"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_conditionnetwork()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bodynetwork()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoopLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoopLayerParams)
  return false;
#undef DO_
}

void LoopLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoopLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 maxLoopIterations = 1;
  if (this->maxloopiterations() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->maxloopiterations(), output);
  }

  // string conditionVar = 2;
  if (this->conditionvar().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->conditionvar().data(), this->conditionvar().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.LoopLayerParams.conditionVar");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->conditionvar(), output);
  }

  // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
  if (this->has_conditionnetwork()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->conditionnetwork_, output);
  }

  // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
  if (this->has_bodynetwork()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->bodynetwork_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoopLayerParams)
}

size_t LoopLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoopLayerParams)
  size_t total_size = 0;

  // string conditionVar = 2;
  if (this->conditionvar().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->conditionvar());
  }

  // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
  if (this->has_conditionnetwork()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->conditionnetwork_);
  }

  // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
  if (this->has_bodynetwork()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bodynetwork_);
  }

  // uint64 maxLoopIterations = 1;
  if (this->maxloopiterations() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->maxloopiterations());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoopLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoopLayerParams*>(&from));
}

void LoopLayerParams::MergeFrom(const LoopLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoopLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.conditionvar().size() > 0) {

    conditionvar_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.conditionvar_);
  }
  if (from.has_conditionnetwork()) {
    mutable_conditionnetwork()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.conditionnetwork());
  }
  if (from.has_bodynetwork()) {
    mutable_bodynetwork()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.bodynetwork());
  }
  if (from.maxloopiterations() != 0) {
    set_maxloopiterations(from.maxloopiterations());
  }
}

void LoopLayerParams::CopyFrom(const LoopLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoopLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoopLayerParams::IsInitialized() const {
  return true;
}

void LoopLayerParams::Swap(LoopLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoopLayerParams::InternalSwap(LoopLayerParams* other) {
  conditionvar_.Swap(&other->conditionvar_);
  std::swap(conditionnetwork_, other->conditionnetwork_);
  std::swap(bodynetwork_, other->bodynetwork_);
  std::swap(maxloopiterations_, other->maxloopiterations_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoopLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoopLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoopLayerParams

// uint64 maxLoopIterations = 1;
void LoopLayerParams::clear_maxloopiterations() {
  maxloopiterations_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 LoopLayerParams::maxloopiterations() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.maxLoopIterations)
  return maxloopiterations_;
}
void LoopLayerParams::set_maxloopiterations(::google::protobuf::uint64 value) {
  
  maxloopiterations_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.maxLoopIterations)
}

// string conditionVar = 2;
void LoopLayerParams::clear_conditionvar() {
  conditionvar_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& LoopLayerParams::conditionvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.GetNoArena();
}
void LoopLayerParams::set_conditionvar(const ::std::string& value) {
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.conditionVar)
}
#if LANG_CXX11
void LoopLayerParams::set_conditionvar(::std::string&& value) {
  
  conditionvar_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.LoopLayerParams.conditionVar)
}
#endif
void LoopLayerParams::set_conditionvar(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.LoopLayerParams.conditionVar)
}
void LoopLayerParams::set_conditionvar(const char* value, size_t size) {
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.LoopLayerParams.conditionVar)
}
::std::string* LoopLayerParams::mutable_conditionvar() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* LoopLayerParams::release_conditionvar() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionVar)
  
  return conditionvar_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void LoopLayerParams::set_allocated_conditionvar(::std::string* conditionvar) {
  if (conditionvar != NULL) {
    
  } else {
    
  }
  conditionvar_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), conditionvar);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionVar)
}

// .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
bool LoopLayerParams::has_conditionnetwork() const {
  return this != internal_default_instance() && conditionnetwork_ != NULL;
}
void LoopLayerParams::clear_conditionnetwork() {
  if (GetArenaNoVirtual() == NULL && conditionnetwork_ != NULL) delete conditionnetwork_;
  conditionnetwork_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::conditionnetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return conditionnetwork_ != NULL ? *conditionnetwork_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_conditionnetwork() {
  
  if (conditionnetwork_ == NULL) {
    conditionnetwork_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return conditionnetwork_;
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_conditionnetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = conditionnetwork_;
  conditionnetwork_ = NULL;
  return temp;
}
void LoopLayerParams::set_allocated_conditionnetwork(::CoreML::Specification::NeuralNetwork* conditionnetwork) {
  delete conditionnetwork_;
  conditionnetwork_ = conditionnetwork;
  if (conditionnetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionNetwork)
}

// .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
bool LoopLayerParams::has_bodynetwork() const {
  return this != internal_default_instance() && bodynetwork_ != NULL;
}
void LoopLayerParams::clear_bodynetwork() {
  if (GetArenaNoVirtual() == NULL && bodynetwork_ != NULL) delete bodynetwork_;
  bodynetwork_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::bodynetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return bodynetwork_ != NULL ? *bodynetwork_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_bodynetwork() {
  
  if (bodynetwork_ == NULL) {
    bodynetwork_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return bodynetwork_;
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_bodynetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.bodyNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = bodynetwork_;
  bodynetwork_ = NULL;
  return temp;
}
void LoopLayerParams::set_allocated_bodynetwork(::CoreML::Specification::NeuralNetwork* bodynetwork) {
  delete bodynetwork_;
  bodynetwork_ = bodynetwork;
  if (bodynetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.bodyNetwork)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoopBreakLayerParams::LoopBreakLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoopBreakLayerParams)
}
LoopBreakLayerParams::LoopBreakLayerParams(const LoopBreakLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoopBreakLayerParams)
}

void LoopBreakLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LoopBreakLayerParams::~LoopBreakLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoopBreakLayerParams)
  SharedDtor();
}

void LoopBreakLayerParams::SharedDtor() {
}

void LoopBreakLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoopBreakLayerParams& LoopBreakLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoopBreakLayerParams* LoopBreakLayerParams::New(::google::protobuf::Arena* arena) const {
  LoopBreakLayerParams* n = new LoopBreakLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoopBreakLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoopBreakLayerParams)
}

bool LoopBreakLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoopBreakLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoopBreakLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoopBreakLayerParams)
  return false;
#undef DO_
}

void LoopBreakLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoopBreakLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoopBreakLayerParams)
}

size_t LoopBreakLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoopBreakLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoopBreakLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoopBreakLayerParams*>(&from));
}

void LoopBreakLayerParams::MergeFrom(const LoopBreakLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoopBreakLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LoopBreakLayerParams::CopyFrom(const LoopBreakLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoopBreakLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoopBreakLayerParams::IsInitialized() const {
  return true;
}

void LoopBreakLayerParams::Swap(LoopBreakLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoopBreakLayerParams::InternalSwap(LoopBreakLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoopBreakLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoopBreakLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoopBreakLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoopContinueLayerParams::LoopContinueLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoopContinueLayerParams)
}
LoopContinueLayerParams::LoopContinueLayerParams(const LoopContinueLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoopContinueLayerParams)
}

void LoopContinueLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LoopContinueLayerParams::~LoopContinueLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoopContinueLayerParams)
  SharedDtor();
}

void LoopContinueLayerParams::SharedDtor() {
}

void LoopContinueLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoopContinueLayerParams& LoopContinueLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoopContinueLayerParams* LoopContinueLayerParams::New(::google::protobuf::Arena* arena) const {
  LoopContinueLayerParams* n = new LoopContinueLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoopContinueLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoopContinueLayerParams)
}

bool LoopContinueLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoopContinueLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoopContinueLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoopContinueLayerParams)
  return false;
#undef DO_
}

void LoopContinueLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoopContinueLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoopContinueLayerParams)
}

size_t LoopContinueLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoopContinueLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoopContinueLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoopContinueLayerParams*>(&from));
}

void LoopContinueLayerParams::MergeFrom(const LoopContinueLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoopContinueLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LoopContinueLayerParams::CopyFrom(const LoopContinueLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoopContinueLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoopContinueLayerParams::IsInitialized() const {
  return true;
}

void LoopContinueLayerParams::Swap(LoopContinueLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoopContinueLayerParams::InternalSwap(LoopContinueLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoopContinueLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoopContinueLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoopContinueLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CopyLayerParams::CopyLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CopyLayerParams)
}
CopyLayerParams::CopyLayerParams(const CopyLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CopyLayerParams)
}

void CopyLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CopyLayerParams::~CopyLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CopyLayerParams)
  SharedDtor();
}

void CopyLayerParams::SharedDtor() {
}

void CopyLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CopyLayerParams& CopyLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CopyLayerParams* CopyLayerParams::New(::google::protobuf::Arena* arena) const {
  CopyLayerParams* n = new CopyLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CopyLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CopyLayerParams)
}

bool CopyLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CopyLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CopyLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CopyLayerParams)
  return false;
#undef DO_
}

void CopyLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CopyLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CopyLayerParams)
}

size_t CopyLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CopyLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CopyLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CopyLayerParams*>(&from));
}

void CopyLayerParams::MergeFrom(const CopyLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CopyLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CopyLayerParams::CopyFrom(const CopyLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CopyLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CopyLayerParams::IsInitialized() const {
  return true;
}

void CopyLayerParams::Swap(CopyLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CopyLayerParams::InternalSwap(CopyLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CopyLayerParams::GetTypeName() const {
  return "CoreML.Specification.CopyLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CopyLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GreaterThanLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GreaterThanLayerParams::GreaterThanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GreaterThanLayerParams)
}
GreaterThanLayerParams::GreaterThanLayerParams(const GreaterThanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GreaterThanLayerParams)
}

void GreaterThanLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

GreaterThanLayerParams::~GreaterThanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GreaterThanLayerParams)
  SharedDtor();
}

void GreaterThanLayerParams::SharedDtor() {
}

void GreaterThanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GreaterThanLayerParams& GreaterThanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GreaterThanLayerParams* GreaterThanLayerParams::New(::google::protobuf::Arena* arena) const {
  GreaterThanLayerParams* n = new GreaterThanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GreaterThanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GreaterThanLayerParams)
  alpha_ = 0;
}

bool GreaterThanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GreaterThanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GreaterThanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GreaterThanLayerParams)
  return false;
#undef DO_
}

void GreaterThanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GreaterThanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GreaterThanLayerParams)
}

size_t GreaterThanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GreaterThanLayerParams)
  size_t total_size = 0;

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GreaterThanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GreaterThanLayerParams*>(&from));
}

void GreaterThanLayerParams::MergeFrom(const GreaterThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GreaterThanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void GreaterThanLayerParams::CopyFrom(const GreaterThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GreaterThanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GreaterThanLayerParams::IsInitialized() const {
  return true;
}

void GreaterThanLayerParams::Swap(GreaterThanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GreaterThanLayerParams::InternalSwap(GreaterThanLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GreaterThanLayerParams::GetTypeName() const {
  return "CoreML.Specification.GreaterThanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GreaterThanLayerParams

// float alpha = 2;
void GreaterThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
float GreaterThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterThanLayerParams.alpha)
  return alpha_;
}
void GreaterThanLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterThanLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GreaterEqualLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GreaterEqualLayerParams::GreaterEqualLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GreaterEqualLayerParams)
}
GreaterEqualLayerParams::GreaterEqualLayerParams(const GreaterEqualLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GreaterEqualLayerParams)
}

void GreaterEqualLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

GreaterEqualLayerParams::~GreaterEqualLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GreaterEqualLayerParams)
  SharedDtor();
}

void GreaterEqualLayerParams::SharedDtor() {
}

void GreaterEqualLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GreaterEqualLayerParams& GreaterEqualLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GreaterEqualLayerParams* GreaterEqualLayerParams::New(::google::protobuf::Arena* arena) const {
  GreaterEqualLayerParams* n = new GreaterEqualLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GreaterEqualLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GreaterEqualLayerParams)
  alpha_ = 0;
}

bool GreaterEqualLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GreaterEqualLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GreaterEqualLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GreaterEqualLayerParams)
  return false;
#undef DO_
}

void GreaterEqualLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GreaterEqualLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GreaterEqualLayerParams)
}

size_t GreaterEqualLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GreaterEqualLayerParams)
  size_t total_size = 0;

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GreaterEqualLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GreaterEqualLayerParams*>(&from));
}

void GreaterEqualLayerParams::MergeFrom(const GreaterEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GreaterEqualLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void GreaterEqualLayerParams::CopyFrom(const GreaterEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GreaterEqualLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GreaterEqualLayerParams::IsInitialized() const {
  return true;
}

void GreaterEqualLayerParams::Swap(GreaterEqualLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GreaterEqualLayerParams::InternalSwap(GreaterEqualLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GreaterEqualLayerParams::GetTypeName() const {
  return "CoreML.Specification.GreaterEqualLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GreaterEqualLayerParams

// float alpha = 2;
void GreaterEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
float GreaterEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterEqualLayerParams.alpha)
  return alpha_;
}
void GreaterEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterEqualLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LessThanLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LessThanLayerParams::LessThanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LessThanLayerParams)
}
LessThanLayerParams::LessThanLayerParams(const LessThanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LessThanLayerParams)
}

void LessThanLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

LessThanLayerParams::~LessThanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LessThanLayerParams)
  SharedDtor();
}

void LessThanLayerParams::SharedDtor() {
}

void LessThanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LessThanLayerParams& LessThanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LessThanLayerParams* LessThanLayerParams::New(::google::protobuf::Arena* arena) const {
  LessThanLayerParams* n = new LessThanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LessThanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LessThanLayerParams)
  alpha_ = 0;
}

bool LessThanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LessThanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LessThanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LessThanLayerParams)
  return false;
#undef DO_
}

void LessThanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LessThanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LessThanLayerParams)
}

size_t LessThanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LessThanLayerParams)
  size_t total_size = 0;

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LessThanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LessThanLayerParams*>(&from));
}

void LessThanLayerParams::MergeFrom(const LessThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LessThanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void LessThanLayerParams::CopyFrom(const LessThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LessThanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LessThanLayerParams::IsInitialized() const {
  return true;
}

void LessThanLayerParams::Swap(LessThanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LessThanLayerParams::InternalSwap(LessThanLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LessThanLayerParams::GetTypeName() const {
  return "CoreML.Specification.LessThanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LessThanLayerParams

// float alpha = 2;
void LessThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LessThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessThanLayerParams.alpha)
  return alpha_;
}
void LessThanLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessThanLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LessEqualLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LessEqualLayerParams::LessEqualLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LessEqualLayerParams)
}
LessEqualLayerParams::LessEqualLayerParams(const LessEqualLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LessEqualLayerParams)
}

void LessEqualLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

LessEqualLayerParams::~LessEqualLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LessEqualLayerParams)
  SharedDtor();
}

void LessEqualLayerParams::SharedDtor() {
}

void LessEqualLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LessEqualLayerParams& LessEqualLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LessEqualLayerParams* LessEqualLayerParams::New(::google::protobuf::Arena* arena) const {
  LessEqualLayerParams* n = new LessEqualLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LessEqualLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LessEqualLayerParams)
  alpha_ = 0;
}

bool LessEqualLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LessEqualLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LessEqualLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LessEqualLayerParams)
  return false;
#undef DO_
}

void LessEqualLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LessEqualLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LessEqualLayerParams)
}

size_t LessEqualLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LessEqualLayerParams)
  size_t total_size = 0;

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LessEqualLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LessEqualLayerParams*>(&from));
}

void LessEqualLayerParams::MergeFrom(const LessEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LessEqualLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void LessEqualLayerParams::CopyFrom(const LessEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LessEqualLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LessEqualLayerParams::IsInitialized() const {
  return true;
}

void LessEqualLayerParams::Swap(LessEqualLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LessEqualLayerParams::InternalSwap(LessEqualLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LessEqualLayerParams::GetTypeName() const {
  return "CoreML.Specification.LessEqualLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LessEqualLayerParams

// float alpha = 2;
void LessEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LessEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessEqualLayerParams.alpha)
  return alpha_;
}
void LessEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessEqualLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EqualLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EqualLayerParams::EqualLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EqualLayerParams)
}
EqualLayerParams::EqualLayerParams(const EqualLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EqualLayerParams)
}

void EqualLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

EqualLayerParams::~EqualLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EqualLayerParams)
  SharedDtor();
}

void EqualLayerParams::SharedDtor() {
}

void EqualLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EqualLayerParams& EqualLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EqualLayerParams* EqualLayerParams::New(::google::protobuf::Arena* arena) const {
  EqualLayerParams* n = new EqualLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EqualLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EqualLayerParams)
  alpha_ = 0;
}

bool EqualLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EqualLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EqualLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EqualLayerParams)
  return false;
#undef DO_
}

void EqualLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EqualLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EqualLayerParams)
}

size_t EqualLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EqualLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EqualLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EqualLayerParams*>(&from));
}

void EqualLayerParams::MergeFrom(const EqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EqualLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void EqualLayerParams::CopyFrom(const EqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EqualLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EqualLayerParams::IsInitialized() const {
  return true;
}

void EqualLayerParams::Swap(EqualLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EqualLayerParams::InternalSwap(EqualLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EqualLayerParams::GetTypeName() const {
  return "CoreML.Specification.EqualLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EqualLayerParams

// float alpha = 1;
void EqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
float EqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EqualLayerParams.alpha)
  return alpha_;
}
void EqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EqualLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NotEqualLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NotEqualLayerParams::NotEqualLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NotEqualLayerParams)
}
NotEqualLayerParams::NotEqualLayerParams(const NotEqualLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NotEqualLayerParams)
}

void NotEqualLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

NotEqualLayerParams::~NotEqualLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NotEqualLayerParams)
  SharedDtor();
}

void NotEqualLayerParams::SharedDtor() {
}

void NotEqualLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NotEqualLayerParams& NotEqualLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NotEqualLayerParams* NotEqualLayerParams::New(::google::protobuf::Arena* arena) const {
  NotEqualLayerParams* n = new NotEqualLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NotEqualLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NotEqualLayerParams)
  alpha_ = 0;
}

bool NotEqualLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NotEqualLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NotEqualLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NotEqualLayerParams)
  return false;
#undef DO_
}

void NotEqualLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NotEqualLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NotEqualLayerParams)
}

size_t NotEqualLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NotEqualLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NotEqualLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NotEqualLayerParams*>(&from));
}

void NotEqualLayerParams::MergeFrom(const NotEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NotEqualLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void NotEqualLayerParams::CopyFrom(const NotEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NotEqualLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NotEqualLayerParams::IsInitialized() const {
  return true;
}

void NotEqualLayerParams::Swap(NotEqualLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NotEqualLayerParams::InternalSwap(NotEqualLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NotEqualLayerParams::GetTypeName() const {
  return "CoreML.Specification.NotEqualLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NotEqualLayerParams

// float alpha = 1;
void NotEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
float NotEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NotEqualLayerParams.alpha)
  return alpha_;
}
void NotEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NotEqualLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalAndLayerParams::LogicalAndLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalAndLayerParams)
}
LogicalAndLayerParams::LogicalAndLayerParams(const LogicalAndLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalAndLayerParams)
}

void LogicalAndLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalAndLayerParams::~LogicalAndLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalAndLayerParams)
  SharedDtor();
}

void LogicalAndLayerParams::SharedDtor() {
}

void LogicalAndLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalAndLayerParams& LogicalAndLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalAndLayerParams* LogicalAndLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalAndLayerParams* n = new LogicalAndLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalAndLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalAndLayerParams)
}

bool LogicalAndLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalAndLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalAndLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalAndLayerParams)
  return false;
#undef DO_
}

void LogicalAndLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalAndLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalAndLayerParams)
}

size_t LogicalAndLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalAndLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalAndLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalAndLayerParams*>(&from));
}

void LogicalAndLayerParams::MergeFrom(const LogicalAndLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalAndLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalAndLayerParams::CopyFrom(const LogicalAndLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalAndLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalAndLayerParams::IsInitialized() const {
  return true;
}

void LogicalAndLayerParams::Swap(LogicalAndLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalAndLayerParams::InternalSwap(LogicalAndLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalAndLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalAndLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalAndLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalOrLayerParams::LogicalOrLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalOrLayerParams)
}
LogicalOrLayerParams::LogicalOrLayerParams(const LogicalOrLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalOrLayerParams)
}

void LogicalOrLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalOrLayerParams::~LogicalOrLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalOrLayerParams)
  SharedDtor();
}

void LogicalOrLayerParams::SharedDtor() {
}

void LogicalOrLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalOrLayerParams& LogicalOrLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalOrLayerParams* LogicalOrLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalOrLayerParams* n = new LogicalOrLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalOrLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalOrLayerParams)
}

bool LogicalOrLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalOrLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalOrLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalOrLayerParams)
  return false;
#undef DO_
}

void LogicalOrLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalOrLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalOrLayerParams)
}

size_t LogicalOrLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalOrLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalOrLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalOrLayerParams*>(&from));
}

void LogicalOrLayerParams::MergeFrom(const LogicalOrLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalOrLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalOrLayerParams::CopyFrom(const LogicalOrLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalOrLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalOrLayerParams::IsInitialized() const {
  return true;
}

void LogicalOrLayerParams::Swap(LogicalOrLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalOrLayerParams::InternalSwap(LogicalOrLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalOrLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalOrLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalOrLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalXorLayerParams::LogicalXorLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalXorLayerParams)
}
LogicalXorLayerParams::LogicalXorLayerParams(const LogicalXorLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalXorLayerParams)
}

void LogicalXorLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalXorLayerParams::~LogicalXorLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalXorLayerParams)
  SharedDtor();
}

void LogicalXorLayerParams::SharedDtor() {
}

void LogicalXorLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalXorLayerParams& LogicalXorLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalXorLayerParams* LogicalXorLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalXorLayerParams* n = new LogicalXorLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalXorLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalXorLayerParams)
}

bool LogicalXorLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalXorLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalXorLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalXorLayerParams)
  return false;
#undef DO_
}

void LogicalXorLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalXorLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalXorLayerParams)
}

size_t LogicalXorLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalXorLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalXorLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalXorLayerParams*>(&from));
}

void LogicalXorLayerParams::MergeFrom(const LogicalXorLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalXorLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalXorLayerParams::CopyFrom(const LogicalXorLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalXorLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalXorLayerParams::IsInitialized() const {
  return true;
}

void LogicalXorLayerParams::Swap(LogicalXorLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalXorLayerParams::InternalSwap(LogicalXorLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalXorLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalXorLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalXorLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalNotLayerParams::LogicalNotLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalNotLayerParams)
}
LogicalNotLayerParams::LogicalNotLayerParams(const LogicalNotLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalNotLayerParams)
}

void LogicalNotLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalNotLayerParams::~LogicalNotLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalNotLayerParams)
  SharedDtor();
}

void LogicalNotLayerParams::SharedDtor() {
}

void LogicalNotLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalNotLayerParams& LogicalNotLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalNotLayerParams* LogicalNotLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalNotLayerParams* n = new LogicalNotLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalNotLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalNotLayerParams)
}

bool LogicalNotLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalNotLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalNotLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalNotLayerParams)
  return false;
#undef DO_
}

void LogicalNotLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalNotLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalNotLayerParams)
}

size_t LogicalNotLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalNotLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalNotLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalNotLayerParams*>(&from));
}

void LogicalNotLayerParams::MergeFrom(const LogicalNotLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalNotLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalNotLayerParams::CopyFrom(const LogicalNotLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalNotLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalNotLayerParams::IsInitialized() const {
  return true;
}

void LogicalNotLayerParams::Swap(LogicalNotLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalNotLayerParams::InternalSwap(LogicalNotLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalNotLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalNotLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalNotLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts_EdgeSizes::kStartEdgeSizeFieldNumber;
const int BorderAmounts_EdgeSizes::kEndEdgeSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}
BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startedgesize_, &from.startedgesize_,
    reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}

void BorderAmounts_EdgeSizes::SharedCtor() {
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  _cached_size_ = 0;
}

BorderAmounts_EdgeSizes::~BorderAmounts_EdgeSizes() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts.EdgeSizes)
  SharedDtor();
}

void BorderAmounts_EdgeSizes::SharedDtor() {
}

void BorderAmounts_EdgeSizes::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts_EdgeSizes& BorderAmounts_EdgeSizes::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BorderAmounts_EdgeSizes* BorderAmounts_EdgeSizes::New(::google::protobuf::Arena* arena) const {
  BorderAmounts_EdgeSizes* n = new BorderAmounts_EdgeSizes;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts_EdgeSizes::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
}

bool BorderAmounts_EdgeSizes::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 startEdgeSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &startedgesize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 endEdgeSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &endedgesize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts.EdgeSizes)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts.EdgeSizes)
  return false;
#undef DO_
}

void BorderAmounts_EdgeSizes::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->startedgesize(), output);
  }

  // uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->endedgesize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts.EdgeSizes)
}

size_t BorderAmounts_EdgeSizes::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  size_t total_size = 0;

  // uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->startedgesize());
  }

  // uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->endedgesize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts_EdgeSizes::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts_EdgeSizes*>(&from));
}

void BorderAmounts_EdgeSizes::MergeFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startedgesize() != 0) {
    set_startedgesize(from.startedgesize());
  }
  if (from.endedgesize() != 0) {
    set_endedgesize(from.endedgesize());
  }
}

void BorderAmounts_EdgeSizes::CopyFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BorderAmounts_EdgeSizes::IsInitialized() const {
  return true;
}

void BorderAmounts_EdgeSizes::Swap(BorderAmounts_EdgeSizes* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts_EdgeSizes::InternalSwap(BorderAmounts_EdgeSizes* other) {
  std::swap(startedgesize_, other->startedgesize_);
  std::swap(endedgesize_, other->endedgesize_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts_EdgeSizes::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts.EdgeSizes";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts_EdgeSizes

// uint64 startEdgeSize = 1;
void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return startedgesize_;
}
void BorderAmounts_EdgeSizes::set_startedgesize(::google::protobuf::uint64 value) {
  
  startedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// uint64 endEdgeSize = 2;
void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return endedgesize_;
}
void BorderAmounts_EdgeSizes::set_endedgesize(::google::protobuf::uint64 value) {
  
  endedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts::kBorderAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts::BorderAmounts()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts)
}
BorderAmounts::BorderAmounts(const BorderAmounts& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      borderamounts_(from.borderamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts)
}

void BorderAmounts::SharedCtor() {
  _cached_size_ = 0;
}

BorderAmounts::~BorderAmounts() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts)
  SharedDtor();
}

void BorderAmounts::SharedDtor() {
}

void BorderAmounts::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts& BorderAmounts::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BorderAmounts* BorderAmounts::New(::google::protobuf::Arena* arena) const {
  BorderAmounts* n = new BorderAmounts;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts)
  borderamounts_.Clear();
}

bool BorderAmounts::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_borderamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts)
  return false;
#undef DO_
}

void BorderAmounts::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  for (unsigned int i = 0, n = this->borderamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->borderamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts)
}

size_t BorderAmounts::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts)
  size_t total_size = 0;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  {
    unsigned int count = this->borderamounts_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->borderamounts(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts*>(&from));
}

void BorderAmounts::MergeFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  borderamounts_.MergeFrom(from.borderamounts_);
}

void BorderAmounts::CopyFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BorderAmounts::IsInitialized() const {
  return true;
}

void BorderAmounts::Swap(BorderAmounts* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts::InternalSwap(BorderAmounts* other) {
  borderamounts_.InternalSwap(&other->borderamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
int BorderAmounts::borderamounts_size() const {
  return borderamounts_.size();
}
void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Get(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ValidPadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ValidPadding::ValidPadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ValidPadding)
}
ValidPadding::ValidPadding(const ValidPadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_paddingamounts()) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts(*from.paddingamounts_);
  } else {
    paddingamounts_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ValidPadding)
}

void ValidPadding::SharedCtor() {
  paddingamounts_ = NULL;
  _cached_size_ = 0;
}

ValidPadding::~ValidPadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ValidPadding)
  SharedDtor();
}

void ValidPadding::SharedDtor() {
  if (this != internal_default_instance()) {
    delete paddingamounts_;
  }
}

void ValidPadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ValidPadding& ValidPadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ValidPadding* ValidPadding::New(::google::protobuf::Arena* arena) const {
  ValidPadding* n = new ValidPadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ValidPadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ValidPadding)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) {
    delete paddingamounts_;
  }
  paddingamounts_ = NULL;
}

bool ValidPadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ValidPadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ValidPadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ValidPadding)
  return false;
#undef DO_
}

void ValidPadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ValidPadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ValidPadding)
}

size_t ValidPadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ValidPadding)
  size_t total_size = 0;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ValidPadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ValidPadding*>(&from));
}

void ValidPadding::MergeFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ValidPadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
}

void ValidPadding::CopyFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ValidPadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ValidPadding::IsInitialized() const {
  return true;
}

void ValidPadding::Swap(ValidPadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ValidPadding::InternalSwap(ValidPadding* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ValidPadding::GetTypeName() const {
  return "CoreML.Specification.ValidPadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ValidPadding

// .CoreML.Specification.BorderAmounts paddingAmounts = 1;
bool ValidPadding::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void ValidPadding::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SamePadding::kAsymmetryModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SamePadding::SamePadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SamePadding)
}
SamePadding::SamePadding(const SamePadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  asymmetrymode_ = from.asymmetrymode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SamePadding)
}

void SamePadding::SharedCtor() {
  asymmetrymode_ = 0;
  _cached_size_ = 0;
}

SamePadding::~SamePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SamePadding)
  SharedDtor();
}

void SamePadding::SharedDtor() {
}

void SamePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SamePadding& SamePadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SamePadding* SamePadding::New(::google::protobuf::Arena* arena) const {
  SamePadding* n = new SamePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SamePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SamePadding)
  asymmetrymode_ = 0;
}

bool SamePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SamePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_asymmetrymode(static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SamePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SamePadding)
  return false;
#undef DO_
}

void SamePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SamePadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->asymmetrymode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SamePadding)
}

size_t SamePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SamePadding)
  size_t total_size = 0;

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->asymmetrymode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SamePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SamePadding*>(&from));
}

void SamePadding::MergeFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SamePadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.asymmetrymode() != 0) {
    set_asymmetrymode(from.asymmetrymode());
  }
}

void SamePadding::CopyFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SamePadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SamePadding::IsInitialized() const {
  return true;
}

void SamePadding::Swap(SamePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SamePadding::InternalSwap(SamePadding* other) {
  std::swap(asymmetrymode_, other->asymmetrymode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SamePadding::GetTypeName() const {
  return "CoreML.Specification.SamePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SamePadding

// .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  
  asymmetrymode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SamplingMode::kSamplingMethodFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SamplingMode::SamplingMode()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SamplingMode)
}
SamplingMode::SamplingMode(const SamplingMode& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  samplingmethod_ = from.samplingmethod_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SamplingMode)
}

void SamplingMode::SharedCtor() {
  samplingmethod_ = 0;
  _cached_size_ = 0;
}

SamplingMode::~SamplingMode() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SamplingMode)
  SharedDtor();
}

void SamplingMode::SharedDtor() {
}

void SamplingMode::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SamplingMode& SamplingMode::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SamplingMode* SamplingMode::New(::google::protobuf::Arena* arena) const {
  SamplingMode* n = new SamplingMode;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SamplingMode::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SamplingMode)
  samplingmethod_ = 0;
}

bool SamplingMode::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SamplingMode)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_samplingmethod(static_cast< ::CoreML::Specification::SamplingMode_Method >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SamplingMode)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SamplingMode)
  return false;
#undef DO_
}

void SamplingMode::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SamplingMode)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
  if (this->samplingmethod() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->samplingmethod(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SamplingMode)
}

size_t SamplingMode::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SamplingMode)
  size_t total_size = 0;

  // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
  if (this->samplingmethod() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->samplingmethod());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SamplingMode::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SamplingMode*>(&from));
}

void SamplingMode::MergeFrom(const SamplingMode& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SamplingMode)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.samplingmethod() != 0) {
    set_samplingmethod(from.samplingmethod());
  }
}

void SamplingMode::CopyFrom(const SamplingMode& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SamplingMode)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SamplingMode::IsInitialized() const {
  return true;
}

void SamplingMode::Swap(SamplingMode* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SamplingMode::InternalSwap(SamplingMode* other) {
  std::swap(samplingmethod_, other->samplingmethod_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SamplingMode::GetTypeName() const {
  return "CoreML.Specification.SamplingMode";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SamplingMode

// .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
void SamplingMode::clear_samplingmethod() {
  samplingmethod_ = 0;
}
::CoreML::Specification::SamplingMode_Method SamplingMode::samplingmethod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamplingMode.samplingMethod)
  return static_cast< ::CoreML::Specification::SamplingMode_Method >(samplingmethod_);
}
void SamplingMode::set_samplingmethod(::CoreML::Specification::SamplingMode_Method value) {
  
  samplingmethod_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamplingMode.samplingMethod)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BoxCoordinatesMode::kBoxModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BoxCoordinatesMode::BoxCoordinatesMode()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BoxCoordinatesMode)
}
BoxCoordinatesMode::BoxCoordinatesMode(const BoxCoordinatesMode& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  boxmode_ = from.boxmode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BoxCoordinatesMode)
}

void BoxCoordinatesMode::SharedCtor() {
  boxmode_ = 0;
  _cached_size_ = 0;
}

BoxCoordinatesMode::~BoxCoordinatesMode() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BoxCoordinatesMode)
  SharedDtor();
}

void BoxCoordinatesMode::SharedDtor() {
}

void BoxCoordinatesMode::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BoxCoordinatesMode& BoxCoordinatesMode::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BoxCoordinatesMode* BoxCoordinatesMode::New(::google::protobuf::Arena* arena) const {
  BoxCoordinatesMode* n = new BoxCoordinatesMode;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BoxCoordinatesMode::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BoxCoordinatesMode)
  boxmode_ = 0;
}

bool BoxCoordinatesMode::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BoxCoordinatesMode)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_boxmode(static_cast< ::CoreML::Specification::BoxCoordinatesMode_Coordinates >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BoxCoordinatesMode)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BoxCoordinatesMode)
  return false;
#undef DO_
}

void BoxCoordinatesMode::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BoxCoordinatesMode)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
  if (this->boxmode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->boxmode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BoxCoordinatesMode)
}

size_t BoxCoordinatesMode::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BoxCoordinatesMode)
  size_t total_size = 0;

  // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
  if (this->boxmode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->boxmode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BoxCoordinatesMode::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BoxCoordinatesMode*>(&from));
}

void BoxCoordinatesMode::MergeFrom(const BoxCoordinatesMode& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BoxCoordinatesMode)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.boxmode() != 0) {
    set_boxmode(from.boxmode());
  }
}

void BoxCoordinatesMode::CopyFrom(const BoxCoordinatesMode& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BoxCoordinatesMode)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BoxCoordinatesMode::IsInitialized() const {
  return true;
}

void BoxCoordinatesMode::Swap(BoxCoordinatesMode* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BoxCoordinatesMode::InternalSwap(BoxCoordinatesMode* other) {
  std::swap(boxmode_, other->boxmode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BoxCoordinatesMode::GetTypeName() const {
  return "CoreML.Specification.BoxCoordinatesMode";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BoxCoordinatesMode

// .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
void BoxCoordinatesMode::clear_boxmode() {
  boxmode_ = 0;
}
::CoreML::Specification::BoxCoordinatesMode_Coordinates BoxCoordinatesMode::boxmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BoxCoordinatesMode.boxMode)
  return static_cast< ::CoreML::Specification::BoxCoordinatesMode_Coordinates >(boxmode_);
}
void BoxCoordinatesMode::set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value) {
  
  boxmode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BoxCoordinatesMode.boxMode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int WeightParams::kFloatValueFieldNumber;
const int WeightParams::kFloat16ValueFieldNumber;
const int WeightParams::kRawValueFieldNumber;
const int WeightParams::kQuantizationFieldNumber;
const int WeightParams::kIsUpdatableFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WeightParams::WeightParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WeightParams)
}
WeightParams::WeightParams(const WeightParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      floatvalue_(from.floatvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  float16value_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.float16value().size() > 0) {
    float16value_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.float16value_);
  }
  rawvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.rawvalue().size() > 0) {
    rawvalue_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.rawvalue_);
  }
  if (from.has_quantization()) {
    quantization_ = new ::CoreML::Specification::QuantizationParams(*from.quantization_);
  } else {
    quantization_ = NULL;
  }
  isupdatable_ = from.isupdatable_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WeightParams)
}

void WeightParams::SharedCtor() {
  float16value_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&quantization_, 0, reinterpret_cast<char*>(&isupdatable_) -
    reinterpret_cast<char*>(&quantization_) + sizeof(isupdatable_));
  _cached_size_ = 0;
}

WeightParams::~WeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WeightParams)
  SharedDtor();
}

void WeightParams::SharedDtor() {
  float16value_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete quantization_;
  }
}

void WeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WeightParams& WeightParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

WeightParams* WeightParams::New(::google::protobuf::Arena* arena) const {
  WeightParams* n = new WeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WeightParams)
  floatvalue_.Clear();
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) {
    delete quantization_;
  }
  quantization_ = NULL;
  isupdatable_ = false;
}

bool WeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bytes float16Value = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->mutable_float16value()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bytes rawValue = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->mutable_rawvalue()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.QuantizationParams quantization = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_quantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isUpdatable = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isupdatable_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WeightParams)
  return false;
#undef DO_
}

void WeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WeightParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->floatvalue().data(), this->floatvalue_size(), output);
  }

  // bytes float16Value = 2;
  if (this->float16value().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(
      2, this->float16value(), output);
  }

  // bytes rawValue = 30;
  if (this->rawvalue().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(
      30, this->rawvalue(), output);
  }

  // .CoreML.Specification.QuantizationParams quantization = 40;
  if (this->has_quantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->quantization_, output);
  }

  // bool isUpdatable = 50;
  if (this->isupdatable() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->isupdatable(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WeightParams)
}

size_t WeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WeightParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    unsigned int count = this->floatvalue_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bytes float16Value = 2;
  if (this->float16value().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::BytesSize(
        this->float16value());
  }

  // bytes rawValue = 30;
  if (this->rawvalue().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::BytesSize(
        this->rawvalue());
  }

  // .CoreML.Specification.QuantizationParams quantization = 40;
  if (this->has_quantization()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->quantization_);
  }

  // bool isUpdatable = 50;
  if (this->isupdatable() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WeightParams*>(&from));
}

void WeightParams::MergeFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WeightParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  floatvalue_.MergeFrom(from.floatvalue_);
  if (from.float16value().size() > 0) {

    float16value_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.float16value_);
  }
  if (from.rawvalue().size() > 0) {

    rawvalue_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.rawvalue_);
  }
  if (from.has_quantization()) {
    mutable_quantization()->::CoreML::Specification::QuantizationParams::MergeFrom(from.quantization());
  }
  if (from.isupdatable() != 0) {
    set_isupdatable(from.isupdatable());
  }
}

void WeightParams::CopyFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WeightParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool WeightParams::IsInitialized() const {
  return true;
}

void WeightParams::Swap(WeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WeightParams::InternalSwap(WeightParams* other) {
  floatvalue_.InternalSwap(&other->floatvalue_);
  float16value_.Swap(&other->float16value_);
  rawvalue_.Swap(&other->rawvalue_);
  std::swap(quantization_, other->quantization_);
  std::swap(isupdatable_, other->isupdatable_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WeightParams::GetTypeName() const {
  return "CoreML.Specification.WeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WeightParams

// repeated float floatValue = 1;
int WeightParams::floatvalue_size() const {
  return floatvalue_.size();
}
void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_.Get(index);
}
void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
void WeightParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return &floatvalue_;
}

// bytes float16Value = 2;
void WeightParams::clear_float16value() {
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& WeightParams::float16value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.float16Value)
  return float16value_.GetNoArena();
}
void WeightParams::set_float16value(const ::std::string& value) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.float16Value)
}
#if LANG_CXX11
void WeightParams::set_float16value(::std::string&& value) {
  
  float16value_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.float16Value)
}
#endif
void WeightParams::set_float16value(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.float16Value)
}
void WeightParams::set_float16value(const void* value, size_t size) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.float16Value)
}
::std::string* WeightParams::mutable_float16value() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.float16Value)
  return float16value_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* WeightParams::release_float16value() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.float16Value)
  
  return float16value_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void WeightParams::set_allocated_float16value(::std::string* float16value) {
  if (float16value != NULL) {
    
  } else {
    
  }
  float16value_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), float16value);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.float16Value)
}

// bytes rawValue = 30;
void WeightParams::clear_rawvalue() {
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& WeightParams::rawvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.GetNoArena();
}
void WeightParams::set_rawvalue(const ::std::string& value) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.rawValue)
}
#if LANG_CXX11
void WeightParams::set_rawvalue(::std::string&& value) {
  
  rawvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.rawValue)
}
#endif
void WeightParams::set_rawvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.rawValue)
}
void WeightParams::set_rawvalue(const void* value, size_t size) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.rawValue)
}
::std::string* WeightParams::mutable_rawvalue() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* WeightParams::release_rawvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.rawValue)
  
  return rawvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void WeightParams::set_allocated_rawvalue(::std::string* rawvalue) {
  if (rawvalue != NULL) {
    
  } else {
    
  }
  rawvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), rawvalue);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.rawValue)
}

// .CoreML.Specification.QuantizationParams quantization = 40;
bool WeightParams::has_quantization() const {
  return this != internal_default_instance() && quantization_ != NULL;
}
void WeightParams::clear_quantization() {
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) delete quantization_;
  quantization_ = NULL;
}
const ::CoreML::Specification::QuantizationParams& WeightParams::quantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.quantization)
  return quantization_ != NULL ? *quantization_
                         : *::CoreML::Specification::QuantizationParams::internal_default_instance();
}
::CoreML::Specification::QuantizationParams* WeightParams::mutable_quantization() {
  
  if (quantization_ == NULL) {
    quantization_ = new ::CoreML::Specification::QuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.quantization)
  return quantization_;
}
::CoreML::Specification::QuantizationParams* WeightParams::release_quantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.quantization)
  
  ::CoreML::Specification::QuantizationParams* temp = quantization_;
  quantization_ = NULL;
  return temp;
}
void WeightParams::set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization) {
  delete quantization_;
  quantization_ = quantization;
  if (quantization) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.quantization)
}

// bool isUpdatable = 50;
void WeightParams::clear_isupdatable() {
  isupdatable_ = false;
}
bool WeightParams::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.isUpdatable)
  return isupdatable_;
}
void WeightParams::set_isupdatable(bool value) {
  
  isupdatable_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.isUpdatable)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int QuantizationParams::kNumberOfBitsFieldNumber;
const int QuantizationParams::kLinearQuantizationFieldNumber;
const int QuantizationParams::kLookupTableQuantizationFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

QuantizationParams::QuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.QuantizationParams)
}
QuantizationParams::QuantizationParams(const QuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  numberofbits_ = from.numberofbits_;
  clear_has_QuantizationType();
  switch (from.QuantizationType_case()) {
    case kLinearQuantization: {
      mutable_linearquantization()->::CoreML::Specification::LinearQuantizationParams::MergeFrom(from.linearquantization());
      break;
    }
    case kLookupTableQuantization: {
      mutable_lookuptablequantization()->::CoreML::Specification::LookUpTableQuantizationParams::MergeFrom(from.lookuptablequantization());
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.QuantizationParams)
}

void QuantizationParams::SharedCtor() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
  clear_has_QuantizationType();
  _cached_size_ = 0;
}

QuantizationParams::~QuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.QuantizationParams)
  SharedDtor();
}

void QuantizationParams::SharedDtor() {
  if (has_QuantizationType()) {
    clear_QuantizationType();
  }
}

void QuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const QuantizationParams& QuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

QuantizationParams* QuantizationParams::New(::google::protobuf::Arena* arena) const {
  QuantizationParams* n = new QuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void QuantizationParams::clear_QuantizationType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.QuantizationParams)
  switch (QuantizationType_case()) {
    case kLinearQuantization: {
      delete QuantizationType_.linearquantization_;
      break;
    }
    case kLookupTableQuantization: {
      delete QuantizationType_.lookuptablequantization_;
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}


void QuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.QuantizationParams)
  numberofbits_ = GOOGLE_ULONGLONG(0);
  clear_QuantizationType();
}

bool QuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.QuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 numberOfBits = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &numberofbits_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
      case 101: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(810u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linearquantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
      case 102: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(818u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lookuptablequantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.QuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.QuantizationParams)
  return false;
#undef DO_
}

void QuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.QuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 numberOfBits = 1;
  if (this->numberofbits() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->numberofbits(), output);
  }

  // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
  if (has_linearquantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *QuantizationType_.linearquantization_, output);
  }

  // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
  if (has_lookuptablequantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      102, *QuantizationType_.lookuptablequantization_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.QuantizationParams)
}

size_t QuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.QuantizationParams)
  size_t total_size = 0;

  // uint64 numberOfBits = 1;
  if (this->numberofbits() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->numberofbits());
  }

  switch (QuantizationType_case()) {
    // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
    case kLinearQuantization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *QuantizationType_.linearquantization_);
      break;
    }
    // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
    case kLookupTableQuantization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *QuantizationType_.lookuptablequantization_);
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void QuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const QuantizationParams*>(&from));
}

void QuantizationParams::MergeFrom(const QuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.QuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.numberofbits() != 0) {
    set_numberofbits(from.numberofbits());
  }
  switch (from.QuantizationType_case()) {
    case kLinearQuantization: {
      mutable_linearquantization()->::CoreML::Specification::LinearQuantizationParams::MergeFrom(from.linearquantization());
      break;
    }
    case kLookupTableQuantization: {
      mutable_lookuptablequantization()->::CoreML::Specification::LookUpTableQuantizationParams::MergeFrom(from.lookuptablequantization());
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
}

void QuantizationParams::CopyFrom(const QuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.QuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool QuantizationParams::IsInitialized() const {
  return true;
}

void QuantizationParams::Swap(QuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void QuantizationParams::InternalSwap(QuantizationParams* other) {
  std::swap(numberofbits_, other->numberofbits_);
  std::swap(QuantizationType_, other->QuantizationType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string QuantizationParams::GetTypeName() const {
  return "CoreML.Specification.QuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// QuantizationParams

// uint64 numberOfBits = 1;
void QuantizationParams::clear_numberofbits() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 QuantizationParams::numberofbits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.numberOfBits)
  return numberofbits_;
}
void QuantizationParams::set_numberofbits(::google::protobuf::uint64 value) {
  
  numberofbits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.QuantizationParams.numberOfBits)
}

// .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
bool QuantizationParams::has_linearquantization() const {
  return QuantizationType_case() == kLinearQuantization;
}
void QuantizationParams::set_has_linearquantization() {
  _oneof_case_[0] = kLinearQuantization;
}
void QuantizationParams::clear_linearquantization() {
  if (has_linearquantization()) {
    delete QuantizationType_.linearquantization_;
    clear_has_QuantizationType();
  }
}
 const ::CoreML::Specification::LinearQuantizationParams& QuantizationParams::linearquantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.linearQuantization)
  return has_linearquantization()
      ? *QuantizationType_.linearquantization_
      : ::CoreML::Specification::LinearQuantizationParams::default_instance();
}
::CoreML::Specification::LinearQuantizationParams* QuantizationParams::mutable_linearquantization() {
  if (!has_linearquantization()) {
    clear_QuantizationType();
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = new ::CoreML::Specification::LinearQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.linearQuantization)
  return QuantizationType_.linearquantization_;
}
::CoreML::Specification::LinearQuantizationParams* QuantizationParams::release_linearquantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.linearQuantization)
  if (has_linearquantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LinearQuantizationParams* temp = QuantizationType_.linearquantization_;
    QuantizationType_.linearquantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void QuantizationParams::set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization) {
  clear_QuantizationType();
  if (linearquantization) {
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = linearquantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.linearQuantization)
}

// .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
bool QuantizationParams::has_lookuptablequantization() const {
  return QuantizationType_case() == kLookupTableQuantization;
}
void QuantizationParams::set_has_lookuptablequantization() {
  _oneof_case_[0] = kLookupTableQuantization;
}
void QuantizationParams::clear_lookuptablequantization() {
  if (has_lookuptablequantization()) {
    delete QuantizationType_.lookuptablequantization_;
    clear_has_QuantizationType();
  }
}
 const ::CoreML::Specification::LookUpTableQuantizationParams& QuantizationParams::lookuptablequantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return has_lookuptablequantization()
      ? *QuantizationType_.lookuptablequantization_
      : ::CoreML::Specification::LookUpTableQuantizationParams::default_instance();
}
::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::mutable_lookuptablequantization() {
  if (!has_lookuptablequantization()) {
    clear_QuantizationType();
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = new ::CoreML::Specification::LookUpTableQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return QuantizationType_.lookuptablequantization_;
}
::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::release_lookuptablequantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  if (has_lookuptablequantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LookUpTableQuantizationParams* temp = QuantizationType_.lookuptablequantization_;
    QuantizationType_.lookuptablequantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void QuantizationParams::set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization) {
  clear_QuantizationType();
  if (lookuptablequantization) {
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = lookuptablequantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.lookupTableQuantization)
}

bool QuantizationParams::has_QuantizationType() const {
  return QuantizationType_case() != QUANTIZATIONTYPE_NOT_SET;
}
void QuantizationParams::clear_has_QuantizationType() {
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}
QuantizationParams::QuantizationTypeCase QuantizationParams::QuantizationType_case() const {
  return QuantizationParams::QuantizationTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LinearQuantizationParams::kScaleFieldNumber;
const int LinearQuantizationParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LinearQuantizationParams::LinearQuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LinearQuantizationParams)
}
LinearQuantizationParams::LinearQuantizationParams(const LinearQuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      scale_(from.scale_),
      bias_(from.bias_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LinearQuantizationParams)
}

void LinearQuantizationParams::SharedCtor() {
  _cached_size_ = 0;
}

LinearQuantizationParams::~LinearQuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LinearQuantizationParams)
  SharedDtor();
}

void LinearQuantizationParams::SharedDtor() {
}

void LinearQuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LinearQuantizationParams& LinearQuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LinearQuantizationParams* LinearQuantizationParams::New(::google::protobuf::Arena* arena) const {
  LinearQuantizationParams* n = new LinearQuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LinearQuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LinearQuantizationParams)
  scale_.Clear();
  bias_.Clear();
}

bool LinearQuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LinearQuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float scale = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_scale())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_scale())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated float bias = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_bias())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(21u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 18u, input, this->mutable_bias())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LinearQuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LinearQuantizationParams)
  return false;
#undef DO_
}

void LinearQuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LinearQuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float scale = 1;
  if (this->scale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scale_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->scale().data(), this->scale_size(), output);
  }

  // repeated float bias = 2;
  if (this->bias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_bias_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->bias().data(), this->bias_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LinearQuantizationParams)
}

size_t LinearQuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LinearQuantizationParams)
  size_t total_size = 0;

  // repeated float scale = 1;
  {
    unsigned int count = this->scale_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated float bias = 2;
  {
    unsigned int count = this->bias_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _bias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LinearQuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LinearQuantizationParams*>(&from));
}

void LinearQuantizationParams::MergeFrom(const LinearQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LinearQuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  scale_.MergeFrom(from.scale_);
  bias_.MergeFrom(from.bias_);
}

void LinearQuantizationParams::CopyFrom(const LinearQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LinearQuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LinearQuantizationParams::IsInitialized() const {
  return true;
}

void LinearQuantizationParams::Swap(LinearQuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LinearQuantizationParams::InternalSwap(LinearQuantizationParams* other) {
  scale_.InternalSwap(&other->scale_);
  bias_.InternalSwap(&other->bias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LinearQuantizationParams::GetTypeName() const {
  return "CoreML.Specification.LinearQuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LinearQuantizationParams

// repeated float scale = 1;
int LinearQuantizationParams::scale_size() const {
  return scale_.size();
}
void LinearQuantizationParams::clear_scale() {
  scale_.Clear();
}
float LinearQuantizationParams::scale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_.Get(index);
}
void LinearQuantizationParams::set_scale(int index, float value) {
  scale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.scale)
}
void LinearQuantizationParams::add_scale(float value) {
  scale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.scale)
}
const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::scale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_;
}
::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_scale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.scale)
  return &scale_;
}

// repeated float bias = 2;
int LinearQuantizationParams::bias_size() const {
  return bias_.size();
}
void LinearQuantizationParams::clear_bias() {
  bias_.Clear();
}
float LinearQuantizationParams::bias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_.Get(index);
}
void LinearQuantizationParams::set_bias(int index, float value) {
  bias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.bias)
}
void LinearQuantizationParams::add_bias(float value) {
  bias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.bias)
}
const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::bias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_;
}
::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_bias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.bias)
  return &bias_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LookUpTableQuantizationParams::kFloatValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LookUpTableQuantizationParams::LookUpTableQuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LookUpTableQuantizationParams)
}
LookUpTableQuantizationParams::LookUpTableQuantizationParams(const LookUpTableQuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      floatvalue_(from.floatvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LookUpTableQuantizationParams)
}

void LookUpTableQuantizationParams::SharedCtor() {
  _cached_size_ = 0;
}

LookUpTableQuantizationParams::~LookUpTableQuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LookUpTableQuantizationParams)
  SharedDtor();
}

void LookUpTableQuantizationParams::SharedDtor() {
}

void LookUpTableQuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LookUpTableQuantizationParams& LookUpTableQuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LookUpTableQuantizationParams* LookUpTableQuantizationParams::New(::google::protobuf::Arena* arena) const {
  LookUpTableQuantizationParams* n = new LookUpTableQuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LookUpTableQuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LookUpTableQuantizationParams)
  floatvalue_.Clear();
}

bool LookUpTableQuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LookUpTableQuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LookUpTableQuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LookUpTableQuantizationParams)
  return false;
#undef DO_
}

void LookUpTableQuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LookUpTableQuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->floatvalue().data(), this->floatvalue_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LookUpTableQuantizationParams)
}

size_t LookUpTableQuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LookUpTableQuantizationParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    unsigned int count = this->floatvalue_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LookUpTableQuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LookUpTableQuantizationParams*>(&from));
}

void LookUpTableQuantizationParams::MergeFrom(const LookUpTableQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LookUpTableQuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  floatvalue_.MergeFrom(from.floatvalue_);
}

void LookUpTableQuantizationParams::CopyFrom(const LookUpTableQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LookUpTableQuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LookUpTableQuantizationParams::IsInitialized() const {
  return true;
}

void LookUpTableQuantizationParams::Swap(LookUpTableQuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LookUpTableQuantizationParams::InternalSwap(LookUpTableQuantizationParams* other) {
  floatvalue_.InternalSwap(&other->floatvalue_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LookUpTableQuantizationParams::GetTypeName() const {
  return "CoreML.Specification.LookUpTableQuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LookUpTableQuantizationParams

// repeated float floatValue = 1;
int LookUpTableQuantizationParams::floatvalue_size() const {
  return floatvalue_.size();
}
void LookUpTableQuantizationParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float LookUpTableQuantizationParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_.Get(index);
}
void LookUpTableQuantizationParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
void LookUpTableQuantizationParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
LookUpTableQuantizationParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
LookUpTableQuantizationParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return &floatvalue_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConvolutionLayerParams::kOutputChannelsFieldNumber;
const int ConvolutionLayerParams::kKernelChannelsFieldNumber;
const int ConvolutionLayerParams::kNGroupsFieldNumber;
const int ConvolutionLayerParams::kKernelSizeFieldNumber;
const int ConvolutionLayerParams::kStrideFieldNumber;
const int ConvolutionLayerParams::kDilationFactorFieldNumber;
const int ConvolutionLayerParams::kValidFieldNumber;
const int ConvolutionLayerParams::kSameFieldNumber;
const int ConvolutionLayerParams::kIsDeconvolutionFieldNumber;
const int ConvolutionLayerParams::kHasBiasFieldNumber;
const int ConvolutionLayerParams::kWeightsFieldNumber;
const int ConvolutionLayerParams::kBiasFieldNumber;
const int ConvolutionLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConvolutionLayerParams::ConvolutionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConvolutionLayerParams)
}
ConvolutionLayerParams::ConvolutionLayerParams(const ConvolutionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      kernelsize_(from.kernelsize_),
      stride_(from.stride_),
      dilationfactor_(from.dilationfactor_),
      outputshape_(from.outputshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&outputchannels_, &from.outputchannels_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConvolutionLayerParams)
}

void ConvolutionLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  _cached_size_ = 0;
}

ConvolutionLayerParams::~ConvolutionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConvolutionLayerParams)
  SharedDtor();
}

void ConvolutionLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
  if (has_ConvolutionPaddingType()) {
    clear_ConvolutionPaddingType();
  }
}

void ConvolutionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConvolutionLayerParams& ConvolutionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConvolutionLayerParams* ConvolutionLayerParams::New(::google::protobuf::Arena* arena) const {
  ConvolutionLayerParams* n = new ConvolutionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConvolutionLayerParams::clear_ConvolutionPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ConvolutionLayerParams)
  switch (ConvolutionPaddingType_case()) {
    case kValid: {
      delete ConvolutionPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete ConvolutionPaddingType_.same_;
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}


void ConvolutionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConvolutionLayerParams)
  kernelsize_.Clear();
  stride_.Clear();
  dilationfactor_.Clear();
  outputshape_.Clear();
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&outputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_ConvolutionPaddingType();
}

bool ConvolutionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConvolutionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 outputChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 kernelChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &kernelchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 nGroups = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &ngroups_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 kernelSize = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(160u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162u, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 stride = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(240u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 242u, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 dilationFactor = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_dilationfactor())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(320u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 322u, input, this->mutable_dilationfactor())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ValidPadding valid = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamePadding same = 51;
      case 51: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isDeconvolution = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(480u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isdeconvolution_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(560u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 90;
      case 90: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 91;
      case 91: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(730u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 outputShape = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(800u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 802u, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConvolutionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConvolutionLayerParams)
  return false;
#undef DO_
}

void ConvolutionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConvolutionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->outputchannels(), output);
  }

  // uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->kernelchannels(), output);
  }

  // uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(10, this->ngroups(), output);
  }

  // repeated uint64 kernelSize = 20;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0, n = this->kernelsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 30;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(30, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0, n = this->stride_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // repeated uint64 dilationFactor = 40;
  if (this->dilationfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(40, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_dilationfactor_cached_byte_size_);
  }
  for (int i = 0, n = this->dilationfactor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->dilationfactor(i), output);
  }

  // .CoreML.Specification.ValidPadding valid = 50;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *ConvolutionPaddingType_.valid_, output);
  }

  // .CoreML.Specification.SamePadding same = 51;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *ConvolutionPaddingType_.same_, output);
  }

  // bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->isdeconvolution(), output);
  }

  // bool hasBias = 70;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(70, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      90, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      91, *this->bias_, output);
  }

  // repeated uint64 outputShape = 100;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(100, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0, n = this->outputshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConvolutionLayerParams)
}

size_t ConvolutionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConvolutionLayerParams)
  size_t total_size = 0;

  // repeated uint64 kernelSize = 20;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->kernelsize_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 30;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->stride_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 dilationFactor = 40;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->dilationfactor_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _dilationfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 outputShape = 100;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->outputshape_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->kernelchannels());
  }

  // uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->ngroups());
  }

  // bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    total_size += 2 + 1;
  }

  // bool hasBias = 70;
  if (this->hasbias() != 0) {
    total_size += 2 + 1;
  }

  switch (ConvolutionPaddingType_case()) {
    // .CoreML.Specification.ValidPadding valid = 50;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.valid_);
      break;
    }
    // .CoreML.Specification.SamePadding same = 51;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.same_);
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConvolutionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConvolutionLayerParams*>(&from));
}

void ConvolutionLayerParams::MergeFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConvolutionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  kernelsize_.MergeFrom(from.kernelsize_);
  stride_.MergeFrom(from.stride_);
  dilationfactor_.MergeFrom(from.dilationfactor_);
  outputshape_.MergeFrom(from.outputshape_);
  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.kernelchannels() != 0) {
    set_kernelchannels(from.kernelchannels());
  }
  if (from.ngroups() != 0) {
    set_ngroups(from.ngroups());
  }
  if (from.isdeconvolution() != 0) {
    set_isdeconvolution(from.isdeconvolution());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void ConvolutionLayerParams::CopyFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConvolutionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConvolutionLayerParams::IsInitialized() const {
  return true;
}

void ConvolutionLayerParams::Swap(ConvolutionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConvolutionLayerParams::InternalSwap(ConvolutionLayerParams* other) {
  kernelsize_.InternalSwap(&other->kernelsize_);
  stride_.InternalSwap(&other->stride_);
  dilationfactor_.InternalSwap(&other->dilationfactor_);
  outputshape_.InternalSwap(&other->outputshape_);
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(kernelchannels_, other->kernelchannels_);
  std::swap(ngroups_, other->ngroups_);
  std::swap(isdeconvolution_, other->isdeconvolution_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(ConvolutionPaddingType_, other->ConvolutionPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConvolutionLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConvolutionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConvolutionLayerParams

// uint64 outputChannels = 1;
void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return outputchannels_;
}
void ConvolutionLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// uint64 kernelChannels = 2;
void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return kernelchannels_;
}
void ConvolutionLayerParams::set_kernelchannels(::google::protobuf::uint64 value) {
  
  kernelchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// uint64 nGroups = 10;
void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return ngroups_;
}
void ConvolutionLayerParams::set_ngroups(::google::protobuf::uint64 value) {
  
  ngroups_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
int ConvolutionLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void ConvolutionLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
void ConvolutionLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 30;
int ConvolutionLayerParams::stride_size() const {
  return stride_.size();
}
void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_.Get(index);
}
void ConvolutionLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
void ConvolutionLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return &stride_;
}

// repeated uint64 dilationFactor = 40;
int ConvolutionLayerParams::dilationfactor_size() const {
  return dilationfactor_.size();
}
void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_.Get(index);
}
void ConvolutionLayerParams::set_dilationfactor(int index, ::google::protobuf::uint64 value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
void ConvolutionLayerParams::add_dilationfactor(::google::protobuf::uint64 value) {
  dilationfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return &dilationfactor_;
}

// .CoreML.Specification.ValidPadding valid = 50;
bool ConvolutionLayerParams::has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void ConvolutionLayerParams::clear_valid() {
  if (has_valid()) {
    delete ConvolutionPaddingType_.valid_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return has_valid()
      ? *ConvolutionPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return ConvolutionPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 51;
bool ConvolutionLayerParams::has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void ConvolutionLayerParams::clear_same() {
  if (has_same()) {
    delete ConvolutionPaddingType_.same_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return has_same()
      ? *ConvolutionPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  if (!has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return ConvolutionPaddingType_.same_;
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}

// bool isDeconvolution = 60;
void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return isdeconvolution_;
}
void ConvolutionLayerParams::set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// bool hasBias = 70;
void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return hasbias_;
}
void ConvolutionLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 90;
bool ConvolutionLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void ConvolutionLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 91;
bool ConvolutionLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ConvolutionLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
int ConvolutionLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_.Get(index);
}
void ConvolutionLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
void ConvolutionLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return &outputshape_;
}

bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InnerProductLayerParams::kInputChannelsFieldNumber;
const int InnerProductLayerParams::kOutputChannelsFieldNumber;
const int InnerProductLayerParams::kHasBiasFieldNumber;
const int InnerProductLayerParams::kWeightsFieldNumber;
const int InnerProductLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InnerProductLayerParams::InnerProductLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.InnerProductLayerParams)
}
InnerProductLayerParams::InnerProductLayerParams(const InnerProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&inputchannels_, &from.inputchannels_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.InnerProductLayerParams)
}

void InnerProductLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

InnerProductLayerParams::~InnerProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.InnerProductLayerParams)
  SharedDtor();
}

void InnerProductLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void InnerProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const InnerProductLayerParams& InnerProductLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

InnerProductLayerParams* InnerProductLayerParams::New(::google::protobuf::Arena* arena) const {
  InnerProductLayerParams* n = new InnerProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void InnerProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.InnerProductLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&inputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
}

bool InnerProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.InnerProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.InnerProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.InnerProductLayerParams)
  return false;
#undef DO_
}

void InnerProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.InnerProductLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputchannels(), output);
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.InnerProductLayerParams)
}

size_t InnerProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.InnerProductLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputchannels());
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void InnerProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const InnerProductLayerParams*>(&from));
}

void InnerProductLayerParams::MergeFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.InnerProductLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.inputchannels() != 0) {
    set_inputchannels(from.inputchannels());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void InnerProductLayerParams::CopyFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.InnerProductLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InnerProductLayerParams::IsInitialized() const {
  return true;
}

void InnerProductLayerParams::Swap(InnerProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void InnerProductLayerParams::InternalSwap(InnerProductLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(inputchannels_, other->inputchannels_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string InnerProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.InnerProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// InnerProductLayerParams

// uint64 inputChannels = 1;
void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return inputchannels_;
}
void InnerProductLayerParams::set_inputchannels(::google::protobuf::uint64 value) {
  
  inputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// uint64 outputChannels = 2;
void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return outputchannels_;
}
void InnerProductLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// bool hasBias = 10;
void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return hasbias_;
}
void InnerProductLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool InnerProductLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void InnerProductLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool InnerProductLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void InnerProductLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EmbeddingLayerParams::kInputDimFieldNumber;
const int EmbeddingLayerParams::kOutputChannelsFieldNumber;
const int EmbeddingLayerParams::kHasBiasFieldNumber;
const int EmbeddingLayerParams::kWeightsFieldNumber;
const int EmbeddingLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EmbeddingLayerParams::EmbeddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EmbeddingLayerParams)
}
EmbeddingLayerParams::EmbeddingLayerParams(const EmbeddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&inputdim_, &from.inputdim_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EmbeddingLayerParams)
}

void EmbeddingLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

EmbeddingLayerParams::~EmbeddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EmbeddingLayerParams)
  SharedDtor();
}

void EmbeddingLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void EmbeddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EmbeddingLayerParams& EmbeddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EmbeddingLayerParams* EmbeddingLayerParams::New(::google::protobuf::Arena* arena) const {
  EmbeddingLayerParams* n = new EmbeddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EmbeddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EmbeddingLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&inputdim_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
}

bool EmbeddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EmbeddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputDim = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputdim_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EmbeddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EmbeddingLayerParams)
  return false;
#undef DO_
}

void EmbeddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EmbeddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputdim(), output);
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EmbeddingLayerParams)
}

size_t EmbeddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EmbeddingLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputdim());
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EmbeddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EmbeddingLayerParams*>(&from));
}

void EmbeddingLayerParams::MergeFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EmbeddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.inputdim() != 0) {
    set_inputdim(from.inputdim());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void EmbeddingLayerParams::CopyFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EmbeddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EmbeddingLayerParams::IsInitialized() const {
  return true;
}

void EmbeddingLayerParams::Swap(EmbeddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EmbeddingLayerParams::InternalSwap(EmbeddingLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(inputdim_, other->inputdim_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EmbeddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.EmbeddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EmbeddingLayerParams

// uint64 inputDim = 1;
void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return inputdim_;
}
void EmbeddingLayerParams::set_inputdim(::google::protobuf::uint64 value) {
  
  inputdim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// uint64 outputChannels = 2;
void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return outputchannels_;
}
void EmbeddingLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// bool hasBias = 10;
void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return hasbias_;
}
void EmbeddingLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool EmbeddingLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void EmbeddingLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool EmbeddingLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void EmbeddingLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EmbeddingNDLayerParams::kVocabSizeFieldNumber;
const int EmbeddingNDLayerParams::kEmbeddingSizeFieldNumber;
const int EmbeddingNDLayerParams::kHasBiasFieldNumber;
const int EmbeddingNDLayerParams::kWeightsFieldNumber;
const int EmbeddingNDLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EmbeddingNDLayerParams::EmbeddingNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EmbeddingNDLayerParams)
}
EmbeddingNDLayerParams::EmbeddingNDLayerParams(const EmbeddingNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&vocabsize_, &from.vocabsize_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&vocabsize_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EmbeddingNDLayerParams)
}

void EmbeddingNDLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

EmbeddingNDLayerParams::~EmbeddingNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EmbeddingNDLayerParams)
  SharedDtor();
}

void EmbeddingNDLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void EmbeddingNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EmbeddingNDLayerParams& EmbeddingNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EmbeddingNDLayerParams* EmbeddingNDLayerParams::New(::google::protobuf::Arena* arena) const {
  EmbeddingNDLayerParams* n = new EmbeddingNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EmbeddingNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EmbeddingNDLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&vocabsize_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&vocabsize_) + sizeof(hasbias_));
}

bool EmbeddingNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EmbeddingNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 vocabSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &vocabsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 embeddingSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &embeddingsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EmbeddingNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EmbeddingNDLayerParams)
  return false;
#undef DO_
}

void EmbeddingNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EmbeddingNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 vocabSize = 1;
  if (this->vocabsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->vocabsize(), output);
  }

  // uint64 embeddingSize = 2;
  if (this->embeddingsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->embeddingsize(), output);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EmbeddingNDLayerParams)
}

size_t EmbeddingNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EmbeddingNDLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 vocabSize = 1;
  if (this->vocabsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->vocabsize());
  }

  // uint64 embeddingSize = 2;
  if (this->embeddingsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->embeddingsize());
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EmbeddingNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EmbeddingNDLayerParams*>(&from));
}

void EmbeddingNDLayerParams::MergeFrom(const EmbeddingNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EmbeddingNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.vocabsize() != 0) {
    set_vocabsize(from.vocabsize());
  }
  if (from.embeddingsize() != 0) {
    set_embeddingsize(from.embeddingsize());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void EmbeddingNDLayerParams::CopyFrom(const EmbeddingNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EmbeddingNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EmbeddingNDLayerParams::IsInitialized() const {
  return true;
}

void EmbeddingNDLayerParams::Swap(EmbeddingNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EmbeddingNDLayerParams::InternalSwap(EmbeddingNDLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(vocabsize_, other->vocabsize_);
  std::swap(embeddingsize_, other->embeddingsize_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EmbeddingNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.EmbeddingNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EmbeddingNDLayerParams

// uint64 vocabSize = 1;
void EmbeddingNDLayerParams::clear_vocabsize() {
  vocabsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingNDLayerParams::vocabsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
  return vocabsize_;
}
void EmbeddingNDLayerParams::set_vocabsize(::google::protobuf::uint64 value) {
  
  vocabsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
}

// uint64 embeddingSize = 2;
void EmbeddingNDLayerParams::clear_embeddingsize() {
  embeddingsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingNDLayerParams::embeddingsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
  return embeddingsize_;
}
void EmbeddingNDLayerParams::set_embeddingsize(::google::protobuf::uint64 value) {
  
  embeddingsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
}

// bool hasBias = 3;
void EmbeddingNDLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool EmbeddingNDLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
  return hasbias_;
}
void EmbeddingNDLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool EmbeddingNDLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void EmbeddingNDLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void EmbeddingNDLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool EmbeddingNDLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void EmbeddingNDLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void EmbeddingNDLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchnormLayerParams::kChannelsFieldNumber;
const int BatchnormLayerParams::kComputeMeanVarFieldNumber;
const int BatchnormLayerParams::kInstanceNormalizationFieldNumber;
const int BatchnormLayerParams::kEpsilonFieldNumber;
const int BatchnormLayerParams::kGammaFieldNumber;
const int BatchnormLayerParams::kBetaFieldNumber;
const int BatchnormLayerParams::kMeanFieldNumber;
const int BatchnormLayerParams::kVarianceFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchnormLayerParams::BatchnormLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BatchnormLayerParams)
}
BatchnormLayerParams::BatchnormLayerParams(const BatchnormLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_gamma()) {
    gamma_ = new ::CoreML::Specification::WeightParams(*from.gamma_);
  } else {
    gamma_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  if (from.has_mean()) {
    mean_ = new ::CoreML::Specification::WeightParams(*from.mean_);
  } else {
    mean_ = NULL;
  }
  if (from.has_variance()) {
    variance_ = new ::CoreML::Specification::WeightParams(*from.variance_);
  } else {
    variance_ = NULL;
  }
  ::memcpy(&channels_, &from.channels_,
    reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BatchnormLayerParams)
}

void BatchnormLayerParams::SharedCtor() {
  ::memset(&gamma_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&gamma_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

BatchnormLayerParams::~BatchnormLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BatchnormLayerParams)
  SharedDtor();
}

void BatchnormLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete gamma_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
  if (this != internal_default_instance()) {
    delete mean_;
  }
  if (this != internal_default_instance()) {
    delete variance_;
  }
}

void BatchnormLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BatchnormLayerParams& BatchnormLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BatchnormLayerParams* BatchnormLayerParams::New(::google::protobuf::Arena* arena) const {
  BatchnormLayerParams* n = new BatchnormLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BatchnormLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BatchnormLayerParams)
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) {
    delete gamma_;
  }
  gamma_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) {
    delete mean_;
  }
  mean_ = NULL;
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) {
    delete variance_;
  }
  variance_ = NULL;
  ::memset(&channels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
}

bool BatchnormLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BatchnormLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 channels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &channels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool computeMeanVar = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &computemeanvar_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool instanceNormalization = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &instancenormalization_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(85u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams gamma = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gamma()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 16;
      case 16: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(130u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams mean = 17;
      case 17: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(138u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mean()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams variance = 18;
      case 18: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(146u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_variance()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BatchnormLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BatchnormLayerParams)
  return false;
#undef DO_
}

void BatchnormLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BatchnormLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 channels = 1;
  if (this->channels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->channels(), output);
  }

  // bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(5, this->computemeanvar(), output);
  }

  // bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(6, this->instancenormalization(), output);
  }

  // float epsilon = 10;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->epsilon(), output);
  }

  // .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->gamma_, output);
  }

  // .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      16, *this->beta_, output);
  }

  // .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      17, *this->mean_, output);
  }

  // .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      18, *this->variance_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BatchnormLayerParams)
}

size_t BatchnormLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BatchnormLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->gamma_);
  }

  // .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  // .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mean_);
  }

  // .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->variance_);
  }

  // uint64 channels = 1;
  if (this->channels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->channels());
  }

  // bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    total_size += 1 + 1;
  }

  // bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    total_size += 1 + 1;
  }

  // float epsilon = 10;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BatchnormLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BatchnormLayerParams*>(&from));
}

void BatchnormLayerParams::MergeFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BatchnormLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_gamma()) {
    mutable_gamma()->::CoreML::Specification::WeightParams::MergeFrom(from.gamma());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
  if (from.has_mean()) {
    mutable_mean()->::CoreML::Specification::WeightParams::MergeFrom(from.mean());
  }
  if (from.has_variance()) {
    mutable_variance()->::CoreML::Specification::WeightParams::MergeFrom(from.variance());
  }
  if (from.channels() != 0) {
    set_channels(from.channels());
  }
  if (from.computemeanvar() != 0) {
    set_computemeanvar(from.computemeanvar());
  }
  if (from.instancenormalization() != 0) {
    set_instancenormalization(from.instancenormalization());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void BatchnormLayerParams::CopyFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BatchnormLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchnormLayerParams::IsInitialized() const {
  return true;
}

void BatchnormLayerParams::Swap(BatchnormLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchnormLayerParams::InternalSwap(BatchnormLayerParams* other) {
  std::swap(gamma_, other->gamma_);
  std::swap(beta_, other->beta_);
  std::swap(mean_, other->mean_);
  std::swap(variance_, other->variance_);
  std::swap(channels_, other->channels_);
  std::swap(computemeanvar_, other->computemeanvar_);
  std::swap(instancenormalization_, other->instancenormalization_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BatchnormLayerParams::GetTypeName() const {
  return "CoreML.Specification.BatchnormLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BatchnormLayerParams

// uint64 channels = 1;
void BatchnormLayerParams::clear_channels() {
  channels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return channels_;
}
void BatchnormLayerParams::set_channels(::google::protobuf::uint64 value) {
  
  channels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// bool computeMeanVar = 5;
void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return computemeanvar_;
}
void BatchnormLayerParams::set_computemeanvar(bool value) {
  
  computemeanvar_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// bool instanceNormalization = 6;
void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return instancenormalization_;
}
void BatchnormLayerParams::set_instancenormalization(bool value) {
  
  instancenormalization_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// float epsilon = 10;
void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return epsilon_;
}
void BatchnormLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// .CoreML.Specification.WeightParams gamma = 15;
bool BatchnormLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
void BatchnormLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 16;
bool BatchnormLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void BatchnormLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// .CoreML.Specification.WeightParams mean = 17;
bool BatchnormLayerParams::has_mean() const {
  return this != internal_default_instance() && mean_ != NULL;
}
void BatchnormLayerParams::clear_mean() {
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_ != NULL ? *mean_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {
  
  if (mean_ == NULL) {
    mean_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  delete mean_;
  mean_ = mean;
  if (mean) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// .CoreML.Specification.WeightParams variance = 18;
bool BatchnormLayerParams::has_variance() const {
  return this != internal_default_instance() && variance_ != NULL;
}
void BatchnormLayerParams::clear_variance() {
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_ != NULL ? *variance_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {
  
  if (variance_ == NULL) {
    variance_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  delete variance_;
  variance_ = variance;
  if (variance) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams_ValidCompletePadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}
PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      paddingamounts_(from.paddingamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

void PoolingLayerParams_ValidCompletePadding::SharedCtor() {
  _cached_size_ = 0;
}

PoolingLayerParams_ValidCompletePadding::~PoolingLayerParams_ValidCompletePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  SharedDtor();
}

void PoolingLayerParams_ValidCompletePadding::SharedDtor() {
}

void PoolingLayerParams_ValidCompletePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams_ValidCompletePadding& PoolingLayerParams_ValidCompletePadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PoolingLayerParams_ValidCompletePadding* PoolingLayerParams_ValidCompletePadding::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams_ValidCompletePadding* n = new PoolingLayerParams_ValidCompletePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams_ValidCompletePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  paddingamounts_.Clear();
}

bool PoolingLayerParams_ValidCompletePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 paddingAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_paddingamounts())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(80u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82u, input, this->mutable_paddingamounts())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return false;
#undef DO_
}

void PoolingLayerParams_ValidCompletePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 paddingAmounts = 10;
  if (this->paddingamounts_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_paddingamounts_cached_byte_size_);
  }
  for (int i = 0, n = this->paddingamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->paddingamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

size_t PoolingLayerParams_ValidCompletePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  size_t total_size = 0;

  // repeated uint64 paddingAmounts = 10;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->paddingamounts_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _paddingamounts_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams_ValidCompletePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams_ValidCompletePadding*>(&from));
}

void PoolingLayerParams_ValidCompletePadding::MergeFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  paddingamounts_.MergeFrom(from.paddingamounts_);
}

void PoolingLayerParams_ValidCompletePadding::CopyFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PoolingLayerParams_ValidCompletePadding::IsInitialized() const {
  return true;
}

void PoolingLayerParams_ValidCompletePadding::Swap(PoolingLayerParams_ValidCompletePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams_ValidCompletePadding::InternalSwap(PoolingLayerParams_ValidCompletePadding* other) {
  paddingamounts_.InternalSwap(&other->paddingamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams_ValidCompletePadding::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams.ValidCompletePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return paddingamounts_.size();
}
void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_.Get(index);
}
void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, ::google::protobuf::uint64 value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(::google::protobuf::uint64 value) {
  paddingamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return &paddingamounts_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams::kTypeFieldNumber;
const int PoolingLayerParams::kKernelSizeFieldNumber;
const int PoolingLayerParams::kStrideFieldNumber;
const int PoolingLayerParams::kValidFieldNumber;
const int PoolingLayerParams::kSameFieldNumber;
const int PoolingLayerParams::kIncludeLastPixelFieldNumber;
const int PoolingLayerParams::kAvgPoolExcludePaddingFieldNumber;
const int PoolingLayerParams::kGlobalPoolingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams::PoolingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams)
}
PoolingLayerParams::PoolingLayerParams(const PoolingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      kernelsize_(from.kernelsize_),
      stride_(from.stride_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&type_, &from.type_,
    reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams)
}

void PoolingLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  _cached_size_ = 0;
}

PoolingLayerParams::~PoolingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams)
  SharedDtor();
}

void PoolingLayerParams::SharedDtor() {
  if (has_PoolingPaddingType()) {
    clear_PoolingPaddingType();
  }
}

void PoolingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams& PoolingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PoolingLayerParams* PoolingLayerParams::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams* n = new PoolingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams::clear_PoolingPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PoolingLayerParams)
  switch (PoolingPaddingType_case()) {
    case kValid: {
      delete PoolingPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete PoolingPaddingType_.same_;
      break;
    }
    case kIncludeLastPixel: {
      delete PoolingPaddingType_.includelastpixel_;
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}


void PoolingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams)
  kernelsize_.Clear();
  stride_.Clear();
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_PoolingPaddingType();
}

bool PoolingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 kernelSize = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(80u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82u, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 stride = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(160u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162u, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ValidPadding valid = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamePadding same = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_includelastpixel()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool avgPoolExcludePadding = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &avgpoolexcludepadding_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool globalPooling = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(480u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &globalpooling_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams)
  return false;
#undef DO_
}

void PoolingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // repeated uint64 kernelSize = 10;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0, n = this->kernelsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 20;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0, n = this->stride_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // .CoreML.Specification.ValidPadding valid = 30;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *PoolingPaddingType_.valid_, output);
  }

  // .CoreML.Specification.SamePadding same = 31;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *PoolingPaddingType_.same_, output);
  }

  // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  if (has_includelastpixel()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *PoolingPaddingType_.includelastpixel_, output);
  }

  // bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->avgpoolexcludepadding(), output);
  }

  // bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->globalpooling(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams)
}

size_t PoolingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams)
  size_t total_size = 0;

  // repeated uint64 kernelSize = 10;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->kernelsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 20;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->stride_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    total_size += 2 + 1;
  }

  // bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    total_size += 2 + 1;
  }

  switch (PoolingPaddingType_case()) {
    // .CoreML.Specification.ValidPadding valid = 30;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.valid_);
      break;
    }
    // .CoreML.Specification.SamePadding same = 31;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.same_);
      break;
    }
    // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
    case kIncludeLastPixel: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.includelastpixel_);
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams*>(&from));
}

void PoolingLayerParams::MergeFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  kernelsize_.MergeFrom(from.kernelsize_);
  stride_.MergeFrom(from.stride_);
  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.avgpoolexcludepadding() != 0) {
    set_avgpoolexcludepadding(from.avgpoolexcludepadding());
  }
  if (from.globalpooling() != 0) {
    set_globalpooling(from.globalpooling());
  }
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void PoolingLayerParams::CopyFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PoolingLayerParams::IsInitialized() const {
  return true;
}

void PoolingLayerParams::Swap(PoolingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams::InternalSwap(PoolingLayerParams* other) {
  kernelsize_.InternalSwap(&other->kernelsize_);
  stride_.InternalSwap(&other->stride_);
  std::swap(type_, other->type_);
  std::swap(avgpoolexcludepadding_, other->avgpoolexcludepadding_);
  std::swap(globalpooling_, other->globalpooling_);
  std::swap(PoolingPaddingType_, other->PoolingPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams

// .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
void PoolingLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
int PoolingLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void PoolingLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
void PoolingLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 20;
int PoolingLayerParams::stride_size() const {
  return stride_.size();
}
void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return stride_.Get(index);
}
void PoolingLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
void PoolingLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return &stride_;
}

// .CoreML.Specification.ValidPadding valid = 30;
bool PoolingLayerParams::has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void PoolingLayerParams::clear_valid() {
  if (has_valid()) {
    delete PoolingPaddingType_.valid_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return has_valid()
      ? *PoolingPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return PoolingPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 31;
bool PoolingLayerParams::has_same() const {
  return PoolingPaddingType_case() == kSame;
}
void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void PoolingLayerParams::clear_same() {
  if (has_same()) {
    delete PoolingPaddingType_.same_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return has_same()
      ? *PoolingPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  if (!has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return PoolingPaddingType_.same_;
}
::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}

// .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
bool PoolingLayerParams::has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
void PoolingLayerParams::clear_includelastpixel() {
  if (has_includelastpixel()) {
    delete PoolingPaddingType_.includelastpixel_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::default_instance();
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  if (!has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = new ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return PoolingPaddingType_.includelastpixel_;
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}

// bool avgPoolExcludePadding = 50;
void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return avgpoolexcludepadding_;
}
void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {
  
  avgpoolexcludepadding_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// bool globalPooling = 60;
void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return globalpooling_;
}
void PoolingLayerParams::set_globalpooling(bool value) {
  
  globalpooling_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams_PaddingConstant::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}
PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

void PaddingLayerParams_PaddingConstant::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingConstant::~PaddingLayerParams_PaddingConstant() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  SharedDtor();
}

void PaddingLayerParams_PaddingConstant::SharedDtor() {
}

void PaddingLayerParams_PaddingConstant::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingConstant& PaddingLayerParams_PaddingConstant::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingConstant* PaddingLayerParams_PaddingConstant::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingConstant* n = new PaddingLayerParams_PaddingConstant;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingConstant::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  value_ = 0;
}

bool PaddingLayerParams_PaddingConstant::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingConstant::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

size_t PaddingLayerParams_PaddingConstant::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingConstant::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingConstant*>(&from));
}

void PaddingLayerParams_PaddingConstant::MergeFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void PaddingLayerParams_PaddingConstant::CopyFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingConstant::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingConstant::Swap(PaddingLayerParams_PaddingConstant* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingConstant::InternalSwap(PaddingLayerParams_PaddingConstant* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingConstant::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingConstant";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingConstant

// float value = 1;
void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return value_;
}
void PaddingLayerParams_PaddingConstant::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}
PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

void PaddingLayerParams_PaddingReflection::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReflection::~PaddingLayerParams_PaddingReflection() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  SharedDtor();
}

void PaddingLayerParams_PaddingReflection::SharedDtor() {
}

void PaddingLayerParams_PaddingReflection::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReflection& PaddingLayerParams_PaddingReflection::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingReflection* PaddingLayerParams_PaddingReflection::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReflection* n = new PaddingLayerParams_PaddingReflection;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReflection::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

bool PaddingLayerParams_PaddingReflection::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReflection::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

size_t PaddingLayerParams_PaddingReflection::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReflection::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReflection*>(&from));
}

void PaddingLayerParams_PaddingReflection::MergeFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PaddingLayerParams_PaddingReflection::CopyFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingReflection::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingReflection::Swap(PaddingLayerParams_PaddingReflection* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReflection::InternalSwap(PaddingLayerParams_PaddingReflection* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReflection::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReflection";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingReflection

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}
PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

void PaddingLayerParams_PaddingReplication::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReplication::~PaddingLayerParams_PaddingReplication() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  SharedDtor();
}

void PaddingLayerParams_PaddingReplication::SharedDtor() {
}

void PaddingLayerParams_PaddingReplication::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReplication& PaddingLayerParams_PaddingReplication::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingReplication* PaddingLayerParams_PaddingReplication::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReplication* n = new PaddingLayerParams_PaddingReplication;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReplication::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

bool PaddingLayerParams_PaddingReplication::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReplication::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

size_t PaddingLayerParams_PaddingReplication::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReplication::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReplication*>(&from));
}

void PaddingLayerParams_PaddingReplication::MergeFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PaddingLayerParams_PaddingReplication::CopyFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingReplication::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingReplication::Swap(PaddingLayerParams_PaddingReplication* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReplication::InternalSwap(PaddingLayerParams_PaddingReplication* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReplication::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReplication";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingReplication

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams::kConstantFieldNumber;
const int PaddingLayerParams::kReflectionFieldNumber;
const int PaddingLayerParams::kReplicationFieldNumber;
const int PaddingLayerParams::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams::PaddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams)
}
PaddingLayerParams::PaddingLayerParams(const PaddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_paddingamounts()) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts(*from.paddingamounts_);
  } else {
    paddingamounts_ = NULL;
  }
  clear_has_PaddingType();
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams)
}

void PaddingLayerParams::SharedCtor() {
  paddingamounts_ = NULL;
  clear_has_PaddingType();
  _cached_size_ = 0;
}

PaddingLayerParams::~PaddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams)
  SharedDtor();
}

void PaddingLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete paddingamounts_;
  }
  if (has_PaddingType()) {
    clear_PaddingType();
  }
}

void PaddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams& PaddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams* PaddingLayerParams::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams* n = new PaddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams::clear_PaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PaddingLayerParams)
  switch (PaddingType_case()) {
    case kConstant: {
      delete PaddingType_.constant_;
      break;
    }
    case kReflection: {
      delete PaddingType_.reflection_;
      break;
    }
    case kReplication: {
      delete PaddingType_.replication_;
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}


void PaddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) {
    delete paddingamounts_;
  }
  paddingamounts_ = NULL;
  clear_PaddingType();
}

bool PaddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_constant()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reflection()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_replication()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams)
  return false;
#undef DO_
}

void PaddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  if (has_constant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *PaddingType_.constant_, output);
  }

  // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  if (has_reflection()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *PaddingType_.reflection_, output);
  }

  // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  if (has_replication()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *PaddingType_.replication_, output);
  }

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams)
}

size_t PaddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  switch (PaddingType_case()) {
    // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
    case kConstant: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.constant_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
    case kReflection: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.reflection_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
    case kReplication: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.replication_);
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams*>(&from));
}

void PaddingLayerParams::MergeFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void PaddingLayerParams::CopyFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams::IsInitialized() const {
  return true;
}

void PaddingLayerParams::Swap(PaddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams::InternalSwap(PaddingLayerParams* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(PaddingType_, other->PaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams

// .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
bool PaddingLayerParams::has_constant() const {
  return PaddingType_case() == kConstant;
}
void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
void PaddingLayerParams::clear_constant() {
  if (has_constant()) {
    delete PaddingType_.constant_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return has_constant()
      ? *PaddingType_.constant_
      : ::CoreML::Specification::PaddingLayerParams_PaddingConstant::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  if (!has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = new ::CoreML::Specification::PaddingLayerParams_PaddingConstant;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return PaddingType_.constant_;
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
bool PaddingLayerParams::has_reflection() const {
  return PaddingType_case() == kReflection;
}
void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
void PaddingLayerParams::clear_reflection() {
  if (has_reflection()) {
    delete PaddingType_.reflection_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return has_reflection()
      ? *PaddingType_.reflection_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReflection::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  if (!has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReflection;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return PaddingType_.reflection_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
bool PaddingLayerParams::has_replication() const {
  return PaddingType_case() == kReplication;
}
void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
void PaddingLayerParams::clear_replication() {
  if (has_replication()) {
    delete PaddingType_.replication_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return has_replication()
      ? *PaddingType_.replication_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReplication::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  if (!has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReplication;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return PaddingType_.replication_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}

// .CoreML.Specification.BorderAmounts paddingAmounts = 10;
bool PaddingLayerParams::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConcatLayerParams::kSequenceConcatFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConcatLayerParams::ConcatLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConcatLayerParams)
}
ConcatLayerParams::ConcatLayerParams(const ConcatLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  sequenceconcat_ = from.sequenceconcat_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConcatLayerParams)
}

void ConcatLayerParams::SharedCtor() {
  sequenceconcat_ = false;
  _cached_size_ = 0;
}

ConcatLayerParams::~ConcatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConcatLayerParams)
  SharedDtor();
}

void ConcatLayerParams::SharedDtor() {
}

void ConcatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConcatLayerParams& ConcatLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConcatLayerParams* ConcatLayerParams::New(::google::protobuf::Arena* arena) const {
  ConcatLayerParams* n = new ConcatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConcatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConcatLayerParams)
  sequenceconcat_ = false;
}

bool ConcatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConcatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool sequenceConcat = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceconcat_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConcatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConcatLayerParams)
  return false;
#undef DO_
}

void ConcatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConcatLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->sequenceconcat(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConcatLayerParams)
}

size_t ConcatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConcatLayerParams)
  size_t total_size = 0;

  // bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConcatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConcatLayerParams*>(&from));
}

void ConcatLayerParams::MergeFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConcatLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.sequenceconcat() != 0) {
    set_sequenceconcat(from.sequenceconcat());
  }
}

void ConcatLayerParams::CopyFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConcatLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConcatLayerParams::IsInitialized() const {
  return true;
}

void ConcatLayerParams::Swap(ConcatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConcatLayerParams::InternalSwap(ConcatLayerParams* other) {
  std::swap(sequenceconcat_, other->sequenceconcat_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConcatLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConcatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConcatLayerParams

// bool sequenceConcat = 100;
void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return sequenceconcat_;
}
void ConcatLayerParams::set_sequenceconcat(bool value) {
  
  sequenceconcat_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LRNLayerParams::kAlphaFieldNumber;
const int LRNLayerParams::kBetaFieldNumber;
const int LRNLayerParams::kLocalSizeFieldNumber;
const int LRNLayerParams::kKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LRNLayerParams::LRNLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LRNLayerParams)
}
LRNLayerParams::LRNLayerParams(const LRNLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LRNLayerParams)
}

void LRNLayerParams::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  _cached_size_ = 0;
}

LRNLayerParams::~LRNLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LRNLayerParams)
  SharedDtor();
}

void LRNLayerParams::SharedDtor() {
}

void LRNLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LRNLayerParams& LRNLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LRNLayerParams* LRNLayerParams::New(::google::protobuf::Arena* arena) const {
  LRNLayerParams* n = new LRNLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LRNLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LRNLayerParams)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
}

bool LRNLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LRNLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 localSize = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &localsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float k = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LRNLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LRNLayerParams)
  return false;
#undef DO_
}

void LRNLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LRNLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // uint64 localSize = 3;
  if (this->localsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->localsize(), output);
  }

  // float k = 4;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->k(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LRNLayerParams)
}

size_t LRNLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LRNLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  // uint64 localSize = 3;
  if (this->localsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->localsize());
  }

  // float k = 4;
  if (this->k() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LRNLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LRNLayerParams*>(&from));
}

void LRNLayerParams::MergeFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LRNLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
  if (from.localsize() != 0) {
    set_localsize(from.localsize());
  }
  if (from.k() != 0) {
    set_k(from.k());
  }
}

void LRNLayerParams::CopyFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LRNLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LRNLayerParams::IsInitialized() const {
  return true;
}

void LRNLayerParams::Swap(LRNLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LRNLayerParams::InternalSwap(LRNLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(localsize_, other->localsize_);
  std::swap(k_, other->k_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LRNLayerParams::GetTypeName() const {
  return "CoreML.Specification.LRNLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LRNLayerParams

// float alpha = 1;
void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return alpha_;
}
void LRNLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// float beta = 2;
void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return beta_;
}
void LRNLayerParams::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// uint64 localSize = 3;
void LRNLayerParams::clear_localsize() {
  localsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return localsize_;
}
void LRNLayerParams::set_localsize(::google::protobuf::uint64 value) {
  
  localsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// float k = 4;
void LRNLayerParams::clear_k() {
  k_ = 0;
}
float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return k_;
}
void LRNLayerParams::set_k(float value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SoftmaxLayerParams::SoftmaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SoftmaxLayerParams)
}
SoftmaxLayerParams::SoftmaxLayerParams(const SoftmaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SoftmaxLayerParams)
}

void SoftmaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SoftmaxLayerParams::~SoftmaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SoftmaxLayerParams)
  SharedDtor();
}

void SoftmaxLayerParams::SharedDtor() {
}

void SoftmaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SoftmaxLayerParams& SoftmaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SoftmaxLayerParams* SoftmaxLayerParams::New(::google::protobuf::Arena* arena) const {
  SoftmaxLayerParams* n = new SoftmaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SoftmaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SoftmaxLayerParams)
}

bool SoftmaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SoftmaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SoftmaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SoftmaxLayerParams)
  return false;
#undef DO_
}

void SoftmaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SoftmaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SoftmaxLayerParams)
}

size_t SoftmaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SoftmaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SoftmaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SoftmaxLayerParams*>(&from));
}

void SoftmaxLayerParams::MergeFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SoftmaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SoftmaxLayerParams::CopyFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SoftmaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SoftmaxLayerParams::IsInitialized() const {
  return true;
}

void SoftmaxLayerParams::Swap(SoftmaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SoftmaxLayerParams::InternalSwap(SoftmaxLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SoftmaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.SoftmaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SoftmaxLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SplitLayerParams::kNOutputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SplitLayerParams::SplitLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SplitLayerParams)
}
SplitLayerParams::SplitLayerParams(const SplitLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  noutputs_ = from.noutputs_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SplitLayerParams)
}

void SplitLayerParams::SharedCtor() {
  noutputs_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SplitLayerParams::~SplitLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SplitLayerParams)
  SharedDtor();
}

void SplitLayerParams::SharedDtor() {
}

void SplitLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SplitLayerParams& SplitLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SplitLayerParams* SplitLayerParams::New(::google::protobuf::Arena* arena) const {
  SplitLayerParams* n = new SplitLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SplitLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SplitLayerParams)
  noutputs_ = GOOGLE_ULONGLONG(0);
}

bool SplitLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SplitLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 nOutputs = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &noutputs_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SplitLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SplitLayerParams)
  return false;
#undef DO_
}

void SplitLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SplitLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->noutputs(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SplitLayerParams)
}

size_t SplitLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SplitLayerParams)
  size_t total_size = 0;

  // uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->noutputs());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SplitLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SplitLayerParams*>(&from));
}

void SplitLayerParams::MergeFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SplitLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.noutputs() != 0) {
    set_noutputs(from.noutputs());
  }
}

void SplitLayerParams::CopyFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SplitLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SplitLayerParams::IsInitialized() const {
  return true;
}

void SplitLayerParams::Swap(SplitLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SplitLayerParams::InternalSwap(SplitLayerParams* other) {
  std::swap(noutputs_, other->noutputs_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SplitLayerParams::GetTypeName() const {
  return "CoreML.Specification.SplitLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SplitLayerParams

// uint64 nOutputs = 1;
void SplitLayerParams::clear_noutputs() {
  noutputs_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return noutputs_;
}
void SplitLayerParams::set_noutputs(::google::protobuf::uint64 value) {
  
  noutputs_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int AddLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AddLayerParams::AddLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AddLayerParams)
}
AddLayerParams::AddLayerParams(const AddLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AddLayerParams)
}

void AddLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

AddLayerParams::~AddLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AddLayerParams)
  SharedDtor();
}

void AddLayerParams::SharedDtor() {
}

void AddLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AddLayerParams& AddLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AddLayerParams* AddLayerParams::New(::google::protobuf::Arena* arena) const {
  AddLayerParams* n = new AddLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AddLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AddLayerParams)
  alpha_ = 0;
}

bool AddLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AddLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AddLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AddLayerParams)
  return false;
#undef DO_
}

void AddLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AddLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AddLayerParams)
}

size_t AddLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AddLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AddLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AddLayerParams*>(&from));
}

void AddLayerParams::MergeFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AddLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void AddLayerParams::CopyFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AddLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AddLayerParams::IsInitialized() const {
  return true;
}

void AddLayerParams::Swap(AddLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AddLayerParams::InternalSwap(AddLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AddLayerParams::GetTypeName() const {
  return "CoreML.Specification.AddLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AddLayerParams

// float alpha = 1;
void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return alpha_;
}
void AddLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiplyLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiplyLayerParams::MultiplyLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MultiplyLayerParams)
}
MultiplyLayerParams::MultiplyLayerParams(const MultiplyLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MultiplyLayerParams)
}

void MultiplyLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

MultiplyLayerParams::~MultiplyLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MultiplyLayerParams)
  SharedDtor();
}

void MultiplyLayerParams::SharedDtor() {
}

void MultiplyLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MultiplyLayerParams& MultiplyLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MultiplyLayerParams* MultiplyLayerParams::New(::google::protobuf::Arena* arena) const {
  MultiplyLayerParams* n = new MultiplyLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MultiplyLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MultiplyLayerParams)
  alpha_ = 0;
}

bool MultiplyLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MultiplyLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MultiplyLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MultiplyLayerParams)
  return false;
#undef DO_
}

void MultiplyLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MultiplyLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MultiplyLayerParams)
}

size_t MultiplyLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MultiplyLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiplyLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MultiplyLayerParams*>(&from));
}

void MultiplyLayerParams::MergeFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MultiplyLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void MultiplyLayerParams::CopyFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MultiplyLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiplyLayerParams::IsInitialized() const {
  return true;
}

void MultiplyLayerParams::Swap(MultiplyLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MultiplyLayerParams::InternalSwap(MultiplyLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MultiplyLayerParams::GetTypeName() const {
  return "CoreML.Specification.MultiplyLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiplyLayerParams

// float alpha = 1;
void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return alpha_;
}
void MultiplyLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UnaryFunctionLayerParams::kTypeFieldNumber;
const int UnaryFunctionLayerParams::kAlphaFieldNumber;
const int UnaryFunctionLayerParams::kEpsilonFieldNumber;
const int UnaryFunctionLayerParams::kShiftFieldNumber;
const int UnaryFunctionLayerParams::kScaleFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UnaryFunctionLayerParams::UnaryFunctionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UnaryFunctionLayerParams)
}
UnaryFunctionLayerParams::UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&type_, &from.type_,
    reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UnaryFunctionLayerParams)
}

void UnaryFunctionLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  _cached_size_ = 0;
}

UnaryFunctionLayerParams::~UnaryFunctionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UnaryFunctionLayerParams)
  SharedDtor();
}

void UnaryFunctionLayerParams::SharedDtor() {
}

void UnaryFunctionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UnaryFunctionLayerParams& UnaryFunctionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UnaryFunctionLayerParams* UnaryFunctionLayerParams::New(::google::protobuf::Arena* arena) const {
  UnaryFunctionLayerParams* n = new UnaryFunctionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UnaryFunctionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UnaryFunctionLayerParams)
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
}

bool UnaryFunctionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UnaryFunctionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float shift = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &shift_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float scale = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(45u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &scale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UnaryFunctionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UnaryFunctionLayerParams)
  return false;
#undef DO_
}

void UnaryFunctionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UnaryFunctionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // float shift = 4;
  if (this->shift() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->shift(), output);
  }

  // float scale = 5;
  if (this->scale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->scale(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UnaryFunctionLayerParams)
}

size_t UnaryFunctionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UnaryFunctionLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // float shift = 4;
  if (this->shift() != 0) {
    total_size += 1 + 4;
  }

  // float scale = 5;
  if (this->scale() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UnaryFunctionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UnaryFunctionLayerParams*>(&from));
}

void UnaryFunctionLayerParams::MergeFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.shift() != 0) {
    set_shift(from.shift());
  }
  if (from.scale() != 0) {
    set_scale(from.scale());
  }
}

void UnaryFunctionLayerParams::CopyFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UnaryFunctionLayerParams::IsInitialized() const {
  return true;
}

void UnaryFunctionLayerParams::Swap(UnaryFunctionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UnaryFunctionLayerParams::InternalSwap(UnaryFunctionLayerParams* other) {
  std::swap(type_, other->type_);
  std::swap(alpha_, other->alpha_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(shift_, other->shift_);
  std::swap(scale_, other->scale_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UnaryFunctionLayerParams::GetTypeName() const {
  return "CoreML.Specification.UnaryFunctionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UnaryFunctionLayerParams

// .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// float alpha = 2;
void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return alpha_;
}
void UnaryFunctionLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// float epsilon = 3;
void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return epsilon_;
}
void UnaryFunctionLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// float shift = 4;
void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return shift_;
}
void UnaryFunctionLayerParams::set_shift(float value) {
  
  shift_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// float scale = 5;
void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return scale_;
}
void UnaryFunctionLayerParams::set_scale(float value) {
  
  scale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UpsampleLayerParams::kScalingFactorFieldNumber;
const int UpsampleLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UpsampleLayerParams::UpsampleLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UpsampleLayerParams)
}
UpsampleLayerParams::UpsampleLayerParams(const UpsampleLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      scalingfactor_(from.scalingfactor_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UpsampleLayerParams)
}

void UpsampleLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

UpsampleLayerParams::~UpsampleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UpsampleLayerParams)
  SharedDtor();
}

void UpsampleLayerParams::SharedDtor() {
}

void UpsampleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UpsampleLayerParams& UpsampleLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UpsampleLayerParams* UpsampleLayerParams::New(::google::protobuf::Arena* arena) const {
  UpsampleLayerParams* n = new UpsampleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UpsampleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UpsampleLayerParams)
  scalingfactor_.Clear();
  mode_ = 0;
}

bool UpsampleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UpsampleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 scalingFactor = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_scalingfactor())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_scalingfactor())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UpsampleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UpsampleLayerParams)
  return false;
#undef DO_
}

void UpsampleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UpsampleLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 scalingFactor = 1;
  if (this->scalingfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scalingfactor_cached_byte_size_);
  }
  for (int i = 0, n = this->scalingfactor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->scalingfactor(i), output);
  }

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UpsampleLayerParams)
}

size_t UpsampleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UpsampleLayerParams)
  size_t total_size = 0;

  // repeated uint64 scalingFactor = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->scalingfactor_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scalingfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UpsampleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UpsampleLayerParams*>(&from));
}

void UpsampleLayerParams::MergeFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UpsampleLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  scalingfactor_.MergeFrom(from.scalingfactor_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void UpsampleLayerParams::CopyFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UpsampleLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UpsampleLayerParams::IsInitialized() const {
  return true;
}

void UpsampleLayerParams::Swap(UpsampleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UpsampleLayerParams::InternalSwap(UpsampleLayerParams* other) {
  scalingfactor_.InternalSwap(&other->scalingfactor_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UpsampleLayerParams::GetTypeName() const {
  return "CoreML.Specification.UpsampleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
int UpsampleLayerParams::scalingfactor_size() const {
  return scalingfactor_.size();
}
void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
::google::protobuf::uint64 UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_.Get(index);
}
void UpsampleLayerParams::set_scalingfactor(int index, ::google::protobuf::uint64 value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
void UpsampleLayerParams::add_scalingfactor(::google::protobuf::uint64 value) {
  scalingfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return &scalingfactor_;
}

// .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
void UpsampleLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::UpsampleLayerParams_InterpolationMode UpsampleLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.mode)
  return static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(mode_);
}
void UpsampleLayerParams::set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ResizeBilinearLayerParams::kTargetSizeFieldNumber;
const int ResizeBilinearLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ResizeBilinearLayerParams::ResizeBilinearLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ResizeBilinearLayerParams)
}
ResizeBilinearLayerParams::ResizeBilinearLayerParams(const ResizeBilinearLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetsize_(from.targetsize_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_mode()) {
    mode_ = new ::CoreML::Specification::SamplingMode(*from.mode_);
  } else {
    mode_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ResizeBilinearLayerParams)
}

void ResizeBilinearLayerParams::SharedCtor() {
  mode_ = NULL;
  _cached_size_ = 0;
}

ResizeBilinearLayerParams::~ResizeBilinearLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ResizeBilinearLayerParams)
  SharedDtor();
}

void ResizeBilinearLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete mode_;
  }
}

void ResizeBilinearLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ResizeBilinearLayerParams& ResizeBilinearLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ResizeBilinearLayerParams* ResizeBilinearLayerParams::New(::google::protobuf::Arena* arena) const {
  ResizeBilinearLayerParams* n = new ResizeBilinearLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ResizeBilinearLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ResizeBilinearLayerParams)
  targetsize_.Clear();
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) {
    delete mode_;
  }
  mode_ = NULL;
}

bool ResizeBilinearLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ResizeBilinearLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 targetSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_targetsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamplingMode mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mode()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ResizeBilinearLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ResizeBilinearLayerParams)
  return false;
#undef DO_
}

void ResizeBilinearLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ResizeBilinearLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 targetSize = 1;
  if (this->targetsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetsize_cached_byte_size_);
  }
  for (int i = 0, n = this->targetsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetsize(i), output);
  }

  // .CoreML.Specification.SamplingMode mode = 2;
  if (this->has_mode()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->mode_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ResizeBilinearLayerParams)
}

size_t ResizeBilinearLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ResizeBilinearLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetSize = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.SamplingMode mode = 2;
  if (this->has_mode()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mode_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ResizeBilinearLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ResizeBilinearLayerParams*>(&from));
}

void ResizeBilinearLayerParams::MergeFrom(const ResizeBilinearLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ResizeBilinearLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetsize_.MergeFrom(from.targetsize_);
  if (from.has_mode()) {
    mutable_mode()->::CoreML::Specification::SamplingMode::MergeFrom(from.mode());
  }
}

void ResizeBilinearLayerParams::CopyFrom(const ResizeBilinearLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ResizeBilinearLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ResizeBilinearLayerParams::IsInitialized() const {
  return true;
}

void ResizeBilinearLayerParams::Swap(ResizeBilinearLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ResizeBilinearLayerParams::InternalSwap(ResizeBilinearLayerParams* other) {
  targetsize_.InternalSwap(&other->targetsize_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ResizeBilinearLayerParams::GetTypeName() const {
  return "CoreML.Specification.ResizeBilinearLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ResizeBilinearLayerParams

// repeated uint64 targetSize = 1;
int ResizeBilinearLayerParams::targetsize_size() const {
  return targetsize_.size();
}
void ResizeBilinearLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
::google::protobuf::uint64 ResizeBilinearLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return targetsize_.Get(index);
}
void ResizeBilinearLayerParams::set_targetsize(int index, ::google::protobuf::uint64 value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
void ResizeBilinearLayerParams::add_targetsize(::google::protobuf::uint64 value) {
  targetsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ResizeBilinearLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return targetsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ResizeBilinearLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return &targetsize_;
}

// .CoreML.Specification.SamplingMode mode = 2;
bool ResizeBilinearLayerParams::has_mode() const {
  return this != internal_default_instance() && mode_ != NULL;
}
void ResizeBilinearLayerParams::clear_mode() {
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) delete mode_;
  mode_ = NULL;
}
const ::CoreML::Specification::SamplingMode& ResizeBilinearLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return mode_ != NULL ? *mode_
                         : *::CoreML::Specification::SamplingMode::internal_default_instance();
}
::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::mutable_mode() {
  
  if (mode_ == NULL) {
    mode_ = new ::CoreML::Specification::SamplingMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return mode_;
}
::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ResizeBilinearLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = NULL;
  return temp;
}
void ResizeBilinearLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  delete mode_;
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ResizeBilinearLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CropResizeLayerParams::kTargetSizeFieldNumber;
const int CropResizeLayerParams::kNormalizedCoordinatesFieldNumber;
const int CropResizeLayerParams::kModeFieldNumber;
const int CropResizeLayerParams::kBoxIndicesModeFieldNumber;
const int CropResizeLayerParams::kSpatialScaleFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CropResizeLayerParams::CropResizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CropResizeLayerParams)
}
CropResizeLayerParams::CropResizeLayerParams(const CropResizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetsize_(from.targetsize_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_mode()) {
    mode_ = new ::CoreML::Specification::SamplingMode(*from.mode_);
  } else {
    mode_ = NULL;
  }
  if (from.has_boxindicesmode()) {
    boxindicesmode_ = new ::CoreML::Specification::BoxCoordinatesMode(*from.boxindicesmode_);
  } else {
    boxindicesmode_ = NULL;
  }
  ::memcpy(&normalizedcoordinates_, &from.normalizedcoordinates_,
    reinterpret_cast<char*>(&spatialscale_) -
    reinterpret_cast<char*>(&normalizedcoordinates_) + sizeof(spatialscale_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CropResizeLayerParams)
}

void CropResizeLayerParams::SharedCtor() {
  ::memset(&mode_, 0, reinterpret_cast<char*>(&spatialscale_) -
    reinterpret_cast<char*>(&mode_) + sizeof(spatialscale_));
  _cached_size_ = 0;
}

CropResizeLayerParams::~CropResizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CropResizeLayerParams)
  SharedDtor();
}

void CropResizeLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete mode_;
  }
  if (this != internal_default_instance()) {
    delete boxindicesmode_;
  }
}

void CropResizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CropResizeLayerParams& CropResizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CropResizeLayerParams* CropResizeLayerParams::New(::google::protobuf::Arena* arena) const {
  CropResizeLayerParams* n = new CropResizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CropResizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CropResizeLayerParams)
  targetsize_.Clear();
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) {
    delete mode_;
  }
  mode_ = NULL;
  if (GetArenaNoVirtual() == NULL && boxindicesmode_ != NULL) {
    delete boxindicesmode_;
  }
  boxindicesmode_ = NULL;
  ::memset(&normalizedcoordinates_, 0, reinterpret_cast<char*>(&spatialscale_) -
    reinterpret_cast<char*>(&normalizedcoordinates_) + sizeof(spatialscale_));
}

bool CropResizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CropResizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 targetSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_targetsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool normalizedCoordinates = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &normalizedcoordinates_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamplingMode mode = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mode()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_boxindicesmode()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float spatialScale = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(45u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &spatialscale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CropResizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CropResizeLayerParams)
  return false;
#undef DO_
}

void CropResizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CropResizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 targetSize = 1;
  if (this->targetsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetsize_cached_byte_size_);
  }
  for (int i = 0, n = this->targetsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetsize(i), output);
  }

  // bool normalizedCoordinates = 2;
  if (this->normalizedcoordinates() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->normalizedcoordinates(), output);
  }

  // .CoreML.Specification.SamplingMode mode = 3;
  if (this->has_mode()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->mode_, output);
  }

  // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
  if (this->has_boxindicesmode()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->boxindicesmode_, output);
  }

  // float spatialScale = 5;
  if (this->spatialscale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->spatialscale(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CropResizeLayerParams)
}

size_t CropResizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CropResizeLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetSize = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.SamplingMode mode = 3;
  if (this->has_mode()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mode_);
  }

  // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
  if (this->has_boxindicesmode()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->boxindicesmode_);
  }

  // bool normalizedCoordinates = 2;
  if (this->normalizedcoordinates() != 0) {
    total_size += 1 + 1;
  }

  // float spatialScale = 5;
  if (this->spatialscale() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CropResizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CropResizeLayerParams*>(&from));
}

void CropResizeLayerParams::MergeFrom(const CropResizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CropResizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetsize_.MergeFrom(from.targetsize_);
  if (from.has_mode()) {
    mutable_mode()->::CoreML::Specification::SamplingMode::MergeFrom(from.mode());
  }
  if (from.has_boxindicesmode()) {
    mutable_boxindicesmode()->::CoreML::Specification::BoxCoordinatesMode::MergeFrom(from.boxindicesmode());
  }
  if (from.normalizedcoordinates() != 0) {
    set_normalizedcoordinates(from.normalizedcoordinates());
  }
  if (from.spatialscale() != 0) {
    set_spatialscale(from.spatialscale());
  }
}

void CropResizeLayerParams::CopyFrom(const CropResizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CropResizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CropResizeLayerParams::IsInitialized() const {
  return true;
}

void CropResizeLayerParams::Swap(CropResizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CropResizeLayerParams::InternalSwap(CropResizeLayerParams* other) {
  targetsize_.InternalSwap(&other->targetsize_);
  std::swap(mode_, other->mode_);
  std::swap(boxindicesmode_, other->boxindicesmode_);
  std::swap(normalizedcoordinates_, other->normalizedcoordinates_);
  std::swap(spatialscale_, other->spatialscale_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CropResizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.CropResizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CropResizeLayerParams

// repeated uint64 targetSize = 1;
int CropResizeLayerParams::targetsize_size() const {
  return targetsize_.size();
}
void CropResizeLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
::google::protobuf::uint64 CropResizeLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.targetSize)
  return targetsize_.Get(index);
}
void CropResizeLayerParams::set_targetsize(int index, ::google::protobuf::uint64 value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.targetSize)
}
void CropResizeLayerParams::add_targetsize(::google::protobuf::uint64 value) {
  targetsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropResizeLayerParams.targetSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropResizeLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return targetsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropResizeLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return &targetsize_;
}

// bool normalizedCoordinates = 2;
void CropResizeLayerParams::clear_normalizedcoordinates() {
  normalizedcoordinates_ = false;
}
bool CropResizeLayerParams::normalizedcoordinates() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
  return normalizedcoordinates_;
}
void CropResizeLayerParams::set_normalizedcoordinates(bool value) {
  
  normalizedcoordinates_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
}

// .CoreML.Specification.SamplingMode mode = 3;
bool CropResizeLayerParams::has_mode() const {
  return this != internal_default_instance() && mode_ != NULL;
}
void CropResizeLayerParams::clear_mode() {
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) delete mode_;
  mode_ = NULL;
}
const ::CoreML::Specification::SamplingMode& CropResizeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.mode)
  return mode_ != NULL ? *mode_
                         : *::CoreML::Specification::SamplingMode::internal_default_instance();
}
::CoreML::Specification::SamplingMode* CropResizeLayerParams::mutable_mode() {
  
  if (mode_ == NULL) {
    mode_ = new ::CoreML::Specification::SamplingMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.mode)
  return mode_;
}
::CoreML::Specification::SamplingMode* CropResizeLayerParams::release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = NULL;
  return temp;
}
void CropResizeLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  delete mode_;
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.mode)
}

// .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
bool CropResizeLayerParams::has_boxindicesmode() const {
  return this != internal_default_instance() && boxindicesmode_ != NULL;
}
void CropResizeLayerParams::clear_boxindicesmode() {
  if (GetArenaNoVirtual() == NULL && boxindicesmode_ != NULL) delete boxindicesmode_;
  boxindicesmode_ = NULL;
}
const ::CoreML::Specification::BoxCoordinatesMode& CropResizeLayerParams::boxindicesmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return boxindicesmode_ != NULL ? *boxindicesmode_
                         : *::CoreML::Specification::BoxCoordinatesMode::internal_default_instance();
}
::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::mutable_boxindicesmode() {
  
  if (boxindicesmode_ == NULL) {
    boxindicesmode_ = new ::CoreML::Specification::BoxCoordinatesMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return boxindicesmode_;
}
::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::release_boxindicesmode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  
  ::CoreML::Specification::BoxCoordinatesMode* temp = boxindicesmode_;
  boxindicesmode_ = NULL;
  return temp;
}
void CropResizeLayerParams::set_allocated_boxindicesmode(::CoreML::Specification::BoxCoordinatesMode* boxindicesmode) {
  delete boxindicesmode_;
  boxindicesmode_ = boxindicesmode;
  if (boxindicesmode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
}

// float spatialScale = 5;
void CropResizeLayerParams::clear_spatialscale() {
  spatialscale_ = 0;
}
float CropResizeLayerParams::spatialscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.spatialScale)
  return spatialscale_;
}
void CropResizeLayerParams::set_spatialscale(float value) {
  
  spatialscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.spatialScale)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiasLayerParams::kShapeFieldNumber;
const int BiasLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiasLayerParams::BiasLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiasLayerParams)
}
BiasLayerParams::BiasLayerParams(const BiasLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiasLayerParams)
}

void BiasLayerParams::SharedCtor() {
  bias_ = NULL;
  _cached_size_ = 0;
}

BiasLayerParams::~BiasLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiasLayerParams)
  SharedDtor();
}

void BiasLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void BiasLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiasLayerParams& BiasLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BiasLayerParams* BiasLayerParams::New(::google::protobuf::Arena* arena) const {
  BiasLayerParams* n = new BiasLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiasLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiasLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
}

bool BiasLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiasLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiasLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiasLayerParams)
  return false;
#undef DO_
}

void BiasLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiasLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiasLayerParams)
}

size_t BiasLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiasLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiasLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiasLayerParams*>(&from));
}

void BiasLayerParams::MergeFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiasLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void BiasLayerParams::CopyFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiasLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BiasLayerParams::IsInitialized() const {
  return true;
}

void BiasLayerParams::Swap(BiasLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiasLayerParams::InternalSwap(BiasLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(bias_, other->bias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiasLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiasLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiasLayerParams

// repeated uint64 shape = 1;
int BiasLayerParams::shape_size() const {
  return shape_.size();
}
void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return shape_.Get(index);
}
void BiasLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
void BiasLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams bias = 2;
bool BiasLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void BiasLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScaleLayerParams::kShapeScaleFieldNumber;
const int ScaleLayerParams::kScaleFieldNumber;
const int ScaleLayerParams::kHasBiasFieldNumber;
const int ScaleLayerParams::kShapeBiasFieldNumber;
const int ScaleLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScaleLayerParams::ScaleLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScaleLayerParams)
}
ScaleLayerParams::ScaleLayerParams(const ScaleLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shapescale_(from.shapescale_),
      shapebias_(from.shapebias_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_scale()) {
    scale_ = new ::CoreML::Specification::WeightParams(*from.scale_);
  } else {
    scale_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  hasbias_ = from.hasbias_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScaleLayerParams)
}

void ScaleLayerParams::SharedCtor() {
  ::memset(&scale_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&scale_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

ScaleLayerParams::~ScaleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScaleLayerParams)
  SharedDtor();
}

void ScaleLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete scale_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void ScaleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScaleLayerParams& ScaleLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScaleLayerParams* ScaleLayerParams::New(::google::protobuf::Arena* arena) const {
  ScaleLayerParams* n = new ScaleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScaleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScaleLayerParams)
  shapescale_.Clear();
  shapebias_.Clear();
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) {
    delete scale_;
  }
  scale_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  hasbias_ = false;
}

bool ScaleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScaleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shapeScale = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapescale())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shapescale())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams scale = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 shapeBias = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapebias())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 34u, input, this->mutable_shapebias())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScaleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScaleLayerParams)
  return false;
#undef DO_
}

void ScaleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScaleLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shapeScale = 1;
  if (this->shapescale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapescale_cached_byte_size_);
  }
  for (int i = 0, n = this->shapescale_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapescale(i), output);
  }

  // .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->scale_, output);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->hasbias(), output);
  }

  // repeated uint64 shapeBias = 4;
  if (this->shapebias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapebias_cached_byte_size_);
  }
  for (int i = 0, n = this->shapebias_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapebias(i), output);
  }

  // .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScaleLayerParams)
}

size_t ScaleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScaleLayerParams)
  size_t total_size = 0;

  // repeated uint64 shapeScale = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shapescale_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapescale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 shapeBias = 4;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shapebias_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapebias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->scale_);
  }

  // .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScaleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScaleLayerParams*>(&from));
}

void ScaleLayerParams::MergeFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScaleLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shapescale_.MergeFrom(from.shapescale_);
  shapebias_.MergeFrom(from.shapebias_);
  if (from.has_scale()) {
    mutable_scale()->::CoreML::Specification::WeightParams::MergeFrom(from.scale());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void ScaleLayerParams::CopyFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScaleLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScaleLayerParams::IsInitialized() const {
  return true;
}

void ScaleLayerParams::Swap(ScaleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScaleLayerParams::InternalSwap(ScaleLayerParams* other) {
  shapescale_.InternalSwap(&other->shapescale_);
  shapebias_.InternalSwap(&other->shapebias_);
  std::swap(scale_, other->scale_);
  std::swap(bias_, other->bias_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScaleLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScaleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScaleLayerParams

// repeated uint64 shapeScale = 1;
int ScaleLayerParams::shapescale_size() const {
  return shapescale_.size();
}
void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_.Get(index);
}
void ScaleLayerParams::set_shapescale(int index, ::google::protobuf::uint64 value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
void ScaleLayerParams::add_shapescale(::google::protobuf::uint64 value) {
  shapescale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return &shapescale_;
}

// .CoreML.Specification.WeightParams scale = 2;
bool ScaleLayerParams::has_scale() const {
  return this != internal_default_instance() && scale_ != NULL;
}
void ScaleLayerParams::clear_scale() {
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return scale_ != NULL ? *scale_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {
  
  if (scale_ == NULL) {
    scale_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return scale_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  delete scale_;
  scale_ = scale;
  if (scale) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// bool hasBias = 3;
void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return hasbias_;
}
void ScaleLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
int ScaleLayerParams::shapebias_size() const {
  return shapebias_.size();
}
void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_.Get(index);
}
void ScaleLayerParams::set_shapebias(int index, ::google::protobuf::uint64 value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
void ScaleLayerParams::add_shapebias(::google::protobuf::uint64 value) {
  shapebias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return &shapebias_;
}

// .CoreML.Specification.WeightParams bias = 5;
bool ScaleLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ScaleLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoadConstantLayerParams::kShapeFieldNumber;
const int LoadConstantLayerParams::kDataFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoadConstantLayerParams::LoadConstantLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoadConstantLayerParams)
}
LoadConstantLayerParams::LoadConstantLayerParams(const LoadConstantLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_data()) {
    data_ = new ::CoreML::Specification::WeightParams(*from.data_);
  } else {
    data_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoadConstantLayerParams)
}

void LoadConstantLayerParams::SharedCtor() {
  data_ = NULL;
  _cached_size_ = 0;
}

LoadConstantLayerParams::~LoadConstantLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoadConstantLayerParams)
  SharedDtor();
}

void LoadConstantLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete data_;
  }
}

void LoadConstantLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoadConstantLayerParams& LoadConstantLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoadConstantLayerParams* LoadConstantLayerParams::New(::google::protobuf::Arena* arena) const {
  LoadConstantLayerParams* n = new LoadConstantLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoadConstantLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoadConstantLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && data_ != NULL) {
    delete data_;
  }
  data_ = NULL;
}

bool LoadConstantLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoadConstantLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams data = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_data()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoadConstantLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoadConstantLayerParams)
  return false;
#undef DO_
}

void LoadConstantLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoadConstantLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->data_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoadConstantLayerParams)
}

size_t LoadConstantLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoadConstantLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->data_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoadConstantLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoadConstantLayerParams*>(&from));
}

void LoadConstantLayerParams::MergeFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoadConstantLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_data()) {
    mutable_data()->::CoreML::Specification::WeightParams::MergeFrom(from.data());
  }
}

void LoadConstantLayerParams::CopyFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoadConstantLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoadConstantLayerParams::IsInitialized() const {
  return true;
}

void LoadConstantLayerParams::Swap(LoadConstantLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoadConstantLayerParams::InternalSwap(LoadConstantLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(data_, other->data_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoadConstantLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoadConstantLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoadConstantLayerParams

// repeated uint64 shape = 1;
int LoadConstantLayerParams::shape_size() const {
  return shape_.size();
}
void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_.Get(index);
}
void LoadConstantLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
void LoadConstantLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
bool LoadConstantLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
void LoadConstantLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return data_;
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int L2NormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

L2NormalizeLayerParams::L2NormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.L2NormalizeLayerParams)
}
L2NormalizeLayerParams::L2NormalizeLayerParams(const L2NormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  epsilon_ = from.epsilon_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.L2NormalizeLayerParams)
}

void L2NormalizeLayerParams::SharedCtor() {
  epsilon_ = 0;
  _cached_size_ = 0;
}

L2NormalizeLayerParams::~L2NormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.L2NormalizeLayerParams)
  SharedDtor();
}

void L2NormalizeLayerParams::SharedDtor() {
}

void L2NormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const L2NormalizeLayerParams& L2NormalizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

L2NormalizeLayerParams* L2NormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  L2NormalizeLayerParams* n = new L2NormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void L2NormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.L2NormalizeLayerParams)
  epsilon_ = 0;
}

bool L2NormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.L2NormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float epsilon = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.L2NormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.L2NormalizeLayerParams)
  return false;
#undef DO_
}

void L2NormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.L2NormalizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float epsilon = 1;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.L2NormalizeLayerParams)
}

size_t L2NormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.L2NormalizeLayerParams)
  size_t total_size = 0;

  // float epsilon = 1;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void L2NormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const L2NormalizeLayerParams*>(&from));
}

void L2NormalizeLayerParams::MergeFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.L2NormalizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void L2NormalizeLayerParams::CopyFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.L2NormalizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool L2NormalizeLayerParams::IsInitialized() const {
  return true;
}

void L2NormalizeLayerParams::Swap(L2NormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void L2NormalizeLayerParams::InternalSwap(L2NormalizeLayerParams* other) {
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string L2NormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.L2NormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// L2NormalizeLayerParams

// float epsilon = 1;
void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return epsilon_;
}
void L2NormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FlattenLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FlattenLayerParams::FlattenLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FlattenLayerParams)
}
FlattenLayerParams::FlattenLayerParams(const FlattenLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FlattenLayerParams)
}

void FlattenLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

FlattenLayerParams::~FlattenLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FlattenLayerParams)
  SharedDtor();
}

void FlattenLayerParams::SharedDtor() {
}

void FlattenLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FlattenLayerParams& FlattenLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FlattenLayerParams* FlattenLayerParams::New(::google::protobuf::Arena* arena) const {
  FlattenLayerParams* n = new FlattenLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FlattenLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FlattenLayerParams)
  mode_ = 0;
}

bool FlattenLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FlattenLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FlattenLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FlattenLayerParams)
  return false;
#undef DO_
}

void FlattenLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FlattenLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FlattenLayerParams)
}

size_t FlattenLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FlattenLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FlattenLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FlattenLayerParams*>(&from));
}

void FlattenLayerParams::MergeFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FlattenLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void FlattenLayerParams::CopyFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FlattenLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FlattenLayerParams::IsInitialized() const {
  return true;
}

void FlattenLayerParams::Swap(FlattenLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FlattenLayerParams::InternalSwap(FlattenLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FlattenLayerParams::GetTypeName() const {
  return "CoreML.Specification.FlattenLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FlattenLayerParams

// .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReshapeLayerParams::kTargetShapeFieldNumber;
const int ReshapeLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeLayerParams::ReshapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeLayerParams)
}
ReshapeLayerParams::ReshapeLayerParams(const ReshapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeLayerParams)
}

void ReshapeLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

ReshapeLayerParams::~ReshapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeLayerParams)
  SharedDtor();
}

void ReshapeLayerParams::SharedDtor() {
}

void ReshapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeLayerParams& ReshapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReshapeLayerParams* ReshapeLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeLayerParams* n = new ReshapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeLayerParams)
  targetshape_.Clear();
  mode_ = 0;
}

bool ReshapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeLayerParams)
  return false;
#undef DO_
}

void ReshapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      2, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeLayerParams)
}

size_t ReshapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeLayerParams)
  size_t total_size = 0;

  // repeated int64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeLayerParams*>(&from));
}

void ReshapeLayerParams::MergeFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReshapeLayerParams::CopyFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReshapeLayerParams::IsInitialized() const {
  return true;
}

void ReshapeLayerParams::Swap(ReshapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeLayerParams::InternalSwap(ReshapeLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeLayerParams

// repeated int64 targetShape = 1;
int ReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
void ReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
void ReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return &targetshape_;
}

// .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PermuteLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PermuteLayerParams::PermuteLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PermuteLayerParams)
}
PermuteLayerParams::PermuteLayerParams(const PermuteLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axis_(from.axis_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PermuteLayerParams)
}

void PermuteLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

PermuteLayerParams::~PermuteLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PermuteLayerParams)
  SharedDtor();
}

void PermuteLayerParams::SharedDtor() {
}

void PermuteLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PermuteLayerParams& PermuteLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PermuteLayerParams* PermuteLayerParams::New(::google::protobuf::Arena* arena) const {
  PermuteLayerParams* n = new PermuteLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PermuteLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PermuteLayerParams)
  axis_.Clear();
}

bool PermuteLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PermuteLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_axis())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_axis())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PermuteLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PermuteLayerParams)
  return false;
#undef DO_
}

void PermuteLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PermuteLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 axis = 1;
  if (this->axis_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axis_cached_byte_size_);
  }
  for (int i = 0, n = this->axis_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->axis(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PermuteLayerParams)
}

size_t PermuteLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PermuteLayerParams)
  size_t total_size = 0;

  // repeated uint64 axis = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->axis_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axis_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PermuteLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PermuteLayerParams*>(&from));
}

void PermuteLayerParams::MergeFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PermuteLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axis_.MergeFrom(from.axis_);
}

void PermuteLayerParams::CopyFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PermuteLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PermuteLayerParams::IsInitialized() const {
  return true;
}

void PermuteLayerParams::Swap(PermuteLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PermuteLayerParams::InternalSwap(PermuteLayerParams* other) {
  axis_.InternalSwap(&other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PermuteLayerParams::GetTypeName() const {
  return "CoreML.Specification.PermuteLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PermuteLayerParams

// repeated uint64 axis = 1;
int PermuteLayerParams::axis_size() const {
  return axis_.size();
}
void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
::google::protobuf::uint64 PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return axis_.Get(index);
}
void PermuteLayerParams::set_axis(int index, ::google::protobuf::uint64 value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
void PermuteLayerParams::add_axis(::google::protobuf::uint64 value) {
  axis_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return axis_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return &axis_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReorganizeDataLayerParams::kModeFieldNumber;
const int ReorganizeDataLayerParams::kBlockSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReorganizeDataLayerParams::ReorganizeDataLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReorganizeDataLayerParams)
}
ReorganizeDataLayerParams::ReorganizeDataLayerParams(const ReorganizeDataLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&blocksize_, &from.blocksize_,
    reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReorganizeDataLayerParams)
}

void ReorganizeDataLayerParams::SharedCtor() {
  ::memset(&blocksize_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
  _cached_size_ = 0;
}

ReorganizeDataLayerParams::~ReorganizeDataLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReorganizeDataLayerParams)
  SharedDtor();
}

void ReorganizeDataLayerParams::SharedDtor() {
}

void ReorganizeDataLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReorganizeDataLayerParams& ReorganizeDataLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReorganizeDataLayerParams* ReorganizeDataLayerParams::New(::google::protobuf::Arena* arena) const {
  ReorganizeDataLayerParams* n = new ReorganizeDataLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReorganizeDataLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReorganizeDataLayerParams)
  ::memset(&blocksize_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
}

bool ReorganizeDataLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReorganizeDataLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 blockSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &blocksize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReorganizeDataLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReorganizeDataLayerParams)
  return false;
#undef DO_
}

void ReorganizeDataLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReorganizeDataLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // uint64 blockSize = 2;
  if (this->blocksize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->blocksize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReorganizeDataLayerParams)
}

size_t ReorganizeDataLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReorganizeDataLayerParams)
  size_t total_size = 0;

  // uint64 blockSize = 2;
  if (this->blocksize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->blocksize());
  }

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReorganizeDataLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReorganizeDataLayerParams*>(&from));
}

void ReorganizeDataLayerParams::MergeFrom(const ReorganizeDataLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReorganizeDataLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.blocksize() != 0) {
    set_blocksize(from.blocksize());
  }
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReorganizeDataLayerParams::CopyFrom(const ReorganizeDataLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReorganizeDataLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReorganizeDataLayerParams::IsInitialized() const {
  return true;
}

void ReorganizeDataLayerParams::Swap(ReorganizeDataLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReorganizeDataLayerParams::InternalSwap(ReorganizeDataLayerParams* other) {
  std::swap(blocksize_, other->blocksize_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReorganizeDataLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReorganizeDataLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReorganizeDataLayerParams

// .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
void ReorganizeDataLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(mode_);
}
void ReorganizeDataLayerParams::set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.mode)
}

// uint64 blockSize = 2;
void ReorganizeDataLayerParams::clear_blocksize() {
  blocksize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ReorganizeDataLayerParams::blocksize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
  return blocksize_;
}
void ReorganizeDataLayerParams::set_blocksize(::google::protobuf::uint64 value) {
  
  blocksize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SliceLayerParams::kStartIndexFieldNumber;
const int SliceLayerParams::kEndIndexFieldNumber;
const int SliceLayerParams::kStrideFieldNumber;
const int SliceLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SliceLayerParams::SliceLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SliceLayerParams)
}
SliceLayerParams::SliceLayerParams(const SliceLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startindex_, &from.startindex_,
    reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SliceLayerParams)
}

void SliceLayerParams::SharedCtor() {
  ::memset(&startindex_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
  _cached_size_ = 0;
}

SliceLayerParams::~SliceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SliceLayerParams)
  SharedDtor();
}

void SliceLayerParams::SharedDtor() {
}

void SliceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SliceLayerParams& SliceLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SliceLayerParams* SliceLayerParams::New(::google::protobuf::Arena* arena) const {
  SliceLayerParams* n = new SliceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SliceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SliceLayerParams)
  ::memset(&startindex_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
}

bool SliceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SliceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 startIndex = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &startindex_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 endIndex = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &endindex_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 stride = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &stride_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_axis(static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SliceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SliceLayerParams)
  return false;
#undef DO_
}

void SliceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SliceLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 startIndex = 1;
  if (this->startindex() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->startindex(), output);
  }

  // int64 endIndex = 2;
  if (this->endindex() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(2, this->endindex(), output);
  }

  // uint64 stride = 3;
  if (this->stride() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->stride(), output);
  }

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      4, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SliceLayerParams)
}

size_t SliceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SliceLayerParams)
  size_t total_size = 0;

  // int64 startIndex = 1;
  if (this->startindex() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->startindex());
  }

  // int64 endIndex = 2;
  if (this->endindex() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->endindex());
  }

  // uint64 stride = 3;
  if (this->stride() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->stride());
  }

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SliceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SliceLayerParams*>(&from));
}

void SliceLayerParams::MergeFrom(const SliceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SliceLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startindex() != 0) {
    set_startindex(from.startindex());
  }
  if (from.endindex() != 0) {
    set_endindex(from.endindex());
  }
  if (from.stride() != 0) {
    set_stride(from.stride());
  }
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void SliceLayerParams::CopyFrom(const SliceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SliceLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SliceLayerParams::IsInitialized() const {
  return true;
}

void SliceLayerParams::Swap(SliceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SliceLayerParams::InternalSwap(SliceLayerParams* other) {
  std::swap(startindex_, other->startindex_);
  std::swap(endindex_, other->endindex_);
  std::swap(stride_, other->stride_);
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SliceLayerParams::GetTypeName() const {
  return "CoreML.Specification.SliceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SliceLayerParams

// int64 startIndex = 1;
void SliceLayerParams::clear_startindex() {
  startindex_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SliceLayerParams::startindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.startIndex)
  return startindex_;
}
void SliceLayerParams::set_startindex(::google::protobuf::int64 value) {
  
  startindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.startIndex)
}

// int64 endIndex = 2;
void SliceLayerParams::clear_endindex() {
  endindex_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SliceLayerParams::endindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.endIndex)
  return endindex_;
}
void SliceLayerParams::set_endindex(::google::protobuf::int64 value) {
  
  endindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.endIndex)
}

// uint64 stride = 3;
void SliceLayerParams::clear_stride() {
  stride_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SliceLayerParams::stride() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.stride)
  return stride_;
}
void SliceLayerParams::set_stride(::google::protobuf::uint64 value) {
  
  stride_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.stride)
}

// .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
void SliceLayerParams::clear_axis() {
  axis_ = 0;
}
::CoreML::Specification::SliceLayerParams_SliceAxis SliceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.axis)
  return static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(axis_);
}
void SliceLayerParams::set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceLayerParams::kModeFieldNumber;
const int ReduceLayerParams::kEpsilonFieldNumber;
const int ReduceLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceLayerParams::ReduceLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceLayerParams)
}
ReduceLayerParams::ReduceLayerParams(const ReduceLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&mode_, &from.mode_,
    reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceLayerParams)
}

void ReduceLayerParams::SharedCtor() {
  ::memset(&mode_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
  _cached_size_ = 0;
}

ReduceLayerParams::~ReduceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceLayerParams)
  SharedDtor();
}

void ReduceLayerParams::SharedDtor() {
}

void ReduceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceLayerParams& ReduceLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceLayerParams* ReduceLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceLayerParams* n = new ReduceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceLayerParams)
  ::memset(&mode_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
}

bool ReduceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_axis(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceLayerParams)
  return false;
#undef DO_
}

void ReduceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // float epsilon = 2;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->epsilon(), output);
  }

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      3, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceLayerParams)
}

size_t ReduceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  // float epsilon = 2;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceLayerParams*>(&from));
}

void ReduceLayerParams::MergeFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void ReduceLayerParams::CopyFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceLayerParams::IsInitialized() const {
  return true;
}

void ReduceLayerParams::Swap(ReduceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceLayerParams::InternalSwap(ReduceLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceLayerParams

// .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// float epsilon = 2;
void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return epsilon_;
}
void ReduceLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

// .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
void ReduceLayerParams::clear_axis() {
  axis_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceAxis ReduceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.axis)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(axis_);
}
void ReduceLayerParams::set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CropLayerParams::kCropAmountsFieldNumber;
const int CropLayerParams::kOffsetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CropLayerParams::CropLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CropLayerParams)
}
CropLayerParams::CropLayerParams(const CropLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      offset_(from.offset_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_cropamounts()) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts(*from.cropamounts_);
  } else {
    cropamounts_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CropLayerParams)
}

void CropLayerParams::SharedCtor() {
  cropamounts_ = NULL;
  _cached_size_ = 0;
}

CropLayerParams::~CropLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CropLayerParams)
  SharedDtor();
}

void CropLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete cropamounts_;
  }
}

void CropLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CropLayerParams& CropLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CropLayerParams* CropLayerParams::New(::google::protobuf::Arena* arena) const {
  CropLayerParams* n = new CropLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CropLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CropLayerParams)
  offset_.Clear();
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) {
    delete cropamounts_;
  }
  cropamounts_ = NULL;
}

bool CropLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CropLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BorderAmounts cropAmounts = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cropamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 offset = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_offset())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(40u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 42u, input, this->mutable_offset())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CropLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CropLayerParams)
  return false;
#undef DO_
}

void CropLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CropLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->cropamounts_, output);
  }

  // repeated uint64 offset = 5;
  if (this->offset_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_offset_cached_byte_size_);
  }
  for (int i = 0, n = this->offset_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->offset(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CropLayerParams)
}

size_t CropLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CropLayerParams)
  size_t total_size = 0;

  // repeated uint64 offset = 5;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->offset_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _offset_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->cropamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CropLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CropLayerParams*>(&from));
}

void CropLayerParams::MergeFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CropLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  offset_.MergeFrom(from.offset_);
  if (from.has_cropamounts()) {
    mutable_cropamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.cropamounts());
  }
}

void CropLayerParams::CopyFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CropLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CropLayerParams::IsInitialized() const {
  return true;
}

void CropLayerParams::Swap(CropLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CropLayerParams::InternalSwap(CropLayerParams* other) {
  offset_.InternalSwap(&other->offset_);
  std::swap(cropamounts_, other->cropamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CropLayerParams::GetTypeName() const {
  return "CoreML.Specification.CropLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CropLayerParams

// .CoreML.Specification.BorderAmounts cropAmounts = 1;
bool CropLayerParams::has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != NULL;
}
void CropLayerParams::clear_cropamounts() {
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_ != NULL ? *cropamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {
  
  if (cropamounts_ == NULL) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_;
}
::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = NULL;
  return temp;
}
void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  delete cropamounts_;
  cropamounts_ = cropamounts;
  if (cropamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
int CropLayerParams::offset_size() const {
  return offset_.size();
}
void CropLayerParams::clear_offset() {
  offset_.Clear();
}
::google::protobuf::uint64 CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return offset_.Get(index);
}
void CropLayerParams::set_offset(int index, ::google::protobuf::uint64 value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
void CropLayerParams::add_offset(::google::protobuf::uint64 value) {
  offset_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return offset_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return &offset_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AverageLayerParams::AverageLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AverageLayerParams)
}
AverageLayerParams::AverageLayerParams(const AverageLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AverageLayerParams)
}

void AverageLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AverageLayerParams::~AverageLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AverageLayerParams)
  SharedDtor();
}

void AverageLayerParams::SharedDtor() {
}

void AverageLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AverageLayerParams& AverageLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AverageLayerParams* AverageLayerParams::New(::google::protobuf::Arena* arena) const {
  AverageLayerParams* n = new AverageLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AverageLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AverageLayerParams)
}

bool AverageLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AverageLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AverageLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AverageLayerParams)
  return false;
#undef DO_
}

void AverageLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AverageLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AverageLayerParams)
}

size_t AverageLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AverageLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AverageLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AverageLayerParams*>(&from));
}

void AverageLayerParams::MergeFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AverageLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AverageLayerParams::CopyFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AverageLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AverageLayerParams::IsInitialized() const {
  return true;
}

void AverageLayerParams::Swap(AverageLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AverageLayerParams::InternalSwap(AverageLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AverageLayerParams::GetTypeName() const {
  return "CoreML.Specification.AverageLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AverageLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MaxLayerParams::MaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MaxLayerParams)
}
MaxLayerParams::MaxLayerParams(const MaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MaxLayerParams)
}

void MaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MaxLayerParams::~MaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MaxLayerParams)
  SharedDtor();
}

void MaxLayerParams::SharedDtor() {
}

void MaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MaxLayerParams& MaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MaxLayerParams* MaxLayerParams::New(::google::protobuf::Arena* arena) const {
  MaxLayerParams* n = new MaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MaxLayerParams)
}

bool MaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MaxLayerParams)
  return false;
#undef DO_
}

void MaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MaxLayerParams)
}

size_t MaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MaxLayerParams*>(&from));
}

void MaxLayerParams::MergeFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MaxLayerParams::CopyFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MaxLayerParams::IsInitialized() const {
  return true;
}

void MaxLayerParams::Swap(MaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MaxLayerParams::InternalSwap(MaxLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.MaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MaxLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MinLayerParams::MinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MinLayerParams)
}
MinLayerParams::MinLayerParams(const MinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MinLayerParams)
}

void MinLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MinLayerParams::~MinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MinLayerParams)
  SharedDtor();
}

void MinLayerParams::SharedDtor() {
}

void MinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MinLayerParams& MinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MinLayerParams* MinLayerParams::New(::google::protobuf::Arena* arena) const {
  MinLayerParams* n = new MinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MinLayerParams)
}

bool MinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MinLayerParams)
  return false;
#undef DO_
}

void MinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MinLayerParams)
}

size_t MinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MinLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MinLayerParams*>(&from));
}

void MinLayerParams::MergeFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MinLayerParams::CopyFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MinLayerParams::IsInitialized() const {
  return true;
}

void MinLayerParams::Swap(MinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MinLayerParams::InternalSwap(MinLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MinLayerParams::GetTypeName() const {
  return "CoreML.Specification.MinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MinLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int DotProductLayerParams::kCosineSimilarityFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DotProductLayerParams::DotProductLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.DotProductLayerParams)
}
DotProductLayerParams::DotProductLayerParams(const DotProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  cosinesimilarity_ = from.cosinesimilarity_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.DotProductLayerParams)
}

void DotProductLayerParams::SharedCtor() {
  cosinesimilarity_ = false;
  _cached_size_ = 0;
}

DotProductLayerParams::~DotProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.DotProductLayerParams)
  SharedDtor();
}

void DotProductLayerParams::SharedDtor() {
}

void DotProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const DotProductLayerParams& DotProductLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

DotProductLayerParams* DotProductLayerParams::New(::google::protobuf::Arena* arena) const {
  DotProductLayerParams* n = new DotProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void DotProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.DotProductLayerParams)
  cosinesimilarity_ = false;
}

bool DotProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.DotProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool cosineSimilarity = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &cosinesimilarity_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.DotProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.DotProductLayerParams)
  return false;
#undef DO_
}

void DotProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.DotProductLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->cosinesimilarity(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.DotProductLayerParams)
}

size_t DotProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.DotProductLayerParams)
  size_t total_size = 0;

  // bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void DotProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const DotProductLayerParams*>(&from));
}

void DotProductLayerParams::MergeFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.DotProductLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.cosinesimilarity() != 0) {
    set_cosinesimilarity(from.cosinesimilarity());
  }
}

void DotProductLayerParams::CopyFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.DotProductLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DotProductLayerParams::IsInitialized() const {
  return true;
}

void DotProductLayerParams::Swap(DotProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DotProductLayerParams::InternalSwap(DotProductLayerParams* other) {
  std::swap(cosinesimilarity_, other->cosinesimilarity_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string DotProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.DotProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// DotProductLayerParams

// bool cosineSimilarity = 1;
void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return cosinesimilarity_;
}
void DotProductLayerParams::set_cosinesimilarity(bool value) {
  
  cosinesimilarity_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MeanVarianceNormalizeLayerParams::kAcrossChannelsFieldNumber;
const int MeanVarianceNormalizeLayerParams::kNormalizeVarianceFieldNumber;
const int MeanVarianceNormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}
MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&acrosschannels_, &from.acrosschannels_,
    reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

void MeanVarianceNormalizeLayerParams::SharedCtor() {
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

MeanVarianceNormalizeLayerParams::~MeanVarianceNormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  SharedDtor();
}

void MeanVarianceNormalizeLayerParams::SharedDtor() {
}

void MeanVarianceNormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MeanVarianceNormalizeLayerParams& MeanVarianceNormalizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MeanVarianceNormalizeLayerParams* MeanVarianceNormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  MeanVarianceNormalizeLayerParams* n = new MeanVarianceNormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MeanVarianceNormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
}

bool MeanVarianceNormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool acrossChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &acrosschannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool normalizeVariance = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &normalizevariance_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return false;
#undef DO_
}

void MeanVarianceNormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->acrosschannels(), output);
  }

  // bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->normalizevariance(), output);
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

size_t MeanVarianceNormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  size_t total_size = 0;

  // bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    total_size += 1 + 1;
  }

  // bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    total_size += 1 + 1;
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MeanVarianceNormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MeanVarianceNormalizeLayerParams*>(&from));
}

void MeanVarianceNormalizeLayerParams::MergeFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.acrosschannels() != 0) {
    set_acrosschannels(from.acrosschannels());
  }
  if (from.normalizevariance() != 0) {
    set_normalizevariance(from.normalizevariance());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void MeanVarianceNormalizeLayerParams::CopyFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MeanVarianceNormalizeLayerParams::IsInitialized() const {
  return true;
}

void MeanVarianceNormalizeLayerParams::Swap(MeanVarianceNormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MeanVarianceNormalizeLayerParams::InternalSwap(MeanVarianceNormalizeLayerParams* other) {
  std::swap(acrosschannels_, other->acrosschannels_);
  std::swap(normalizevariance_, other->normalizevariance_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MeanVarianceNormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.MeanVarianceNormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MeanVarianceNormalizeLayerParams

// bool acrossChannels = 1;
void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return acrosschannels_;
}
void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {
  
  acrosschannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// bool normalizeVariance = 2;
void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return normalizevariance_;
}
void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {
  
  normalizevariance_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// float epsilon = 3;
void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return epsilon_;
}
void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SequenceRepeatLayerParams::kNRepetitionsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SequenceRepeatLayerParams::SequenceRepeatLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SequenceRepeatLayerParams)
}
SequenceRepeatLayerParams::SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  nrepetitions_ = from.nrepetitions_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SequenceRepeatLayerParams)
}

void SequenceRepeatLayerParams::SharedCtor() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SequenceRepeatLayerParams::~SequenceRepeatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SequenceRepeatLayerParams)
  SharedDtor();
}

void SequenceRepeatLayerParams::SharedDtor() {
}

void SequenceRepeatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SequenceRepeatLayerParams& SequenceRepeatLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SequenceRepeatLayerParams* SequenceRepeatLayerParams::New(::google::protobuf::Arena* arena) const {
  SequenceRepeatLayerParams* n = new SequenceRepeatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SequenceRepeatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SequenceRepeatLayerParams)
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}

bool SequenceRepeatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SequenceRepeatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 nRepetitions = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &nrepetitions_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SequenceRepeatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SequenceRepeatLayerParams)
  return false;
#undef DO_
}

void SequenceRepeatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SequenceRepeatLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->nrepetitions(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SequenceRepeatLayerParams)
}

size_t SequenceRepeatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SequenceRepeatLayerParams)
  size_t total_size = 0;

  // uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->nrepetitions());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SequenceRepeatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SequenceRepeatLayerParams*>(&from));
}

void SequenceRepeatLayerParams::MergeFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.nrepetitions() != 0) {
    set_nrepetitions(from.nrepetitions());
  }
}

void SequenceRepeatLayerParams::CopyFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SequenceRepeatLayerParams::IsInitialized() const {
  return true;
}

void SequenceRepeatLayerParams::Swap(SequenceRepeatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SequenceRepeatLayerParams::InternalSwap(SequenceRepeatLayerParams* other) {
  std::swap(nrepetitions_, other->nrepetitions_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SequenceRepeatLayerParams::GetTypeName() const {
  return "CoreML.Specification.SequenceRepeatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SequenceRepeatLayerParams

// uint64 nRepetitions = 1;
void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return nrepetitions_;
}
void SequenceRepeatLayerParams::set_nrepetitions(::google::protobuf::uint64 value) {
  
  nrepetitions_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SimpleRecurrentLayerParams::kInputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kOutputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kActivationFieldNumber;
const int SimpleRecurrentLayerParams::kSequenceOutputFieldNumber;
const int SimpleRecurrentLayerParams::kHasBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kWeightMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kRecursionMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SimpleRecurrentLayerParams::SimpleRecurrentLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}
SimpleRecurrentLayerParams::SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_activation()) {
    activation_ = new ::CoreML::Specification::ActivationParams(*from.activation_);
  } else {
    activation_ = NULL;
  }
  if (from.has_weightmatrix()) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams(*from.weightmatrix_);
  } else {
    weightmatrix_ = NULL;
  }
  if (from.has_recursionmatrix()) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.recursionmatrix_);
  } else {
    recursionmatrix_ = NULL;
  }
  if (from.has_biasvector()) {
    biasvector_ = new ::CoreML::Specification::WeightParams(*from.biasvector_);
  } else {
    biasvector_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}

void SimpleRecurrentLayerParams::SharedCtor() {
  ::memset(&activation_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&activation_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

SimpleRecurrentLayerParams::~SimpleRecurrentLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SimpleRecurrentLayerParams)
  SharedDtor();
}

void SimpleRecurrentLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete activation_;
  }
  if (this != internal_default_instance()) {
    delete weightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete recursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete biasvector_;
  }
}

void SimpleRecurrentLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SimpleRecurrentLayerParams& SimpleRecurrentLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SimpleRecurrentLayerParams* SimpleRecurrentLayerParams::New(::google::protobuf::Arena* arena) const {
  SimpleRecurrentLayerParams* n = new SimpleRecurrentLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SimpleRecurrentLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) {
    delete activation_;
  }
  activation_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) {
    delete weightmatrix_;
  }
  weightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) {
    delete recursionmatrix_;
  }
  recursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) {
    delete biasvector_;
  }
  biasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool SimpleRecurrentLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SimpleRecurrentLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParams activation = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool sequenceOutput = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(120u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVector = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvector_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weightMatrix = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams recursionMatrix = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_recursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams biasVector = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_biasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SimpleRecurrentLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SimpleRecurrentLayerParams)
  return false;
#undef DO_
}

void SimpleRecurrentLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SimpleRecurrentLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->activation_, output);
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvector(), output);
  }

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->weightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->recursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->biasvector_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SimpleRecurrentLayerParams)
}

size_t SimpleRecurrentLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SimpleRecurrentLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->activation_);
  }

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightmatrix_);
  }

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->recursionmatrix_);
  }

  // .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->biasvector_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    total_size += 2 + 1;
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SimpleRecurrentLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SimpleRecurrentLayerParams*>(&from));
}

void SimpleRecurrentLayerParams::MergeFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_activation()) {
    mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
  }
  if (from.has_weightmatrix()) {
    mutable_weightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.weightmatrix());
  }
  if (from.has_recursionmatrix()) {
    mutable_recursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.recursionmatrix());
  }
  if (from.has_biasvector()) {
    mutable_biasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.biasvector());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvector() != 0) {
    set_hasbiasvector(from.hasbiasvector());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void SimpleRecurrentLayerParams::CopyFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SimpleRecurrentLayerParams::IsInitialized() const {
  return true;
}

void SimpleRecurrentLayerParams::Swap(SimpleRecurrentLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SimpleRecurrentLayerParams::InternalSwap(SimpleRecurrentLayerParams* other) {
  std::swap(activation_, other->activation_);
  std::swap(weightmatrix_, other->weightmatrix_);
  std::swap(recursionmatrix_, other->recursionmatrix_);
  std::swap(biasvector_, other->biasvector_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvector_, other->hasbiasvector_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SimpleRecurrentLayerParams::GetTypeName() const {
  return "CoreML.Specification.SimpleRecurrentLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SimpleRecurrentLayerParams

// uint64 inputVectorSize = 1;
void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void SimpleRecurrentLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void SimpleRecurrentLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// .CoreML.Specification.ActivationParams activation = 10;
bool SimpleRecurrentLayerParams::has_activation() const {
  return this != internal_default_instance() && activation_ != NULL;
}
void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
}
const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_ != NULL ? *activation_
                         : *::CoreML::Specification::ActivationParams::internal_default_instance();
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {
  
  if (activation_ == NULL) {
    activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_;
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  delete activation_;
  activation_ = activation;
  if (activation) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// bool sequenceOutput = 15;
void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return sequenceoutput_;
}
void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// bool hasBiasVector = 20;
void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return hasbiasvector_;
}
void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {
  
  hasbiasvector_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// .CoreML.Specification.WeightParams weightMatrix = 30;
bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_ != NULL ? *weightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {
  
  if (weightmatrix_ == NULL) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  delete weightmatrix_;
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// .CoreML.Specification.WeightParams recursionMatrix = 31;
bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_ != NULL ? *recursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {
  
  if (recursionmatrix_ == NULL) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  delete recursionmatrix_;
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// .CoreML.Specification.WeightParams biasVector = 32;
bool SimpleRecurrentLayerParams::has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != NULL;
}
void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_ != NULL ? *biasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {
  
  if (biasvector_ == NULL) {
    biasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  delete biasvector_;
  biasvector_ = biasvector;
  if (biasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// bool reverseInput = 100;
void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return reverseinput_;
}
void SimpleRecurrentLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GRULayerParams::kInputVectorSizeFieldNumber;
const int GRULayerParams::kOutputVectorSizeFieldNumber;
const int GRULayerParams::kActivationsFieldNumber;
const int GRULayerParams::kSequenceOutputFieldNumber;
const int GRULayerParams::kHasBiasVectorsFieldNumber;
const int GRULayerParams::kUpdateGateWeightMatrixFieldNumber;
const int GRULayerParams::kResetGateWeightMatrixFieldNumber;
const int GRULayerParams::kOutputGateWeightMatrixFieldNumber;
const int GRULayerParams::kUpdateGateRecursionMatrixFieldNumber;
const int GRULayerParams::kResetGateRecursionMatrixFieldNumber;
const int GRULayerParams::kOutputGateRecursionMatrixFieldNumber;
const int GRULayerParams::kUpdateGateBiasVectorFieldNumber;
const int GRULayerParams::kResetGateBiasVectorFieldNumber;
const int GRULayerParams::kOutputGateBiasVectorFieldNumber;
const int GRULayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GRULayerParams::GRULayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GRULayerParams)
}
GRULayerParams::GRULayerParams(const GRULayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activations_(from.activations_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updategateweightmatrix()) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.updategateweightmatrix_);
  } else {
    updategateweightmatrix_ = NULL;
  }
  if (from.has_resetgateweightmatrix()) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.resetgateweightmatrix_);
  } else {
    resetgateweightmatrix_ = NULL;
  }
  if (from.has_outputgateweightmatrix()) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgateweightmatrix_);
  } else {
    outputgateweightmatrix_ = NULL;
  }
  if (from.has_updategaterecursionmatrix()) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.updategaterecursionmatrix_);
  } else {
    updategaterecursionmatrix_ = NULL;
  }
  if (from.has_resetgaterecursionmatrix()) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.resetgaterecursionmatrix_);
  } else {
    resetgaterecursionmatrix_ = NULL;
  }
  if (from.has_outputgaterecursionmatrix()) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgaterecursionmatrix_);
  } else {
    outputgaterecursionmatrix_ = NULL;
  }
  if (from.has_updategatebiasvector()) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.updategatebiasvector_);
  } else {
    updategatebiasvector_ = NULL;
  }
  if (from.has_resetgatebiasvector()) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.resetgatebiasvector_);
  } else {
    resetgatebiasvector_ = NULL;
  }
  if (from.has_outputgatebiasvector()) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.outputgatebiasvector_);
  } else {
    outputgatebiasvector_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GRULayerParams)
}

void GRULayerParams::SharedCtor() {
  ::memset(&updategateweightmatrix_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&updategateweightmatrix_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

GRULayerParams::~GRULayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GRULayerParams)
  SharedDtor();
}

void GRULayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updategateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete resetgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete updategaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete resetgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete updategatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete resetgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatebiasvector_;
  }
}

void GRULayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GRULayerParams& GRULayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GRULayerParams* GRULayerParams::New(::google::protobuf::Arena* arena) const {
  GRULayerParams* n = new GRULayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GRULayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GRULayerParams)
  activations_.Clear();
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) {
    delete updategateweightmatrix_;
  }
  updategateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) {
    delete resetgateweightmatrix_;
  }
  resetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) {
    delete updategaterecursionmatrix_;
  }
  updategaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) {
    delete resetgaterecursionmatrix_;
  }
  resetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) {
    delete updategatebiasvector_;
  }
  updategatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) {
    delete resetgatebiasvector_;
  }
  resetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool GRULayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GRULayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool sequenceOutput = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(120u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVectors = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
      case 51: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
      case 52: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(418u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
      case 71: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(570u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
      case 72: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(578u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GRULayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GRULayerParams)
  return false;
#undef DO_
}

void GRULayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GRULayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->updategateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->resetgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->outputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *this->updategaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *this->resetgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      52, *this->outputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *this->updategatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *this->resetgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      72, *this->outputgatebiasvector_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GRULayerParams)
}

size_t GRULayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GRULayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategatebiasvector_);
  }

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GRULayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GRULayerParams*>(&from));
}

void GRULayerParams::MergeFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GRULayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activations_.MergeFrom(from.activations_);
  if (from.has_updategateweightmatrix()) {
    mutable_updategateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategateweightmatrix());
  }
  if (from.has_resetgateweightmatrix()) {
    mutable_resetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgateweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_updategaterecursionmatrix()) {
    mutable_updategaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategaterecursionmatrix());
  }
  if (from.has_resetgaterecursionmatrix()) {
    mutable_resetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgaterecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_updategatebiasvector()) {
    mutable_updategatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.updategatebiasvector());
  }
  if (from.has_resetgatebiasvector()) {
    mutable_resetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgatebiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void GRULayerParams::CopyFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GRULayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GRULayerParams::IsInitialized() const {
  return true;
}

void GRULayerParams::Swap(GRULayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GRULayerParams::InternalSwap(GRULayerParams* other) {
  activations_.InternalSwap(&other->activations_);
  std::swap(updategateweightmatrix_, other->updategateweightmatrix_);
  std::swap(resetgateweightmatrix_, other->resetgateweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(updategaterecursionmatrix_, other->updategaterecursionmatrix_);
  std::swap(resetgaterecursionmatrix_, other->resetgaterecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(updategatebiasvector_, other->updategatebiasvector_);
  std::swap(resetgatebiasvector_, other->resetgatebiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GRULayerParams::GetTypeName() const {
  return "CoreML.Specification.GRULayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GRULayerParams

// uint64 inputVectorSize = 1;
void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return inputvectorsize_;
}
void GRULayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return outputvectorsize_;
}
void GRULayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int GRULayerParams::activations_size() const {
  return activations_.size();
}
void GRULayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// bool sequenceOutput = 15;
void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return sequenceoutput_;
}
void GRULayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return hasbiasvectors_;
}
void GRULayerParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
bool GRULayerParams::has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != NULL;
}
void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_ != NULL ? *updategateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {
  
  if (updategateweightmatrix_ == NULL) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  delete updategateweightmatrix_;
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
bool GRULayerParams::has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_ != NULL ? *resetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {
  
  if (resetgateweightmatrix_ == NULL) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  delete resetgateweightmatrix_;
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
bool GRULayerParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
bool GRULayerParams::has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_ != NULL ? *updategaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {
  
  if (updategaterecursionmatrix_ == NULL) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_ != NULL ? *resetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {
  
  if (resetgaterecursionmatrix_ == NULL) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams updateGateBiasVector = 70;
bool GRULayerParams::has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != NULL;
}
void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_ != NULL ? *updategatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {
  
  if (updategatebiasvector_ == NULL) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  delete updategatebiasvector_;
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// .CoreML.Specification.WeightParams resetGateBiasVector = 71;
bool GRULayerParams::has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != NULL;
}
void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_ != NULL ? *resetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {
  
  if (resetgatebiasvector_ == NULL) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  delete resetgatebiasvector_;
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 72;
bool GRULayerParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// bool reverseInput = 100;
void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return reverseinput_;
}
void GRULayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMParams::kSequenceOutputFieldNumber;
const int LSTMParams::kHasBiasVectorsFieldNumber;
const int LSTMParams::kForgetBiasFieldNumber;
const int LSTMParams::kHasPeepholeVectorsFieldNumber;
const int LSTMParams::kCoupledInputAndForgetGateFieldNumber;
const int LSTMParams::kCellClipThresholdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMParams::LSTMParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMParams)
}
LSTMParams::LSTMParams(const LSTMParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&cellclipthreshold_, &from.cellclipthreshold_,
    reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMParams)
}

void LSTMParams::SharedCtor() {
  ::memset(&cellclipthreshold_, 0, reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
  _cached_size_ = 0;
}

LSTMParams::~LSTMParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMParams)
  SharedDtor();
}

void LSTMParams::SharedDtor() {
}

void LSTMParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMParams& LSTMParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LSTMParams* LSTMParams::New(::google::protobuf::Arena* arena) const {
  LSTMParams* n = new LSTMParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMParams)
  ::memset(&cellclipthreshold_, 0, reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
}

bool LSTMParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool sequenceOutput = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVectors = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool forgetBias = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(240u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &forgetbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasPeepholeVectors = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(320u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &haspeepholevectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool coupledInputAndForgetGate = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &coupledinputandforgetgate_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float cellClipThreshold = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(485u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &cellclipthreshold_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMParams)
  return false;
#undef DO_
}

void LSTMParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->sequenceoutput(), output);
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(30, this->forgetbias(), output);
  }

  // bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(40, this->haspeepholevectors(), output);
  }

  // bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->coupledinputandforgetgate(), output);
  }

  // float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(60, this->cellclipthreshold(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMParams)
}

size_t LSTMParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMParams)
  size_t total_size = 0;

  // float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    total_size += 2 + 4;
  }

  // bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    total_size += 2 + 1;
  }

  // bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    total_size += 2 + 1;
  }

  // bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMParams*>(&from));
}

void LSTMParams::MergeFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.cellclipthreshold() != 0) {
    set_cellclipthreshold(from.cellclipthreshold());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.forgetbias() != 0) {
    set_forgetbias(from.forgetbias());
  }
  if (from.haspeepholevectors() != 0) {
    set_haspeepholevectors(from.haspeepholevectors());
  }
  if (from.coupledinputandforgetgate() != 0) {
    set_coupledinputandforgetgate(from.coupledinputandforgetgate());
  }
}

void LSTMParams::CopyFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LSTMParams::IsInitialized() const {
  return true;
}

void LSTMParams::Swap(LSTMParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMParams::InternalSwap(LSTMParams* other) {
  std::swap(cellclipthreshold_, other->cellclipthreshold_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(forgetbias_, other->forgetbias_);
  std::swap(haspeepholevectors_, other->haspeepholevectors_);
  std::swap(coupledinputandforgetgate_, other->coupledinputandforgetgate_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMParams::GetTypeName() const {
  return "CoreML.Specification.LSTMParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMParams

// bool sequenceOutput = 10;
void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return sequenceoutput_;
}
void LSTMParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return hasbiasvectors_;
}
void LSTMParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// bool forgetBias = 30;
void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return forgetbias_;
}
void LSTMParams::set_forgetbias(bool value) {
  
  forgetbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// bool hasPeepholeVectors = 40;
void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return haspeepholevectors_;
}
void LSTMParams::set_haspeepholevectors(bool value) {
  
  haspeepholevectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// bool coupledInputAndForgetGate = 50;
void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return coupledinputandforgetgate_;
}
void LSTMParams::set_coupledinputandforgetgate(bool value) {
  
  coupledinputandforgetgate_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// float cellClipThreshold = 60;
void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return cellclipthreshold_;
}
void LSTMParams::set_cellclipthreshold(float value) {
  
  cellclipthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMWeightParams::kInputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputWeightMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kInputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputRecursionMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kInputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kForgetGateBiasVectorFieldNumber;
const int LSTMWeightParams::kBlockInputBiasVectorFieldNumber;
const int LSTMWeightParams::kOutputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kInputGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kForgetGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kOutputGatePeepholeVectorFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMWeightParams::LSTMWeightParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMWeightParams)
}
LSTMWeightParams::LSTMWeightParams(const LSTMWeightParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_inputgateweightmatrix()) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.inputgateweightmatrix_);
  } else {
    inputgateweightmatrix_ = NULL;
  }
  if (from.has_forgetgateweightmatrix()) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.forgetgateweightmatrix_);
  } else {
    forgetgateweightmatrix_ = NULL;
  }
  if (from.has_blockinputweightmatrix()) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.blockinputweightmatrix_);
  } else {
    blockinputweightmatrix_ = NULL;
  }
  if (from.has_outputgateweightmatrix()) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgateweightmatrix_);
  } else {
    outputgateweightmatrix_ = NULL;
  }
  if (from.has_inputgaterecursionmatrix()) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.inputgaterecursionmatrix_);
  } else {
    inputgaterecursionmatrix_ = NULL;
  }
  if (from.has_forgetgaterecursionmatrix()) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.forgetgaterecursionmatrix_);
  } else {
    forgetgaterecursionmatrix_ = NULL;
  }
  if (from.has_blockinputrecursionmatrix()) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.blockinputrecursionmatrix_);
  } else {
    blockinputrecursionmatrix_ = NULL;
  }
  if (from.has_outputgaterecursionmatrix()) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgaterecursionmatrix_);
  } else {
    outputgaterecursionmatrix_ = NULL;
  }
  if (from.has_inputgatebiasvector()) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.inputgatebiasvector_);
  } else {
    inputgatebiasvector_ = NULL;
  }
  if (from.has_forgetgatebiasvector()) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.forgetgatebiasvector_);
  } else {
    forgetgatebiasvector_ = NULL;
  }
  if (from.has_blockinputbiasvector()) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams(*from.blockinputbiasvector_);
  } else {
    blockinputbiasvector_ = NULL;
  }
  if (from.has_outputgatebiasvector()) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.outputgatebiasvector_);
  } else {
    outputgatebiasvector_ = NULL;
  }
  if (from.has_inputgatepeepholevector()) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.inputgatepeepholevector_);
  } else {
    inputgatepeepholevector_ = NULL;
  }
  if (from.has_forgetgatepeepholevector()) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.forgetgatepeepholevector_);
  } else {
    forgetgatepeepholevector_ = NULL;
  }
  if (from.has_outputgatepeepholevector()) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.outputgatepeepholevector_);
  } else {
    outputgatepeepholevector_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMWeightParams)
}

void LSTMWeightParams::SharedCtor() {
  ::memset(&inputgateweightmatrix_, 0, reinterpret_cast<char*>(&outputgatepeepholevector_) -
    reinterpret_cast<char*>(&inputgateweightmatrix_) + sizeof(outputgatepeepholevector_));
  _cached_size_ = 0;
}

LSTMWeightParams::~LSTMWeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMWeightParams)
  SharedDtor();
}

void LSTMWeightParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete inputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete forgetgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete blockinputweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete inputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete forgetgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete blockinputrecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete inputgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete forgetgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete blockinputbiasvector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete inputgatepeepholevector_;
  }
  if (this != internal_default_instance()) {
    delete forgetgatepeepholevector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatepeepholevector_;
  }
}

void LSTMWeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMWeightParams& LSTMWeightParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LSTMWeightParams* LSTMWeightParams::New(::google::protobuf::Arena* arena) const {
  LSTMWeightParams* n = new LSTMWeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMWeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMWeightParams)
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) {
    delete inputgateweightmatrix_;
  }
  inputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) {
    delete forgetgateweightmatrix_;
  }
  forgetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) {
    delete blockinputweightmatrix_;
  }
  blockinputweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) {
    delete inputgaterecursionmatrix_;
  }
  inputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) {
    delete forgetgaterecursionmatrix_;
  }
  forgetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) {
    delete blockinputrecursionmatrix_;
  }
  blockinputrecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) {
    delete inputgatebiasvector_;
  }
  inputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) {
    delete forgetgatebiasvector_;
  }
  forgetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) {
    delete blockinputbiasvector_;
  }
  blockinputbiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) {
    delete inputgatepeepholevector_;
  }
  inputgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) {
    delete forgetgatepeepholevector_;
  }
  forgetgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) {
    delete outputgatepeepholevector_;
  }
  outputgatepeepholevector_ = NULL;
}

bool LSTMWeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMWeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
      case 22: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(178u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputrecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
      case 23: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(186u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
      case 41: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(330u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
      case 42: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(338u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputbiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
      case 43: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(346u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
      case 61: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(490u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
      case 62: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(498u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMWeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMWeightParams)
  return false;
#undef DO_
}

void LSTMWeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMWeightParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->inputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->forgetgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->blockinputweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->outputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->inputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->forgetgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      22, *this->blockinputrecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      23, *this->outputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->inputgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *this->forgetgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      42, *this->blockinputbiasvector_, output);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      43, *this->outputgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *this->inputgatepeepholevector_, output);
  }

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      61, *this->forgetgatepeepholevector_, output);
  }

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      62, *this->outputgatepeepholevector_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMWeightParams)
}

size_t LSTMWeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMWeightParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputweightmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputrecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputbiasvector_);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatepeepholevector_);
  }

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatepeepholevector_);
  }

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatepeepholevector_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMWeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMWeightParams*>(&from));
}

void LSTMWeightParams::MergeFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMWeightParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_inputgateweightmatrix()) {
    mutable_inputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgateweightmatrix());
  }
  if (from.has_forgetgateweightmatrix()) {
    mutable_forgetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgateweightmatrix());
  }
  if (from.has_blockinputweightmatrix()) {
    mutable_blockinputweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_inputgaterecursionmatrix()) {
    mutable_inputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgaterecursionmatrix());
  }
  if (from.has_forgetgaterecursionmatrix()) {
    mutable_forgetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgaterecursionmatrix());
  }
  if (from.has_blockinputrecursionmatrix()) {
    mutable_blockinputrecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputrecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_inputgatebiasvector()) {
    mutable_inputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatebiasvector());
  }
  if (from.has_forgetgatebiasvector()) {
    mutable_forgetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatebiasvector());
  }
  if (from.has_blockinputbiasvector()) {
    mutable_blockinputbiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputbiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.has_inputgatepeepholevector()) {
    mutable_inputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatepeepholevector());
  }
  if (from.has_forgetgatepeepholevector()) {
    mutable_forgetgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatepeepholevector());
  }
  if (from.has_outputgatepeepholevector()) {
    mutable_outputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatepeepholevector());
  }
}

void LSTMWeightParams::CopyFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMWeightParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LSTMWeightParams::IsInitialized() const {
  return true;
}

void LSTMWeightParams::Swap(LSTMWeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMWeightParams::InternalSwap(LSTMWeightParams* other) {
  std::swap(inputgateweightmatrix_, other->inputgateweightmatrix_);
  std::swap(forgetgateweightmatrix_, other->forgetgateweightmatrix_);
  std::swap(blockinputweightmatrix_, other->blockinputweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(inputgaterecursionmatrix_, other->inputgaterecursionmatrix_);
  std::swap(forgetgaterecursionmatrix_, other->forgetgaterecursionmatrix_);
  std::swap(blockinputrecursionmatrix_, other->blockinputrecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(inputgatebiasvector_, other->inputgatebiasvector_);
  std::swap(forgetgatebiasvector_, other->forgetgatebiasvector_);
  std::swap(blockinputbiasvector_, other->blockinputbiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputgatepeepholevector_, other->inputgatepeepholevector_);
  std::swap(forgetgatepeepholevector_, other->forgetgatepeepholevector_);
  std::swap(outputgatepeepholevector_, other->outputgatepeepholevector_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMWeightParams::GetTypeName() const {
  return "CoreML.Specification.LSTMWeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMWeightParams

// .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_ != NULL ? *inputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {
  
  if (inputgateweightmatrix_ == NULL) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  delete inputgateweightmatrix_;
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_ != NULL ? *forgetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {
  
  if (forgetgateweightmatrix_ == NULL) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_ != NULL ? *blockinputweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {
  
  if (blockinputweightmatrix_ == NULL) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  delete blockinputweightmatrix_;
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_ != NULL ? *inputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {
  
  if (inputgaterecursionmatrix_ == NULL) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_ != NULL ? *forgetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {
  
  if (forgetgaterecursionmatrix_ == NULL) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_ != NULL ? *blockinputrecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {
  
  if (blockinputrecursionmatrix_ == NULL) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams inputGateBiasVector = 40;
bool LSTMWeightParams::has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_ != NULL ? *inputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {
  
  if (inputgatebiasvector_ == NULL) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  delete inputgatebiasvector_;
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_ != NULL ? *forgetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {
  
  if (forgetgatebiasvector_ == NULL) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  delete forgetgatebiasvector_;
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// .CoreML.Specification.WeightParams blockInputBiasVector = 42;
bool LSTMWeightParams::has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != NULL;
}
void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_ != NULL ? *blockinputbiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {
  
  if (blockinputbiasvector_ == NULL) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  delete blockinputbiasvector_;
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 43;
bool LSTMWeightParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_ != NULL ? *inputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {
  
  if (inputgatepeepholevector_ == NULL) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  delete inputgatepeepholevector_;
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_ != NULL ? *forgetgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {
  
  if (forgetgatepeepholevector_ == NULL) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_ != NULL ? *outputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {
  
  if (outputgatepeepholevector_ == NULL) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  delete outputgatepeepholevector_;
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UniDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kActivationsFieldNumber;
const int UniDirectionalLSTMLayerParams::kParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}
UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activations_(from.activations_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_params()) {
    params_ = new ::CoreML::Specification::LSTMParams(*from.params_);
  } else {
    params_ = NULL;
  }
  if (from.has_weightparams()) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams(*from.weightparams_);
  } else {
    weightparams_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

void UniDirectionalLSTMLayerParams::SharedCtor() {
  ::memset(&params_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&params_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

UniDirectionalLSTMLayerParams::~UniDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
  SharedDtor();
}

void UniDirectionalLSTMLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete params_;
  }
  if (this != internal_default_instance()) {
    delete weightparams_;
  }
}

void UniDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UniDirectionalLSTMLayerParams& UniDirectionalLSTMLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UniDirectionalLSTMLayerParams* UniDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  UniDirectionalLSTMLayerParams* n = new UniDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UniDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  activations_.Clear();
  if (GetArenaNoVirtual() == NULL && params_ != NULL) {
    delete params_;
  }
  params_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) {
    delete weightparams_;
  }
  weightparams_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool UniDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void UniDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weightparams_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

size_t UniDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightparams_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UniDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UniDirectionalLSTMLayerParams*>(&from));
}

void UniDirectionalLSTMLayerParams::MergeFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activations_.MergeFrom(from.activations_);
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.has_weightparams()) {
    mutable_weightparams()->::CoreML::Specification::LSTMWeightParams::MergeFrom(from.weightparams());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void UniDirectionalLSTMLayerParams::CopyFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UniDirectionalLSTMLayerParams::IsInitialized() const {
  return true;
}

void UniDirectionalLSTMLayerParams::Swap(UniDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UniDirectionalLSTMLayerParams::InternalSwap(UniDirectionalLSTMLayerParams* other) {
  activations_.InternalSwap(&other->activations_);
  std::swap(params_, other->params_);
  std::swap(weightparams_, other->weightparams_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UniDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.UniDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UniDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int UniDirectionalLSTMLayerParams::activations_size() const {
  return activations_.size();
}
void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// .CoreML.Specification.LSTMParams params = 15;
bool UniDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// .CoreML.Specification.LSTMWeightParams weightParams = 20;
bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;
}
const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_ != NULL ? *weightparams_
                         : *::CoreML::Specification::LSTMWeightParams::internal_default_instance();
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {
  
  if (weightparams_ == NULL) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  delete weightparams_;
  weightparams_ = weightparams;
  if (weightparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// bool reverseInput = 100;
void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return reverseinput_;
}
void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsForwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsBackwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kParamsFieldNumber;
const int BiDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}
BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activationsforwardlstm_(from.activationsforwardlstm_),
      activationsbackwardlstm_(from.activationsbackwardlstm_),
      weightparams_(from.weightparams_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_params()) {
    params_ = new ::CoreML::Specification::LSTMParams(*from.params_);
  } else {
    params_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

void BiDirectionalLSTMLayerParams::SharedCtor() {
  ::memset(&params_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&params_) + sizeof(outputvectorsize_));
  _cached_size_ = 0;
}

BiDirectionalLSTMLayerParams::~BiDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
  SharedDtor();
}

void BiDirectionalLSTMLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete params_;
  }
}

void BiDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiDirectionalLSTMLayerParams& BiDirectionalLSTMLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BiDirectionalLSTMLayerParams* BiDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  BiDirectionalLSTMLayerParams* n = new BiDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  activationsforwardlstm_.Clear();
  activationsbackwardlstm_.Clear();
  weightparams_.Clear();
  if (GetArenaNoVirtual() == NULL && params_ != NULL) {
    delete params_;
  }
  params_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
}

bool BiDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activationsforwardlstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activationsbackwardlstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_weightparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void BiDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  for (unsigned int i = 0, n = this->activationsforwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activationsforwardlstm(i), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  for (unsigned int i = 0, n = this->activationsbackwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, this->activationsbackwardlstm(i), output);
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  for (unsigned int i = 0, n = this->weightparams_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weightparams(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

size_t BiDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  {
    unsigned int count = this->activationsforwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsforwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  {
    unsigned int count = this->activationsbackwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsbackwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  {
    unsigned int count = this->weightparams_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weightparams(i));
    }
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiDirectionalLSTMLayerParams*>(&from));
}

void BiDirectionalLSTMLayerParams::MergeFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activationsforwardlstm_.MergeFrom(from.activationsforwardlstm_);
  activationsbackwardlstm_.MergeFrom(from.activationsbackwardlstm_);
  weightparams_.MergeFrom(from.weightparams_);
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
}

void BiDirectionalLSTMLayerParams::CopyFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BiDirectionalLSTMLayerParams::IsInitialized() const {
  return true;
}

void BiDirectionalLSTMLayerParams::Swap(BiDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiDirectionalLSTMLayerParams::InternalSwap(BiDirectionalLSTMLayerParams* other) {
  activationsforwardlstm_.InternalSwap(&other->activationsforwardlstm_);
  activationsbackwardlstm_.InternalSwap(&other->activationsbackwardlstm_);
  weightparams_.InternalSwap(&other->weightparams_);
  std::swap(params_, other->params_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// .CoreML.Specification.LSTMParams params = 15;
bool BiDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return weightparams_.size();
}
void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Get(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CustomLayerParams_CustomLayerParamValue::kDoubleValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kStringValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kIntValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kLongValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kBoolValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CustomLayerParams_CustomLayerParamValue::CustomLayerParams_CustomLayerParamValue()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}
CustomLayerParams_CustomLayerParamValue::CustomLayerParams_CustomLayerParamValue(const CustomLayerParams_CustomLayerParamValue& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_value();
  switch (from.value_case()) {
    case kDoubleValue: {
      set_doublevalue(from.doublevalue());
      break;
    }
    case kStringValue: {
      set_stringvalue(from.stringvalue());
      break;
    }
    case kIntValue: {
      set_intvalue(from.intvalue());
      break;
    }
    case kLongValue: {
      set_longvalue(from.longvalue());
      break;
    }
    case kBoolValue: {
      set_boolvalue(from.boolvalue());
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}

void CustomLayerParams_CustomLayerParamValue::SharedCtor() {
  clear_has_value();
  _cached_size_ = 0;
}

CustomLayerParams_CustomLayerParamValue::~CustomLayerParams_CustomLayerParamValue() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  SharedDtor();
}

void CustomLayerParams_CustomLayerParamValue::SharedDtor() {
  if (has_value()) {
    clear_value();
  }
}

void CustomLayerParams_CustomLayerParamValue::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CustomLayerParams_CustomLayerParamValue& CustomLayerParams_CustomLayerParamValue::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CustomLayerParams_CustomLayerParamValue* CustomLayerParams_CustomLayerParamValue::New(::google::protobuf::Arena* arena) const {
  CustomLayerParams_CustomLayerParamValue* n = new CustomLayerParams_CustomLayerParamValue;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CustomLayerParams_CustomLayerParamValue::clear_value() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  switch (value_case()) {
    case kDoubleValue: {
      // No need to clear
      break;
    }
    case kStringValue: {
      value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
      break;
    }
    case kIntValue: {
      // No need to clear
      break;
    }
    case kLongValue: {
      // No need to clear
      break;
    }
    case kBoolValue: {
      // No need to clear
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = VALUE_NOT_SET;
}


void CustomLayerParams_CustomLayerParamValue::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  clear_value();
}

bool CustomLayerParams_CustomLayerParamValue::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // double doubleValue = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(81u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &value_.doublevalue_)));
          set_has_doublevalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string stringValue = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_stringvalue()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->stringvalue().data(), this->stringvalue().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int32 intValue = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(240u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int32, ::google::protobuf::internal::WireFormatLite::TYPE_INT32>(
                 input, &value_.intvalue_)));
          set_has_intvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 longValue = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(320u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &value_.longvalue_)));
          set_has_longvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool boolValue = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &value_.boolvalue_)));
          set_has_boolvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  return false;
#undef DO_
}

void CustomLayerParams_CustomLayerParamValue::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // double doubleValue = 10;
  if (has_doublevalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(10, this->doublevalue(), output);
  }

  // string stringValue = 20;
  if (has_stringvalue()) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->stringvalue().data(), this->stringvalue().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      20, this->stringvalue(), output);
  }

  // int32 intValue = 30;
  if (has_intvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteInt32(30, this->intvalue(), output);
  }

  // int64 longValue = 40;
  if (has_longvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(40, this->longvalue(), output);
  }

  // bool boolValue = 50;
  if (has_boolvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->boolvalue(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}

size_t CustomLayerParams_CustomLayerParamValue::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  size_t total_size = 0;

  switch (value_case()) {
    // double doubleValue = 10;
    case kDoubleValue: {
      total_size += 1 + 8;
      break;
    }
    // string stringValue = 20;
    case kStringValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::StringSize(
          this->stringvalue());
      break;
    }
    // int32 intValue = 30;
    case kIntValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(
          this->intvalue());
      break;
    }
    // int64 longValue = 40;
    case kLongValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int64Size(
          this->longvalue());
      break;
    }
    // bool boolValue = 50;
    case kBoolValue: {
      total_size += 2 + 1;
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CustomLayerParams_CustomLayerParamValue::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CustomLayerParams_CustomLayerParamValue*>(&from));
}

void CustomLayerParams_CustomLayerParamValue::MergeFrom(const CustomLayerParams_CustomLayerParamValue& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.value_case()) {
    case kDoubleValue: {
      set_doublevalue(from.doublevalue());
      break;
    }
    case kStringValue: {
      set_stringvalue(from.stringvalue());
      break;
    }
    case kIntValue: {
      set_intvalue(from.intvalue());
      break;
    }
    case kLongValue: {
      set_longvalue(from.longvalue());
      break;
    }
    case kBoolValue: {
      set_boolvalue(from.boolvalue());
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
}

void CustomLayerParams_CustomLayerParamValue::CopyFrom(const CustomLayerParams_CustomLayerParamValue& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CustomLayerParams_CustomLayerParamValue::IsInitialized() const {
  return true;
}

void CustomLayerParams_CustomLayerParamValue::Swap(CustomLayerParams_CustomLayerParamValue* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CustomLayerParams_CustomLayerParamValue::InternalSwap(CustomLayerParams_CustomLayerParamValue* other) {
  std::swap(value_, other->value_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CustomLayerParams_CustomLayerParamValue::GetTypeName() const {
  return "CoreML.Specification.CustomLayerParams.CustomLayerParamValue";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CustomLayerParams_CustomLayerParamValue

// double doubleValue = 10;
bool CustomLayerParams_CustomLayerParamValue::has_doublevalue() const {
  return value_case() == kDoubleValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_doublevalue() {
  _oneof_case_[0] = kDoubleValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_doublevalue() {
  if (has_doublevalue()) {
    value_.doublevalue_ = 0;
    clear_has_value();
  }
}
double CustomLayerParams_CustomLayerParamValue::doublevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
  if (has_doublevalue()) {
    return value_.doublevalue_;
  }
  return 0;
}
void CustomLayerParams_CustomLayerParamValue::set_doublevalue(double value) {
  if (!has_doublevalue()) {
    clear_value();
    set_has_doublevalue();
  }
  value_.doublevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
}

// string stringValue = 20;
bool CustomLayerParams_CustomLayerParamValue::has_stringvalue() const {
  return value_case() == kStringValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_stringvalue() {
  _oneof_case_[0] = kStringValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_stringvalue() {
  if (has_stringvalue()) {
    value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
    clear_has_value();
  }
}
const ::std::string& CustomLayerParams_CustomLayerParamValue::stringvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    return value_.stringvalue_.GetNoArena();
  }
  return *&::google::protobuf::internal::GetEmptyStringAlreadyInited();
}
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#if LANG_CXX11
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#endif
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value, size_t size) {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
::std::string* CustomLayerParams_CustomLayerParamValue::mutable_stringvalue() {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  return value_.stringvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams_CustomLayerParamValue::release_stringvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    clear_has_value();
    return value_.stringvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  } else {
    return NULL;
  }
}
void CustomLayerParams_CustomLayerParamValue::set_allocated_stringvalue(::std::string* stringvalue) {
  if (!has_stringvalue()) {
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  clear_value();
  if (stringvalue != NULL) {
    set_has_stringvalue();
    value_.stringvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
        stringvalue);
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}

// int32 intValue = 30;
bool CustomLayerParams_CustomLayerParamValue::has_intvalue() const {
  return value_case() == kIntValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_intvalue() {
  _oneof_case_[0] = kIntValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_intvalue() {
  if (has_intvalue()) {
    value_.intvalue_ = 0;
    clear_has_value();
  }
}
::google::protobuf::int32 CustomLayerParams_CustomLayerParamValue::intvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
  if (has_intvalue()) {
    return value_.intvalue_;
  }
  return 0;
}
void CustomLayerParams_CustomLayerParamValue::set_intvalue(::google::protobuf::int32 value) {
  if (!has_intvalue()) {
    clear_value();
    set_has_intvalue();
  }
  value_.intvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
}

// int64 longValue = 40;
bool CustomLayerParams_CustomLayerParamValue::has_longvalue() const {
  return value_case() == kLongValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_longvalue() {
  _oneof_case_[0] = kLongValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_longvalue() {
  if (has_longvalue()) {
    value_.longvalue_ = GOOGLE_LONGLONG(0);
    clear_has_value();
  }
}
::google::protobuf::int64 CustomLayerParams_CustomLayerParamValue::longvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
  if (has_longvalue()) {
    return value_.longvalue_;
  }
  return GOOGLE_LONGLONG(0);
}
void CustomLayerParams_CustomLayerParamValue::set_longvalue(::google::protobuf::int64 value) {
  if (!has_longvalue()) {
    clear_value();
    set_has_longvalue();
  }
  value_.longvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
}

// bool boolValue = 50;
bool CustomLayerParams_CustomLayerParamValue::has_boolvalue() const {
  return value_case() == kBoolValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_boolvalue() {
  _oneof_case_[0] = kBoolValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_boolvalue() {
  if (has_boolvalue()) {
    value_.boolvalue_ = false;
    clear_has_value();
  }
}
bool CustomLayerParams_CustomLayerParamValue::boolvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
  if (has_boolvalue()) {
    return value_.boolvalue_;
  }
  return false;
}
void CustomLayerParams_CustomLayerParamValue::set_boolvalue(bool value) {
  if (!has_boolvalue()) {
    clear_value();
    set_has_boolvalue();
  }
  value_.boolvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
}

bool CustomLayerParams_CustomLayerParamValue::has_value() const {
  return value_case() != VALUE_NOT_SET;
}
void CustomLayerParams_CustomLayerParamValue::clear_has_value() {
  _oneof_case_[0] = VALUE_NOT_SET;
}
CustomLayerParams_CustomLayerParamValue::ValueCase CustomLayerParams_CustomLayerParamValue::value_case() const {
  return CustomLayerParams_CustomLayerParamValue::ValueCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if PROTOBUF_INLINE_NOT_IN_HEADERS
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CustomLayerParams::kClassNameFieldNumber;
const int CustomLayerParams::kWeightsFieldNumber;
const int CustomLayerParams::kParametersFieldNumber;
const int CustomLayerParams::kDescriptionFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CustomLayerParams::CustomLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CustomLayerParams)
}
CustomLayerParams::CustomLayerParams(const CustomLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      weights_(from.weights_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  parameters_.MergeFrom(from.parameters_);
  classname_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.classname().size() > 0) {
    classname_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.classname_);
  }
  description_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.description().size() > 0) {
    description_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.description_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CustomLayerParams)
}

void CustomLayerParams::SharedCtor() {
  classname_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

CustomLayerParams::~CustomLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CustomLayerParams)
  SharedDtor();
}

void CustomLayerParams::SharedDtor() {
  classname_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void CustomLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CustomLayerParams& CustomLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CustomLayerParams* CustomLayerParams::New(::google::protobuf::Arena* arena) const {
  CustomLayerParams* n = new CustomLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CustomLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CustomLayerParams)
  weights_.Clear();
  parameters_.Clear();
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool CustomLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CustomLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string className = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_classname()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->classname().data(), this->classname().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.className"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          CustomLayerParams_ParametersEntry::Parser< ::google::protobuf::internal::MapFieldLite<
              CustomLayerParams_ParametersEntry,
              ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue > > parser(&parameters_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), parser.key().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.ParametersEntry.key"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string description = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_description()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->description().data(), this->description().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.description"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CustomLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CustomLayerParams)
  return false;
#undef DO_
}

void CustomLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CustomLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string className = 10;
  if (this->classname().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->classname().data(), this->classname().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.className");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      10, this->classname(), output);
  }

  // repeated .CoreML.Specification.WeightParams weights = 20;
  for (unsigned int i = 0, n = this->weights_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weights(i), output);
  }

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  if (!this->parameters().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "CoreML.Specification.CustomLayerParams.ParametersEntry.key");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->parameters().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->parameters().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
          it = this->parameters().begin();
          it != this->parameters().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(parameters_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessage(
            30, *entry, output);
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
          it = this->parameters().begin();
          it != this->parameters().end(); ++it) {
        entry.reset(parameters_.NewEntryWrapper(
            it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessage(
            30, *entry, output);
        Utf8Check::Check(&*it);
      }
    }
  }

  // string description = 40;
  if (this->description().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->description().data(), this->description().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.description");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      40, this->description(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CustomLayerParams)
}

size_t CustomLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CustomLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.WeightParams weights = 20;
  {
    unsigned int count = this->weights_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weights(i));
    }
  }

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  total_size += 2 *
      ::google::protobuf::internal::FromIntSize(this->parameters_size());
  {
    ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
    for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
        it = this->parameters().begin();
        it != this->parameters().end(); ++it) {
      entry.reset(parameters_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
  }

  // string className = 10;
  if (this->classname().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->classname());
  }

  // string description = 40;
  if (this->description().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->description());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CustomLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CustomLayerParams*>(&from));
}

void CustomLayerParams::MergeFrom(const CustomLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CustomLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  weights_.MergeFrom(from.weights_);
  parameters_.MergeFrom(from.parameters_);
  if (from.classname().size() > 0) {

    classname_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.classname_);
  }
  if (from.description().size() > 0) {

    description_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.description_);
  }
}

void CustomLayerParams::CopyFrom(const CustomLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CustomLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CustomLayerParams::IsInitialized() const {
  return true;
}

void CustomLayerParams::Swap(CustomLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CustomLayerParams::InternalSwap(CustomLayerParams* other) {
  weights_.InternalSwap(&other->weights_);
  parameters_.Swap(&other->parameters_);
  classname_.Swap(&other->classname_);
  description_.Swap(&other->description_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CustomLayerParams::GetTypeName() const {
  return "CoreML.Specification.CustomLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CustomLayerParams

// string className = 10;
void CustomLayerParams::clear_classname() {
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CustomLayerParams::classname() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.className)
  return classname_.GetNoArena();
}
void CustomLayerParams::set_classname(const ::std::string& value) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.className)
}
#if LANG_CXX11
void CustomLayerParams::set_classname(::std::string&& value) {
  
  classname_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.className)
}
#endif
void CustomLayerParams::set_classname(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.className)
}
void CustomLayerParams::set_classname(const char* value, size_t size) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.className)
}
::std::string* CustomLayerParams::mutable_classname() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.className)
  return classname_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams::release_classname() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.className)
  
  return classname_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CustomLayerParams::set_allocated_classname(::std::string* classname) {
  if (classname != NULL) {
    
  } else {
    
  }
  classname_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), classname);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.className)
}

// repeated .CoreML.Specification.WeightParams weights = 20;
int CustomLayerParams::weights_size() const {
  return weights_.size();
}
void CustomLayerParams::clear_weights() {
  weights_.Clear();
}
const ::CoreML::Specification::WeightParams& CustomLayerParams::weights(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Get(index);
}
::CoreML::Specification::WeightParams* CustomLayerParams::mutable_weights(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Mutable(index);
}
::CoreML::Specification::WeightParams* CustomLayerParams::add_weights() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
CustomLayerParams::mutable_weights() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CustomLayerParams.weights)
  return &weights_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
CustomLayerParams::weights() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CustomLayerParams.weights)
  return weights_;
}

// map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
int CustomLayerParams::parameters_size() const {
  return parameters_.size();
}
void CustomLayerParams::clear_parameters() {
  parameters_.Clear();
}
 const ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
CustomLayerParams::parameters() const {
  // @@protoc_insertion_point(field_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.GetMap();
}
 ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
CustomLayerParams::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.MutableMap();
}

// string description = 40;
void CustomLayerParams::clear_description() {
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CustomLayerParams::description() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.description)
  return description_.GetNoArena();
}
void CustomLayerParams::set_description(const ::std::string& value) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.description)
}
#if LANG_CXX11
void CustomLayerParams::set_description(::std::string&& value) {
  
  description_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.description)
}
#endif
void CustomLayerParams::set_description(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.description)
}
void CustomLayerParams::set_description(const char* value, size_t size) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.description)
}
::std::string* CustomLayerParams::mutable_description() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.description)
  return description_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams::release_description() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.description)
  
  return description_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CustomLayerParams::set_allocated_description(::std::string* description) {
  if (description != NULL) {
    
  } else {
    
  }
  description_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), description);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.description)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int TransposeLayerParams::kAxesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TransposeLayerParams::TransposeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TransposeLayerParams)
}
TransposeLayerParams::TransposeLayerParams(const TransposeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TransposeLayerParams)
}

void TransposeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

TransposeLayerParams::~TransposeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TransposeLayerParams)
  SharedDtor();
}

void TransposeLayerParams::SharedDtor() {
}

void TransposeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TransposeLayerParams& TransposeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TransposeLayerParams* TransposeLayerParams::New(::google::protobuf::Arena* arena) const {
  TransposeLayerParams* n = new TransposeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TransposeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TransposeLayerParams)
  axes_.Clear();
}

bool TransposeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TransposeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TransposeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TransposeLayerParams)
  return false;
#undef DO_
}

void TransposeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TransposeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->axes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TransposeLayerParams)
}

size_t TransposeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TransposeLayerParams)
  size_t total_size = 0;

  // repeated uint64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TransposeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TransposeLayerParams*>(&from));
}

void TransposeLayerParams::MergeFrom(const TransposeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TransposeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
}

void TransposeLayerParams::CopyFrom(const TransposeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TransposeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TransposeLayerParams::IsInitialized() const {
  return true;
}

void TransposeLayerParams::Swap(TransposeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TransposeLayerParams::InternalSwap(TransposeLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TransposeLayerParams::GetTypeName() const {
  return "CoreML.Specification.TransposeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TransposeLayerParams

// repeated uint64 axes = 1;
int TransposeLayerParams::axes_size() const {
  return axes_.size();
}
void TransposeLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::uint64 TransposeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TransposeLayerParams.axes)
  return axes_.Get(index);
}
void TransposeLayerParams::set_axes(int index, ::google::protobuf::uint64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TransposeLayerParams.axes)
}
void TransposeLayerParams::add_axes(::google::protobuf::uint64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TransposeLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
TransposeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TransposeLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
TransposeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TransposeLayerParams.axes)
  return &axes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchedMatMulLayerParams::kTransposeAFieldNumber;
const int BatchedMatMulLayerParams::kTransposeBFieldNumber;
const int BatchedMatMulLayerParams::kWeightMatrixFirstDimensionFieldNumber;
const int BatchedMatMulLayerParams::kWeightMatrixSecondDimensionFieldNumber;
const int BatchedMatMulLayerParams::kHasBiasFieldNumber;
const int BatchedMatMulLayerParams::kWeightsFieldNumber;
const int BatchedMatMulLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchedMatMulLayerParams::BatchedMatMulLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BatchedMatMulLayerParams)
}
BatchedMatMulLayerParams::BatchedMatMulLayerParams(const BatchedMatMulLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&weightmatrixfirstdimension_, &from.weightmatrixfirstdimension_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weightmatrixfirstdimension_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BatchedMatMulLayerParams)
}

void BatchedMatMulLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

BatchedMatMulLayerParams::~BatchedMatMulLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BatchedMatMulLayerParams)
  SharedDtor();
}

void BatchedMatMulLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void BatchedMatMulLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BatchedMatMulLayerParams& BatchedMatMulLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BatchedMatMulLayerParams* BatchedMatMulLayerParams::New(::google::protobuf::Arena* arena) const {
  BatchedMatMulLayerParams* n = new BatchedMatMulLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BatchedMatMulLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BatchedMatMulLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&weightmatrixfirstdimension_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weightmatrixfirstdimension_) + sizeof(hasbias_));
}

bool BatchedMatMulLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BatchedMatMulLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool transposeA = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &transposea_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool transposeB = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &transposeb_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 weightMatrixFirstDimension = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &weightmatrixfirstdimension_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 weightMatrixSecondDimension = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &weightmatrixseconddimension_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 7;
      case 7: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(56u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 8;
      case 8: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(66u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 9;
      case 9: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(74u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BatchedMatMulLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BatchedMatMulLayerParams)
  return false;
#undef DO_
}

void BatchedMatMulLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BatchedMatMulLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool transposeA = 1;
  if (this->transposea() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->transposea(), output);
  }

  // bool transposeB = 2;
  if (this->transposeb() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->transposeb(), output);
  }

  // uint64 weightMatrixFirstDimension = 5;
  if (this->weightmatrixfirstdimension() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(5, this->weightmatrixfirstdimension(), output);
  }

  // uint64 weightMatrixSecondDimension = 6;
  if (this->weightmatrixseconddimension() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(6, this->weightmatrixseconddimension(), output);
  }

  // bool hasBias = 7;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(7, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 8;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      8, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 9;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      9, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BatchedMatMulLayerParams)
}

size_t BatchedMatMulLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BatchedMatMulLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 8;
  if (this->has_weights()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 9;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 weightMatrixFirstDimension = 5;
  if (this->weightmatrixfirstdimension() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->weightmatrixfirstdimension());
  }

  // uint64 weightMatrixSecondDimension = 6;
  if (this->weightmatrixseconddimension() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->weightmatrixseconddimension());
  }

  // bool transposeA = 1;
  if (this->transposea() != 0) {
    total_size += 1 + 1;
  }

  // bool transposeB = 2;
  if (this->transposeb() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBias = 7;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BatchedMatMulLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BatchedMatMulLayerParams*>(&from));
}

void BatchedMatMulLayerParams::MergeFrom(const BatchedMatMulLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BatchedMatMulLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.weightmatrixfirstdimension() != 0) {
    set_weightmatrixfirstdimension(from.weightmatrixfirstdimension());
  }
  if (from.weightmatrixseconddimension() != 0) {
    set_weightmatrixseconddimension(from.weightmatrixseconddimension());
  }
  if (from.transposea() != 0) {
    set_transposea(from.transposea());
  }
  if (from.transposeb() != 0) {
    set_transposeb(from.transposeb());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void BatchedMatMulLayerParams::CopyFrom(const BatchedMatMulLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BatchedMatMulLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchedMatMulLayerParams::IsInitialized() const {
  return true;
}

void BatchedMatMulLayerParams::Swap(BatchedMatMulLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchedMatMulLayerParams::InternalSwap(BatchedMatMulLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(weightmatrixfirstdimension_, other->weightmatrixfirstdimension_);
  std::swap(weightmatrixseconddimension_, other->weightmatrixseconddimension_);
  std::swap(transposea_, other->transposea_);
  std::swap(transposeb_, other->transposeb_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BatchedMatMulLayerParams::GetTypeName() const {
  return "CoreML.Specification.BatchedMatMulLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BatchedMatMulLayerParams

// bool transposeA = 1;
void BatchedMatMulLayerParams::clear_transposea() {
  transposea_ = false;
}
bool BatchedMatMulLayerParams::transposea() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.transposeA)
  return transposea_;
}
void BatchedMatMulLayerParams::set_transposea(bool value) {
  
  transposea_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.transposeA)
}

// bool transposeB = 2;
void BatchedMatMulLayerParams::clear_transposeb() {
  transposeb_ = false;
}
bool BatchedMatMulLayerParams::transposeb() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.transposeB)
  return transposeb_;
}
void BatchedMatMulLayerParams::set_transposeb(bool value) {
  
  transposeb_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.transposeB)
}

// uint64 weightMatrixFirstDimension = 5;
void BatchedMatMulLayerParams::clear_weightmatrixfirstdimension() {
  weightmatrixfirstdimension_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchedMatMulLayerParams::weightmatrixfirstdimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixFirstDimension)
  return weightmatrixfirstdimension_;
}
void BatchedMatMulLayerParams::set_weightmatrixfirstdimension(::google::protobuf::uint64 value) {
  
  weightmatrixfirstdimension_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixFirstDimension)
}

// uint64 weightMatrixSecondDimension = 6;
void BatchedMatMulLayerParams::clear_weightmatrixseconddimension() {
  weightmatrixseconddimension_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchedMatMulLayerParams::weightmatrixseconddimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixSecondDimension)
  return weightmatrixseconddimension_;
}
void BatchedMatMulLayerParams::set_weightmatrixseconddimension(::google::protobuf::uint64 value) {
  
  weightmatrixseconddimension_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.weightMatrixSecondDimension)
}

// bool hasBias = 7;
void BatchedMatMulLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool BatchedMatMulLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.hasBias)
  return hasbias_;
}
void BatchedMatMulLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 8;
bool BatchedMatMulLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void BatchedMatMulLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void BatchedMatMulLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 9;
bool BatchedMatMulLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void BatchedMatMulLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchedMatMulLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* BatchedMatMulLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void BatchedMatMulLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConcatNDLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConcatNDLayerParams::ConcatNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConcatNDLayerParams)
}
ConcatNDLayerParams::ConcatNDLayerParams(const ConcatNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConcatNDLayerParams)
}

void ConcatNDLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

ConcatNDLayerParams::~ConcatNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConcatNDLayerParams)
  SharedDtor();
}

void ConcatNDLayerParams::SharedDtor() {
}

void ConcatNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConcatNDLayerParams& ConcatNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConcatNDLayerParams* ConcatNDLayerParams::New(::google::protobuf::Arena* arena) const {
  ConcatNDLayerParams* n = new ConcatNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConcatNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConcatNDLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool ConcatNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConcatNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConcatNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConcatNDLayerParams)
  return false;
#undef DO_
}

void ConcatNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConcatNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConcatNDLayerParams)
}

size_t ConcatNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConcatNDLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConcatNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConcatNDLayerParams*>(&from));
}

void ConcatNDLayerParams::MergeFrom(const ConcatNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConcatNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void ConcatNDLayerParams::CopyFrom(const ConcatNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConcatNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConcatNDLayerParams::IsInitialized() const {
  return true;
}

void ConcatNDLayerParams::Swap(ConcatNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConcatNDLayerParams::InternalSwap(ConcatNDLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConcatNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConcatNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConcatNDLayerParams

// int64 axis = 1;
void ConcatNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ConcatNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatNDLayerParams.axis)
  return axis_;
}
void ConcatNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatNDLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SoftmaxNDLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SoftmaxNDLayerParams::SoftmaxNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SoftmaxNDLayerParams)
}
SoftmaxNDLayerParams::SoftmaxNDLayerParams(const SoftmaxNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SoftmaxNDLayerParams)
}

void SoftmaxNDLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

SoftmaxNDLayerParams::~SoftmaxNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SoftmaxNDLayerParams)
  SharedDtor();
}

void SoftmaxNDLayerParams::SharedDtor() {
}

void SoftmaxNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SoftmaxNDLayerParams& SoftmaxNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SoftmaxNDLayerParams* SoftmaxNDLayerParams::New(::google::protobuf::Arena* arena) const {
  SoftmaxNDLayerParams* n = new SoftmaxNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SoftmaxNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SoftmaxNDLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool SoftmaxNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SoftmaxNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SoftmaxNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SoftmaxNDLayerParams)
  return false;
#undef DO_
}

void SoftmaxNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SoftmaxNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SoftmaxNDLayerParams)
}

size_t SoftmaxNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SoftmaxNDLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SoftmaxNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SoftmaxNDLayerParams*>(&from));
}

void SoftmaxNDLayerParams::MergeFrom(const SoftmaxNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SoftmaxNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void SoftmaxNDLayerParams::CopyFrom(const SoftmaxNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SoftmaxNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SoftmaxNDLayerParams::IsInitialized() const {
  return true;
}

void SoftmaxNDLayerParams::Swap(SoftmaxNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SoftmaxNDLayerParams::InternalSwap(SoftmaxNDLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SoftmaxNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.SoftmaxNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SoftmaxNDLayerParams

// int64 axis = 1;
void SoftmaxNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SoftmaxNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SoftmaxNDLayerParams.axis)
  return axis_;
}
void SoftmaxNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SoftmaxNDLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReverseLayerParams::kReverseDimFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReverseLayerParams::ReverseLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReverseLayerParams)
}
ReverseLayerParams::ReverseLayerParams(const ReverseLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      reversedim_(from.reversedim_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReverseLayerParams)
}

void ReverseLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ReverseLayerParams::~ReverseLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReverseLayerParams)
  SharedDtor();
}

void ReverseLayerParams::SharedDtor() {
}

void ReverseLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReverseLayerParams& ReverseLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReverseLayerParams* ReverseLayerParams::New(::google::protobuf::Arena* arena) const {
  ReverseLayerParams* n = new ReverseLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReverseLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReverseLayerParams)
  reversedim_.Clear();
}

bool ReverseLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReverseLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated bool reverseDim = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_reversedim())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 10u, input, this->mutable_reversedim())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReverseLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReverseLayerParams)
  return false;
#undef DO_
}

void ReverseLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReverseLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated bool reverseDim = 1;
  if (this->reversedim_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_reversedim_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->reversedim().data(), this->reversedim_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReverseLayerParams)
}

size_t ReverseLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReverseLayerParams)
  size_t total_size = 0;

  // repeated bool reverseDim = 1;
  {
    unsigned int count = this->reversedim_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _reversedim_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReverseLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReverseLayerParams*>(&from));
}

void ReverseLayerParams::MergeFrom(const ReverseLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReverseLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  reversedim_.MergeFrom(from.reversedim_);
}

void ReverseLayerParams::CopyFrom(const ReverseLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReverseLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReverseLayerParams::IsInitialized() const {
  return true;
}

void ReverseLayerParams::Swap(ReverseLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReverseLayerParams::InternalSwap(ReverseLayerParams* other) {
  reversedim_.InternalSwap(&other->reversedim_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReverseLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReverseLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReverseLayerParams

// repeated bool reverseDim = 1;
int ReverseLayerParams::reversedim_size() const {
  return reversedim_.size();
}
void ReverseLayerParams::clear_reversedim() {
  reversedim_.Clear();
}
bool ReverseLayerParams::reversedim(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseLayerParams.reverseDim)
  return reversedim_.Get(index);
}
void ReverseLayerParams::set_reversedim(int index, bool value) {
  reversedim_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseLayerParams.reverseDim)
}
void ReverseLayerParams::add_reversedim(bool value) {
  reversedim_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReverseLayerParams.reverseDim)
}
const ::google::protobuf::RepeatedField< bool >&
ReverseLayerParams::reversedim() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReverseLayerParams.reverseDim)
  return reversedim_;
}
::google::protobuf::RepeatedField< bool >*
ReverseLayerParams::mutable_reversedim() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReverseLayerParams.reverseDim)
  return &reversedim_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReverseSeqLayerParams::kBatchAxisFieldNumber;
const int ReverseSeqLayerParams::kSequenceAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReverseSeqLayerParams::ReverseSeqLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReverseSeqLayerParams)
}
ReverseSeqLayerParams::ReverseSeqLayerParams(const ReverseSeqLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&batchaxis_, &from.batchaxis_,
    reinterpret_cast<char*>(&sequenceaxis_) -
    reinterpret_cast<char*>(&batchaxis_) + sizeof(sequenceaxis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReverseSeqLayerParams)
}

void ReverseSeqLayerParams::SharedCtor() {
  ::memset(&batchaxis_, 0, reinterpret_cast<char*>(&sequenceaxis_) -
    reinterpret_cast<char*>(&batchaxis_) + sizeof(sequenceaxis_));
  _cached_size_ = 0;
}

ReverseSeqLayerParams::~ReverseSeqLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReverseSeqLayerParams)
  SharedDtor();
}

void ReverseSeqLayerParams::SharedDtor() {
}

void ReverseSeqLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReverseSeqLayerParams& ReverseSeqLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReverseSeqLayerParams* ReverseSeqLayerParams::New(::google::protobuf::Arena* arena) const {
  ReverseSeqLayerParams* n = new ReverseSeqLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReverseSeqLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReverseSeqLayerParams)
  ::memset(&batchaxis_, 0, reinterpret_cast<char*>(&sequenceaxis_) -
    reinterpret_cast<char*>(&batchaxis_) + sizeof(sequenceaxis_));
}

bool ReverseSeqLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReverseSeqLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 batchAxis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &batchaxis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 sequenceAxis = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &sequenceaxis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReverseSeqLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReverseSeqLayerParams)
  return false;
#undef DO_
}

void ReverseSeqLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReverseSeqLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 batchAxis = 1;
  if (this->batchaxis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->batchaxis(), output);
  }

  // int64 sequenceAxis = 2;
  if (this->sequenceaxis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(2, this->sequenceaxis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReverseSeqLayerParams)
}

size_t ReverseSeqLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReverseSeqLayerParams)
  size_t total_size = 0;

  // int64 batchAxis = 1;
  if (this->batchaxis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->batchaxis());
  }

  // int64 sequenceAxis = 2;
  if (this->sequenceaxis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->sequenceaxis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReverseSeqLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReverseSeqLayerParams*>(&from));
}

void ReverseSeqLayerParams::MergeFrom(const ReverseSeqLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReverseSeqLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.batchaxis() != 0) {
    set_batchaxis(from.batchaxis());
  }
  if (from.sequenceaxis() != 0) {
    set_sequenceaxis(from.sequenceaxis());
  }
}

void ReverseSeqLayerParams::CopyFrom(const ReverseSeqLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReverseSeqLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReverseSeqLayerParams::IsInitialized() const {
  return true;
}

void ReverseSeqLayerParams::Swap(ReverseSeqLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReverseSeqLayerParams::InternalSwap(ReverseSeqLayerParams* other) {
  std::swap(batchaxis_, other->batchaxis_);
  std::swap(sequenceaxis_, other->sequenceaxis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReverseSeqLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReverseSeqLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReverseSeqLayerParams

// int64 batchAxis = 1;
void ReverseSeqLayerParams::clear_batchaxis() {
  batchaxis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ReverseSeqLayerParams::batchaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseSeqLayerParams.batchAxis)
  return batchaxis_;
}
void ReverseSeqLayerParams::set_batchaxis(::google::protobuf::int64 value) {
  
  batchaxis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseSeqLayerParams.batchAxis)
}

// int64 sequenceAxis = 2;
void ReverseSeqLayerParams::clear_sequenceaxis() {
  sequenceaxis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ReverseSeqLayerParams::sequenceaxis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReverseSeqLayerParams.sequenceAxis)
  return sequenceaxis_;
}
void ReverseSeqLayerParams::set_sequenceaxis(::google::protobuf::int64 value) {
  
  sequenceaxis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReverseSeqLayerParams.sequenceAxis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoadConstantNDLayerParams::kShapeFieldNumber;
const int LoadConstantNDLayerParams::kDataFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoadConstantNDLayerParams::LoadConstantNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoadConstantNDLayerParams)
}
LoadConstantNDLayerParams::LoadConstantNDLayerParams(const LoadConstantNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_data()) {
    data_ = new ::CoreML::Specification::WeightParams(*from.data_);
  } else {
    data_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoadConstantNDLayerParams)
}

void LoadConstantNDLayerParams::SharedCtor() {
  data_ = NULL;
  _cached_size_ = 0;
}

LoadConstantNDLayerParams::~LoadConstantNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoadConstantNDLayerParams)
  SharedDtor();
}

void LoadConstantNDLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete data_;
  }
}

void LoadConstantNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoadConstantNDLayerParams& LoadConstantNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoadConstantNDLayerParams* LoadConstantNDLayerParams::New(::google::protobuf::Arena* arena) const {
  LoadConstantNDLayerParams* n = new LoadConstantNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoadConstantNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoadConstantNDLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && data_ != NULL) {
    delete data_;
  }
  data_ = NULL;
}

bool LoadConstantNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoadConstantNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams data = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_data()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoadConstantNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoadConstantNDLayerParams)
  return false;
#undef DO_
}

void LoadConstantNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoadConstantNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->data_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoadConstantNDLayerParams)
}

size_t LoadConstantNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoadConstantNDLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->data_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoadConstantNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoadConstantNDLayerParams*>(&from));
}

void LoadConstantNDLayerParams::MergeFrom(const LoadConstantNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoadConstantNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_data()) {
    mutable_data()->::CoreML::Specification::WeightParams::MergeFrom(from.data());
  }
}

void LoadConstantNDLayerParams::CopyFrom(const LoadConstantNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoadConstantNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoadConstantNDLayerParams::IsInitialized() const {
  return true;
}

void LoadConstantNDLayerParams::Swap(LoadConstantNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoadConstantNDLayerParams::InternalSwap(LoadConstantNDLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(data_, other->data_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoadConstantNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoadConstantNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoadConstantNDLayerParams

// repeated uint64 shape = 1;
int LoadConstantNDLayerParams::shape_size() const {
  return shape_.size();
}
void LoadConstantNDLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 LoadConstantNDLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return shape_.Get(index);
}
void LoadConstantNDLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
void LoadConstantNDLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantNDLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantNDLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
bool LoadConstantNDLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
void LoadConstantNDLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
const ::CoreML::Specification::WeightParams& LoadConstantNDLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantNDLayerParams.data)
  return data_;
}
::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantNDLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
void LoadConstantNDLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantNDLayerParams.data)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FillLikeLayerParams::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FillLikeLayerParams::FillLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FillLikeLayerParams)
}
FillLikeLayerParams::FillLikeLayerParams(const FillLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FillLikeLayerParams)
}

void FillLikeLayerParams::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

FillLikeLayerParams::~FillLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FillLikeLayerParams)
  SharedDtor();
}

void FillLikeLayerParams::SharedDtor() {
}

void FillLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FillLikeLayerParams& FillLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FillLikeLayerParams* FillLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  FillLikeLayerParams* n = new FillLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FillLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FillLikeLayerParams)
  value_ = 0;
}

bool FillLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FillLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FillLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FillLikeLayerParams)
  return false;
#undef DO_
}

void FillLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FillLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FillLikeLayerParams)
}

size_t FillLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FillLikeLayerParams)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FillLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FillLikeLayerParams*>(&from));
}

void FillLikeLayerParams::MergeFrom(const FillLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FillLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void FillLikeLayerParams::CopyFrom(const FillLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FillLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FillLikeLayerParams::IsInitialized() const {
  return true;
}

void FillLikeLayerParams::Swap(FillLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FillLikeLayerParams::InternalSwap(FillLikeLayerParams* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FillLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.FillLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FillLikeLayerParams

// float value = 1;
void FillLikeLayerParams::clear_value() {
  value_ = 0;
}
float FillLikeLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillLikeLayerParams.value)
  return value_;
}
void FillLikeLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillLikeLayerParams.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FillStaticLayerParams::kValueFieldNumber;
const int FillStaticLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FillStaticLayerParams::FillStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FillStaticLayerParams)
}
FillStaticLayerParams::FillStaticLayerParams(const FillStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FillStaticLayerParams)
}

void FillStaticLayerParams::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

FillStaticLayerParams::~FillStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FillStaticLayerParams)
  SharedDtor();
}

void FillStaticLayerParams::SharedDtor() {
}

void FillStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FillStaticLayerParams& FillStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FillStaticLayerParams* FillStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  FillStaticLayerParams* n = new FillStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FillStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FillStaticLayerParams)
  targetshape_.Clear();
  value_ = 0;
}

bool FillStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FillStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 targetShape = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 18u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FillStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FillStaticLayerParams)
  return false;
#undef DO_
}

void FillStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FillStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // repeated uint64 targetShape = 2;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FillStaticLayerParams)
}

size_t FillStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FillStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetShape = 2;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FillStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FillStaticLayerParams*>(&from));
}

void FillStaticLayerParams::MergeFrom(const FillStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FillStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
  if (from.value() != 0) {
    set_value(from.value());
  }
}

void FillStaticLayerParams::CopyFrom(const FillStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FillStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FillStaticLayerParams::IsInitialized() const {
  return true;
}

void FillStaticLayerParams::Swap(FillStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FillStaticLayerParams::InternalSwap(FillStaticLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FillStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.FillStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FillStaticLayerParams

// float value = 1;
void FillStaticLayerParams::clear_value() {
  value_ = 0;
}
float FillStaticLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.value)
  return value_;
}
void FillStaticLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.value)
}

// repeated uint64 targetShape = 2;
int FillStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void FillStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::uint64 FillStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
void FillStaticLayerParams::set_targetshape(int index, ::google::protobuf::uint64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.targetShape)
}
void FillStaticLayerParams::add_targetshape(::google::protobuf::uint64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.FillStaticLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
FillStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
FillStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FillDynamicLayerParams::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FillDynamicLayerParams::FillDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FillDynamicLayerParams)
}
FillDynamicLayerParams::FillDynamicLayerParams(const FillDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FillDynamicLayerParams)
}

void FillDynamicLayerParams::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

FillDynamicLayerParams::~FillDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FillDynamicLayerParams)
  SharedDtor();
}

void FillDynamicLayerParams::SharedDtor() {
}

void FillDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FillDynamicLayerParams& FillDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FillDynamicLayerParams* FillDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  FillDynamicLayerParams* n = new FillDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FillDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FillDynamicLayerParams)
  value_ = 0;
}

bool FillDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FillDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FillDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FillDynamicLayerParams)
  return false;
#undef DO_
}

void FillDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FillDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FillDynamicLayerParams)
}

size_t FillDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FillDynamicLayerParams)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FillDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FillDynamicLayerParams*>(&from));
}

void FillDynamicLayerParams::MergeFrom(const FillDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FillDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void FillDynamicLayerParams::CopyFrom(const FillDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FillDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FillDynamicLayerParams::IsInitialized() const {
  return true;
}

void FillDynamicLayerParams::Swap(FillDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FillDynamicLayerParams::InternalSwap(FillDynamicLayerParams* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FillDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.FillDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FillDynamicLayerParams

// float value = 1;
void FillDynamicLayerParams::clear_value() {
  value_ = 0;
}
float FillDynamicLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillDynamicLayerParams.value)
  return value_;
}
void FillDynamicLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillDynamicLayerParams.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WhereBroadcastableLayerParams::WhereBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WhereBroadcastableLayerParams)
}
WhereBroadcastableLayerParams::WhereBroadcastableLayerParams(const WhereBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WhereBroadcastableLayerParams)
}

void WhereBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

WhereBroadcastableLayerParams::~WhereBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WhereBroadcastableLayerParams)
  SharedDtor();
}

void WhereBroadcastableLayerParams::SharedDtor() {
}

void WhereBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WhereBroadcastableLayerParams& WhereBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

WhereBroadcastableLayerParams* WhereBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  WhereBroadcastableLayerParams* n = new WhereBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WhereBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WhereBroadcastableLayerParams)
}

bool WhereBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WhereBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WhereBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WhereBroadcastableLayerParams)
  return false;
#undef DO_
}

void WhereBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WhereBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WhereBroadcastableLayerParams)
}

size_t WhereBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WhereBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WhereBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WhereBroadcastableLayerParams*>(&from));
}

void WhereBroadcastableLayerParams::MergeFrom(const WhereBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WhereBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void WhereBroadcastableLayerParams::CopyFrom(const WhereBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WhereBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool WhereBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void WhereBroadcastableLayerParams::Swap(WhereBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WhereBroadcastableLayerParams::InternalSwap(WhereBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WhereBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.WhereBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WhereBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SinLayerParams::SinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SinLayerParams)
}
SinLayerParams::SinLayerParams(const SinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SinLayerParams)
}

void SinLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SinLayerParams::~SinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SinLayerParams)
  SharedDtor();
}

void SinLayerParams::SharedDtor() {
}

void SinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SinLayerParams& SinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SinLayerParams* SinLayerParams::New(::google::protobuf::Arena* arena) const {
  SinLayerParams* n = new SinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SinLayerParams)
}

bool SinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SinLayerParams)
  return false;
#undef DO_
}

void SinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SinLayerParams)
}

size_t SinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SinLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SinLayerParams*>(&from));
}

void SinLayerParams::MergeFrom(const SinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SinLayerParams::CopyFrom(const SinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SinLayerParams::IsInitialized() const {
  return true;
}

void SinLayerParams::Swap(SinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SinLayerParams::InternalSwap(SinLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SinLayerParams::GetTypeName() const {
  return "CoreML.Specification.SinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SinLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CosLayerParams::CosLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CosLayerParams)
}
CosLayerParams::CosLayerParams(const CosLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CosLayerParams)
}

void CosLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CosLayerParams::~CosLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CosLayerParams)
  SharedDtor();
}

void CosLayerParams::SharedDtor() {
}

void CosLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CosLayerParams& CosLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CosLayerParams* CosLayerParams::New(::google::protobuf::Arena* arena) const {
  CosLayerParams* n = new CosLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CosLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CosLayerParams)
}

bool CosLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CosLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CosLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CosLayerParams)
  return false;
#undef DO_
}

void CosLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CosLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CosLayerParams)
}

size_t CosLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CosLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CosLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CosLayerParams*>(&from));
}

void CosLayerParams::MergeFrom(const CosLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CosLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CosLayerParams::CopyFrom(const CosLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CosLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CosLayerParams::IsInitialized() const {
  return true;
}

void CosLayerParams::Swap(CosLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CosLayerParams::InternalSwap(CosLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CosLayerParams::GetTypeName() const {
  return "CoreML.Specification.CosLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CosLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TanLayerParams::TanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TanLayerParams)
}
TanLayerParams::TanLayerParams(const TanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TanLayerParams)
}

void TanLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

TanLayerParams::~TanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TanLayerParams)
  SharedDtor();
}

void TanLayerParams::SharedDtor() {
}

void TanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TanLayerParams& TanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TanLayerParams* TanLayerParams::New(::google::protobuf::Arena* arena) const {
  TanLayerParams* n = new TanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TanLayerParams)
}

bool TanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TanLayerParams)
  return false;
#undef DO_
}

void TanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TanLayerParams)
}

size_t TanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TanLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TanLayerParams*>(&from));
}

void TanLayerParams::MergeFrom(const TanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void TanLayerParams::CopyFrom(const TanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TanLayerParams::IsInitialized() const {
  return true;
}

void TanLayerParams::Swap(TanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TanLayerParams::InternalSwap(TanLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TanLayerParams::GetTypeName() const {
  return "CoreML.Specification.TanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TanLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AsinLayerParams::AsinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AsinLayerParams)
}
AsinLayerParams::AsinLayerParams(const AsinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AsinLayerParams)
}

void AsinLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AsinLayerParams::~AsinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AsinLayerParams)
  SharedDtor();
}

void AsinLayerParams::SharedDtor() {
}

void AsinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AsinLayerParams& AsinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AsinLayerParams* AsinLayerParams::New(::google::protobuf::Arena* arena) const {
  AsinLayerParams* n = new AsinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AsinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AsinLayerParams)
}

bool AsinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AsinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AsinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AsinLayerParams)
  return false;
#undef DO_
}

void AsinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AsinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AsinLayerParams)
}

size_t AsinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AsinLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AsinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AsinLayerParams*>(&from));
}

void AsinLayerParams::MergeFrom(const AsinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AsinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AsinLayerParams::CopyFrom(const AsinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AsinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AsinLayerParams::IsInitialized() const {
  return true;
}

void AsinLayerParams::Swap(AsinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AsinLayerParams::InternalSwap(AsinLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AsinLayerParams::GetTypeName() const {
  return "CoreML.Specification.AsinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AsinLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AcosLayerParams::AcosLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AcosLayerParams)
}
AcosLayerParams::AcosLayerParams(const AcosLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AcosLayerParams)
}

void AcosLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AcosLayerParams::~AcosLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AcosLayerParams)
  SharedDtor();
}

void AcosLayerParams::SharedDtor() {
}

void AcosLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AcosLayerParams& AcosLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AcosLayerParams* AcosLayerParams::New(::google::protobuf::Arena* arena) const {
  AcosLayerParams* n = new AcosLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AcosLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AcosLayerParams)
}

bool AcosLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AcosLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AcosLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AcosLayerParams)
  return false;
#undef DO_
}

void AcosLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AcosLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AcosLayerParams)
}

size_t AcosLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AcosLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AcosLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AcosLayerParams*>(&from));
}

void AcosLayerParams::MergeFrom(const AcosLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AcosLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AcosLayerParams::CopyFrom(const AcosLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AcosLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AcosLayerParams::IsInitialized() const {
  return true;
}

void AcosLayerParams::Swap(AcosLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AcosLayerParams::InternalSwap(AcosLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AcosLayerParams::GetTypeName() const {
  return "CoreML.Specification.AcosLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AcosLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AtanLayerParams::AtanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AtanLayerParams)
}
AtanLayerParams::AtanLayerParams(const AtanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AtanLayerParams)
}

void AtanLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AtanLayerParams::~AtanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AtanLayerParams)
  SharedDtor();
}

void AtanLayerParams::SharedDtor() {
}

void AtanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AtanLayerParams& AtanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AtanLayerParams* AtanLayerParams::New(::google::protobuf::Arena* arena) const {
  AtanLayerParams* n = new AtanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AtanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AtanLayerParams)
}

bool AtanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AtanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AtanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AtanLayerParams)
  return false;
#undef DO_
}

void AtanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AtanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AtanLayerParams)
}

size_t AtanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AtanLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AtanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AtanLayerParams*>(&from));
}

void AtanLayerParams::MergeFrom(const AtanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AtanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AtanLayerParams::CopyFrom(const AtanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AtanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AtanLayerParams::IsInitialized() const {
  return true;
}

void AtanLayerParams::Swap(AtanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AtanLayerParams::InternalSwap(AtanLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AtanLayerParams::GetTypeName() const {
  return "CoreML.Specification.AtanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AtanLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SinhLayerParams::SinhLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SinhLayerParams)
}
SinhLayerParams::SinhLayerParams(const SinhLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SinhLayerParams)
}

void SinhLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SinhLayerParams::~SinhLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SinhLayerParams)
  SharedDtor();
}

void SinhLayerParams::SharedDtor() {
}

void SinhLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SinhLayerParams& SinhLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SinhLayerParams* SinhLayerParams::New(::google::protobuf::Arena* arena) const {
  SinhLayerParams* n = new SinhLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SinhLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SinhLayerParams)
}

bool SinhLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SinhLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SinhLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SinhLayerParams)
  return false;
#undef DO_
}

void SinhLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SinhLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SinhLayerParams)
}

size_t SinhLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SinhLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SinhLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SinhLayerParams*>(&from));
}

void SinhLayerParams::MergeFrom(const SinhLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SinhLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SinhLayerParams::CopyFrom(const SinhLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SinhLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SinhLayerParams::IsInitialized() const {
  return true;
}

void SinhLayerParams::Swap(SinhLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SinhLayerParams::InternalSwap(SinhLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SinhLayerParams::GetTypeName() const {
  return "CoreML.Specification.SinhLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SinhLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CoshLayerParams::CoshLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CoshLayerParams)
}
CoshLayerParams::CoshLayerParams(const CoshLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CoshLayerParams)
}

void CoshLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CoshLayerParams::~CoshLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CoshLayerParams)
  SharedDtor();
}

void CoshLayerParams::SharedDtor() {
}

void CoshLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CoshLayerParams& CoshLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CoshLayerParams* CoshLayerParams::New(::google::protobuf::Arena* arena) const {
  CoshLayerParams* n = new CoshLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CoshLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CoshLayerParams)
}

bool CoshLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CoshLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CoshLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CoshLayerParams)
  return false;
#undef DO_
}

void CoshLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CoshLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CoshLayerParams)
}

size_t CoshLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CoshLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CoshLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CoshLayerParams*>(&from));
}

void CoshLayerParams::MergeFrom(const CoshLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CoshLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CoshLayerParams::CopyFrom(const CoshLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CoshLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CoshLayerParams::IsInitialized() const {
  return true;
}

void CoshLayerParams::Swap(CoshLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CoshLayerParams::InternalSwap(CoshLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CoshLayerParams::GetTypeName() const {
  return "CoreML.Specification.CoshLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CoshLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TanhLayerParams::TanhLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TanhLayerParams)
}
TanhLayerParams::TanhLayerParams(const TanhLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TanhLayerParams)
}

void TanhLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

TanhLayerParams::~TanhLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TanhLayerParams)
  SharedDtor();
}

void TanhLayerParams::SharedDtor() {
}

void TanhLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TanhLayerParams& TanhLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TanhLayerParams* TanhLayerParams::New(::google::protobuf::Arena* arena) const {
  TanhLayerParams* n = new TanhLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TanhLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TanhLayerParams)
}

bool TanhLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TanhLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TanhLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TanhLayerParams)
  return false;
#undef DO_
}

void TanhLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TanhLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TanhLayerParams)
}

size_t TanhLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TanhLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TanhLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TanhLayerParams*>(&from));
}

void TanhLayerParams::MergeFrom(const TanhLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TanhLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void TanhLayerParams::CopyFrom(const TanhLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TanhLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TanhLayerParams::IsInitialized() const {
  return true;
}

void TanhLayerParams::Swap(TanhLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TanhLayerParams::InternalSwap(TanhLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TanhLayerParams::GetTypeName() const {
  return "CoreML.Specification.TanhLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TanhLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AsinhLayerParams::AsinhLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AsinhLayerParams)
}
AsinhLayerParams::AsinhLayerParams(const AsinhLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AsinhLayerParams)
}

void AsinhLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AsinhLayerParams::~AsinhLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AsinhLayerParams)
  SharedDtor();
}

void AsinhLayerParams::SharedDtor() {
}

void AsinhLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AsinhLayerParams& AsinhLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AsinhLayerParams* AsinhLayerParams::New(::google::protobuf::Arena* arena) const {
  AsinhLayerParams* n = new AsinhLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AsinhLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AsinhLayerParams)
}

bool AsinhLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AsinhLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AsinhLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AsinhLayerParams)
  return false;
#undef DO_
}

void AsinhLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AsinhLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AsinhLayerParams)
}

size_t AsinhLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AsinhLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AsinhLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AsinhLayerParams*>(&from));
}

void AsinhLayerParams::MergeFrom(const AsinhLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AsinhLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AsinhLayerParams::CopyFrom(const AsinhLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AsinhLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AsinhLayerParams::IsInitialized() const {
  return true;
}

void AsinhLayerParams::Swap(AsinhLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AsinhLayerParams::InternalSwap(AsinhLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AsinhLayerParams::GetTypeName() const {
  return "CoreML.Specification.AsinhLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AsinhLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AcoshLayerParams::AcoshLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AcoshLayerParams)
}
AcoshLayerParams::AcoshLayerParams(const AcoshLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AcoshLayerParams)
}

void AcoshLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AcoshLayerParams::~AcoshLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AcoshLayerParams)
  SharedDtor();
}

void AcoshLayerParams::SharedDtor() {
}

void AcoshLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AcoshLayerParams& AcoshLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AcoshLayerParams* AcoshLayerParams::New(::google::protobuf::Arena* arena) const {
  AcoshLayerParams* n = new AcoshLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AcoshLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AcoshLayerParams)
}

bool AcoshLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AcoshLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AcoshLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AcoshLayerParams)
  return false;
#undef DO_
}

void AcoshLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AcoshLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AcoshLayerParams)
}

size_t AcoshLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AcoshLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AcoshLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AcoshLayerParams*>(&from));
}

void AcoshLayerParams::MergeFrom(const AcoshLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AcoshLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AcoshLayerParams::CopyFrom(const AcoshLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AcoshLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AcoshLayerParams::IsInitialized() const {
  return true;
}

void AcoshLayerParams::Swap(AcoshLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AcoshLayerParams::InternalSwap(AcoshLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AcoshLayerParams::GetTypeName() const {
  return "CoreML.Specification.AcoshLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AcoshLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AtanhLayerParams::AtanhLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AtanhLayerParams)
}
AtanhLayerParams::AtanhLayerParams(const AtanhLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AtanhLayerParams)
}

void AtanhLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AtanhLayerParams::~AtanhLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AtanhLayerParams)
  SharedDtor();
}

void AtanhLayerParams::SharedDtor() {
}

void AtanhLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AtanhLayerParams& AtanhLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AtanhLayerParams* AtanhLayerParams::New(::google::protobuf::Arena* arena) const {
  AtanhLayerParams* n = new AtanhLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AtanhLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AtanhLayerParams)
}

bool AtanhLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AtanhLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AtanhLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AtanhLayerParams)
  return false;
#undef DO_
}

void AtanhLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AtanhLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AtanhLayerParams)
}

size_t AtanhLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AtanhLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AtanhLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AtanhLayerParams*>(&from));
}

void AtanhLayerParams::MergeFrom(const AtanhLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AtanhLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AtanhLayerParams::CopyFrom(const AtanhLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AtanhLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AtanhLayerParams::IsInitialized() const {
  return true;
}

void AtanhLayerParams::Swap(AtanhLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AtanhLayerParams::InternalSwap(AtanhLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AtanhLayerParams::GetTypeName() const {
  return "CoreML.Specification.AtanhLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AtanhLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PowBroadcastableLayerParams::PowBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PowBroadcastableLayerParams)
}
PowBroadcastableLayerParams::PowBroadcastableLayerParams(const PowBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PowBroadcastableLayerParams)
}

void PowBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

PowBroadcastableLayerParams::~PowBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PowBroadcastableLayerParams)
  SharedDtor();
}

void PowBroadcastableLayerParams::SharedDtor() {
}

void PowBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PowBroadcastableLayerParams& PowBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PowBroadcastableLayerParams* PowBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  PowBroadcastableLayerParams* n = new PowBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PowBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PowBroadcastableLayerParams)
}

bool PowBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PowBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PowBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PowBroadcastableLayerParams)
  return false;
#undef DO_
}

void PowBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PowBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PowBroadcastableLayerParams)
}

size_t PowBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PowBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PowBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PowBroadcastableLayerParams*>(&from));
}

void PowBroadcastableLayerParams::MergeFrom(const PowBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PowBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PowBroadcastableLayerParams::CopyFrom(const PowBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PowBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PowBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void PowBroadcastableLayerParams::Swap(PowBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PowBroadcastableLayerParams::InternalSwap(PowBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PowBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.PowBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PowBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Exp2LayerParams::Exp2LayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.Exp2LayerParams)
}
Exp2LayerParams::Exp2LayerParams(const Exp2LayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.Exp2LayerParams)
}

void Exp2LayerParams::SharedCtor() {
  _cached_size_ = 0;
}

Exp2LayerParams::~Exp2LayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.Exp2LayerParams)
  SharedDtor();
}

void Exp2LayerParams::SharedDtor() {
}

void Exp2LayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const Exp2LayerParams& Exp2LayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

Exp2LayerParams* Exp2LayerParams::New(::google::protobuf::Arena* arena) const {
  Exp2LayerParams* n = new Exp2LayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void Exp2LayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.Exp2LayerParams)
}

bool Exp2LayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.Exp2LayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.Exp2LayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.Exp2LayerParams)
  return false;
#undef DO_
}

void Exp2LayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.Exp2LayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.Exp2LayerParams)
}

size_t Exp2LayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.Exp2LayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Exp2LayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const Exp2LayerParams*>(&from));
}

void Exp2LayerParams::MergeFrom(const Exp2LayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.Exp2LayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void Exp2LayerParams::CopyFrom(const Exp2LayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.Exp2LayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Exp2LayerParams::IsInitialized() const {
  return true;
}

void Exp2LayerParams::Swap(Exp2LayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void Exp2LayerParams::InternalSwap(Exp2LayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string Exp2LayerParams::GetTypeName() const {
  return "CoreML.Specification.Exp2LayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Exp2LayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WhereNonZeroLayerParams::WhereNonZeroLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WhereNonZeroLayerParams)
}
WhereNonZeroLayerParams::WhereNonZeroLayerParams(const WhereNonZeroLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WhereNonZeroLayerParams)
}

void WhereNonZeroLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

WhereNonZeroLayerParams::~WhereNonZeroLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WhereNonZeroLayerParams)
  SharedDtor();
}

void WhereNonZeroLayerParams::SharedDtor() {
}

void WhereNonZeroLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WhereNonZeroLayerParams& WhereNonZeroLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

WhereNonZeroLayerParams* WhereNonZeroLayerParams::New(::google::protobuf::Arena* arena) const {
  WhereNonZeroLayerParams* n = new WhereNonZeroLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WhereNonZeroLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WhereNonZeroLayerParams)
}

bool WhereNonZeroLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WhereNonZeroLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WhereNonZeroLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WhereNonZeroLayerParams)
  return false;
#undef DO_
}

void WhereNonZeroLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WhereNonZeroLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WhereNonZeroLayerParams)
}

size_t WhereNonZeroLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WhereNonZeroLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WhereNonZeroLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WhereNonZeroLayerParams*>(&from));
}

void WhereNonZeroLayerParams::MergeFrom(const WhereNonZeroLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WhereNonZeroLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void WhereNonZeroLayerParams::CopyFrom(const WhereNonZeroLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WhereNonZeroLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool WhereNonZeroLayerParams::IsInitialized() const {
  return true;
}

void WhereNonZeroLayerParams::Swap(WhereNonZeroLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WhereNonZeroLayerParams::InternalSwap(WhereNonZeroLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WhereNonZeroLayerParams::GetTypeName() const {
  return "CoreML.Specification.WhereNonZeroLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WhereNonZeroLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MatrixBandPartLayerParams::kNumLowerFieldNumber;
const int MatrixBandPartLayerParams::kNumUpperFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MatrixBandPartLayerParams::MatrixBandPartLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MatrixBandPartLayerParams)
}
MatrixBandPartLayerParams::MatrixBandPartLayerParams(const MatrixBandPartLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&numlower_, &from.numlower_,
    reinterpret_cast<char*>(&numupper_) -
    reinterpret_cast<char*>(&numlower_) + sizeof(numupper_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MatrixBandPartLayerParams)
}

void MatrixBandPartLayerParams::SharedCtor() {
  ::memset(&numlower_, 0, reinterpret_cast<char*>(&numupper_) -
    reinterpret_cast<char*>(&numlower_) + sizeof(numupper_));
  _cached_size_ = 0;
}

MatrixBandPartLayerParams::~MatrixBandPartLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MatrixBandPartLayerParams)
  SharedDtor();
}

void MatrixBandPartLayerParams::SharedDtor() {
}

void MatrixBandPartLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MatrixBandPartLayerParams& MatrixBandPartLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MatrixBandPartLayerParams* MatrixBandPartLayerParams::New(::google::protobuf::Arena* arena) const {
  MatrixBandPartLayerParams* n = new MatrixBandPartLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MatrixBandPartLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MatrixBandPartLayerParams)
  ::memset(&numlower_, 0, reinterpret_cast<char*>(&numupper_) -
    reinterpret_cast<char*>(&numlower_) + sizeof(numupper_));
}

bool MatrixBandPartLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MatrixBandPartLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 numLower = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &numlower_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 numUpper = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &numupper_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MatrixBandPartLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MatrixBandPartLayerParams)
  return false;
#undef DO_
}

void MatrixBandPartLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MatrixBandPartLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 numLower = 1;
  if (this->numlower() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->numlower(), output);
  }

  // int64 numUpper = 2;
  if (this->numupper() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(2, this->numupper(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MatrixBandPartLayerParams)
}

size_t MatrixBandPartLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MatrixBandPartLayerParams)
  size_t total_size = 0;

  // int64 numLower = 1;
  if (this->numlower() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->numlower());
  }

  // int64 numUpper = 2;
  if (this->numupper() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->numupper());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MatrixBandPartLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MatrixBandPartLayerParams*>(&from));
}

void MatrixBandPartLayerParams::MergeFrom(const MatrixBandPartLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MatrixBandPartLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.numlower() != 0) {
    set_numlower(from.numlower());
  }
  if (from.numupper() != 0) {
    set_numupper(from.numupper());
  }
}

void MatrixBandPartLayerParams::CopyFrom(const MatrixBandPartLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MatrixBandPartLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MatrixBandPartLayerParams::IsInitialized() const {
  return true;
}

void MatrixBandPartLayerParams::Swap(MatrixBandPartLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MatrixBandPartLayerParams::InternalSwap(MatrixBandPartLayerParams* other) {
  std::swap(numlower_, other->numlower_);
  std::swap(numupper_, other->numupper_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MatrixBandPartLayerParams::GetTypeName() const {
  return "CoreML.Specification.MatrixBandPartLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MatrixBandPartLayerParams

// int64 numLower = 1;
void MatrixBandPartLayerParams::clear_numlower() {
  numlower_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 MatrixBandPartLayerParams::numlower() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MatrixBandPartLayerParams.numLower)
  return numlower_;
}
void MatrixBandPartLayerParams::set_numlower(::google::protobuf::int64 value) {
  
  numlower_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MatrixBandPartLayerParams.numLower)
}

// int64 numUpper = 2;
void MatrixBandPartLayerParams::clear_numupper() {
  numupper_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 MatrixBandPartLayerParams::numupper() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MatrixBandPartLayerParams.numUpper)
  return numupper_;
}
void MatrixBandPartLayerParams::set_numupper(::google::protobuf::int64 value) {
  
  numupper_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MatrixBandPartLayerParams.numUpper)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UpperTriangularLayerParams::kKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UpperTriangularLayerParams::UpperTriangularLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UpperTriangularLayerParams)
}
UpperTriangularLayerParams::UpperTriangularLayerParams(const UpperTriangularLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  k_ = from.k_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UpperTriangularLayerParams)
}

void UpperTriangularLayerParams::SharedCtor() {
  k_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

UpperTriangularLayerParams::~UpperTriangularLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UpperTriangularLayerParams)
  SharedDtor();
}

void UpperTriangularLayerParams::SharedDtor() {
}

void UpperTriangularLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UpperTriangularLayerParams& UpperTriangularLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UpperTriangularLayerParams* UpperTriangularLayerParams::New(::google::protobuf::Arena* arena) const {
  UpperTriangularLayerParams* n = new UpperTriangularLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UpperTriangularLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UpperTriangularLayerParams)
  k_ = GOOGLE_LONGLONG(0);
}

bool UpperTriangularLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UpperTriangularLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 k = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UpperTriangularLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UpperTriangularLayerParams)
  return false;
#undef DO_
}

void UpperTriangularLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UpperTriangularLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 k = 1;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->k(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UpperTriangularLayerParams)
}

size_t UpperTriangularLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UpperTriangularLayerParams)
  size_t total_size = 0;

  // int64 k = 1;
  if (this->k() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->k());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UpperTriangularLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UpperTriangularLayerParams*>(&from));
}

void UpperTriangularLayerParams::MergeFrom(const UpperTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UpperTriangularLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.k() != 0) {
    set_k(from.k());
  }
}

void UpperTriangularLayerParams::CopyFrom(const UpperTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UpperTriangularLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UpperTriangularLayerParams::IsInitialized() const {
  return true;
}

void UpperTriangularLayerParams::Swap(UpperTriangularLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UpperTriangularLayerParams::InternalSwap(UpperTriangularLayerParams* other) {
  std::swap(k_, other->k_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UpperTriangularLayerParams::GetTypeName() const {
  return "CoreML.Specification.UpperTriangularLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UpperTriangularLayerParams

// int64 k = 1;
void UpperTriangularLayerParams::clear_k() {
  k_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 UpperTriangularLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpperTriangularLayerParams.k)
  return k_;
}
void UpperTriangularLayerParams::set_k(::google::protobuf::int64 value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpperTriangularLayerParams.k)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LowerTriangularLayerParams::kKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LowerTriangularLayerParams::LowerTriangularLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LowerTriangularLayerParams)
}
LowerTriangularLayerParams::LowerTriangularLayerParams(const LowerTriangularLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  k_ = from.k_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LowerTriangularLayerParams)
}

void LowerTriangularLayerParams::SharedCtor() {
  k_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

LowerTriangularLayerParams::~LowerTriangularLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LowerTriangularLayerParams)
  SharedDtor();
}

void LowerTriangularLayerParams::SharedDtor() {
}

void LowerTriangularLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LowerTriangularLayerParams& LowerTriangularLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LowerTriangularLayerParams* LowerTriangularLayerParams::New(::google::protobuf::Arena* arena) const {
  LowerTriangularLayerParams* n = new LowerTriangularLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LowerTriangularLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LowerTriangularLayerParams)
  k_ = GOOGLE_LONGLONG(0);
}

bool LowerTriangularLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LowerTriangularLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 k = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LowerTriangularLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LowerTriangularLayerParams)
  return false;
#undef DO_
}

void LowerTriangularLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LowerTriangularLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 k = 1;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->k(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LowerTriangularLayerParams)
}

size_t LowerTriangularLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LowerTriangularLayerParams)
  size_t total_size = 0;

  // int64 k = 1;
  if (this->k() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->k());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LowerTriangularLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LowerTriangularLayerParams*>(&from));
}

void LowerTriangularLayerParams::MergeFrom(const LowerTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LowerTriangularLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.k() != 0) {
    set_k(from.k());
  }
}

void LowerTriangularLayerParams::CopyFrom(const LowerTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LowerTriangularLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LowerTriangularLayerParams::IsInitialized() const {
  return true;
}

void LowerTriangularLayerParams::Swap(LowerTriangularLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LowerTriangularLayerParams::InternalSwap(LowerTriangularLayerParams* other) {
  std::swap(k_, other->k_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LowerTriangularLayerParams::GetTypeName() const {
  return "CoreML.Specification.LowerTriangularLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LowerTriangularLayerParams

// int64 k = 1;
void LowerTriangularLayerParams::clear_k() {
  k_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 LowerTriangularLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LowerTriangularLayerParams.k)
  return k_;
}
void LowerTriangularLayerParams::set_k(::google::protobuf::int64 value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LowerTriangularLayerParams.k)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BroadcastToLikeLayerParams::BroadcastToLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BroadcastToLikeLayerParams)
}
BroadcastToLikeLayerParams::BroadcastToLikeLayerParams(const BroadcastToLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BroadcastToLikeLayerParams)
}

void BroadcastToLikeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

BroadcastToLikeLayerParams::~BroadcastToLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BroadcastToLikeLayerParams)
  SharedDtor();
}

void BroadcastToLikeLayerParams::SharedDtor() {
}

void BroadcastToLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BroadcastToLikeLayerParams& BroadcastToLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BroadcastToLikeLayerParams* BroadcastToLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  BroadcastToLikeLayerParams* n = new BroadcastToLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BroadcastToLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BroadcastToLikeLayerParams)
}

bool BroadcastToLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BroadcastToLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BroadcastToLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BroadcastToLikeLayerParams)
  return false;
#undef DO_
}

void BroadcastToLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BroadcastToLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BroadcastToLikeLayerParams)
}

size_t BroadcastToLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BroadcastToLikeLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BroadcastToLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BroadcastToLikeLayerParams*>(&from));
}

void BroadcastToLikeLayerParams::MergeFrom(const BroadcastToLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BroadcastToLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void BroadcastToLikeLayerParams::CopyFrom(const BroadcastToLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BroadcastToLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BroadcastToLikeLayerParams::IsInitialized() const {
  return true;
}

void BroadcastToLikeLayerParams::Swap(BroadcastToLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BroadcastToLikeLayerParams::InternalSwap(BroadcastToLikeLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BroadcastToLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.BroadcastToLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BroadcastToLikeLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BroadcastToStaticLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BroadcastToStaticLayerParams::BroadcastToStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BroadcastToStaticLayerParams)
}
BroadcastToStaticLayerParams::BroadcastToStaticLayerParams(const BroadcastToStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BroadcastToStaticLayerParams)
}

void BroadcastToStaticLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

BroadcastToStaticLayerParams::~BroadcastToStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BroadcastToStaticLayerParams)
  SharedDtor();
}

void BroadcastToStaticLayerParams::SharedDtor() {
}

void BroadcastToStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BroadcastToStaticLayerParams& BroadcastToStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BroadcastToStaticLayerParams* BroadcastToStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  BroadcastToStaticLayerParams* n = new BroadcastToStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BroadcastToStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BroadcastToStaticLayerParams)
  targetshape_.Clear();
}

bool BroadcastToStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BroadcastToStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BroadcastToStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BroadcastToStaticLayerParams)
  return false;
#undef DO_
}

void BroadcastToStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BroadcastToStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BroadcastToStaticLayerParams)
}

size_t BroadcastToStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BroadcastToStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BroadcastToStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BroadcastToStaticLayerParams*>(&from));
}

void BroadcastToStaticLayerParams::MergeFrom(const BroadcastToStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BroadcastToStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
}

void BroadcastToStaticLayerParams::CopyFrom(const BroadcastToStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BroadcastToStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BroadcastToStaticLayerParams::IsInitialized() const {
  return true;
}

void BroadcastToStaticLayerParams::Swap(BroadcastToStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BroadcastToStaticLayerParams::InternalSwap(BroadcastToStaticLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BroadcastToStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.BroadcastToStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BroadcastToStaticLayerParams

// repeated uint64 targetShape = 1;
int BroadcastToStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void BroadcastToStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::uint64 BroadcastToStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
void BroadcastToStaticLayerParams::set_targetshape(int index, ::google::protobuf::uint64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
void BroadcastToStaticLayerParams::add_targetshape(::google::protobuf::uint64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BroadcastToStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BroadcastToStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BroadcastToDynamicLayerParams::BroadcastToDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BroadcastToDynamicLayerParams)
}
BroadcastToDynamicLayerParams::BroadcastToDynamicLayerParams(const BroadcastToDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BroadcastToDynamicLayerParams)
}

void BroadcastToDynamicLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

BroadcastToDynamicLayerParams::~BroadcastToDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BroadcastToDynamicLayerParams)
  SharedDtor();
}

void BroadcastToDynamicLayerParams::SharedDtor() {
}

void BroadcastToDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BroadcastToDynamicLayerParams& BroadcastToDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BroadcastToDynamicLayerParams* BroadcastToDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  BroadcastToDynamicLayerParams* n = new BroadcastToDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BroadcastToDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BroadcastToDynamicLayerParams)
}

bool BroadcastToDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BroadcastToDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BroadcastToDynamicLayerParams)
  return false;
#undef DO_
}

void BroadcastToDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BroadcastToDynamicLayerParams)
}

size_t BroadcastToDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BroadcastToDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BroadcastToDynamicLayerParams*>(&from));
}

void BroadcastToDynamicLayerParams::MergeFrom(const BroadcastToDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void BroadcastToDynamicLayerParams::CopyFrom(const BroadcastToDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BroadcastToDynamicLayerParams::IsInitialized() const {
  return true;
}

void BroadcastToDynamicLayerParams::Swap(BroadcastToDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BroadcastToDynamicLayerParams::InternalSwap(BroadcastToDynamicLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BroadcastToDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.BroadcastToDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BroadcastToDynamicLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AddBroadcastableLayerParams::AddBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AddBroadcastableLayerParams)
}
AddBroadcastableLayerParams::AddBroadcastableLayerParams(const AddBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AddBroadcastableLayerParams)
}

void AddBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AddBroadcastableLayerParams::~AddBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AddBroadcastableLayerParams)
  SharedDtor();
}

void AddBroadcastableLayerParams::SharedDtor() {
}

void AddBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AddBroadcastableLayerParams& AddBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AddBroadcastableLayerParams* AddBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  AddBroadcastableLayerParams* n = new AddBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AddBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AddBroadcastableLayerParams)
}

bool AddBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AddBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AddBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AddBroadcastableLayerParams)
  return false;
#undef DO_
}

void AddBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AddBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AddBroadcastableLayerParams)
}

size_t AddBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AddBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AddBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AddBroadcastableLayerParams*>(&from));
}

void AddBroadcastableLayerParams::MergeFrom(const AddBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AddBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AddBroadcastableLayerParams::CopyFrom(const AddBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AddBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AddBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void AddBroadcastableLayerParams::Swap(AddBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AddBroadcastableLayerParams::InternalSwap(AddBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AddBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.AddBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AddBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MaxBroadcastableLayerParams::MaxBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MaxBroadcastableLayerParams)
}
MaxBroadcastableLayerParams::MaxBroadcastableLayerParams(const MaxBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MaxBroadcastableLayerParams)
}

void MaxBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MaxBroadcastableLayerParams::~MaxBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MaxBroadcastableLayerParams)
  SharedDtor();
}

void MaxBroadcastableLayerParams::SharedDtor() {
}

void MaxBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MaxBroadcastableLayerParams& MaxBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MaxBroadcastableLayerParams* MaxBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  MaxBroadcastableLayerParams* n = new MaxBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MaxBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MaxBroadcastableLayerParams)
}

bool MaxBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MaxBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MaxBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MaxBroadcastableLayerParams)
  return false;
#undef DO_
}

void MaxBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MaxBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MaxBroadcastableLayerParams)
}

size_t MaxBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MaxBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MaxBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MaxBroadcastableLayerParams*>(&from));
}

void MaxBroadcastableLayerParams::MergeFrom(const MaxBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MaxBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MaxBroadcastableLayerParams::CopyFrom(const MaxBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MaxBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MaxBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void MaxBroadcastableLayerParams::Swap(MaxBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MaxBroadcastableLayerParams::InternalSwap(MaxBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MaxBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.MaxBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MaxBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MinBroadcastableLayerParams::MinBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MinBroadcastableLayerParams)
}
MinBroadcastableLayerParams::MinBroadcastableLayerParams(const MinBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MinBroadcastableLayerParams)
}

void MinBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MinBroadcastableLayerParams::~MinBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MinBroadcastableLayerParams)
  SharedDtor();
}

void MinBroadcastableLayerParams::SharedDtor() {
}

void MinBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MinBroadcastableLayerParams& MinBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MinBroadcastableLayerParams* MinBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  MinBroadcastableLayerParams* n = new MinBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MinBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MinBroadcastableLayerParams)
}

bool MinBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MinBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MinBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MinBroadcastableLayerParams)
  return false;
#undef DO_
}

void MinBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MinBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MinBroadcastableLayerParams)
}

size_t MinBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MinBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MinBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MinBroadcastableLayerParams*>(&from));
}

void MinBroadcastableLayerParams::MergeFrom(const MinBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MinBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MinBroadcastableLayerParams::CopyFrom(const MinBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MinBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MinBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void MinBroadcastableLayerParams::Swap(MinBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MinBroadcastableLayerParams::InternalSwap(MinBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MinBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.MinBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MinBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ModBroadcastableLayerParams::ModBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ModBroadcastableLayerParams)
}
ModBroadcastableLayerParams::ModBroadcastableLayerParams(const ModBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ModBroadcastableLayerParams)
}

void ModBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ModBroadcastableLayerParams::~ModBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ModBroadcastableLayerParams)
  SharedDtor();
}

void ModBroadcastableLayerParams::SharedDtor() {
}

void ModBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ModBroadcastableLayerParams& ModBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ModBroadcastableLayerParams* ModBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  ModBroadcastableLayerParams* n = new ModBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ModBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ModBroadcastableLayerParams)
}

bool ModBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ModBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ModBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ModBroadcastableLayerParams)
  return false;
#undef DO_
}

void ModBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ModBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ModBroadcastableLayerParams)
}

size_t ModBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ModBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ModBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ModBroadcastableLayerParams*>(&from));
}

void ModBroadcastableLayerParams::MergeFrom(const ModBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ModBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ModBroadcastableLayerParams::CopyFrom(const ModBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ModBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ModBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void ModBroadcastableLayerParams::Swap(ModBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ModBroadcastableLayerParams::InternalSwap(ModBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ModBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.ModBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ModBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FloorDivBroadcastableLayerParams::FloorDivBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FloorDivBroadcastableLayerParams)
}
FloorDivBroadcastableLayerParams::FloorDivBroadcastableLayerParams(const FloorDivBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FloorDivBroadcastableLayerParams)
}

void FloorDivBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

FloorDivBroadcastableLayerParams::~FloorDivBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FloorDivBroadcastableLayerParams)
  SharedDtor();
}

void FloorDivBroadcastableLayerParams::SharedDtor() {
}

void FloorDivBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FloorDivBroadcastableLayerParams& FloorDivBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FloorDivBroadcastableLayerParams* FloorDivBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  FloorDivBroadcastableLayerParams* n = new FloorDivBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FloorDivBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FloorDivBroadcastableLayerParams)
}

bool FloorDivBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FloorDivBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FloorDivBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FloorDivBroadcastableLayerParams)
  return false;
#undef DO_
}

void FloorDivBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FloorDivBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FloorDivBroadcastableLayerParams)
}

size_t FloorDivBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FloorDivBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FloorDivBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FloorDivBroadcastableLayerParams*>(&from));
}

void FloorDivBroadcastableLayerParams::MergeFrom(const FloorDivBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FloorDivBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void FloorDivBroadcastableLayerParams::CopyFrom(const FloorDivBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FloorDivBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FloorDivBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void FloorDivBroadcastableLayerParams::Swap(FloorDivBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FloorDivBroadcastableLayerParams::InternalSwap(FloorDivBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FloorDivBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.FloorDivBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FloorDivBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SubtractBroadcastableLayerParams::SubtractBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SubtractBroadcastableLayerParams)
}
SubtractBroadcastableLayerParams::SubtractBroadcastableLayerParams(const SubtractBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SubtractBroadcastableLayerParams)
}

void SubtractBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SubtractBroadcastableLayerParams::~SubtractBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SubtractBroadcastableLayerParams)
  SharedDtor();
}

void SubtractBroadcastableLayerParams::SharedDtor() {
}

void SubtractBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SubtractBroadcastableLayerParams& SubtractBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SubtractBroadcastableLayerParams* SubtractBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  SubtractBroadcastableLayerParams* n = new SubtractBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SubtractBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SubtractBroadcastableLayerParams)
}

bool SubtractBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SubtractBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SubtractBroadcastableLayerParams)
  return false;
#undef DO_
}

void SubtractBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SubtractBroadcastableLayerParams)
}

size_t SubtractBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SubtractBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SubtractBroadcastableLayerParams*>(&from));
}

void SubtractBroadcastableLayerParams::MergeFrom(const SubtractBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SubtractBroadcastableLayerParams::CopyFrom(const SubtractBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SubtractBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void SubtractBroadcastableLayerParams::Swap(SubtractBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SubtractBroadcastableLayerParams::InternalSwap(SubtractBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SubtractBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.SubtractBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SubtractBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiplyBroadcastableLayerParams::MultiplyBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MultiplyBroadcastableLayerParams)
}
MultiplyBroadcastableLayerParams::MultiplyBroadcastableLayerParams(const MultiplyBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MultiplyBroadcastableLayerParams)
}

void MultiplyBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MultiplyBroadcastableLayerParams::~MultiplyBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MultiplyBroadcastableLayerParams)
  SharedDtor();
}

void MultiplyBroadcastableLayerParams::SharedDtor() {
}

void MultiplyBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MultiplyBroadcastableLayerParams& MultiplyBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MultiplyBroadcastableLayerParams* MultiplyBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  MultiplyBroadcastableLayerParams* n = new MultiplyBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MultiplyBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
}

bool MultiplyBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MultiplyBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MultiplyBroadcastableLayerParams)
  return false;
#undef DO_
}

void MultiplyBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MultiplyBroadcastableLayerParams)
}

size_t MultiplyBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiplyBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MultiplyBroadcastableLayerParams*>(&from));
}

void MultiplyBroadcastableLayerParams::MergeFrom(const MultiplyBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MultiplyBroadcastableLayerParams::CopyFrom(const MultiplyBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiplyBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void MultiplyBroadcastableLayerParams::Swap(MultiplyBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MultiplyBroadcastableLayerParams::InternalSwap(MultiplyBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MultiplyBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.MultiplyBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiplyBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DivideBroadcastableLayerParams::DivideBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.DivideBroadcastableLayerParams)
}
DivideBroadcastableLayerParams::DivideBroadcastableLayerParams(const DivideBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.DivideBroadcastableLayerParams)
}

void DivideBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

DivideBroadcastableLayerParams::~DivideBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.DivideBroadcastableLayerParams)
  SharedDtor();
}

void DivideBroadcastableLayerParams::SharedDtor() {
}

void DivideBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const DivideBroadcastableLayerParams& DivideBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

DivideBroadcastableLayerParams* DivideBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  DivideBroadcastableLayerParams* n = new DivideBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void DivideBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.DivideBroadcastableLayerParams)
}

bool DivideBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.DivideBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.DivideBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.DivideBroadcastableLayerParams)
  return false;
#undef DO_
}

void DivideBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.DivideBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.DivideBroadcastableLayerParams)
}

size_t DivideBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.DivideBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void DivideBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const DivideBroadcastableLayerParams*>(&from));
}

void DivideBroadcastableLayerParams::MergeFrom(const DivideBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.DivideBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void DivideBroadcastableLayerParams::CopyFrom(const DivideBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.DivideBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DivideBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void DivideBroadcastableLayerParams::Swap(DivideBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DivideBroadcastableLayerParams::InternalSwap(DivideBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string DivideBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.DivideBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// DivideBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GatherLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GatherLayerParams::GatherLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GatherLayerParams)
}
GatherLayerParams::GatherLayerParams(const GatherLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GatherLayerParams)
}

void GatherLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

GatherLayerParams::~GatherLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GatherLayerParams)
  SharedDtor();
}

void GatherLayerParams::SharedDtor() {
}

void GatherLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GatherLayerParams& GatherLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GatherLayerParams* GatherLayerParams::New(::google::protobuf::Arena* arena) const {
  GatherLayerParams* n = new GatherLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GatherLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GatherLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool GatherLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GatherLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GatherLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GatherLayerParams)
  return false;
#undef DO_
}

void GatherLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GatherLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GatherLayerParams)
}

size_t GatherLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GatherLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GatherLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GatherLayerParams*>(&from));
}

void GatherLayerParams::MergeFrom(const GatherLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GatherLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void GatherLayerParams::CopyFrom(const GatherLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GatherLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GatherLayerParams::IsInitialized() const {
  return true;
}

void GatherLayerParams::Swap(GatherLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GatherLayerParams::InternalSwap(GatherLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GatherLayerParams::GetTypeName() const {
  return "CoreML.Specification.GatherLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GatherLayerParams

// int64 axis = 1;
void GatherLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 GatherLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherLayerParams.axis)
  return axis_;
}
void GatherLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScatterLayerParams::kAxisFieldNumber;
const int ScatterLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScatterLayerParams::ScatterLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScatterLayerParams)
}
ScatterLayerParams::ScatterLayerParams(const ScatterLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&axis_) + sizeof(mode_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScatterLayerParams)
}

void ScatterLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&axis_) + sizeof(mode_));
  _cached_size_ = 0;
}

ScatterLayerParams::~ScatterLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScatterLayerParams)
  SharedDtor();
}

void ScatterLayerParams::SharedDtor() {
}

void ScatterLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScatterLayerParams& ScatterLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScatterLayerParams* ScatterLayerParams::New(::google::protobuf::Arena* arena) const {
  ScatterLayerParams* n = new ScatterLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScatterLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScatterLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&axis_) + sizeof(mode_));
}

bool ScatterLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScatterLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScatterMode mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ScatterMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScatterLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScatterLayerParams)
  return false;
#undef DO_
}

void ScatterLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScatterLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // .CoreML.Specification.ScatterMode mode = 2;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      2, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScatterLayerParams)
}

size_t ScatterLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScatterLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // .CoreML.Specification.ScatterMode mode = 2;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScatterLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScatterLayerParams*>(&from));
}

void ScatterLayerParams::MergeFrom(const ScatterLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScatterLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ScatterLayerParams::CopyFrom(const ScatterLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScatterLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScatterLayerParams::IsInitialized() const {
  return true;
}

void ScatterLayerParams::Swap(ScatterLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScatterLayerParams::InternalSwap(ScatterLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScatterLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScatterLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScatterLayerParams

// int64 axis = 1;
void ScatterLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ScatterLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterLayerParams.axis)
  return axis_;
}
void ScatterLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterLayerParams.axis)
}

// .CoreML.Specification.ScatterMode mode = 2;
void ScatterLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ScatterMode ScatterLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterLayerParams.mode)
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
void ScatterLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GatherNDLayerParams::GatherNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GatherNDLayerParams)
}
GatherNDLayerParams::GatherNDLayerParams(const GatherNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GatherNDLayerParams)
}

void GatherNDLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

GatherNDLayerParams::~GatherNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GatherNDLayerParams)
  SharedDtor();
}

void GatherNDLayerParams::SharedDtor() {
}

void GatherNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GatherNDLayerParams& GatherNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GatherNDLayerParams* GatherNDLayerParams::New(::google::protobuf::Arena* arena) const {
  GatherNDLayerParams* n = new GatherNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GatherNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GatherNDLayerParams)
}

bool GatherNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GatherNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GatherNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GatherNDLayerParams)
  return false;
#undef DO_
}

void GatherNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GatherNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GatherNDLayerParams)
}

size_t GatherNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GatherNDLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GatherNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GatherNDLayerParams*>(&from));
}

void GatherNDLayerParams::MergeFrom(const GatherNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GatherNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void GatherNDLayerParams::CopyFrom(const GatherNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GatherNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GatherNDLayerParams::IsInitialized() const {
  return true;
}

void GatherNDLayerParams::Swap(GatherNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GatherNDLayerParams::InternalSwap(GatherNDLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GatherNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.GatherNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GatherNDLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScatterNDLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScatterNDLayerParams::ScatterNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScatterNDLayerParams)
}
ScatterNDLayerParams::ScatterNDLayerParams(const ScatterNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScatterNDLayerParams)
}

void ScatterNDLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

ScatterNDLayerParams::~ScatterNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScatterNDLayerParams)
  SharedDtor();
}

void ScatterNDLayerParams::SharedDtor() {
}

void ScatterNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScatterNDLayerParams& ScatterNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScatterNDLayerParams* ScatterNDLayerParams::New(::google::protobuf::Arena* arena) const {
  ScatterNDLayerParams* n = new ScatterNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScatterNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScatterNDLayerParams)
  mode_ = 0;
}

bool ScatterNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScatterNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ScatterMode mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ScatterMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScatterNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScatterNDLayerParams)
  return false;
#undef DO_
}

void ScatterNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScatterNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ScatterMode mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScatterNDLayerParams)
}

size_t ScatterNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScatterNDLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ScatterMode mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScatterNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScatterNDLayerParams*>(&from));
}

void ScatterNDLayerParams::MergeFrom(const ScatterNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScatterNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ScatterNDLayerParams::CopyFrom(const ScatterNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScatterNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScatterNDLayerParams::IsInitialized() const {
  return true;
}

void ScatterNDLayerParams::Swap(ScatterNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScatterNDLayerParams::InternalSwap(ScatterNDLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScatterNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScatterNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScatterNDLayerParams

// .CoreML.Specification.ScatterMode mode = 1;
void ScatterNDLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ScatterMode ScatterNDLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterNDLayerParams.mode)
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
void ScatterNDLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterNDLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GatherAlongAxisLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GatherAlongAxisLayerParams::GatherAlongAxisLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GatherAlongAxisLayerParams)
}
GatherAlongAxisLayerParams::GatherAlongAxisLayerParams(const GatherAlongAxisLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GatherAlongAxisLayerParams)
}

void GatherAlongAxisLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

GatherAlongAxisLayerParams::~GatherAlongAxisLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GatherAlongAxisLayerParams)
  SharedDtor();
}

void GatherAlongAxisLayerParams::SharedDtor() {
}

void GatherAlongAxisLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GatherAlongAxisLayerParams& GatherAlongAxisLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GatherAlongAxisLayerParams* GatherAlongAxisLayerParams::New(::google::protobuf::Arena* arena) const {
  GatherAlongAxisLayerParams* n = new GatherAlongAxisLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GatherAlongAxisLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GatherAlongAxisLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool GatherAlongAxisLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GatherAlongAxisLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GatherAlongAxisLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GatherAlongAxisLayerParams)
  return false;
#undef DO_
}

void GatherAlongAxisLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GatherAlongAxisLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GatherAlongAxisLayerParams)
}

size_t GatherAlongAxisLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GatherAlongAxisLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GatherAlongAxisLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GatherAlongAxisLayerParams*>(&from));
}

void GatherAlongAxisLayerParams::MergeFrom(const GatherAlongAxisLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GatherAlongAxisLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void GatherAlongAxisLayerParams::CopyFrom(const GatherAlongAxisLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GatherAlongAxisLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GatherAlongAxisLayerParams::IsInitialized() const {
  return true;
}

void GatherAlongAxisLayerParams::Swap(GatherAlongAxisLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GatherAlongAxisLayerParams::InternalSwap(GatherAlongAxisLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GatherAlongAxisLayerParams::GetTypeName() const {
  return "CoreML.Specification.GatherAlongAxisLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GatherAlongAxisLayerParams

// int64 axis = 1;
void GatherAlongAxisLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 GatherAlongAxisLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherAlongAxisLayerParams.axis)
  return axis_;
}
void GatherAlongAxisLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherAlongAxisLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScatterAlongAxisLayerParams::kAxisFieldNumber;
const int ScatterAlongAxisLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScatterAlongAxisLayerParams::ScatterAlongAxisLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScatterAlongAxisLayerParams)
}
ScatterAlongAxisLayerParams::ScatterAlongAxisLayerParams(const ScatterAlongAxisLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&axis_) + sizeof(mode_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScatterAlongAxisLayerParams)
}

void ScatterAlongAxisLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&axis_) + sizeof(mode_));
  _cached_size_ = 0;
}

ScatterAlongAxisLayerParams::~ScatterAlongAxisLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScatterAlongAxisLayerParams)
  SharedDtor();
}

void ScatterAlongAxisLayerParams::SharedDtor() {
}

void ScatterAlongAxisLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScatterAlongAxisLayerParams& ScatterAlongAxisLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScatterAlongAxisLayerParams* ScatterAlongAxisLayerParams::New(::google::protobuf::Arena* arena) const {
  ScatterAlongAxisLayerParams* n = new ScatterAlongAxisLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScatterAlongAxisLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScatterAlongAxisLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&axis_) + sizeof(mode_));
}

bool ScatterAlongAxisLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScatterAlongAxisLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScatterMode mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ScatterMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScatterAlongAxisLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScatterAlongAxisLayerParams)
  return false;
#undef DO_
}

void ScatterAlongAxisLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScatterAlongAxisLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // .CoreML.Specification.ScatterMode mode = 2;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      2, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScatterAlongAxisLayerParams)
}

size_t ScatterAlongAxisLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScatterAlongAxisLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // .CoreML.Specification.ScatterMode mode = 2;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScatterAlongAxisLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScatterAlongAxisLayerParams*>(&from));
}

void ScatterAlongAxisLayerParams::MergeFrom(const ScatterAlongAxisLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScatterAlongAxisLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ScatterAlongAxisLayerParams::CopyFrom(const ScatterAlongAxisLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScatterAlongAxisLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScatterAlongAxisLayerParams::IsInitialized() const {
  return true;
}

void ScatterAlongAxisLayerParams::Swap(ScatterAlongAxisLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScatterAlongAxisLayerParams::InternalSwap(ScatterAlongAxisLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScatterAlongAxisLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScatterAlongAxisLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScatterAlongAxisLayerParams

// int64 axis = 1;
void ScatterAlongAxisLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ScatterAlongAxisLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterAlongAxisLayerParams.axis)
  return axis_;
}
void ScatterAlongAxisLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterAlongAxisLayerParams.axis)
}

// .CoreML.Specification.ScatterMode mode = 2;
void ScatterAlongAxisLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ScatterMode ScatterAlongAxisLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScatterAlongAxisLayerParams.mode)
  return static_cast< ::CoreML::Specification::ScatterMode >(mode_);
}
void ScatterAlongAxisLayerParams::set_mode(::CoreML::Specification::ScatterMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScatterAlongAxisLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int StackLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

StackLayerParams::StackLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.StackLayerParams)
}
StackLayerParams::StackLayerParams(const StackLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.StackLayerParams)
}

void StackLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

StackLayerParams::~StackLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.StackLayerParams)
  SharedDtor();
}

void StackLayerParams::SharedDtor() {
}

void StackLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const StackLayerParams& StackLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

StackLayerParams* StackLayerParams::New(::google::protobuf::Arena* arena) const {
  StackLayerParams* n = new StackLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void StackLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.StackLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool StackLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.StackLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.StackLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.StackLayerParams)
  return false;
#undef DO_
}

void StackLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.StackLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.StackLayerParams)
}

size_t StackLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.StackLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void StackLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const StackLayerParams*>(&from));
}

void StackLayerParams::MergeFrom(const StackLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.StackLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void StackLayerParams::CopyFrom(const StackLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.StackLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StackLayerParams::IsInitialized() const {
  return true;
}

void StackLayerParams::Swap(StackLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void StackLayerParams::InternalSwap(StackLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string StackLayerParams::GetTypeName() const {
  return "CoreML.Specification.StackLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// StackLayerParams

// int64 axis = 1;
void StackLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 StackLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.StackLayerParams.axis)
  return axis_;
}
void StackLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.StackLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RankPreservingReshapeLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RankPreservingReshapeLayerParams::RankPreservingReshapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RankPreservingReshapeLayerParams)
}
RankPreservingReshapeLayerParams::RankPreservingReshapeLayerParams(const RankPreservingReshapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RankPreservingReshapeLayerParams)
}

void RankPreservingReshapeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

RankPreservingReshapeLayerParams::~RankPreservingReshapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RankPreservingReshapeLayerParams)
  SharedDtor();
}

void RankPreservingReshapeLayerParams::SharedDtor() {
}

void RankPreservingReshapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RankPreservingReshapeLayerParams& RankPreservingReshapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RankPreservingReshapeLayerParams* RankPreservingReshapeLayerParams::New(::google::protobuf::Arena* arena) const {
  RankPreservingReshapeLayerParams* n = new RankPreservingReshapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RankPreservingReshapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  targetshape_.Clear();
}

bool RankPreservingReshapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RankPreservingReshapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RankPreservingReshapeLayerParams)
  return false;
#undef DO_
}

void RankPreservingReshapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RankPreservingReshapeLayerParams)
}

size_t RankPreservingReshapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  size_t total_size = 0;

  // repeated int64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RankPreservingReshapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RankPreservingReshapeLayerParams*>(&from));
}

void RankPreservingReshapeLayerParams::MergeFrom(const RankPreservingReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
}

void RankPreservingReshapeLayerParams::CopyFrom(const RankPreservingReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RankPreservingReshapeLayerParams::IsInitialized() const {
  return true;
}

void RankPreservingReshapeLayerParams::Swap(RankPreservingReshapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RankPreservingReshapeLayerParams::InternalSwap(RankPreservingReshapeLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RankPreservingReshapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.RankPreservingReshapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RankPreservingReshapeLayerParams

// repeated int64 targetShape = 1;
int RankPreservingReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void RankPreservingReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 RankPreservingReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
void RankPreservingReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
void RankPreservingReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
RankPreservingReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
RankPreservingReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConstantPaddingLayerParams::kValueFieldNumber;
const int ConstantPaddingLayerParams::kPadAmountsFieldNumber;
const int ConstantPaddingLayerParams::kPadToGivenOutputSizeModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConstantPaddingLayerParams::ConstantPaddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConstantPaddingLayerParams)
}
ConstantPaddingLayerParams::ConstantPaddingLayerParams(const ConstantPaddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      padamounts_(from.padamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&value_, &from.value_,
    reinterpret_cast<char*>(&padtogivenoutputsizemode_) -
    reinterpret_cast<char*>(&value_) + sizeof(padtogivenoutputsizemode_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConstantPaddingLayerParams)
}

void ConstantPaddingLayerParams::SharedCtor() {
  ::memset(&value_, 0, reinterpret_cast<char*>(&padtogivenoutputsizemode_) -
    reinterpret_cast<char*>(&value_) + sizeof(padtogivenoutputsizemode_));
  _cached_size_ = 0;
}

ConstantPaddingLayerParams::~ConstantPaddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConstantPaddingLayerParams)
  SharedDtor();
}

void ConstantPaddingLayerParams::SharedDtor() {
}

void ConstantPaddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConstantPaddingLayerParams& ConstantPaddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConstantPaddingLayerParams* ConstantPaddingLayerParams::New(::google::protobuf::Arena* arena) const {
  ConstantPaddingLayerParams* n = new ConstantPaddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConstantPaddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConstantPaddingLayerParams)
  padamounts_.Clear();
  ::memset(&value_, 0, reinterpret_cast<char*>(&padtogivenoutputsizemode_) -
    reinterpret_cast<char*>(&value_) + sizeof(padtogivenoutputsizemode_));
}

bool ConstantPaddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConstantPaddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 padAmounts = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_padamounts())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 18u, input, this->mutable_padamounts())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool padToGivenOutputSizeMode = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &padtogivenoutputsizemode_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConstantPaddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConstantPaddingLayerParams)
  return false;
#undef DO_
}

void ConstantPaddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConstantPaddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // repeated uint64 padAmounts = 2;
  if (this->padamounts_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_padamounts_cached_byte_size_);
  }
  for (int i = 0, n = this->padamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->padamounts(i), output);
  }

  // bool padToGivenOutputSizeMode = 3;
  if (this->padtogivenoutputsizemode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->padtogivenoutputsizemode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConstantPaddingLayerParams)
}

size_t ConstantPaddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConstantPaddingLayerParams)
  size_t total_size = 0;

  // repeated uint64 padAmounts = 2;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->padamounts_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _padamounts_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  // bool padToGivenOutputSizeMode = 3;
  if (this->padtogivenoutputsizemode() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConstantPaddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConstantPaddingLayerParams*>(&from));
}

void ConstantPaddingLayerParams::MergeFrom(const ConstantPaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConstantPaddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  padamounts_.MergeFrom(from.padamounts_);
  if (from.value() != 0) {
    set_value(from.value());
  }
  if (from.padtogivenoutputsizemode() != 0) {
    set_padtogivenoutputsizemode(from.padtogivenoutputsizemode());
  }
}

void ConstantPaddingLayerParams::CopyFrom(const ConstantPaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConstantPaddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConstantPaddingLayerParams::IsInitialized() const {
  return true;
}

void ConstantPaddingLayerParams::Swap(ConstantPaddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConstantPaddingLayerParams::InternalSwap(ConstantPaddingLayerParams* other) {
  padamounts_.InternalSwap(&other->padamounts_);
  std::swap(value_, other->value_);
  std::swap(padtogivenoutputsizemode_, other->padtogivenoutputsizemode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConstantPaddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConstantPaddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConstantPaddingLayerParams

// float value = 1;
void ConstantPaddingLayerParams::clear_value() {
  value_ = 0;
}
float ConstantPaddingLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.value)
  return value_;
}
void ConstantPaddingLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.value)
}

// repeated uint64 padAmounts = 2;
int ConstantPaddingLayerParams::padamounts_size() const {
  return padamounts_.size();
}
void ConstantPaddingLayerParams::clear_padamounts() {
  padamounts_.Clear();
}
::google::protobuf::uint64 ConstantPaddingLayerParams::padamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return padamounts_.Get(index);
}
void ConstantPaddingLayerParams::set_padamounts(int index, ::google::protobuf::uint64 value) {
  padamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
}
void ConstantPaddingLayerParams::add_padamounts(::google::protobuf::uint64 value) {
  padamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConstantPaddingLayerParams::padamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return padamounts_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConstantPaddingLayerParams::mutable_padamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConstantPaddingLayerParams.padAmounts)
  return &padamounts_;
}

// bool padToGivenOutputSizeMode = 3;
void ConstantPaddingLayerParams::clear_padtogivenoutputsizemode() {
  padtogivenoutputsizemode_ = false;
}
bool ConstantPaddingLayerParams::padtogivenoutputsizemode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConstantPaddingLayerParams.padToGivenOutputSizeMode)
  return padtogivenoutputsizemode_;
}
void ConstantPaddingLayerParams::set_padtogivenoutputsizemode(bool value) {
  
  padtogivenoutputsizemode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConstantPaddingLayerParams.padToGivenOutputSizeMode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomNormalLikeLayerParams::kSeedFieldNumber;
const int RandomNormalLikeLayerParams::kMeanFieldNumber;
const int RandomNormalLikeLayerParams::kStdDevFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomNormalLikeLayerParams::RandomNormalLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomNormalLikeLayerParams)
}
RandomNormalLikeLayerParams::RandomNormalLikeLayerParams(const RandomNormalLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomNormalLikeLayerParams)
}

void RandomNormalLikeLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
  _cached_size_ = 0;
}

RandomNormalLikeLayerParams::~RandomNormalLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomNormalLikeLayerParams)
  SharedDtor();
}

void RandomNormalLikeLayerParams::SharedDtor() {
}

void RandomNormalLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomNormalLikeLayerParams& RandomNormalLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomNormalLikeLayerParams* RandomNormalLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomNormalLikeLayerParams* n = new RandomNormalLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomNormalLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomNormalLikeLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
}

bool RandomNormalLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomNormalLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float mean = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &mean_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float stdDev = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &stddev_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomNormalLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomNormalLikeLayerParams)
  return false;
#undef DO_
}

void RandomNormalLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomNormalLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float mean = 2;
  if (this->mean() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->mean(), output);
  }

  // float stdDev = 3;
  if (this->stddev() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->stddev(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomNormalLikeLayerParams)
}

size_t RandomNormalLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomNormalLikeLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float mean = 2;
  if (this->mean() != 0) {
    total_size += 1 + 4;
  }

  // float stdDev = 3;
  if (this->stddev() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomNormalLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomNormalLikeLayerParams*>(&from));
}

void RandomNormalLikeLayerParams::MergeFrom(const RandomNormalLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomNormalLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.mean() != 0) {
    set_mean(from.mean());
  }
  if (from.stddev() != 0) {
    set_stddev(from.stddev());
  }
}

void RandomNormalLikeLayerParams::CopyFrom(const RandomNormalLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomNormalLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomNormalLikeLayerParams::IsInitialized() const {
  return true;
}

void RandomNormalLikeLayerParams::Swap(RandomNormalLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomNormalLikeLayerParams::InternalSwap(RandomNormalLikeLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(mean_, other->mean_);
  std::swap(stddev_, other->stddev_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomNormalLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomNormalLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomNormalLikeLayerParams

// int64 seed = 1;
void RandomNormalLikeLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomNormalLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.seed)
  return seed_;
}
void RandomNormalLikeLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.seed)
}

// float mean = 2;
void RandomNormalLikeLayerParams::clear_mean() {
  mean_ = 0;
}
float RandomNormalLikeLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.mean)
  return mean_;
}
void RandomNormalLikeLayerParams::set_mean(float value) {
  
  mean_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.mean)
}

// float stdDev = 3;
void RandomNormalLikeLayerParams::clear_stddev() {
  stddev_ = 0;
}
float RandomNormalLikeLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalLikeLayerParams.stdDev)
  return stddev_;
}
void RandomNormalLikeLayerParams::set_stddev(float value) {
  
  stddev_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalLikeLayerParams.stdDev)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomNormalStaticLayerParams::kSeedFieldNumber;
const int RandomNormalStaticLayerParams::kMeanFieldNumber;
const int RandomNormalStaticLayerParams::kStdDevFieldNumber;
const int RandomNormalStaticLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomNormalStaticLayerParams::RandomNormalStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomNormalStaticLayerParams)
}
RandomNormalStaticLayerParams::RandomNormalStaticLayerParams(const RandomNormalStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      outputshape_(from.outputshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomNormalStaticLayerParams)
}

void RandomNormalStaticLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
  _cached_size_ = 0;
}

RandomNormalStaticLayerParams::~RandomNormalStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomNormalStaticLayerParams)
  SharedDtor();
}

void RandomNormalStaticLayerParams::SharedDtor() {
}

void RandomNormalStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomNormalStaticLayerParams& RandomNormalStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomNormalStaticLayerParams* RandomNormalStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomNormalStaticLayerParams* n = new RandomNormalStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomNormalStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomNormalStaticLayerParams)
  outputshape_.Clear();
  ::memset(&seed_, 0, reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
}

bool RandomNormalStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomNormalStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float mean = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &mean_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float stdDev = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &stddev_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 outputShape = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 34u, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomNormalStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomNormalStaticLayerParams)
  return false;
#undef DO_
}

void RandomNormalStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomNormalStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float mean = 2;
  if (this->mean() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->mean(), output);
  }

  // float stdDev = 3;
  if (this->stddev() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->stddev(), output);
  }

  // repeated uint64 outputShape = 4;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0, n = this->outputshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomNormalStaticLayerParams)
}

size_t RandomNormalStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomNormalStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 outputShape = 4;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->outputshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float mean = 2;
  if (this->mean() != 0) {
    total_size += 1 + 4;
  }

  // float stdDev = 3;
  if (this->stddev() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomNormalStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomNormalStaticLayerParams*>(&from));
}

void RandomNormalStaticLayerParams::MergeFrom(const RandomNormalStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomNormalStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  outputshape_.MergeFrom(from.outputshape_);
  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.mean() != 0) {
    set_mean(from.mean());
  }
  if (from.stddev() != 0) {
    set_stddev(from.stddev());
  }
}

void RandomNormalStaticLayerParams::CopyFrom(const RandomNormalStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomNormalStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomNormalStaticLayerParams::IsInitialized() const {
  return true;
}

void RandomNormalStaticLayerParams::Swap(RandomNormalStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomNormalStaticLayerParams::InternalSwap(RandomNormalStaticLayerParams* other) {
  outputshape_.InternalSwap(&other->outputshape_);
  std::swap(seed_, other->seed_);
  std::swap(mean_, other->mean_);
  std::swap(stddev_, other->stddev_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomNormalStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomNormalStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomNormalStaticLayerParams

// int64 seed = 1;
void RandomNormalStaticLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomNormalStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.seed)
  return seed_;
}
void RandomNormalStaticLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.seed)
}

// float mean = 2;
void RandomNormalStaticLayerParams::clear_mean() {
  mean_ = 0;
}
float RandomNormalStaticLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.mean)
  return mean_;
}
void RandomNormalStaticLayerParams::set_mean(float value) {
  
  mean_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.mean)
}

// float stdDev = 3;
void RandomNormalStaticLayerParams::clear_stddev() {
  stddev_ = 0;
}
float RandomNormalStaticLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.stdDev)
  return stddev_;
}
void RandomNormalStaticLayerParams::set_stddev(float value) {
  
  stddev_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.stdDev)
}

// repeated uint64 outputShape = 4;
int RandomNormalStaticLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void RandomNormalStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 RandomNormalStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return outputshape_.Get(index);
}
void RandomNormalStaticLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
}
void RandomNormalStaticLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
RandomNormalStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
RandomNormalStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomNormalStaticLayerParams.outputShape)
  return &outputshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomNormalDynamicLayerParams::kSeedFieldNumber;
const int RandomNormalDynamicLayerParams::kMeanFieldNumber;
const int RandomNormalDynamicLayerParams::kStdDevFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomNormalDynamicLayerParams::RandomNormalDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomNormalDynamicLayerParams)
}
RandomNormalDynamicLayerParams::RandomNormalDynamicLayerParams(const RandomNormalDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomNormalDynamicLayerParams)
}

void RandomNormalDynamicLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
  _cached_size_ = 0;
}

RandomNormalDynamicLayerParams::~RandomNormalDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomNormalDynamicLayerParams)
  SharedDtor();
}

void RandomNormalDynamicLayerParams::SharedDtor() {
}

void RandomNormalDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomNormalDynamicLayerParams& RandomNormalDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomNormalDynamicLayerParams* RandomNormalDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomNormalDynamicLayerParams* n = new RandomNormalDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomNormalDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomNormalDynamicLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&stddev_) -
    reinterpret_cast<char*>(&seed_) + sizeof(stddev_));
}

bool RandomNormalDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomNormalDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float mean = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &mean_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float stdDev = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &stddev_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomNormalDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomNormalDynamicLayerParams)
  return false;
#undef DO_
}

void RandomNormalDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomNormalDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float mean = 2;
  if (this->mean() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->mean(), output);
  }

  // float stdDev = 3;
  if (this->stddev() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->stddev(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomNormalDynamicLayerParams)
}

size_t RandomNormalDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomNormalDynamicLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float mean = 2;
  if (this->mean() != 0) {
    total_size += 1 + 4;
  }

  // float stdDev = 3;
  if (this->stddev() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomNormalDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomNormalDynamicLayerParams*>(&from));
}

void RandomNormalDynamicLayerParams::MergeFrom(const RandomNormalDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomNormalDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.mean() != 0) {
    set_mean(from.mean());
  }
  if (from.stddev() != 0) {
    set_stddev(from.stddev());
  }
}

void RandomNormalDynamicLayerParams::CopyFrom(const RandomNormalDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomNormalDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomNormalDynamicLayerParams::IsInitialized() const {
  return true;
}

void RandomNormalDynamicLayerParams::Swap(RandomNormalDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomNormalDynamicLayerParams::InternalSwap(RandomNormalDynamicLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(mean_, other->mean_);
  std::swap(stddev_, other->stddev_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomNormalDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomNormalDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomNormalDynamicLayerParams

// int64 seed = 1;
void RandomNormalDynamicLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomNormalDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.seed)
  return seed_;
}
void RandomNormalDynamicLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.seed)
}

// float mean = 2;
void RandomNormalDynamicLayerParams::clear_mean() {
  mean_ = 0;
}
float RandomNormalDynamicLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.mean)
  return mean_;
}
void RandomNormalDynamicLayerParams::set_mean(float value) {
  
  mean_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.mean)
}

// float stdDev = 3;
void RandomNormalDynamicLayerParams::clear_stddev() {
  stddev_ = 0;
}
float RandomNormalDynamicLayerParams::stddev() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomNormalDynamicLayerParams.stdDev)
  return stddev_;
}
void RandomNormalDynamicLayerParams::set_stddev(float value) {
  
  stddev_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomNormalDynamicLayerParams.stdDev)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomUniformLikeLayerParams::kSeedFieldNumber;
const int RandomUniformLikeLayerParams::kMinValFieldNumber;
const int RandomUniformLikeLayerParams::kMaxValFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomUniformLikeLayerParams::RandomUniformLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomUniformLikeLayerParams)
}
RandomUniformLikeLayerParams::RandomUniformLikeLayerParams(const RandomUniformLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomUniformLikeLayerParams)
}

void RandomUniformLikeLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
  _cached_size_ = 0;
}

RandomUniformLikeLayerParams::~RandomUniformLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomUniformLikeLayerParams)
  SharedDtor();
}

void RandomUniformLikeLayerParams::SharedDtor() {
}

void RandomUniformLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomUniformLikeLayerParams& RandomUniformLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomUniformLikeLayerParams* RandomUniformLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomUniformLikeLayerParams* n = new RandomUniformLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomUniformLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomUniformLikeLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
}

bool RandomUniformLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomUniformLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float minVal = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &minval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float maxVal = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &maxval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomUniformLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomUniformLikeLayerParams)
  return false;
#undef DO_
}

void RandomUniformLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomUniformLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float minVal = 2;
  if (this->minval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->minval(), output);
  }

  // float maxVal = 3;
  if (this->maxval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->maxval(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomUniformLikeLayerParams)
}

size_t RandomUniformLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomUniformLikeLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float minVal = 2;
  if (this->minval() != 0) {
    total_size += 1 + 4;
  }

  // float maxVal = 3;
  if (this->maxval() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomUniformLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomUniformLikeLayerParams*>(&from));
}

void RandomUniformLikeLayerParams::MergeFrom(const RandomUniformLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomUniformLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.minval() != 0) {
    set_minval(from.minval());
  }
  if (from.maxval() != 0) {
    set_maxval(from.maxval());
  }
}

void RandomUniformLikeLayerParams::CopyFrom(const RandomUniformLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomUniformLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomUniformLikeLayerParams::IsInitialized() const {
  return true;
}

void RandomUniformLikeLayerParams::Swap(RandomUniformLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomUniformLikeLayerParams::InternalSwap(RandomUniformLikeLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(minval_, other->minval_);
  std::swap(maxval_, other->maxval_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomUniformLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomUniformLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomUniformLikeLayerParams

// int64 seed = 1;
void RandomUniformLikeLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomUniformLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.seed)
  return seed_;
}
void RandomUniformLikeLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.seed)
}

// float minVal = 2;
void RandomUniformLikeLayerParams::clear_minval() {
  minval_ = 0;
}
float RandomUniformLikeLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.minVal)
  return minval_;
}
void RandomUniformLikeLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.minVal)
}

// float maxVal = 3;
void RandomUniformLikeLayerParams::clear_maxval() {
  maxval_ = 0;
}
float RandomUniformLikeLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformLikeLayerParams.maxVal)
  return maxval_;
}
void RandomUniformLikeLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformLikeLayerParams.maxVal)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomUniformStaticLayerParams::kSeedFieldNumber;
const int RandomUniformStaticLayerParams::kMinValFieldNumber;
const int RandomUniformStaticLayerParams::kMaxValFieldNumber;
const int RandomUniformStaticLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomUniformStaticLayerParams::RandomUniformStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomUniformStaticLayerParams)
}
RandomUniformStaticLayerParams::RandomUniformStaticLayerParams(const RandomUniformStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      outputshape_(from.outputshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomUniformStaticLayerParams)
}

void RandomUniformStaticLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
  _cached_size_ = 0;
}

RandomUniformStaticLayerParams::~RandomUniformStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomUniformStaticLayerParams)
  SharedDtor();
}

void RandomUniformStaticLayerParams::SharedDtor() {
}

void RandomUniformStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomUniformStaticLayerParams& RandomUniformStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomUniformStaticLayerParams* RandomUniformStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomUniformStaticLayerParams* n = new RandomUniformStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomUniformStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomUniformStaticLayerParams)
  outputshape_.Clear();
  ::memset(&seed_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
}

bool RandomUniformStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomUniformStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float minVal = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &minval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float maxVal = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &maxval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 outputShape = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 34u, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomUniformStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomUniformStaticLayerParams)
  return false;
#undef DO_
}

void RandomUniformStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomUniformStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float minVal = 2;
  if (this->minval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->minval(), output);
  }

  // float maxVal = 3;
  if (this->maxval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->maxval(), output);
  }

  // repeated uint64 outputShape = 4;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0, n = this->outputshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomUniformStaticLayerParams)
}

size_t RandomUniformStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomUniformStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 outputShape = 4;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->outputshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float minVal = 2;
  if (this->minval() != 0) {
    total_size += 1 + 4;
  }

  // float maxVal = 3;
  if (this->maxval() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomUniformStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomUniformStaticLayerParams*>(&from));
}

void RandomUniformStaticLayerParams::MergeFrom(const RandomUniformStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomUniformStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  outputshape_.MergeFrom(from.outputshape_);
  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.minval() != 0) {
    set_minval(from.minval());
  }
  if (from.maxval() != 0) {
    set_maxval(from.maxval());
  }
}

void RandomUniformStaticLayerParams::CopyFrom(const RandomUniformStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomUniformStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomUniformStaticLayerParams::IsInitialized() const {
  return true;
}

void RandomUniformStaticLayerParams::Swap(RandomUniformStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomUniformStaticLayerParams::InternalSwap(RandomUniformStaticLayerParams* other) {
  outputshape_.InternalSwap(&other->outputshape_);
  std::swap(seed_, other->seed_);
  std::swap(minval_, other->minval_);
  std::swap(maxval_, other->maxval_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomUniformStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomUniformStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomUniformStaticLayerParams

// int64 seed = 1;
void RandomUniformStaticLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomUniformStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.seed)
  return seed_;
}
void RandomUniformStaticLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.seed)
}

// float minVal = 2;
void RandomUniformStaticLayerParams::clear_minval() {
  minval_ = 0;
}
float RandomUniformStaticLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.minVal)
  return minval_;
}
void RandomUniformStaticLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.minVal)
}

// float maxVal = 3;
void RandomUniformStaticLayerParams::clear_maxval() {
  maxval_ = 0;
}
float RandomUniformStaticLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.maxVal)
  return maxval_;
}
void RandomUniformStaticLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.maxVal)
}

// repeated uint64 outputShape = 4;
int RandomUniformStaticLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void RandomUniformStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 RandomUniformStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return outputshape_.Get(index);
}
void RandomUniformStaticLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
}
void RandomUniformStaticLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
RandomUniformStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
RandomUniformStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomUniformStaticLayerParams.outputShape)
  return &outputshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomUniformDynamicLayerParams::kSeedFieldNumber;
const int RandomUniformDynamicLayerParams::kMinValFieldNumber;
const int RandomUniformDynamicLayerParams::kMaxValFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomUniformDynamicLayerParams::RandomUniformDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomUniformDynamicLayerParams)
}
RandomUniformDynamicLayerParams::RandomUniformDynamicLayerParams(const RandomUniformDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomUniformDynamicLayerParams)
}

void RandomUniformDynamicLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
  _cached_size_ = 0;
}

RandomUniformDynamicLayerParams::~RandomUniformDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomUniformDynamicLayerParams)
  SharedDtor();
}

void RandomUniformDynamicLayerParams::SharedDtor() {
}

void RandomUniformDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomUniformDynamicLayerParams& RandomUniformDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomUniformDynamicLayerParams* RandomUniformDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomUniformDynamicLayerParams* n = new RandomUniformDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomUniformDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomUniformDynamicLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&seed_) + sizeof(maxval_));
}

bool RandomUniformDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomUniformDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float minVal = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &minval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float maxVal = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &maxval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomUniformDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomUniformDynamicLayerParams)
  return false;
#undef DO_
}

void RandomUniformDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomUniformDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float minVal = 2;
  if (this->minval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->minval(), output);
  }

  // float maxVal = 3;
  if (this->maxval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->maxval(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomUniformDynamicLayerParams)
}

size_t RandomUniformDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomUniformDynamicLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float minVal = 2;
  if (this->minval() != 0) {
    total_size += 1 + 4;
  }

  // float maxVal = 3;
  if (this->maxval() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomUniformDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomUniformDynamicLayerParams*>(&from));
}

void RandomUniformDynamicLayerParams::MergeFrom(const RandomUniformDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomUniformDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.minval() != 0) {
    set_minval(from.minval());
  }
  if (from.maxval() != 0) {
    set_maxval(from.maxval());
  }
}

void RandomUniformDynamicLayerParams::CopyFrom(const RandomUniformDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomUniformDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomUniformDynamicLayerParams::IsInitialized() const {
  return true;
}

void RandomUniformDynamicLayerParams::Swap(RandomUniformDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomUniformDynamicLayerParams::InternalSwap(RandomUniformDynamicLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(minval_, other->minval_);
  std::swap(maxval_, other->maxval_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomUniformDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomUniformDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomUniformDynamicLayerParams

// int64 seed = 1;
void RandomUniformDynamicLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomUniformDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.seed)
  return seed_;
}
void RandomUniformDynamicLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.seed)
}

// float minVal = 2;
void RandomUniformDynamicLayerParams::clear_minval() {
  minval_ = 0;
}
float RandomUniformDynamicLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.minVal)
  return minval_;
}
void RandomUniformDynamicLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.minVal)
}

// float maxVal = 3;
void RandomUniformDynamicLayerParams::clear_maxval() {
  maxval_ = 0;
}
float RandomUniformDynamicLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomUniformDynamicLayerParams.maxVal)
  return maxval_;
}
void RandomUniformDynamicLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomUniformDynamicLayerParams.maxVal)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomBernoulliLikeLayerParams::kSeedFieldNumber;
const int RandomBernoulliLikeLayerParams::kProbFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomBernoulliLikeLayerParams::RandomBernoulliLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomBernoulliLikeLayerParams)
}
RandomBernoulliLikeLayerParams::RandomBernoulliLikeLayerParams(const RandomBernoulliLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomBernoulliLikeLayerParams)
}

void RandomBernoulliLikeLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
  _cached_size_ = 0;
}

RandomBernoulliLikeLayerParams::~RandomBernoulliLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomBernoulliLikeLayerParams)
  SharedDtor();
}

void RandomBernoulliLikeLayerParams::SharedDtor() {
}

void RandomBernoulliLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomBernoulliLikeLayerParams& RandomBernoulliLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomBernoulliLikeLayerParams* RandomBernoulliLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomBernoulliLikeLayerParams* n = new RandomBernoulliLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomBernoulliLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomBernoulliLikeLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
}

bool RandomBernoulliLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomBernoulliLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float prob = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &prob_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomBernoulliLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomBernoulliLikeLayerParams)
  return false;
#undef DO_
}

void RandomBernoulliLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomBernoulliLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float prob = 2;
  if (this->prob() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->prob(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomBernoulliLikeLayerParams)
}

size_t RandomBernoulliLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomBernoulliLikeLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float prob = 2;
  if (this->prob() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomBernoulliLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomBernoulliLikeLayerParams*>(&from));
}

void RandomBernoulliLikeLayerParams::MergeFrom(const RandomBernoulliLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomBernoulliLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.prob() != 0) {
    set_prob(from.prob());
  }
}

void RandomBernoulliLikeLayerParams::CopyFrom(const RandomBernoulliLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomBernoulliLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomBernoulliLikeLayerParams::IsInitialized() const {
  return true;
}

void RandomBernoulliLikeLayerParams::Swap(RandomBernoulliLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomBernoulliLikeLayerParams::InternalSwap(RandomBernoulliLikeLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(prob_, other->prob_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomBernoulliLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomBernoulliLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomBernoulliLikeLayerParams

// int64 seed = 1;
void RandomBernoulliLikeLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomBernoulliLikeLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliLikeLayerParams.seed)
  return seed_;
}
void RandomBernoulliLikeLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliLikeLayerParams.seed)
}

// float prob = 2;
void RandomBernoulliLikeLayerParams::clear_prob() {
  prob_ = 0;
}
float RandomBernoulliLikeLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliLikeLayerParams.prob)
  return prob_;
}
void RandomBernoulliLikeLayerParams::set_prob(float value) {
  
  prob_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliLikeLayerParams.prob)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomBernoulliStaticLayerParams::kSeedFieldNumber;
const int RandomBernoulliStaticLayerParams::kProbFieldNumber;
const int RandomBernoulliStaticLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomBernoulliStaticLayerParams::RandomBernoulliStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomBernoulliStaticLayerParams)
}
RandomBernoulliStaticLayerParams::RandomBernoulliStaticLayerParams(const RandomBernoulliStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      outputshape_(from.outputshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomBernoulliStaticLayerParams)
}

void RandomBernoulliStaticLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
  _cached_size_ = 0;
}

RandomBernoulliStaticLayerParams::~RandomBernoulliStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomBernoulliStaticLayerParams)
  SharedDtor();
}

void RandomBernoulliStaticLayerParams::SharedDtor() {
}

void RandomBernoulliStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomBernoulliStaticLayerParams& RandomBernoulliStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomBernoulliStaticLayerParams* RandomBernoulliStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomBernoulliStaticLayerParams* n = new RandomBernoulliStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomBernoulliStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomBernoulliStaticLayerParams)
  outputshape_.Clear();
  ::memset(&seed_, 0, reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
}

bool RandomBernoulliStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomBernoulliStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float prob = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &prob_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 outputShape = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(24u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 26u, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomBernoulliStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomBernoulliStaticLayerParams)
  return false;
#undef DO_
}

void RandomBernoulliStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomBernoulliStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float prob = 2;
  if (this->prob() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->prob(), output);
  }

  // repeated uint64 outputShape = 3;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0, n = this->outputshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomBernoulliStaticLayerParams)
}

size_t RandomBernoulliStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomBernoulliStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 outputShape = 3;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->outputshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float prob = 2;
  if (this->prob() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomBernoulliStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomBernoulliStaticLayerParams*>(&from));
}

void RandomBernoulliStaticLayerParams::MergeFrom(const RandomBernoulliStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomBernoulliStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  outputshape_.MergeFrom(from.outputshape_);
  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.prob() != 0) {
    set_prob(from.prob());
  }
}

void RandomBernoulliStaticLayerParams::CopyFrom(const RandomBernoulliStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomBernoulliStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomBernoulliStaticLayerParams::IsInitialized() const {
  return true;
}

void RandomBernoulliStaticLayerParams::Swap(RandomBernoulliStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomBernoulliStaticLayerParams::InternalSwap(RandomBernoulliStaticLayerParams* other) {
  outputshape_.InternalSwap(&other->outputshape_);
  std::swap(seed_, other->seed_);
  std::swap(prob_, other->prob_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomBernoulliStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomBernoulliStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomBernoulliStaticLayerParams

// int64 seed = 1;
void RandomBernoulliStaticLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomBernoulliStaticLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.seed)
  return seed_;
}
void RandomBernoulliStaticLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.seed)
}

// float prob = 2;
void RandomBernoulliStaticLayerParams::clear_prob() {
  prob_ = 0;
}
float RandomBernoulliStaticLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.prob)
  return prob_;
}
void RandomBernoulliStaticLayerParams::set_prob(float value) {
  
  prob_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.prob)
}

// repeated uint64 outputShape = 3;
int RandomBernoulliStaticLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void RandomBernoulliStaticLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 RandomBernoulliStaticLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return outputshape_.Get(index);
}
void RandomBernoulliStaticLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
}
void RandomBernoulliStaticLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
RandomBernoulliStaticLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
RandomBernoulliStaticLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RandomBernoulliStaticLayerParams.outputShape)
  return &outputshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RandomBernoulliDynamicLayerParams::kSeedFieldNumber;
const int RandomBernoulliDynamicLayerParams::kProbFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RandomBernoulliDynamicLayerParams::RandomBernoulliDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RandomBernoulliDynamicLayerParams)
}
RandomBernoulliDynamicLayerParams::RandomBernoulliDynamicLayerParams(const RandomBernoulliDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RandomBernoulliDynamicLayerParams)
}

void RandomBernoulliDynamicLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
  _cached_size_ = 0;
}

RandomBernoulliDynamicLayerParams::~RandomBernoulliDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  SharedDtor();
}

void RandomBernoulliDynamicLayerParams::SharedDtor() {
}

void RandomBernoulliDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RandomBernoulliDynamicLayerParams& RandomBernoulliDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RandomBernoulliDynamicLayerParams* RandomBernoulliDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  RandomBernoulliDynamicLayerParams* n = new RandomBernoulliDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RandomBernoulliDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&prob_) -
    reinterpret_cast<char*>(&seed_) + sizeof(prob_));
}

bool RandomBernoulliDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float prob = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &prob_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  return false;
#undef DO_
}

void RandomBernoulliDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // float prob = 2;
  if (this->prob() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->prob(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RandomBernoulliDynamicLayerParams)
}

size_t RandomBernoulliDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // float prob = 2;
  if (this->prob() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RandomBernoulliDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RandomBernoulliDynamicLayerParams*>(&from));
}

void RandomBernoulliDynamicLayerParams::MergeFrom(const RandomBernoulliDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.prob() != 0) {
    set_prob(from.prob());
  }
}

void RandomBernoulliDynamicLayerParams::CopyFrom(const RandomBernoulliDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RandomBernoulliDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RandomBernoulliDynamicLayerParams::IsInitialized() const {
  return true;
}

void RandomBernoulliDynamicLayerParams::Swap(RandomBernoulliDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RandomBernoulliDynamicLayerParams::InternalSwap(RandomBernoulliDynamicLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(prob_, other->prob_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RandomBernoulliDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.RandomBernoulliDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RandomBernoulliDynamicLayerParams

// int64 seed = 1;
void RandomBernoulliDynamicLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 RandomBernoulliDynamicLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliDynamicLayerParams.seed)
  return seed_;
}
void RandomBernoulliDynamicLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliDynamicLayerParams.seed)
}

// float prob = 2;
void RandomBernoulliDynamicLayerParams::clear_prob() {
  prob_ = 0;
}
float RandomBernoulliDynamicLayerParams::prob() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RandomBernoulliDynamicLayerParams.prob)
  return prob_;
}
void RandomBernoulliDynamicLayerParams::set_prob(float value) {
  
  prob_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RandomBernoulliDynamicLayerParams.prob)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CategoricalDistributionLayerParams::kSeedFieldNumber;
const int CategoricalDistributionLayerParams::kNumSamplesFieldNumber;
const int CategoricalDistributionLayerParams::kIsLogitsFieldNumber;
const int CategoricalDistributionLayerParams::kEpsFieldNumber;
const int CategoricalDistributionLayerParams::kTemperatureFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CategoricalDistributionLayerParams::CategoricalDistributionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CategoricalDistributionLayerParams)
}
CategoricalDistributionLayerParams::CategoricalDistributionLayerParams(const CategoricalDistributionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&seed_, &from.seed_,
    reinterpret_cast<char*>(&temperature_) -
    reinterpret_cast<char*>(&seed_) + sizeof(temperature_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CategoricalDistributionLayerParams)
}

void CategoricalDistributionLayerParams::SharedCtor() {
  ::memset(&seed_, 0, reinterpret_cast<char*>(&temperature_) -
    reinterpret_cast<char*>(&seed_) + sizeof(temperature_));
  _cached_size_ = 0;
}

CategoricalDistributionLayerParams::~CategoricalDistributionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CategoricalDistributionLayerParams)
  SharedDtor();
}

void CategoricalDistributionLayerParams::SharedDtor() {
}

void CategoricalDistributionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CategoricalDistributionLayerParams& CategoricalDistributionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CategoricalDistributionLayerParams* CategoricalDistributionLayerParams::New(::google::protobuf::Arena* arena) const {
  CategoricalDistributionLayerParams* n = new CategoricalDistributionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CategoricalDistributionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CategoricalDistributionLayerParams)
  ::memset(&seed_, 0, reinterpret_cast<char*>(&temperature_) -
    reinterpret_cast<char*>(&seed_) + sizeof(temperature_));
}

bool CategoricalDistributionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CategoricalDistributionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 seed = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &seed_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 numSamples = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &numsamples_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isLogits = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &islogits_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float eps = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &eps_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float temperature = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(45u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &temperature_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CategoricalDistributionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CategoricalDistributionLayerParams)
  return false;
#undef DO_
}

void CategoricalDistributionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CategoricalDistributionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 seed = 1;
  if (this->seed() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->seed(), output);
  }

  // int64 numSamples = 2;
  if (this->numsamples() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(2, this->numsamples(), output);
  }

  // bool isLogits = 3;
  if (this->islogits() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->islogits(), output);
  }

  // float eps = 4;
  if (this->eps() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->eps(), output);
  }

  // float temperature = 5;
  if (this->temperature() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->temperature(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CategoricalDistributionLayerParams)
}

size_t CategoricalDistributionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CategoricalDistributionLayerParams)
  size_t total_size = 0;

  // int64 seed = 1;
  if (this->seed() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->seed());
  }

  // int64 numSamples = 2;
  if (this->numsamples() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->numsamples());
  }

  // bool isLogits = 3;
  if (this->islogits() != 0) {
    total_size += 1 + 1;
  }

  // float eps = 4;
  if (this->eps() != 0) {
    total_size += 1 + 4;
  }

  // float temperature = 5;
  if (this->temperature() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CategoricalDistributionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CategoricalDistributionLayerParams*>(&from));
}

void CategoricalDistributionLayerParams::MergeFrom(const CategoricalDistributionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CategoricalDistributionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.seed() != 0) {
    set_seed(from.seed());
  }
  if (from.numsamples() != 0) {
    set_numsamples(from.numsamples());
  }
  if (from.islogits() != 0) {
    set_islogits(from.islogits());
  }
  if (from.eps() != 0) {
    set_eps(from.eps());
  }
  if (from.temperature() != 0) {
    set_temperature(from.temperature());
  }
}

void CategoricalDistributionLayerParams::CopyFrom(const CategoricalDistributionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CategoricalDistributionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CategoricalDistributionLayerParams::IsInitialized() const {
  return true;
}

void CategoricalDistributionLayerParams::Swap(CategoricalDistributionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CategoricalDistributionLayerParams::InternalSwap(CategoricalDistributionLayerParams* other) {
  std::swap(seed_, other->seed_);
  std::swap(numsamples_, other->numsamples_);
  std::swap(islogits_, other->islogits_);
  std::swap(eps_, other->eps_);
  std::swap(temperature_, other->temperature_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CategoricalDistributionLayerParams::GetTypeName() const {
  return "CoreML.Specification.CategoricalDistributionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CategoricalDistributionLayerParams

// int64 seed = 1;
void CategoricalDistributionLayerParams::clear_seed() {
  seed_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 CategoricalDistributionLayerParams::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.seed)
  return seed_;
}
void CategoricalDistributionLayerParams::set_seed(::google::protobuf::int64 value) {
  
  seed_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.seed)
}

// int64 numSamples = 2;
void CategoricalDistributionLayerParams::clear_numsamples() {
  numsamples_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 CategoricalDistributionLayerParams::numsamples() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.numSamples)
  return numsamples_;
}
void CategoricalDistributionLayerParams::set_numsamples(::google::protobuf::int64 value) {
  
  numsamples_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.numSamples)
}

// bool isLogits = 3;
void CategoricalDistributionLayerParams::clear_islogits() {
  islogits_ = false;
}
bool CategoricalDistributionLayerParams::islogits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.isLogits)
  return islogits_;
}
void CategoricalDistributionLayerParams::set_islogits(bool value) {
  
  islogits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.isLogits)
}

// float eps = 4;
void CategoricalDistributionLayerParams::clear_eps() {
  eps_ = 0;
}
float CategoricalDistributionLayerParams::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.eps)
  return eps_;
}
void CategoricalDistributionLayerParams::set_eps(float value) {
  
  eps_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.eps)
}

// float temperature = 5;
void CategoricalDistributionLayerParams::clear_temperature() {
  temperature_ = 0;
}
float CategoricalDistributionLayerParams::temperature() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalDistributionLayerParams.temperature)
  return temperature_;
}
void CategoricalDistributionLayerParams::set_temperature(float value) {
  
  temperature_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalDistributionLayerParams.temperature)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceL1LayerParams::kAxesFieldNumber;
const int ReduceL1LayerParams::kKeepDimsFieldNumber;
const int ReduceL1LayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceL1LayerParams::ReduceL1LayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceL1LayerParams)
}
ReduceL1LayerParams::ReduceL1LayerParams(const ReduceL1LayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceL1LayerParams)
}

void ReduceL1LayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceL1LayerParams::~ReduceL1LayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceL1LayerParams)
  SharedDtor();
}

void ReduceL1LayerParams::SharedDtor() {
}

void ReduceL1LayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceL1LayerParams& ReduceL1LayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceL1LayerParams* ReduceL1LayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceL1LayerParams* n = new ReduceL1LayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceL1LayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceL1LayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceL1LayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceL1LayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceL1LayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceL1LayerParams)
  return false;
#undef DO_
}

void ReduceL1LayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceL1LayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceL1LayerParams)
}

size_t ReduceL1LayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceL1LayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceL1LayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceL1LayerParams*>(&from));
}

void ReduceL1LayerParams::MergeFrom(const ReduceL1LayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceL1LayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceL1LayerParams::CopyFrom(const ReduceL1LayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceL1LayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceL1LayerParams::IsInitialized() const {
  return true;
}

void ReduceL1LayerParams::Swap(ReduceL1LayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceL1LayerParams::InternalSwap(ReduceL1LayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceL1LayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceL1LayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceL1LayerParams

// repeated int64 axes = 1;
int ReduceL1LayerParams::axes_size() const {
  return axes_.size();
}
void ReduceL1LayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceL1LayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.axes)
  return axes_.Get(index);
}
void ReduceL1LayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.axes)
}
void ReduceL1LayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceL1LayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceL1LayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceL1LayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceL1LayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceL1LayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceL1LayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceL1LayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.keepDims)
  return keepdims_;
}
void ReduceL1LayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceL1LayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceL1LayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL1LayerParams.reduceAll)
  return reduceall_;
}
void ReduceL1LayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL1LayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceL2LayerParams::kAxesFieldNumber;
const int ReduceL2LayerParams::kKeepDimsFieldNumber;
const int ReduceL2LayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceL2LayerParams::ReduceL2LayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceL2LayerParams)
}
ReduceL2LayerParams::ReduceL2LayerParams(const ReduceL2LayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceL2LayerParams)
}

void ReduceL2LayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceL2LayerParams::~ReduceL2LayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceL2LayerParams)
  SharedDtor();
}

void ReduceL2LayerParams::SharedDtor() {
}

void ReduceL2LayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceL2LayerParams& ReduceL2LayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceL2LayerParams* ReduceL2LayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceL2LayerParams* n = new ReduceL2LayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceL2LayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceL2LayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceL2LayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceL2LayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceL2LayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceL2LayerParams)
  return false;
#undef DO_
}

void ReduceL2LayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceL2LayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceL2LayerParams)
}

size_t ReduceL2LayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceL2LayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceL2LayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceL2LayerParams*>(&from));
}

void ReduceL2LayerParams::MergeFrom(const ReduceL2LayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceL2LayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceL2LayerParams::CopyFrom(const ReduceL2LayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceL2LayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceL2LayerParams::IsInitialized() const {
  return true;
}

void ReduceL2LayerParams::Swap(ReduceL2LayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceL2LayerParams::InternalSwap(ReduceL2LayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceL2LayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceL2LayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceL2LayerParams

// repeated int64 axes = 1;
int ReduceL2LayerParams::axes_size() const {
  return axes_.size();
}
void ReduceL2LayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceL2LayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.axes)
  return axes_.Get(index);
}
void ReduceL2LayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.axes)
}
void ReduceL2LayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceL2LayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceL2LayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceL2LayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceL2LayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceL2LayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceL2LayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceL2LayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.keepDims)
  return keepdims_;
}
void ReduceL2LayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceL2LayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceL2LayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceL2LayerParams.reduceAll)
  return reduceall_;
}
void ReduceL2LayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceL2LayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceMaxLayerParams::kAxesFieldNumber;
const int ReduceMaxLayerParams::kKeepDimsFieldNumber;
const int ReduceMaxLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceMaxLayerParams::ReduceMaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceMaxLayerParams)
}
ReduceMaxLayerParams::ReduceMaxLayerParams(const ReduceMaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceMaxLayerParams)
}

void ReduceMaxLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceMaxLayerParams::~ReduceMaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceMaxLayerParams)
  SharedDtor();
}

void ReduceMaxLayerParams::SharedDtor() {
}

void ReduceMaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceMaxLayerParams& ReduceMaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceMaxLayerParams* ReduceMaxLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceMaxLayerParams* n = new ReduceMaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceMaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceMaxLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceMaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceMaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceMaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceMaxLayerParams)
  return false;
#undef DO_
}

void ReduceMaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceMaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceMaxLayerParams)
}

size_t ReduceMaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceMaxLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceMaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceMaxLayerParams*>(&from));
}

void ReduceMaxLayerParams::MergeFrom(const ReduceMaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceMaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceMaxLayerParams::CopyFrom(const ReduceMaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceMaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceMaxLayerParams::IsInitialized() const {
  return true;
}

void ReduceMaxLayerParams::Swap(ReduceMaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceMaxLayerParams::InternalSwap(ReduceMaxLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceMaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceMaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceMaxLayerParams

// repeated int64 axes = 1;
int ReduceMaxLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceMaxLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceMaxLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.axes)
  return axes_.Get(index);
}
void ReduceMaxLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.axes)
}
void ReduceMaxLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMaxLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceMaxLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMaxLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceMaxLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMaxLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceMaxLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceMaxLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.keepDims)
  return keepdims_;
}
void ReduceMaxLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceMaxLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceMaxLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMaxLayerParams.reduceAll)
  return reduceall_;
}
void ReduceMaxLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMaxLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceMinLayerParams::kAxesFieldNumber;
const int ReduceMinLayerParams::kKeepDimsFieldNumber;
const int ReduceMinLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceMinLayerParams::ReduceMinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceMinLayerParams)
}
ReduceMinLayerParams::ReduceMinLayerParams(const ReduceMinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceMinLayerParams)
}

void ReduceMinLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceMinLayerParams::~ReduceMinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceMinLayerParams)
  SharedDtor();
}

void ReduceMinLayerParams::SharedDtor() {
}

void ReduceMinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceMinLayerParams& ReduceMinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceMinLayerParams* ReduceMinLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceMinLayerParams* n = new ReduceMinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceMinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceMinLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceMinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceMinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceMinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceMinLayerParams)
  return false;
#undef DO_
}

void ReduceMinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceMinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceMinLayerParams)
}

size_t ReduceMinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceMinLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceMinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceMinLayerParams*>(&from));
}

void ReduceMinLayerParams::MergeFrom(const ReduceMinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceMinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceMinLayerParams::CopyFrom(const ReduceMinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceMinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceMinLayerParams::IsInitialized() const {
  return true;
}

void ReduceMinLayerParams::Swap(ReduceMinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceMinLayerParams::InternalSwap(ReduceMinLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceMinLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceMinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceMinLayerParams

// repeated int64 axes = 1;
int ReduceMinLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceMinLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceMinLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.axes)
  return axes_.Get(index);
}
void ReduceMinLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.axes)
}
void ReduceMinLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMinLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceMinLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMinLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceMinLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMinLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceMinLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceMinLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.keepDims)
  return keepdims_;
}
void ReduceMinLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceMinLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceMinLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMinLayerParams.reduceAll)
  return reduceall_;
}
void ReduceMinLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMinLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceSumLayerParams::kAxesFieldNumber;
const int ReduceSumLayerParams::kKeepDimsFieldNumber;
const int ReduceSumLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceSumLayerParams::ReduceSumLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceSumLayerParams)
}
ReduceSumLayerParams::ReduceSumLayerParams(const ReduceSumLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceSumLayerParams)
}

void ReduceSumLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceSumLayerParams::~ReduceSumLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceSumLayerParams)
  SharedDtor();
}

void ReduceSumLayerParams::SharedDtor() {
}

void ReduceSumLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceSumLayerParams& ReduceSumLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceSumLayerParams* ReduceSumLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceSumLayerParams* n = new ReduceSumLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceSumLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceSumLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceSumLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceSumLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceSumLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceSumLayerParams)
  return false;
#undef DO_
}

void ReduceSumLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceSumLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceSumLayerParams)
}

size_t ReduceSumLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceSumLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceSumLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceSumLayerParams*>(&from));
}

void ReduceSumLayerParams::MergeFrom(const ReduceSumLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceSumLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceSumLayerParams::CopyFrom(const ReduceSumLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceSumLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceSumLayerParams::IsInitialized() const {
  return true;
}

void ReduceSumLayerParams::Swap(ReduceSumLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceSumLayerParams::InternalSwap(ReduceSumLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceSumLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceSumLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceSumLayerParams

// repeated int64 axes = 1;
int ReduceSumLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceSumLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceSumLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.axes)
  return axes_.Get(index);
}
void ReduceSumLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.axes)
}
void ReduceSumLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceSumLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceSumLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceSumLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceSumLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceSumLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceSumLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceSumLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.keepDims)
  return keepdims_;
}
void ReduceSumLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceSumLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceSumLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumLayerParams.reduceAll)
  return reduceall_;
}
void ReduceSumLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceProdLayerParams::kAxesFieldNumber;
const int ReduceProdLayerParams::kKeepDimsFieldNumber;
const int ReduceProdLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceProdLayerParams::ReduceProdLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceProdLayerParams)
}
ReduceProdLayerParams::ReduceProdLayerParams(const ReduceProdLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceProdLayerParams)
}

void ReduceProdLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceProdLayerParams::~ReduceProdLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceProdLayerParams)
  SharedDtor();
}

void ReduceProdLayerParams::SharedDtor() {
}

void ReduceProdLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceProdLayerParams& ReduceProdLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceProdLayerParams* ReduceProdLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceProdLayerParams* n = new ReduceProdLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceProdLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceProdLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceProdLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceProdLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceProdLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceProdLayerParams)
  return false;
#undef DO_
}

void ReduceProdLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceProdLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceProdLayerParams)
}

size_t ReduceProdLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceProdLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceProdLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceProdLayerParams*>(&from));
}

void ReduceProdLayerParams::MergeFrom(const ReduceProdLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceProdLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceProdLayerParams::CopyFrom(const ReduceProdLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceProdLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceProdLayerParams::IsInitialized() const {
  return true;
}

void ReduceProdLayerParams::Swap(ReduceProdLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceProdLayerParams::InternalSwap(ReduceProdLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceProdLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceProdLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceProdLayerParams

// repeated int64 axes = 1;
int ReduceProdLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceProdLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceProdLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.axes)
  return axes_.Get(index);
}
void ReduceProdLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.axes)
}
void ReduceProdLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceProdLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceProdLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceProdLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceProdLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceProdLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceProdLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceProdLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.keepDims)
  return keepdims_;
}
void ReduceProdLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceProdLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceProdLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceProdLayerParams.reduceAll)
  return reduceall_;
}
void ReduceProdLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceProdLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceMeanLayerParams::kAxesFieldNumber;
const int ReduceMeanLayerParams::kKeepDimsFieldNumber;
const int ReduceMeanLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceMeanLayerParams::ReduceMeanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceMeanLayerParams)
}
ReduceMeanLayerParams::ReduceMeanLayerParams(const ReduceMeanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceMeanLayerParams)
}

void ReduceMeanLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceMeanLayerParams::~ReduceMeanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceMeanLayerParams)
  SharedDtor();
}

void ReduceMeanLayerParams::SharedDtor() {
}

void ReduceMeanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceMeanLayerParams& ReduceMeanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceMeanLayerParams* ReduceMeanLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceMeanLayerParams* n = new ReduceMeanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceMeanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceMeanLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceMeanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceMeanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceMeanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceMeanLayerParams)
  return false;
#undef DO_
}

void ReduceMeanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceMeanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceMeanLayerParams)
}

size_t ReduceMeanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceMeanLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceMeanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceMeanLayerParams*>(&from));
}

void ReduceMeanLayerParams::MergeFrom(const ReduceMeanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceMeanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceMeanLayerParams::CopyFrom(const ReduceMeanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceMeanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceMeanLayerParams::IsInitialized() const {
  return true;
}

void ReduceMeanLayerParams::Swap(ReduceMeanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceMeanLayerParams::InternalSwap(ReduceMeanLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceMeanLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceMeanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceMeanLayerParams

// repeated int64 axes = 1;
int ReduceMeanLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceMeanLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceMeanLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.axes)
  return axes_.Get(index);
}
void ReduceMeanLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.axes)
}
void ReduceMeanLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceMeanLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceMeanLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceMeanLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceMeanLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceMeanLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceMeanLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceMeanLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.keepDims)
  return keepdims_;
}
void ReduceMeanLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceMeanLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceMeanLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceMeanLayerParams.reduceAll)
  return reduceall_;
}
void ReduceMeanLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceMeanLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceLogSumLayerParams::kAxesFieldNumber;
const int ReduceLogSumLayerParams::kKeepDimsFieldNumber;
const int ReduceLogSumLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceLogSumLayerParams::ReduceLogSumLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceLogSumLayerParams)
}
ReduceLogSumLayerParams::ReduceLogSumLayerParams(const ReduceLogSumLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceLogSumLayerParams)
}

void ReduceLogSumLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceLogSumLayerParams::~ReduceLogSumLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceLogSumLayerParams)
  SharedDtor();
}

void ReduceLogSumLayerParams::SharedDtor() {
}

void ReduceLogSumLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceLogSumLayerParams& ReduceLogSumLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceLogSumLayerParams* ReduceLogSumLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceLogSumLayerParams* n = new ReduceLogSumLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceLogSumLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceLogSumLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceLogSumLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceLogSumLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceLogSumLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceLogSumLayerParams)
  return false;
#undef DO_
}

void ReduceLogSumLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceLogSumLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceLogSumLayerParams)
}

size_t ReduceLogSumLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceLogSumLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceLogSumLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceLogSumLayerParams*>(&from));
}

void ReduceLogSumLayerParams::MergeFrom(const ReduceLogSumLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceLogSumLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceLogSumLayerParams::CopyFrom(const ReduceLogSumLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceLogSumLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceLogSumLayerParams::IsInitialized() const {
  return true;
}

void ReduceLogSumLayerParams::Swap(ReduceLogSumLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceLogSumLayerParams::InternalSwap(ReduceLogSumLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceLogSumLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceLogSumLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceLogSumLayerParams

// repeated int64 axes = 1;
int ReduceLogSumLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceLogSumLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceLogSumLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return axes_.Get(index);
}
void ReduceLogSumLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.axes)
}
void ReduceLogSumLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceLogSumLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceLogSumLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceLogSumLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceLogSumLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceLogSumLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceLogSumLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.keepDims)
  return keepdims_;
}
void ReduceLogSumLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceLogSumLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceLogSumLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumLayerParams.reduceAll)
  return reduceall_;
}
void ReduceLogSumLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceSumSquareLayerParams::kAxesFieldNumber;
const int ReduceSumSquareLayerParams::kKeepDimsFieldNumber;
const int ReduceSumSquareLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceSumSquareLayerParams::ReduceSumSquareLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceSumSquareLayerParams)
}
ReduceSumSquareLayerParams::ReduceSumSquareLayerParams(const ReduceSumSquareLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceSumSquareLayerParams)
}

void ReduceSumSquareLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceSumSquareLayerParams::~ReduceSumSquareLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceSumSquareLayerParams)
  SharedDtor();
}

void ReduceSumSquareLayerParams::SharedDtor() {
}

void ReduceSumSquareLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceSumSquareLayerParams& ReduceSumSquareLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceSumSquareLayerParams* ReduceSumSquareLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceSumSquareLayerParams* n = new ReduceSumSquareLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceSumSquareLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceSumSquareLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceSumSquareLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceSumSquareLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceSumSquareLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceSumSquareLayerParams)
  return false;
#undef DO_
}

void ReduceSumSquareLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceSumSquareLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceSumSquareLayerParams)
}

size_t ReduceSumSquareLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceSumSquareLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceSumSquareLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceSumSquareLayerParams*>(&from));
}

void ReduceSumSquareLayerParams::MergeFrom(const ReduceSumSquareLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceSumSquareLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceSumSquareLayerParams::CopyFrom(const ReduceSumSquareLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceSumSquareLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceSumSquareLayerParams::IsInitialized() const {
  return true;
}

void ReduceSumSquareLayerParams::Swap(ReduceSumSquareLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceSumSquareLayerParams::InternalSwap(ReduceSumSquareLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceSumSquareLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceSumSquareLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceSumSquareLayerParams

// repeated int64 axes = 1;
int ReduceSumSquareLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceSumSquareLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceSumSquareLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return axes_.Get(index);
}
void ReduceSumSquareLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.axes)
}
void ReduceSumSquareLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceSumSquareLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceSumSquareLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceSumSquareLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceSumSquareLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceSumSquareLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceSumSquareLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.keepDims)
  return keepdims_;
}
void ReduceSumSquareLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceSumSquareLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceSumSquareLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceSumSquareLayerParams.reduceAll)
  return reduceall_;
}
void ReduceSumSquareLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceSumSquareLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceLogSumExpLayerParams::kAxesFieldNumber;
const int ReduceLogSumExpLayerParams::kKeepDimsFieldNumber;
const int ReduceLogSumExpLayerParams::kReduceAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceLogSumExpLayerParams::ReduceLogSumExpLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceLogSumExpLayerParams)
}
ReduceLogSumExpLayerParams::ReduceLogSumExpLayerParams(const ReduceLogSumExpLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&keepdims_, &from.keepdims_,
    reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceLogSumExpLayerParams)
}

void ReduceLogSumExpLayerParams::SharedCtor() {
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
  _cached_size_ = 0;
}

ReduceLogSumExpLayerParams::~ReduceLogSumExpLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceLogSumExpLayerParams)
  SharedDtor();
}

void ReduceLogSumExpLayerParams::SharedDtor() {
}

void ReduceLogSumExpLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceLogSumExpLayerParams& ReduceLogSumExpLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceLogSumExpLayerParams* ReduceLogSumExpLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceLogSumExpLayerParams* n = new ReduceLogSumExpLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceLogSumExpLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceLogSumExpLayerParams)
  axes_.Clear();
  ::memset(&keepdims_, 0, reinterpret_cast<char*>(&reduceall_) -
    reinterpret_cast<char*>(&keepdims_) + sizeof(reduceall_));
}

bool ReduceLogSumExpLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceLogSumExpLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool keepDims = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &keepdims_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reduceAll = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reduceall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceLogSumExpLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceLogSumExpLayerParams)
  return false;
#undef DO_
}

void ReduceLogSumExpLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceLogSumExpLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->keepdims(), output);
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->reduceall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceLogSumExpLayerParams)
}

size_t ReduceLogSumExpLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceLogSumExpLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool keepDims = 2;
  if (this->keepdims() != 0) {
    total_size += 1 + 1;
  }

  // bool reduceAll = 3;
  if (this->reduceall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceLogSumExpLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceLogSumExpLayerParams*>(&from));
}

void ReduceLogSumExpLayerParams::MergeFrom(const ReduceLogSumExpLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceLogSumExpLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.keepdims() != 0) {
    set_keepdims(from.keepdims());
  }
  if (from.reduceall() != 0) {
    set_reduceall(from.reduceall());
  }
}

void ReduceLogSumExpLayerParams::CopyFrom(const ReduceLogSumExpLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceLogSumExpLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceLogSumExpLayerParams::IsInitialized() const {
  return true;
}

void ReduceLogSumExpLayerParams::Swap(ReduceLogSumExpLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceLogSumExpLayerParams::InternalSwap(ReduceLogSumExpLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(keepdims_, other->keepdims_);
  std::swap(reduceall_, other->reduceall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceLogSumExpLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceLogSumExpLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceLogSumExpLayerParams

// repeated int64 axes = 1;
int ReduceLogSumExpLayerParams::axes_size() const {
  return axes_.size();
}
void ReduceLogSumExpLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ReduceLogSumExpLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return axes_.Get(index);
}
void ReduceLogSumExpLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
}
void ReduceLogSumExpLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReduceLogSumExpLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReduceLogSumExpLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReduceLogSumExpLayerParams.axes)
  return &axes_;
}

// bool keepDims = 2;
void ReduceLogSumExpLayerParams::clear_keepdims() {
  keepdims_ = false;
}
bool ReduceLogSumExpLayerParams::keepdims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.keepDims)
  return keepdims_;
}
void ReduceLogSumExpLayerParams::set_keepdims(bool value) {
  
  keepdims_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.keepDims)
}

// bool reduceAll = 3;
void ReduceLogSumExpLayerParams::clear_reduceall() {
  reduceall_ = false;
}
bool ReduceLogSumExpLayerParams::reduceall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLogSumExpLayerParams.reduceAll)
  return reduceall_;
}
void ReduceLogSumExpLayerParams::set_reduceall(bool value) {
  
  reduceall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLogSumExpLayerParams.reduceAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ExpandDimsLayerParams::kAxesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ExpandDimsLayerParams::ExpandDimsLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ExpandDimsLayerParams)
}
ExpandDimsLayerParams::ExpandDimsLayerParams(const ExpandDimsLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ExpandDimsLayerParams)
}

void ExpandDimsLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ExpandDimsLayerParams::~ExpandDimsLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ExpandDimsLayerParams)
  SharedDtor();
}

void ExpandDimsLayerParams::SharedDtor() {
}

void ExpandDimsLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ExpandDimsLayerParams& ExpandDimsLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ExpandDimsLayerParams* ExpandDimsLayerParams::New(::google::protobuf::Arena* arena) const {
  ExpandDimsLayerParams* n = new ExpandDimsLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ExpandDimsLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ExpandDimsLayerParams)
  axes_.Clear();
}

bool ExpandDimsLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ExpandDimsLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ExpandDimsLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ExpandDimsLayerParams)
  return false;
#undef DO_
}

void ExpandDimsLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ExpandDimsLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ExpandDimsLayerParams)
}

size_t ExpandDimsLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ExpandDimsLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ExpandDimsLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ExpandDimsLayerParams*>(&from));
}

void ExpandDimsLayerParams::MergeFrom(const ExpandDimsLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ExpandDimsLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
}

void ExpandDimsLayerParams::CopyFrom(const ExpandDimsLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ExpandDimsLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ExpandDimsLayerParams::IsInitialized() const {
  return true;
}

void ExpandDimsLayerParams::Swap(ExpandDimsLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ExpandDimsLayerParams::InternalSwap(ExpandDimsLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ExpandDimsLayerParams::GetTypeName() const {
  return "CoreML.Specification.ExpandDimsLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ExpandDimsLayerParams

// repeated int64 axes = 1;
int ExpandDimsLayerParams::axes_size() const {
  return axes_.size();
}
void ExpandDimsLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ExpandDimsLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ExpandDimsLayerParams.axes)
  return axes_.Get(index);
}
void ExpandDimsLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ExpandDimsLayerParams.axes)
}
void ExpandDimsLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ExpandDimsLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ExpandDimsLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ExpandDimsLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return &axes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FlattenTo2DLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FlattenTo2DLayerParams::FlattenTo2DLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FlattenTo2DLayerParams)
}
FlattenTo2DLayerParams::FlattenTo2DLayerParams(const FlattenTo2DLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FlattenTo2DLayerParams)
}

void FlattenTo2DLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

FlattenTo2DLayerParams::~FlattenTo2DLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FlattenTo2DLayerParams)
  SharedDtor();
}

void FlattenTo2DLayerParams::SharedDtor() {
}

void FlattenTo2DLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FlattenTo2DLayerParams& FlattenTo2DLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FlattenTo2DLayerParams* FlattenTo2DLayerParams::New(::google::protobuf::Arena* arena) const {
  FlattenTo2DLayerParams* n = new FlattenTo2DLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FlattenTo2DLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FlattenTo2DLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool FlattenTo2DLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FlattenTo2DLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FlattenTo2DLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FlattenTo2DLayerParams)
  return false;
#undef DO_
}

void FlattenTo2DLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FlattenTo2DLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FlattenTo2DLayerParams)
}

size_t FlattenTo2DLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FlattenTo2DLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FlattenTo2DLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FlattenTo2DLayerParams*>(&from));
}

void FlattenTo2DLayerParams::MergeFrom(const FlattenTo2DLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FlattenTo2DLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void FlattenTo2DLayerParams::CopyFrom(const FlattenTo2DLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FlattenTo2DLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FlattenTo2DLayerParams::IsInitialized() const {
  return true;
}

void FlattenTo2DLayerParams::Swap(FlattenTo2DLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FlattenTo2DLayerParams::InternalSwap(FlattenTo2DLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FlattenTo2DLayerParams::GetTypeName() const {
  return "CoreML.Specification.FlattenTo2DLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FlattenTo2DLayerParams

// int64 axis = 1;
void FlattenTo2DLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 FlattenTo2DLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenTo2DLayerParams.axis)
  return axis_;
}
void FlattenTo2DLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenTo2DLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReshapeStaticLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeStaticLayerParams::ReshapeStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeStaticLayerParams)
}
ReshapeStaticLayerParams::ReshapeStaticLayerParams(const ReshapeStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeStaticLayerParams)
}

void ReshapeStaticLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ReshapeStaticLayerParams::~ReshapeStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeStaticLayerParams)
  SharedDtor();
}

void ReshapeStaticLayerParams::SharedDtor() {
}

void ReshapeStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeStaticLayerParams& ReshapeStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReshapeStaticLayerParams* ReshapeStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeStaticLayerParams* n = new ReshapeStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeStaticLayerParams)
  targetshape_.Clear();
}

bool ReshapeStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeStaticLayerParams)
  return false;
#undef DO_
}

void ReshapeStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeStaticLayerParams)
}

size_t ReshapeStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeStaticLayerParams)
  size_t total_size = 0;

  // repeated int64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeStaticLayerParams*>(&from));
}

void ReshapeStaticLayerParams::MergeFrom(const ReshapeStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
}

void ReshapeStaticLayerParams::CopyFrom(const ReshapeStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReshapeStaticLayerParams::IsInitialized() const {
  return true;
}

void ReshapeStaticLayerParams::Swap(ReshapeStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeStaticLayerParams::InternalSwap(ReshapeStaticLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeStaticLayerParams

// repeated int64 targetShape = 1;
int ReshapeStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void ReshapeStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 ReshapeStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
void ReshapeStaticLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
}
void ReshapeStaticLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeStaticLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeLikeLayerParams::ReshapeLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeLikeLayerParams)
}
ReshapeLikeLayerParams::ReshapeLikeLayerParams(const ReshapeLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeLikeLayerParams)
}

void ReshapeLikeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ReshapeLikeLayerParams::~ReshapeLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeLikeLayerParams)
  SharedDtor();
}

void ReshapeLikeLayerParams::SharedDtor() {
}

void ReshapeLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeLikeLayerParams& ReshapeLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReshapeLikeLayerParams* ReshapeLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeLikeLayerParams* n = new ReshapeLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeLikeLayerParams)
}

bool ReshapeLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeLikeLayerParams)
  return false;
#undef DO_
}

void ReshapeLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeLikeLayerParams)
}

size_t ReshapeLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeLikeLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeLikeLayerParams*>(&from));
}

void ReshapeLikeLayerParams::MergeFrom(const ReshapeLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ReshapeLikeLayerParams::CopyFrom(const ReshapeLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReshapeLikeLayerParams::IsInitialized() const {
  return true;
}

void ReshapeLikeLayerParams::Swap(ReshapeLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeLikeLayerParams::InternalSwap(ReshapeLikeLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeLikeLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeDynamicLayerParams::ReshapeDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeDynamicLayerParams)
}
ReshapeDynamicLayerParams::ReshapeDynamicLayerParams(const ReshapeDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeDynamicLayerParams)
}

void ReshapeDynamicLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ReshapeDynamicLayerParams::~ReshapeDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeDynamicLayerParams)
  SharedDtor();
}

void ReshapeDynamicLayerParams::SharedDtor() {
}

void ReshapeDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeDynamicLayerParams& ReshapeDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReshapeDynamicLayerParams* ReshapeDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeDynamicLayerParams* n = new ReshapeDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeDynamicLayerParams)
}

bool ReshapeDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeDynamicLayerParams)
  return false;
#undef DO_
}

void ReshapeDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeDynamicLayerParams)
}

size_t ReshapeDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeDynamicLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeDynamicLayerParams*>(&from));
}

void ReshapeDynamicLayerParams::MergeFrom(const ReshapeDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ReshapeDynamicLayerParams::CopyFrom(const ReshapeDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReshapeDynamicLayerParams::IsInitialized() const {
  return true;
}

void ReshapeDynamicLayerParams::Swap(ReshapeDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeDynamicLayerParams::InternalSwap(ReshapeDynamicLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeDynamicLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SqueezeLayerParams::kAxesFieldNumber;
const int SqueezeLayerParams::kSqueezeAllFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SqueezeLayerParams::SqueezeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SqueezeLayerParams)
}
SqueezeLayerParams::SqueezeLayerParams(const SqueezeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  squeezeall_ = from.squeezeall_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SqueezeLayerParams)
}

void SqueezeLayerParams::SharedCtor() {
  squeezeall_ = false;
  _cached_size_ = 0;
}

SqueezeLayerParams::~SqueezeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SqueezeLayerParams)
  SharedDtor();
}

void SqueezeLayerParams::SharedDtor() {
}

void SqueezeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SqueezeLayerParams& SqueezeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SqueezeLayerParams* SqueezeLayerParams::New(::google::protobuf::Arena* arena) const {
  SqueezeLayerParams* n = new SqueezeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SqueezeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SqueezeLayerParams)
  axes_.Clear();
  squeezeall_ = false;
}

bool SqueezeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SqueezeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool squeezeAll = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &squeezeall_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SqueezeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SqueezeLayerParams)
  return false;
#undef DO_
}

void SqueezeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SqueezeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // bool squeezeAll = 2;
  if (this->squeezeall() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->squeezeall(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SqueezeLayerParams)
}

size_t SqueezeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SqueezeLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bool squeezeAll = 2;
  if (this->squeezeall() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SqueezeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SqueezeLayerParams*>(&from));
}

void SqueezeLayerParams::MergeFrom(const SqueezeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SqueezeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
  if (from.squeezeall() != 0) {
    set_squeezeall(from.squeezeall());
  }
}

void SqueezeLayerParams::CopyFrom(const SqueezeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SqueezeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SqueezeLayerParams::IsInitialized() const {
  return true;
}

void SqueezeLayerParams::Swap(SqueezeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SqueezeLayerParams::InternalSwap(SqueezeLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(squeezeall_, other->squeezeall_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SqueezeLayerParams::GetTypeName() const {
  return "CoreML.Specification.SqueezeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SqueezeLayerParams

// repeated int64 axes = 1;
int SqueezeLayerParams::axes_size() const {
  return axes_.size();
}
void SqueezeLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 SqueezeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.axes)
  return axes_.Get(index);
}
void SqueezeLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.axes)
}
void SqueezeLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SqueezeLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SqueezeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SqueezeLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SqueezeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SqueezeLayerParams.axes)
  return &axes_;
}

// bool squeezeAll = 2;
void SqueezeLayerParams::clear_squeezeall() {
  squeezeall_ = false;
}
bool SqueezeLayerParams::squeezeall() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.squeezeAll)
  return squeezeall_;
}
void SqueezeLayerParams::set_squeezeall(bool value) {
  
  squeezeall_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.squeezeAll)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int TopKLayerParams::kAxisFieldNumber;
const int TopKLayerParams::kKFieldNumber;
const int TopKLayerParams::kUseBottomKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TopKLayerParams::TopKLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TopKLayerParams)
}
TopKLayerParams::TopKLayerParams(const TopKLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&usebottomk_) -
    reinterpret_cast<char*>(&axis_) + sizeof(usebottomk_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TopKLayerParams)
}

void TopKLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&usebottomk_) -
    reinterpret_cast<char*>(&axis_) + sizeof(usebottomk_));
  _cached_size_ = 0;
}

TopKLayerParams::~TopKLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TopKLayerParams)
  SharedDtor();
}

void TopKLayerParams::SharedDtor() {
}

void TopKLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TopKLayerParams& TopKLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TopKLayerParams* TopKLayerParams::New(::google::protobuf::Arena* arena) const {
  TopKLayerParams* n = new TopKLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TopKLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TopKLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&usebottomk_) -
    reinterpret_cast<char*>(&axis_) + sizeof(usebottomk_));
}

bool TopKLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TopKLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 K = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool useBottomK = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &usebottomk_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TopKLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TopKLayerParams)
  return false;
#undef DO_
}

void TopKLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TopKLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // uint64 K = 2;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->k(), output);
  }

  // bool useBottomK = 3;
  if (this->usebottomk() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->usebottomk(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TopKLayerParams)
}

size_t TopKLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TopKLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // uint64 K = 2;
  if (this->k() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->k());
  }

  // bool useBottomK = 3;
  if (this->usebottomk() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TopKLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TopKLayerParams*>(&from));
}

void TopKLayerParams::MergeFrom(const TopKLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TopKLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.k() != 0) {
    set_k(from.k());
  }
  if (from.usebottomk() != 0) {
    set_usebottomk(from.usebottomk());
  }
}

void TopKLayerParams::CopyFrom(const TopKLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TopKLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TopKLayerParams::IsInitialized() const {
  return true;
}

void TopKLayerParams::Swap(TopKLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TopKLayerParams::InternalSwap(TopKLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(k_, other->k_);
  std::swap(usebottomk_, other->usebottomk_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TopKLayerParams::GetTypeName() const {
  return "CoreML.Specification.TopKLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TopKLayerParams

// int64 axis = 1;
void TopKLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 TopKLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.axis)
  return axis_;
}
void TopKLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.axis)
}

// uint64 K = 2;
void TopKLayerParams::clear_k() {
  k_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 TopKLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.K)
  return k_;
}
void TopKLayerParams::set_k(::google::protobuf::uint64 value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.K)
}

// bool useBottomK = 3;
void TopKLayerParams::clear_usebottomk() {
  usebottomk_ = false;
}
bool TopKLayerParams::usebottomk() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TopKLayerParams.useBottomK)
  return usebottomk_;
}
void TopKLayerParams::set_usebottomk(bool value) {
  
  usebottomk_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.TopKLayerParams.useBottomK)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ArgMaxLayerParams::kAxisFieldNumber;
const int ArgMaxLayerParams::kRemoveDimFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ArgMaxLayerParams::ArgMaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ArgMaxLayerParams)
}
ArgMaxLayerParams::ArgMaxLayerParams(const ArgMaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&removedim_) -
    reinterpret_cast<char*>(&axis_) + sizeof(removedim_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ArgMaxLayerParams)
}

void ArgMaxLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&removedim_) -
    reinterpret_cast<char*>(&axis_) + sizeof(removedim_));
  _cached_size_ = 0;
}

ArgMaxLayerParams::~ArgMaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ArgMaxLayerParams)
  SharedDtor();
}

void ArgMaxLayerParams::SharedDtor() {
}

void ArgMaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ArgMaxLayerParams& ArgMaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ArgMaxLayerParams* ArgMaxLayerParams::New(::google::protobuf::Arena* arena) const {
  ArgMaxLayerParams* n = new ArgMaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ArgMaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ArgMaxLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&removedim_) -
    reinterpret_cast<char*>(&axis_) + sizeof(removedim_));
}

bool ArgMaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ArgMaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool removeDim = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &removedim_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ArgMaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ArgMaxLayerParams)
  return false;
#undef DO_
}

void ArgMaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ArgMaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // bool removeDim = 2;
  if (this->removedim() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->removedim(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ArgMaxLayerParams)
}

size_t ArgMaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ArgMaxLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // bool removeDim = 2;
  if (this->removedim() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ArgMaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ArgMaxLayerParams*>(&from));
}

void ArgMaxLayerParams::MergeFrom(const ArgMaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ArgMaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.removedim() != 0) {
    set_removedim(from.removedim());
  }
}

void ArgMaxLayerParams::CopyFrom(const ArgMaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ArgMaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ArgMaxLayerParams::IsInitialized() const {
  return true;
}

void ArgMaxLayerParams::Swap(ArgMaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ArgMaxLayerParams::InternalSwap(ArgMaxLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(removedim_, other->removedim_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ArgMaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.ArgMaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ArgMaxLayerParams

// int64 axis = 1;
void ArgMaxLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ArgMaxLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMaxLayerParams.axis)
  return axis_;
}
void ArgMaxLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMaxLayerParams.axis)
}

// bool removeDim = 2;
void ArgMaxLayerParams::clear_removedim() {
  removedim_ = false;
}
bool ArgMaxLayerParams::removedim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMaxLayerParams.removeDim)
  return removedim_;
}
void ArgMaxLayerParams::set_removedim(bool value) {
  
  removedim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMaxLayerParams.removeDim)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ArgMinLayerParams::kAxisFieldNumber;
const int ArgMinLayerParams::kRemoveDimFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ArgMinLayerParams::ArgMinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ArgMinLayerParams)
}
ArgMinLayerParams::ArgMinLayerParams(const ArgMinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&removedim_) -
    reinterpret_cast<char*>(&axis_) + sizeof(removedim_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ArgMinLayerParams)
}

void ArgMinLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&removedim_) -
    reinterpret_cast<char*>(&axis_) + sizeof(removedim_));
  _cached_size_ = 0;
}

ArgMinLayerParams::~ArgMinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ArgMinLayerParams)
  SharedDtor();
}

void ArgMinLayerParams::SharedDtor() {
}

void ArgMinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ArgMinLayerParams& ArgMinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ArgMinLayerParams* ArgMinLayerParams::New(::google::protobuf::Arena* arena) const {
  ArgMinLayerParams* n = new ArgMinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ArgMinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ArgMinLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&removedim_) -
    reinterpret_cast<char*>(&axis_) + sizeof(removedim_));
}

bool ArgMinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ArgMinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool removeDim = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &removedim_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ArgMinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ArgMinLayerParams)
  return false;
#undef DO_
}

void ArgMinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ArgMinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // bool removeDim = 2;
  if (this->removedim() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->removedim(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ArgMinLayerParams)
}

size_t ArgMinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ArgMinLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // bool removeDim = 2;
  if (this->removedim() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ArgMinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ArgMinLayerParams*>(&from));
}

void ArgMinLayerParams::MergeFrom(const ArgMinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ArgMinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.removedim() != 0) {
    set_removedim(from.removedim());
  }
}

void ArgMinLayerParams::CopyFrom(const ArgMinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ArgMinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ArgMinLayerParams::IsInitialized() const {
  return true;
}

void ArgMinLayerParams::Swap(ArgMinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ArgMinLayerParams::InternalSwap(ArgMinLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(removedim_, other->removedim_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ArgMinLayerParams::GetTypeName() const {
  return "CoreML.Specification.ArgMinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ArgMinLayerParams

// int64 axis = 1;
void ArgMinLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ArgMinLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMinLayerParams.axis)
  return axis_;
}
void ArgMinLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMinLayerParams.axis)
}

// bool removeDim = 2;
void ArgMinLayerParams::clear_removedim() {
  removedim_ = false;
}
bool ArgMinLayerParams::removedim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ArgMinLayerParams.removeDim)
  return removedim_;
}
void ArgMinLayerParams::set_removedim(bool value) {
  
  removedim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ArgMinLayerParams.removeDim)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SplitNDLayerParams::kAxisFieldNumber;
const int SplitNDLayerParams::kNumSplitsFieldNumber;
const int SplitNDLayerParams::kSplitSizesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SplitNDLayerParams::SplitNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SplitNDLayerParams)
}
SplitNDLayerParams::SplitNDLayerParams(const SplitNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      splitsizes_(from.splitsizes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&numsplits_) -
    reinterpret_cast<char*>(&axis_) + sizeof(numsplits_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SplitNDLayerParams)
}

void SplitNDLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&numsplits_) -
    reinterpret_cast<char*>(&axis_) + sizeof(numsplits_));
  _cached_size_ = 0;
}

SplitNDLayerParams::~SplitNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SplitNDLayerParams)
  SharedDtor();
}

void SplitNDLayerParams::SharedDtor() {
}

void SplitNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SplitNDLayerParams& SplitNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SplitNDLayerParams* SplitNDLayerParams::New(::google::protobuf::Arena* arena) const {
  SplitNDLayerParams* n = new SplitNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SplitNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SplitNDLayerParams)
  splitsizes_.Clear();
  ::memset(&axis_, 0, reinterpret_cast<char*>(&numsplits_) -
    reinterpret_cast<char*>(&axis_) + sizeof(numsplits_));
}

bool SplitNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SplitNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 numSplits = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &numsplits_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 splitSizes = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_splitsizes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(24u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 26u, input, this->mutable_splitsizes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SplitNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SplitNDLayerParams)
  return false;
#undef DO_
}

void SplitNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SplitNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // uint64 numSplits = 2;
  if (this->numsplits() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->numsplits(), output);
  }

  // repeated uint64 splitSizes = 3;
  if (this->splitsizes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_splitsizes_cached_byte_size_);
  }
  for (int i = 0, n = this->splitsizes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->splitsizes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SplitNDLayerParams)
}

size_t SplitNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SplitNDLayerParams)
  size_t total_size = 0;

  // repeated uint64 splitSizes = 3;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->splitsizes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _splitsizes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // uint64 numSplits = 2;
  if (this->numsplits() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->numsplits());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SplitNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SplitNDLayerParams*>(&from));
}

void SplitNDLayerParams::MergeFrom(const SplitNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SplitNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  splitsizes_.MergeFrom(from.splitsizes_);
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.numsplits() != 0) {
    set_numsplits(from.numsplits());
  }
}

void SplitNDLayerParams::CopyFrom(const SplitNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SplitNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SplitNDLayerParams::IsInitialized() const {
  return true;
}

void SplitNDLayerParams::Swap(SplitNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SplitNDLayerParams::InternalSwap(SplitNDLayerParams* other) {
  splitsizes_.InternalSwap(&other->splitsizes_);
  std::swap(axis_, other->axis_);
  std::swap(numsplits_, other->numsplits_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SplitNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.SplitNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SplitNDLayerParams

// int64 axis = 1;
void SplitNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SplitNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.axis)
  return axis_;
}
void SplitNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.axis)
}

// uint64 numSplits = 2;
void SplitNDLayerParams::clear_numsplits() {
  numsplits_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SplitNDLayerParams::numsplits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.numSplits)
  return numsplits_;
}
void SplitNDLayerParams::set_numsplits(::google::protobuf::uint64 value) {
  
  numsplits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.numSplits)
}

// repeated uint64 splitSizes = 3;
int SplitNDLayerParams::splitsizes_size() const {
  return splitsizes_.size();
}
void SplitNDLayerParams::clear_splitsizes() {
  splitsizes_.Clear();
}
::google::protobuf::uint64 SplitNDLayerParams::splitsizes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return splitsizes_.Get(index);
}
void SplitNDLayerParams::set_splitsizes(int index, ::google::protobuf::uint64 value) {
  splitsizes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
void SplitNDLayerParams::add_splitsizes(::google::protobuf::uint64 value) {
  splitsizes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
SplitNDLayerParams::splitsizes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return splitsizes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
SplitNDLayerParams::mutable_splitsizes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return &splitsizes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CeilLayerParams::CeilLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CeilLayerParams)
}
CeilLayerParams::CeilLayerParams(const CeilLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CeilLayerParams)
}

void CeilLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CeilLayerParams::~CeilLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CeilLayerParams)
  SharedDtor();
}

void CeilLayerParams::SharedDtor() {
}

void CeilLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CeilLayerParams& CeilLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CeilLayerParams* CeilLayerParams::New(::google::protobuf::Arena* arena) const {
  CeilLayerParams* n = new CeilLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CeilLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CeilLayerParams)
}

bool CeilLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CeilLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CeilLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CeilLayerParams)
  return false;
#undef DO_
}

void CeilLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CeilLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CeilLayerParams)
}

size_t CeilLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CeilLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CeilLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CeilLayerParams*>(&from));
}

void CeilLayerParams::MergeFrom(const CeilLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CeilLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CeilLayerParams::CopyFrom(const CeilLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CeilLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CeilLayerParams::IsInitialized() const {
  return true;
}

void CeilLayerParams::Swap(CeilLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CeilLayerParams::InternalSwap(CeilLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CeilLayerParams::GetTypeName() const {
  return "CoreML.Specification.CeilLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CeilLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RoundLayerParams::RoundLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RoundLayerParams)
}
RoundLayerParams::RoundLayerParams(const RoundLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RoundLayerParams)
}

void RoundLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

RoundLayerParams::~RoundLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RoundLayerParams)
  SharedDtor();
}

void RoundLayerParams::SharedDtor() {
}

void RoundLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RoundLayerParams& RoundLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RoundLayerParams* RoundLayerParams::New(::google::protobuf::Arena* arena) const {
  RoundLayerParams* n = new RoundLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RoundLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RoundLayerParams)
}

bool RoundLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RoundLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RoundLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RoundLayerParams)
  return false;
#undef DO_
}

void RoundLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RoundLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RoundLayerParams)
}

size_t RoundLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RoundLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RoundLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RoundLayerParams*>(&from));
}

void RoundLayerParams::MergeFrom(const RoundLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RoundLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void RoundLayerParams::CopyFrom(const RoundLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RoundLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RoundLayerParams::IsInitialized() const {
  return true;
}

void RoundLayerParams::Swap(RoundLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RoundLayerParams::InternalSwap(RoundLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RoundLayerParams::GetTypeName() const {
  return "CoreML.Specification.RoundLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RoundLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FloorLayerParams::FloorLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FloorLayerParams)
}
FloorLayerParams::FloorLayerParams(const FloorLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FloorLayerParams)
}

void FloorLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

FloorLayerParams::~FloorLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FloorLayerParams)
  SharedDtor();
}

void FloorLayerParams::SharedDtor() {
}

void FloorLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FloorLayerParams& FloorLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FloorLayerParams* FloorLayerParams::New(::google::protobuf::Arena* arena) const {
  FloorLayerParams* n = new FloorLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FloorLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FloorLayerParams)
}

bool FloorLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FloorLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FloorLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FloorLayerParams)
  return false;
#undef DO_
}

void FloorLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FloorLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FloorLayerParams)
}

size_t FloorLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FloorLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FloorLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FloorLayerParams*>(&from));
}

void FloorLayerParams::MergeFrom(const FloorLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FloorLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void FloorLayerParams::CopyFrom(const FloorLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FloorLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FloorLayerParams::IsInitialized() const {
  return true;
}

void FloorLayerParams::Swap(FloorLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FloorLayerParams::InternalSwap(FloorLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FloorLayerParams::GetTypeName() const {
  return "CoreML.Specification.FloorLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FloorLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SignLayerParams::SignLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SignLayerParams)
}
SignLayerParams::SignLayerParams(const SignLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SignLayerParams)
}

void SignLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SignLayerParams::~SignLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SignLayerParams)
  SharedDtor();
}

void SignLayerParams::SharedDtor() {
}

void SignLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SignLayerParams& SignLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SignLayerParams* SignLayerParams::New(::google::protobuf::Arena* arena) const {
  SignLayerParams* n = new SignLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SignLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SignLayerParams)
}

bool SignLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SignLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SignLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SignLayerParams)
  return false;
#undef DO_
}

void SignLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SignLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SignLayerParams)
}

size_t SignLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SignLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SignLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SignLayerParams*>(&from));
}

void SignLayerParams::MergeFrom(const SignLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SignLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SignLayerParams::CopyFrom(const SignLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SignLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SignLayerParams::IsInitialized() const {
  return true;
}

void SignLayerParams::Swap(SignLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SignLayerParams::InternalSwap(SignLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SignLayerParams::GetTypeName() const {
  return "CoreML.Specification.SignLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SignLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ClipLayerParams::kMinValFieldNumber;
const int ClipLayerParams::kMaxValFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ClipLayerParams::ClipLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ClipLayerParams)
}
ClipLayerParams::ClipLayerParams(const ClipLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&minval_, &from.minval_,
    reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&minval_) + sizeof(maxval_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ClipLayerParams)
}

void ClipLayerParams::SharedCtor() {
  ::memset(&minval_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&minval_) + sizeof(maxval_));
  _cached_size_ = 0;
}

ClipLayerParams::~ClipLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ClipLayerParams)
  SharedDtor();
}

void ClipLayerParams::SharedDtor() {
}

void ClipLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ClipLayerParams& ClipLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ClipLayerParams* ClipLayerParams::New(::google::protobuf::Arena* arena) const {
  ClipLayerParams* n = new ClipLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ClipLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ClipLayerParams)
  ::memset(&minval_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&minval_) + sizeof(maxval_));
}

bool ClipLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ClipLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float minVal = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &minval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float maxVal = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &maxval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ClipLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ClipLayerParams)
  return false;
#undef DO_
}

void ClipLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ClipLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float minVal = 1;
  if (this->minval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->minval(), output);
  }

  // float maxVal = 2;
  if (this->maxval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->maxval(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ClipLayerParams)
}

size_t ClipLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ClipLayerParams)
  size_t total_size = 0;

  // float minVal = 1;
  if (this->minval() != 0) {
    total_size += 1 + 4;
  }

  // float maxVal = 2;
  if (this->maxval() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ClipLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ClipLayerParams*>(&from));
}

void ClipLayerParams::MergeFrom(const ClipLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ClipLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.minval() != 0) {
    set_minval(from.minval());
  }
  if (from.maxval() != 0) {
    set_maxval(from.maxval());
  }
}

void ClipLayerParams::CopyFrom(const ClipLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ClipLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ClipLayerParams::IsInitialized() const {
  return true;
}

void ClipLayerParams::Swap(ClipLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ClipLayerParams::InternalSwap(ClipLayerParams* other) {
  std::swap(minval_, other->minval_);
  std::swap(maxval_, other->maxval_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ClipLayerParams::GetTypeName() const {
  return "CoreML.Specification.ClipLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ClipLayerParams

// float minVal = 1;
void ClipLayerParams::clear_minval() {
  minval_ = 0;
}
float ClipLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.minVal)
  return minval_;
}
void ClipLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.minVal)
}

// float maxVal = 2;
void ClipLayerParams::clear_maxval() {
  maxval_ = 0;
}
float ClipLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.maxVal)
  return maxval_;
}
void ClipLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.maxVal)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SliceStaticLayerParams::kBeginIdsFieldNumber;
const int SliceStaticLayerParams::kBeginMasksFieldNumber;
const int SliceStaticLayerParams::kEndIdsFieldNumber;
const int SliceStaticLayerParams::kEndMasksFieldNumber;
const int SliceStaticLayerParams::kStridesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SliceStaticLayerParams::SliceStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SliceStaticLayerParams)
}
SliceStaticLayerParams::SliceStaticLayerParams(const SliceStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      beginids_(from.beginids_),
      beginmasks_(from.beginmasks_),
      endids_(from.endids_),
      endmasks_(from.endmasks_),
      strides_(from.strides_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SliceStaticLayerParams)
}

void SliceStaticLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SliceStaticLayerParams::~SliceStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SliceStaticLayerParams)
  SharedDtor();
}

void SliceStaticLayerParams::SharedDtor() {
}

void SliceStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SliceStaticLayerParams& SliceStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SliceStaticLayerParams* SliceStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  SliceStaticLayerParams* n = new SliceStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SliceStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SliceStaticLayerParams)
  beginids_.Clear();
  beginmasks_.Clear();
  endids_.Clear();
  endmasks_.Clear();
  strides_.Clear();
}

bool SliceStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SliceStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 beginIds = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_beginids())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_beginids())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated bool beginMasks = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_beginmasks())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 18u, input, this->mutable_beginmasks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 endIds = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_endids())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(24u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 26u, input, this->mutable_endids())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated bool endMasks = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_endmasks())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 34u, input, this->mutable_endmasks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 strides = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_strides())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(40u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 42u, input, this->mutable_strides())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SliceStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SliceStaticLayerParams)
  return false;
#undef DO_
}

void SliceStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SliceStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 beginIds = 1;
  if (this->beginids_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_beginids_cached_byte_size_);
  }
  for (int i = 0, n = this->beginids_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->beginids(i), output);
  }

  // repeated bool beginMasks = 2;
  if (this->beginmasks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_beginmasks_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->beginmasks().data(), this->beginmasks_size(), output);
  }

  // repeated int64 endIds = 3;
  if (this->endids_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_endids_cached_byte_size_);
  }
  for (int i = 0, n = this->endids_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->endids(i), output);
  }

  // repeated bool endMasks = 4;
  if (this->endmasks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_endmasks_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->endmasks().data(), this->endmasks_size(), output);
  }

  // repeated int64 strides = 5;
  if (this->strides_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_strides_cached_byte_size_);
  }
  for (int i = 0, n = this->strides_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->strides(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SliceStaticLayerParams)
}

size_t SliceStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SliceStaticLayerParams)
  size_t total_size = 0;

  // repeated int64 beginIds = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->beginids_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _beginids_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated bool beginMasks = 2;
  {
    unsigned int count = this->beginmasks_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _beginmasks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated int64 endIds = 3;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->endids_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _endids_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated bool endMasks = 4;
  {
    unsigned int count = this->endmasks_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _endmasks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated int64 strides = 5;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->strides_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _strides_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SliceStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SliceStaticLayerParams*>(&from));
}

void SliceStaticLayerParams::MergeFrom(const SliceStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SliceStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  beginids_.MergeFrom(from.beginids_);
  beginmasks_.MergeFrom(from.beginmasks_);
  endids_.MergeFrom(from.endids_);
  endmasks_.MergeFrom(from.endmasks_);
  strides_.MergeFrom(from.strides_);
}

void SliceStaticLayerParams::CopyFrom(const SliceStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SliceStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SliceStaticLayerParams::IsInitialized() const {
  return true;
}

void SliceStaticLayerParams::Swap(SliceStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SliceStaticLayerParams::InternalSwap(SliceStaticLayerParams* other) {
  beginids_.InternalSwap(&other->beginids_);
  beginmasks_.InternalSwap(&other->beginmasks_);
  endids_.InternalSwap(&other->endids_);
  endmasks_.InternalSwap(&other->endmasks_);
  strides_.InternalSwap(&other->strides_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SliceStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.SliceStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SliceStaticLayerParams

// repeated int64 beginIds = 1;
int SliceStaticLayerParams::beginids_size() const {
  return beginids_.size();
}
void SliceStaticLayerParams::clear_beginids() {
  beginids_.Clear();
}
::google::protobuf::int64 SliceStaticLayerParams::beginids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return beginids_.Get(index);
}
void SliceStaticLayerParams::set_beginids(int index, ::google::protobuf::int64 value) {
  beginids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.beginIds)
}
void SliceStaticLayerParams::add_beginids(::google::protobuf::int64 value) {
  beginids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.beginIds)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceStaticLayerParams::beginids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return beginids_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceStaticLayerParams::mutable_beginids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.beginIds)
  return &beginids_;
}

// repeated bool beginMasks = 2;
int SliceStaticLayerParams::beginmasks_size() const {
  return beginmasks_.size();
}
void SliceStaticLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
bool SliceStaticLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return beginmasks_.Get(index);
}
void SliceStaticLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.beginMasks)
}
void SliceStaticLayerParams::add_beginmasks(bool value) {
  beginmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.beginMasks)
}
const ::google::protobuf::RepeatedField< bool >&
SliceStaticLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return beginmasks_;
}
::google::protobuf::RepeatedField< bool >*
SliceStaticLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.beginMasks)
  return &beginmasks_;
}

// repeated int64 endIds = 3;
int SliceStaticLayerParams::endids_size() const {
  return endids_.size();
}
void SliceStaticLayerParams::clear_endids() {
  endids_.Clear();
}
::google::protobuf::int64 SliceStaticLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.endIds)
  return endids_.Get(index);
}
void SliceStaticLayerParams::set_endids(int index, ::google::protobuf::int64 value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.endIds)
}
void SliceStaticLayerParams::add_endids(::google::protobuf::int64 value) {
  endids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.endIds)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceStaticLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.endIds)
  return endids_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceStaticLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.endIds)
  return &endids_;
}

// repeated bool endMasks = 4;
int SliceStaticLayerParams::endmasks_size() const {
  return endmasks_.size();
}
void SliceStaticLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
bool SliceStaticLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return endmasks_.Get(index);
}
void SliceStaticLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.endMasks)
}
void SliceStaticLayerParams::add_endmasks(bool value) {
  endmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.endMasks)
}
const ::google::protobuf::RepeatedField< bool >&
SliceStaticLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return endmasks_;
}
::google::protobuf::RepeatedField< bool >*
SliceStaticLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.endMasks)
  return &endmasks_;
}

// repeated int64 strides = 5;
int SliceStaticLayerParams::strides_size() const {
  return strides_.size();
}
void SliceStaticLayerParams::clear_strides() {
  strides_.Clear();
}
::google::protobuf::int64 SliceStaticLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceStaticLayerParams.strides)
  return strides_.Get(index);
}
void SliceStaticLayerParams::set_strides(int index, ::google::protobuf::int64 value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceStaticLayerParams.strides)
}
void SliceStaticLayerParams::add_strides(::google::protobuf::int64 value) {
  strides_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceStaticLayerParams.strides)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceStaticLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceStaticLayerParams.strides)
  return strides_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceStaticLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceStaticLayerParams.strides)
  return &strides_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SliceDynamicLayerParams::kBeginMasksFieldNumber;
const int SliceDynamicLayerParams::kEndIdsFieldNumber;
const int SliceDynamicLayerParams::kEndMasksFieldNumber;
const int SliceDynamicLayerParams::kStridesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SliceDynamicLayerParams::SliceDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SliceDynamicLayerParams)
}
SliceDynamicLayerParams::SliceDynamicLayerParams(const SliceDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      beginmasks_(from.beginmasks_),
      endids_(from.endids_),
      endmasks_(from.endmasks_),
      strides_(from.strides_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SliceDynamicLayerParams)
}

void SliceDynamicLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SliceDynamicLayerParams::~SliceDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SliceDynamicLayerParams)
  SharedDtor();
}

void SliceDynamicLayerParams::SharedDtor() {
}

void SliceDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SliceDynamicLayerParams& SliceDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SliceDynamicLayerParams* SliceDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  SliceDynamicLayerParams* n = new SliceDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SliceDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SliceDynamicLayerParams)
  beginmasks_.Clear();
  endids_.Clear();
  endmasks_.Clear();
  strides_.Clear();
}

bool SliceDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SliceDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated bool beginMasks = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_beginmasks())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 18u, input, this->mutable_beginmasks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 endIds = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_endids())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(24u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 26u, input, this->mutable_endids())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated bool endMasks = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_endmasks())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 34u, input, this->mutable_endmasks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 strides = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_strides())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(40u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 42u, input, this->mutable_strides())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SliceDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SliceDynamicLayerParams)
  return false;
#undef DO_
}

void SliceDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SliceDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated bool beginMasks = 2;
  if (this->beginmasks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_beginmasks_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->beginmasks().data(), this->beginmasks_size(), output);
  }

  // repeated int64 endIds = 3;
  if (this->endids_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_endids_cached_byte_size_);
  }
  for (int i = 0, n = this->endids_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->endids(i), output);
  }

  // repeated bool endMasks = 4;
  if (this->endmasks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_endmasks_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->endmasks().data(), this->endmasks_size(), output);
  }

  // repeated int64 strides = 5;
  if (this->strides_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_strides_cached_byte_size_);
  }
  for (int i = 0, n = this->strides_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->strides(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SliceDynamicLayerParams)
}

size_t SliceDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SliceDynamicLayerParams)
  size_t total_size = 0;

  // repeated bool beginMasks = 2;
  {
    unsigned int count = this->beginmasks_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _beginmasks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated int64 endIds = 3;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->endids_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _endids_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated bool endMasks = 4;
  {
    unsigned int count = this->endmasks_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _endmasks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated int64 strides = 5;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->strides_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _strides_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SliceDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SliceDynamicLayerParams*>(&from));
}

void SliceDynamicLayerParams::MergeFrom(const SliceDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SliceDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  beginmasks_.MergeFrom(from.beginmasks_);
  endids_.MergeFrom(from.endids_);
  endmasks_.MergeFrom(from.endmasks_);
  strides_.MergeFrom(from.strides_);
}

void SliceDynamicLayerParams::CopyFrom(const SliceDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SliceDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SliceDynamicLayerParams::IsInitialized() const {
  return true;
}

void SliceDynamicLayerParams::Swap(SliceDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SliceDynamicLayerParams::InternalSwap(SliceDynamicLayerParams* other) {
  beginmasks_.InternalSwap(&other->beginmasks_);
  endids_.InternalSwap(&other->endids_);
  endmasks_.InternalSwap(&other->endmasks_);
  strides_.InternalSwap(&other->strides_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SliceDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.SliceDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SliceDynamicLayerParams

// repeated bool beginMasks = 2;
int SliceDynamicLayerParams::beginmasks_size() const {
  return beginmasks_.size();
}
void SliceDynamicLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
bool SliceDynamicLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return beginmasks_.Get(index);
}
void SliceDynamicLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
}
void SliceDynamicLayerParams::add_beginmasks(bool value) {
  beginmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
}
const ::google::protobuf::RepeatedField< bool >&
SliceDynamicLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return beginmasks_;
}
::google::protobuf::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.beginMasks)
  return &beginmasks_;
}

// repeated int64 endIds = 3;
int SliceDynamicLayerParams::endids_size() const {
  return endids_.size();
}
void SliceDynamicLayerParams::clear_endids() {
  endids_.Clear();
}
::google::protobuf::int64 SliceDynamicLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return endids_.Get(index);
}
void SliceDynamicLayerParams::set_endids(int index, ::google::protobuf::int64 value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.endIds)
}
void SliceDynamicLayerParams::add_endids(::google::protobuf::int64 value) {
  endids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.endIds)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceDynamicLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return endids_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceDynamicLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.endIds)
  return &endids_;
}

// repeated bool endMasks = 4;
int SliceDynamicLayerParams::endmasks_size() const {
  return endmasks_.size();
}
void SliceDynamicLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
bool SliceDynamicLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return endmasks_.Get(index);
}
void SliceDynamicLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.endMasks)
}
void SliceDynamicLayerParams::add_endmasks(bool value) {
  endmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.endMasks)
}
const ::google::protobuf::RepeatedField< bool >&
SliceDynamicLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return endmasks_;
}
::google::protobuf::RepeatedField< bool >*
SliceDynamicLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.endMasks)
  return &endmasks_;
}

// repeated int64 strides = 5;
int SliceDynamicLayerParams::strides_size() const {
  return strides_.size();
}
void SliceDynamicLayerParams::clear_strides() {
  strides_.Clear();
}
::google::protobuf::int64 SliceDynamicLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceDynamicLayerParams.strides)
  return strides_.Get(index);
}
void SliceDynamicLayerParams::set_strides(int index, ::google::protobuf::int64 value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceDynamicLayerParams.strides)
}
void SliceDynamicLayerParams::add_strides(::google::protobuf::int64 value) {
  strides_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceDynamicLayerParams.strides)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceDynamicLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceDynamicLayerParams.strides)
  return strides_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceDynamicLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceDynamicLayerParams.strides)
  return &strides_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int TileLayerParams::kRepsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TileLayerParams::TileLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TileLayerParams)
}
TileLayerParams::TileLayerParams(const TileLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      reps_(from.reps_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TileLayerParams)
}

void TileLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

TileLayerParams::~TileLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TileLayerParams)
  SharedDtor();
}

void TileLayerParams::SharedDtor() {
}

void TileLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TileLayerParams& TileLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TileLayerParams* TileLayerParams::New(::google::protobuf::Arena* arena) const {
  TileLayerParams* n = new TileLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TileLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TileLayerParams)
  reps_.Clear();
}

bool TileLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TileLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 reps = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_reps())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_reps())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TileLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TileLayerParams)
  return false;
#undef DO_
}

void TileLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TileLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 reps = 1;
  if (this->reps_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_reps_cached_byte_size_);
  }
  for (int i = 0, n = this->reps_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->reps(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TileLayerParams)
}

size_t TileLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TileLayerParams)
  size_t total_size = 0;

  // repeated uint64 reps = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->reps_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _reps_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TileLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TileLayerParams*>(&from));
}

void TileLayerParams::MergeFrom(const TileLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TileLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  reps_.MergeFrom(from.reps_);
}

void TileLayerParams::CopyFrom(const TileLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TileLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TileLayerParams::IsInitialized() const {
  return true;
}

void TileLayerParams::Swap(TileLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TileLayerParams::InternalSwap(TileLayerParams* other) {
  reps_.InternalSwap(&other->reps_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TileLayerParams::GetTypeName() const {
  return "CoreML.Specification.TileLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TileLayerParams

// repeated uint64 reps = 1;
int TileLayerParams::reps_size() const {
  return reps_.size();
}
void TileLayerParams::clear_reps() {
  reps_.Clear();
}
::google::protobuf::uint64 TileLayerParams::reps(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TileLayerParams.reps)
  return reps_.Get(index);
}
void TileLayerParams::set_reps(int index, ::google::protobuf::uint64 value) {
  reps_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TileLayerParams.reps)
}
void TileLayerParams::add_reps(::google::protobuf::uint64 value) {
  reps_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TileLayerParams.reps)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
TileLayerParams::reps() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TileLayerParams.reps)
  return reps_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
TileLayerParams::mutable_reps() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TileLayerParams.reps)
  return &reps_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GetShapeLayerParams::GetShapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GetShapeLayerParams)
}
GetShapeLayerParams::GetShapeLayerParams(const GetShapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GetShapeLayerParams)
}

void GetShapeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

GetShapeLayerParams::~GetShapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GetShapeLayerParams)
  SharedDtor();
}

void GetShapeLayerParams::SharedDtor() {
}

void GetShapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GetShapeLayerParams& GetShapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GetShapeLayerParams* GetShapeLayerParams::New(::google::protobuf::Arena* arena) const {
  GetShapeLayerParams* n = new GetShapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GetShapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GetShapeLayerParams)
}

bool GetShapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GetShapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GetShapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GetShapeLayerParams)
  return false;
#undef DO_
}

void GetShapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GetShapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GetShapeLayerParams)
}

size_t GetShapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GetShapeLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GetShapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GetShapeLayerParams*>(&from));
}

void GetShapeLayerParams::MergeFrom(const GetShapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GetShapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void GetShapeLayerParams::CopyFrom(const GetShapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GetShapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GetShapeLayerParams::IsInitialized() const {
  return true;
}

void GetShapeLayerParams::Swap(GetShapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GetShapeLayerParams::InternalSwap(GetShapeLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GetShapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.GetShapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GetShapeLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ErfLayerParams::ErfLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ErfLayerParams)
}
ErfLayerParams::ErfLayerParams(const ErfLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ErfLayerParams)
}

void ErfLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ErfLayerParams::~ErfLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ErfLayerParams)
  SharedDtor();
}

void ErfLayerParams::SharedDtor() {
}

void ErfLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ErfLayerParams& ErfLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ErfLayerParams* ErfLayerParams::New(::google::protobuf::Arena* arena) const {
  ErfLayerParams* n = new ErfLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ErfLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ErfLayerParams)
}

bool ErfLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ErfLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ErfLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ErfLayerParams)
  return false;
#undef DO_
}

void ErfLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ErfLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ErfLayerParams)
}

size_t ErfLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ErfLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ErfLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ErfLayerParams*>(&from));
}

void ErfLayerParams::MergeFrom(const ErfLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ErfLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ErfLayerParams::CopyFrom(const ErfLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ErfLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ErfLayerParams::IsInitialized() const {
  return true;
}

void ErfLayerParams::Swap(ErfLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ErfLayerParams::InternalSwap(ErfLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ErfLayerParams::GetTypeName() const {
  return "CoreML.Specification.ErfLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ErfLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GeluLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GeluLayerParams::GeluLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GeluLayerParams)
}
GeluLayerParams::GeluLayerParams(const GeluLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GeluLayerParams)
}

void GeluLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

GeluLayerParams::~GeluLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GeluLayerParams)
  SharedDtor();
}

void GeluLayerParams::SharedDtor() {
}

void GeluLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GeluLayerParams& GeluLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GeluLayerParams* GeluLayerParams::New(::google::protobuf::Arena* arena) const {
  GeluLayerParams* n = new GeluLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GeluLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GeluLayerParams)
  mode_ = 0;
}

bool GeluLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GeluLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::GeluLayerParams_GeluMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GeluLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GeluLayerParams)
  return false;
#undef DO_
}

void GeluLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GeluLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GeluLayerParams)
}

size_t GeluLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GeluLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GeluLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GeluLayerParams*>(&from));
}

void GeluLayerParams::MergeFrom(const GeluLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GeluLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void GeluLayerParams::CopyFrom(const GeluLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GeluLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GeluLayerParams::IsInitialized() const {
  return true;
}

void GeluLayerParams::Swap(GeluLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GeluLayerParams::InternalSwap(GeluLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GeluLayerParams::GetTypeName() const {
  return "CoreML.Specification.GeluLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GeluLayerParams

// .CoreML.Specification.GeluLayerParams.GeluMode mode = 1;
void GeluLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::GeluLayerParams_GeluMode GeluLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GeluLayerParams.mode)
  return static_cast< ::CoreML::Specification::GeluLayerParams_GeluMode >(mode_);
}
void GeluLayerParams::set_mode(::CoreML::Specification::GeluLayerParams_GeluMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GeluLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RangeStaticLayerParams::kEndValueFieldNumber;
const int RangeStaticLayerParams::kStartValueFieldNumber;
const int RangeStaticLayerParams::kStepSizeValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RangeStaticLayerParams::RangeStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RangeStaticLayerParams)
}
RangeStaticLayerParams::RangeStaticLayerParams(const RangeStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&endvalue_, &from.endvalue_,
    reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&endvalue_) + sizeof(stepsizevalue_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RangeStaticLayerParams)
}

void RangeStaticLayerParams::SharedCtor() {
  ::memset(&endvalue_, 0, reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&endvalue_) + sizeof(stepsizevalue_));
  _cached_size_ = 0;
}

RangeStaticLayerParams::~RangeStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RangeStaticLayerParams)
  SharedDtor();
}

void RangeStaticLayerParams::SharedDtor() {
}

void RangeStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RangeStaticLayerParams& RangeStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RangeStaticLayerParams* RangeStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  RangeStaticLayerParams* n = new RangeStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RangeStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RangeStaticLayerParams)
  ::memset(&endvalue_, 0, reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&endvalue_) + sizeof(stepsizevalue_));
}

bool RangeStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RangeStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float endValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &endvalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float startValue = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &startvalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float stepSizeValue = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &stepsizevalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RangeStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RangeStaticLayerParams)
  return false;
#undef DO_
}

void RangeStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RangeStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float endValue = 1;
  if (this->endvalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->endvalue(), output);
  }

  // float startValue = 2;
  if (this->startvalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->startvalue(), output);
  }

  // float stepSizeValue = 3;
  if (this->stepsizevalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->stepsizevalue(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RangeStaticLayerParams)
}

size_t RangeStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RangeStaticLayerParams)
  size_t total_size = 0;

  // float endValue = 1;
  if (this->endvalue() != 0) {
    total_size += 1 + 4;
  }

  // float startValue = 2;
  if (this->startvalue() != 0) {
    total_size += 1 + 4;
  }

  // float stepSizeValue = 3;
  if (this->stepsizevalue() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RangeStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RangeStaticLayerParams*>(&from));
}

void RangeStaticLayerParams::MergeFrom(const RangeStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RangeStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.endvalue() != 0) {
    set_endvalue(from.endvalue());
  }
  if (from.startvalue() != 0) {
    set_startvalue(from.startvalue());
  }
  if (from.stepsizevalue() != 0) {
    set_stepsizevalue(from.stepsizevalue());
  }
}

void RangeStaticLayerParams::CopyFrom(const RangeStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RangeStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RangeStaticLayerParams::IsInitialized() const {
  return true;
}

void RangeStaticLayerParams::Swap(RangeStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RangeStaticLayerParams::InternalSwap(RangeStaticLayerParams* other) {
  std::swap(endvalue_, other->endvalue_);
  std::swap(startvalue_, other->startvalue_);
  std::swap(stepsizevalue_, other->stepsizevalue_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RangeStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.RangeStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RangeStaticLayerParams

// float endValue = 1;
void RangeStaticLayerParams::clear_endvalue() {
  endvalue_ = 0;
}
float RangeStaticLayerParams::endvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.endValue)
  return endvalue_;
}
void RangeStaticLayerParams::set_endvalue(float value) {
  
  endvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.endValue)
}

// float startValue = 2;
void RangeStaticLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
float RangeStaticLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.startValue)
  return startvalue_;
}
void RangeStaticLayerParams::set_startvalue(float value) {
  
  startvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.startValue)
}

// float stepSizeValue = 3;
void RangeStaticLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
float RangeStaticLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeStaticLayerParams.stepSizeValue)
  return stepsizevalue_;
}
void RangeStaticLayerParams::set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeStaticLayerParams.stepSizeValue)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RangeDynamicLayerParams::kStartValueFieldNumber;
const int RangeDynamicLayerParams::kStepSizeValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RangeDynamicLayerParams::RangeDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RangeDynamicLayerParams)
}
RangeDynamicLayerParams::RangeDynamicLayerParams(const RangeDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startvalue_, &from.startvalue_,
    reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&startvalue_) + sizeof(stepsizevalue_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RangeDynamicLayerParams)
}

void RangeDynamicLayerParams::SharedCtor() {
  ::memset(&startvalue_, 0, reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&startvalue_) + sizeof(stepsizevalue_));
  _cached_size_ = 0;
}

RangeDynamicLayerParams::~RangeDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RangeDynamicLayerParams)
  SharedDtor();
}

void RangeDynamicLayerParams::SharedDtor() {
}

void RangeDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RangeDynamicLayerParams& RangeDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RangeDynamicLayerParams* RangeDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  RangeDynamicLayerParams* n = new RangeDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RangeDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RangeDynamicLayerParams)
  ::memset(&startvalue_, 0, reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&startvalue_) + sizeof(stepsizevalue_));
}

bool RangeDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RangeDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float startValue = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &startvalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float stepSizeValue = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &stepsizevalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RangeDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RangeDynamicLayerParams)
  return false;
#undef DO_
}

void RangeDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RangeDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float startValue = 2;
  if (this->startvalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->startvalue(), output);
  }

  // float stepSizeValue = 3;
  if (this->stepsizevalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->stepsizevalue(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RangeDynamicLayerParams)
}

size_t RangeDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RangeDynamicLayerParams)
  size_t total_size = 0;

  // float startValue = 2;
  if (this->startvalue() != 0) {
    total_size += 1 + 4;
  }

  // float stepSizeValue = 3;
  if (this->stepsizevalue() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RangeDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RangeDynamicLayerParams*>(&from));
}

void RangeDynamicLayerParams::MergeFrom(const RangeDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RangeDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startvalue() != 0) {
    set_startvalue(from.startvalue());
  }
  if (from.stepsizevalue() != 0) {
    set_stepsizevalue(from.stepsizevalue());
  }
}

void RangeDynamicLayerParams::CopyFrom(const RangeDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RangeDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RangeDynamicLayerParams::IsInitialized() const {
  return true;
}

void RangeDynamicLayerParams::Swap(RangeDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RangeDynamicLayerParams::InternalSwap(RangeDynamicLayerParams* other) {
  std::swap(startvalue_, other->startvalue_);
  std::swap(stepsizevalue_, other->stepsizevalue_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RangeDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.RangeDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RangeDynamicLayerParams

// float startValue = 2;
void RangeDynamicLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
float RangeDynamicLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeDynamicLayerParams.startValue)
  return startvalue_;
}
void RangeDynamicLayerParams::set_startvalue(float value) {
  
  startvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeDynamicLayerParams.startValue)
}

// float stepSizeValue = 3;
void RangeDynamicLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
float RangeDynamicLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeDynamicLayerParams.stepSizeValue)
  return stepsizevalue_;
}
void RangeDynamicLayerParams::set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeDynamicLayerParams.stepSizeValue)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SlidingWindowsLayerParams::kAxisFieldNumber;
const int SlidingWindowsLayerParams::kWindowSizeFieldNumber;
const int SlidingWindowsLayerParams::kStepFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SlidingWindowsLayerParams::SlidingWindowsLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SlidingWindowsLayerParams)
}
SlidingWindowsLayerParams::SlidingWindowsLayerParams(const SlidingWindowsLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&step_) -
    reinterpret_cast<char*>(&axis_) + sizeof(step_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SlidingWindowsLayerParams)
}

void SlidingWindowsLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&step_) -
    reinterpret_cast<char*>(&axis_) + sizeof(step_));
  _cached_size_ = 0;
}

SlidingWindowsLayerParams::~SlidingWindowsLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SlidingWindowsLayerParams)
  SharedDtor();
}

void SlidingWindowsLayerParams::SharedDtor() {
}

void SlidingWindowsLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SlidingWindowsLayerParams& SlidingWindowsLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SlidingWindowsLayerParams* SlidingWindowsLayerParams::New(::google::protobuf::Arena* arena) const {
  SlidingWindowsLayerParams* n = new SlidingWindowsLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SlidingWindowsLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SlidingWindowsLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&step_) -
    reinterpret_cast<char*>(&axis_) + sizeof(step_));
}

bool SlidingWindowsLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SlidingWindowsLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 windowSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &windowsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 step = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &step_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SlidingWindowsLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SlidingWindowsLayerParams)
  return false;
#undef DO_
}

void SlidingWindowsLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SlidingWindowsLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // uint64 windowSize = 2;
  if (this->windowsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->windowsize(), output);
  }

  // uint64 step = 3;
  if (this->step() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->step(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SlidingWindowsLayerParams)
}

size_t SlidingWindowsLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SlidingWindowsLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // uint64 windowSize = 2;
  if (this->windowsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->windowsize());
  }

  // uint64 step = 3;
  if (this->step() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->step());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SlidingWindowsLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SlidingWindowsLayerParams*>(&from));
}

void SlidingWindowsLayerParams::MergeFrom(const SlidingWindowsLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SlidingWindowsLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.windowsize() != 0) {
    set_windowsize(from.windowsize());
  }
  if (from.step() != 0) {
    set_step(from.step());
  }
}

void SlidingWindowsLayerParams::CopyFrom(const SlidingWindowsLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SlidingWindowsLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SlidingWindowsLayerParams::IsInitialized() const {
  return true;
}

void SlidingWindowsLayerParams::Swap(SlidingWindowsLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SlidingWindowsLayerParams::InternalSwap(SlidingWindowsLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(windowsize_, other->windowsize_);
  std::swap(step_, other->step_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SlidingWindowsLayerParams::GetTypeName() const {
  return "CoreML.Specification.SlidingWindowsLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SlidingWindowsLayerParams

// int64 axis = 1;
void SlidingWindowsLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SlidingWindowsLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.axis)
  return axis_;
}
void SlidingWindowsLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.axis)
}

// uint64 windowSize = 2;
void SlidingWindowsLayerParams::clear_windowsize() {
  windowsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SlidingWindowsLayerParams::windowsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
  return windowsize_;
}
void SlidingWindowsLayerParams::set_windowsize(::google::protobuf::uint64 value) {
  
  windowsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
}

// uint64 step = 3;
void SlidingWindowsLayerParams::clear_step() {
  step_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SlidingWindowsLayerParams::step() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.step)
  return step_;
}
void SlidingWindowsLayerParams::set_step(::google::protobuf::uint64 value) {
  
  step_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.step)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LayerNormalizationLayerParams::kNormalizedShapeFieldNumber;
const int LayerNormalizationLayerParams::kEpsFieldNumber;
const int LayerNormalizationLayerParams::kGammaFieldNumber;
const int LayerNormalizationLayerParams::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LayerNormalizationLayerParams::LayerNormalizationLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LayerNormalizationLayerParams)
}
LayerNormalizationLayerParams::LayerNormalizationLayerParams(const LayerNormalizationLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      normalizedshape_(from.normalizedshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_gamma()) {
    gamma_ = new ::CoreML::Specification::WeightParams(*from.gamma_);
  } else {
    gamma_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  eps_ = from.eps_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LayerNormalizationLayerParams)
}

void LayerNormalizationLayerParams::SharedCtor() {
  ::memset(&gamma_, 0, reinterpret_cast<char*>(&eps_) -
    reinterpret_cast<char*>(&gamma_) + sizeof(eps_));
  _cached_size_ = 0;
}

LayerNormalizationLayerParams::~LayerNormalizationLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LayerNormalizationLayerParams)
  SharedDtor();
}

void LayerNormalizationLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete gamma_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
}

void LayerNormalizationLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LayerNormalizationLayerParams& LayerNormalizationLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LayerNormalizationLayerParams* LayerNormalizationLayerParams::New(::google::protobuf::Arena* arena) const {
  LayerNormalizationLayerParams* n = new LayerNormalizationLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LayerNormalizationLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LayerNormalizationLayerParams)
  normalizedshape_.Clear();
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) {
    delete gamma_;
  }
  gamma_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
  eps_ = 0;
}

bool LayerNormalizationLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LayerNormalizationLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 normalizedShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_normalizedshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_normalizedshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float eps = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &eps_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams gamma = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gamma()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LayerNormalizationLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LayerNormalizationLayerParams)
  return false;
#undef DO_
}

void LayerNormalizationLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LayerNormalizationLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 normalizedShape = 1;
  if (this->normalizedshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_normalizedshape_cached_byte_size_);
  }
  for (int i = 0, n = this->normalizedshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->normalizedshape(i), output);
  }

  // float eps = 2;
  if (this->eps() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->eps(), output);
  }

  // .CoreML.Specification.WeightParams gamma = 3;
  if (this->has_gamma()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->gamma_, output);
  }

  // .CoreML.Specification.WeightParams beta = 4;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->beta_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LayerNormalizationLayerParams)
}

size_t LayerNormalizationLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LayerNormalizationLayerParams)
  size_t total_size = 0;

  // repeated int64 normalizedShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->normalizedshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _normalizedshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams gamma = 3;
  if (this->has_gamma()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->gamma_);
  }

  // .CoreML.Specification.WeightParams beta = 4;
  if (this->has_beta()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  // float eps = 2;
  if (this->eps() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LayerNormalizationLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LayerNormalizationLayerParams*>(&from));
}

void LayerNormalizationLayerParams::MergeFrom(const LayerNormalizationLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LayerNormalizationLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  normalizedshape_.MergeFrom(from.normalizedshape_);
  if (from.has_gamma()) {
    mutable_gamma()->::CoreML::Specification::WeightParams::MergeFrom(from.gamma());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
  if (from.eps() != 0) {
    set_eps(from.eps());
  }
}

void LayerNormalizationLayerParams::CopyFrom(const LayerNormalizationLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LayerNormalizationLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LayerNormalizationLayerParams::IsInitialized() const {
  return true;
}

void LayerNormalizationLayerParams::Swap(LayerNormalizationLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LayerNormalizationLayerParams::InternalSwap(LayerNormalizationLayerParams* other) {
  normalizedshape_.InternalSwap(&other->normalizedshape_);
  std::swap(gamma_, other->gamma_);
  std::swap(beta_, other->beta_);
  std::swap(eps_, other->eps_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LayerNormalizationLayerParams::GetTypeName() const {
  return "CoreML.Specification.LayerNormalizationLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LayerNormalizationLayerParams

// repeated int64 normalizedShape = 1;
int LayerNormalizationLayerParams::normalizedshape_size() const {
  return normalizedshape_.size();
}
void LayerNormalizationLayerParams::clear_normalizedshape() {
  normalizedshape_.Clear();
}
::google::protobuf::int64 LayerNormalizationLayerParams::normalizedshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return normalizedshape_.Get(index);
}
void LayerNormalizationLayerParams::set_normalizedshape(int index, ::google::protobuf::int64 value) {
  normalizedshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
}
void LayerNormalizationLayerParams::add_normalizedshape(::google::protobuf::int64 value) {
  normalizedshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
LayerNormalizationLayerParams::normalizedshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return normalizedshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
LayerNormalizationLayerParams::mutable_normalizedshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LayerNormalizationLayerParams.normalizedShape)
  return &normalizedshape_;
}

// float eps = 2;
void LayerNormalizationLayerParams::clear_eps() {
  eps_ = 0;
}
float LayerNormalizationLayerParams::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.eps)
  return eps_;
}
void LayerNormalizationLayerParams::set_eps(float value) {
  
  eps_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LayerNormalizationLayerParams.eps)
}

// .CoreML.Specification.WeightParams gamma = 3;
bool LayerNormalizationLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
void LayerNormalizationLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  return gamma_;
}
::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LayerNormalizationLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
void LayerNormalizationLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 4;
bool LayerNormalizationLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void LayerNormalizationLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& LayerNormalizationLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LayerNormalizationLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LayerNormalizationLayerParams.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* LayerNormalizationLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LayerNormalizationLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void LayerNormalizationLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LayerNormalizationLayerParams.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NonMaximumSuppressionLayerParams::kIouThresholdFieldNumber;
const int NonMaximumSuppressionLayerParams::kScoreThresholdFieldNumber;
const int NonMaximumSuppressionLayerParams::kMaxBoxesFieldNumber;
const int NonMaximumSuppressionLayerParams::kPerClassSuppressionFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NonMaximumSuppressionLayerParams::NonMaximumSuppressionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NonMaximumSuppressionLayerParams)
}
NonMaximumSuppressionLayerParams::NonMaximumSuppressionLayerParams(const NonMaximumSuppressionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&iouthreshold_, &from.iouthreshold_,
    reinterpret_cast<char*>(&perclasssuppression_) -
    reinterpret_cast<char*>(&iouthreshold_) + sizeof(perclasssuppression_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NonMaximumSuppressionLayerParams)
}

void NonMaximumSuppressionLayerParams::SharedCtor() {
  ::memset(&iouthreshold_, 0, reinterpret_cast<char*>(&perclasssuppression_) -
    reinterpret_cast<char*>(&iouthreshold_) + sizeof(perclasssuppression_));
  _cached_size_ = 0;
}

NonMaximumSuppressionLayerParams::~NonMaximumSuppressionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NonMaximumSuppressionLayerParams)
  SharedDtor();
}

void NonMaximumSuppressionLayerParams::SharedDtor() {
}

void NonMaximumSuppressionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NonMaximumSuppressionLayerParams& NonMaximumSuppressionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NonMaximumSuppressionLayerParams* NonMaximumSuppressionLayerParams::New(::google::protobuf::Arena* arena) const {
  NonMaximumSuppressionLayerParams* n = new NonMaximumSuppressionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NonMaximumSuppressionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NonMaximumSuppressionLayerParams)
  ::memset(&iouthreshold_, 0, reinterpret_cast<char*>(&perclasssuppression_) -
    reinterpret_cast<char*>(&iouthreshold_) + sizeof(perclasssuppression_));
}

bool NonMaximumSuppressionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NonMaximumSuppressionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float iouThreshold = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &iouthreshold_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float scoreThreshold = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &scorethreshold_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 maxBoxes = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &maxboxes_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool perClassSuppression = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &perclasssuppression_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NonMaximumSuppressionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NonMaximumSuppressionLayerParams)
  return false;
#undef DO_
}

void NonMaximumSuppressionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NonMaximumSuppressionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float iouThreshold = 1;
  if (this->iouthreshold() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->iouthreshold(), output);
  }

  // float scoreThreshold = 2;
  if (this->scorethreshold() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->scorethreshold(), output);
  }

  // uint64 maxBoxes = 3;
  if (this->maxboxes() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->maxboxes(), output);
  }

  // bool perClassSuppression = 4;
  if (this->perclasssuppression() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(4, this->perclasssuppression(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NonMaximumSuppressionLayerParams)
}

size_t NonMaximumSuppressionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NonMaximumSuppressionLayerParams)
  size_t total_size = 0;

  // float iouThreshold = 1;
  if (this->iouthreshold() != 0) {
    total_size += 1 + 4;
  }

  // float scoreThreshold = 2;
  if (this->scorethreshold() != 0) {
    total_size += 1 + 4;
  }

  // uint64 maxBoxes = 3;
  if (this->maxboxes() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->maxboxes());
  }

  // bool perClassSuppression = 4;
  if (this->perclasssuppression() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NonMaximumSuppressionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NonMaximumSuppressionLayerParams*>(&from));
}

void NonMaximumSuppressionLayerParams::MergeFrom(const NonMaximumSuppressionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NonMaximumSuppressionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.iouthreshold() != 0) {
    set_iouthreshold(from.iouthreshold());
  }
  if (from.scorethreshold() != 0) {
    set_scorethreshold(from.scorethreshold());
  }
  if (from.maxboxes() != 0) {
    set_maxboxes(from.maxboxes());
  }
  if (from.perclasssuppression() != 0) {
    set_perclasssuppression(from.perclasssuppression());
  }
}

void NonMaximumSuppressionLayerParams::CopyFrom(const NonMaximumSuppressionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NonMaximumSuppressionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NonMaximumSuppressionLayerParams::IsInitialized() const {
  return true;
}

void NonMaximumSuppressionLayerParams::Swap(NonMaximumSuppressionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NonMaximumSuppressionLayerParams::InternalSwap(NonMaximumSuppressionLayerParams* other) {
  std::swap(iouthreshold_, other->iouthreshold_);
  std::swap(scorethreshold_, other->scorethreshold_);
  std::swap(maxboxes_, other->maxboxes_);
  std::swap(perclasssuppression_, other->perclasssuppression_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NonMaximumSuppressionLayerParams::GetTypeName() const {
  return "CoreML.Specification.NonMaximumSuppressionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NonMaximumSuppressionLayerParams

// float iouThreshold = 1;
void NonMaximumSuppressionLayerParams::clear_iouthreshold() {
  iouthreshold_ = 0;
}
float NonMaximumSuppressionLayerParams::iouthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.iouThreshold)
  return iouthreshold_;
}
void NonMaximumSuppressionLayerParams::set_iouthreshold(float value) {
  
  iouthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.iouThreshold)
}

// float scoreThreshold = 2;
void NonMaximumSuppressionLayerParams::clear_scorethreshold() {
  scorethreshold_ = 0;
}
float NonMaximumSuppressionLayerParams::scorethreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.scoreThreshold)
  return scorethreshold_;
}
void NonMaximumSuppressionLayerParams::set_scorethreshold(float value) {
  
  scorethreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.scoreThreshold)
}

// uint64 maxBoxes = 3;
void NonMaximumSuppressionLayerParams::clear_maxboxes() {
  maxboxes_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 NonMaximumSuppressionLayerParams::maxboxes() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.maxBoxes)
  return maxboxes_;
}
void NonMaximumSuppressionLayerParams::set_maxboxes(::google::protobuf::uint64 value) {
  
  maxboxes_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.maxBoxes)
}

// bool perClassSuppression = 4;
void NonMaximumSuppressionLayerParams::clear_perclasssuppression() {
  perclasssuppression_ = false;
}
bool NonMaximumSuppressionLayerParams::perclasssuppression() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NonMaximumSuppressionLayerParams.perClassSuppression)
  return perclasssuppression_;
}
void NonMaximumSuppressionLayerParams::set_perclasssuppression(bool value) {
  
  perclasssuppression_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NonMaximumSuppressionLayerParams.perClassSuppression)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkClassifier::kLayersFieldNumber;
const int NeuralNetworkClassifier::kPreprocessingFieldNumber;
const int NeuralNetworkClassifier::kArrayInputShapeMappingFieldNumber;
const int NeuralNetworkClassifier::kImageInputShapeMappingFieldNumber;
const int NeuralNetworkClassifier::kUpdateParamsFieldNumber;
const int NeuralNetworkClassifier::kStringClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kInt64ClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kLabelProbabilityLayerNameFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkClassifier::NeuralNetworkClassifier()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkClassifier)
}
NeuralNetworkClassifier::NeuralNetworkClassifier(const NeuralNetworkClassifier& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.labelprobabilitylayername().size() > 0) {
    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
  if (from.has_updateparams()) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters(*from.updateparams_);
  } else {
    updateparams_ = NULL;
  }
  ::memcpy(&arrayinputshapemapping_, &from.arrayinputshapemapping_,
    reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  clear_has_ClassLabels();
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkClassifier)
}

void NeuralNetworkClassifier::SharedCtor() {
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&updateparams_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&updateparams_) + sizeof(imageinputshapemapping_));
  clear_has_ClassLabels();
  _cached_size_ = 0;
}

NeuralNetworkClassifier::~NeuralNetworkClassifier() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkClassifier)
  SharedDtor();
}

void NeuralNetworkClassifier::SharedDtor() {
  labelprobabilitylayername_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete updateparams_;
  }
  if (has_ClassLabels()) {
    clear_ClassLabels();
  }
}

void NeuralNetworkClassifier::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkClassifier& NeuralNetworkClassifier::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkClassifier* NeuralNetworkClassifier::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkClassifier* n = new NeuralNetworkClassifier;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkClassifier::clear_ClassLabels() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  switch (ClassLabels_case()) {
    case kStringClassLabels: {
      delete ClassLabels_.stringclasslabels_;
      break;
    }
    case kInt64ClassLabels: {
      delete ClassLabels_.int64classlabels_;
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}


void NeuralNetworkClassifier::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  layers_.Clear();
  preprocessing_.Clear();
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) {
    delete updateparams_;
  }
  updateparams_ = NULL;
  ::memset(&arrayinputshapemapping_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  clear_ClassLabels();
}

bool NeuralNetworkClassifier::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkClassifier)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_arrayinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_imageinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updateparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.StringVector stringClassLabels = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_stringclasslabels()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
      case 101: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(810u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_int64classlabels()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string labelProbabilityLayerName = 200;
      case 200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_labelprobabilitylayername()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkClassifier)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkClassifier)
  return false;
#undef DO_
}

void NeuralNetworkClassifier::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkClassifier)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->arrayinputshapemapping(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      6, this->imageinputshapemapping(), output);
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->updateparams_, output);
  }

  // .CoreML.Specification.StringVector stringClassLabels = 100;
  if (has_stringclasslabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *ClassLabels_.stringclasslabels_, output);
  }

  // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  if (has_int64classlabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *ClassLabels_.int64classlabels_, output);
  }

  // string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      200, this->labelprobabilitylayername(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkClassifier)
}

size_t NeuralNetworkClassifier::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkClassifier)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->labelprobabilitylayername());
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updateparams_);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->arrayinputshapemapping());
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->imageinputshapemapping());
  }

  switch (ClassLabels_case()) {
    // .CoreML.Specification.StringVector stringClassLabels = 100;
    case kStringClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.stringclasslabels_);
      break;
    }
    // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
    case kInt64ClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.int64classlabels_);
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkClassifier::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkClassifier*>(&from));
}

void NeuralNetworkClassifier::MergeFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkClassifier)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.labelprobabilitylayername().size() > 0) {

    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
  if (from.has_updateparams()) {
    mutable_updateparams()->::CoreML::Specification::NetworkUpdateParameters::MergeFrom(from.updateparams());
  }
  if (from.arrayinputshapemapping() != 0) {
    set_arrayinputshapemapping(from.arrayinputshapemapping());
  }
  if (from.imageinputshapemapping() != 0) {
    set_imageinputshapemapping(from.imageinputshapemapping());
  }
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkClassifier::CopyFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkClassifier)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkClassifier::IsInitialized() const {
  return true;
}

void NeuralNetworkClassifier::Swap(NeuralNetworkClassifier* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkClassifier::InternalSwap(NeuralNetworkClassifier* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  labelprobabilitylayername_.Swap(&other->labelprobabilitylayername_);
  std::swap(updateparams_, other->updateparams_);
  std::swap(arrayinputshapemapping_, other->arrayinputshapemapping_);
  std::swap(imageinputshapemapping_, other->imageinputshapemapping_);
  std::swap(ClassLabels_, other->ClassLabels_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkClassifier::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkClassifier";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkClassifier::layers_size() const {
  return layers_.size();
}
void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkClassifier::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
void NeuralNetworkClassifier::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkClassifier::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
void NeuralNetworkClassifier::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
void NeuralNetworkClassifier::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkClassifier::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
void NeuralNetworkClassifier::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
bool NeuralNetworkClassifier::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
void NeuralNetworkClassifier::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkClassifier::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return updateparams_;
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
void NeuralNetworkClassifier::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.updateParams)
}

// .CoreML.Specification.StringVector stringClassLabels = 100;
bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
void NeuralNetworkClassifier::clear_stringclasslabels() {
  if (has_stringclasslabels()) {
    delete ClassLabels_.stringclasslabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : ::CoreML::Specification::StringVector::default_instance();
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  if (!has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = new ::CoreML::Specification::StringVector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return ClassLabels_.stringclasslabels_;
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}

// .CoreML.Specification.Int64Vector int64ClassLabels = 101;
bool NeuralNetworkClassifier::has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
void NeuralNetworkClassifier::clear_int64classlabels() {
  if (has_int64classlabels()) {
    delete ClassLabels_.int64classlabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : ::CoreML::Specification::Int64Vector::default_instance();
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  if (!has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = new ::CoreML::Specification::Int64Vector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return ClassLabels_.int64classlabels_;
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}

// string labelProbabilityLayerName = 200;
void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.GetNoArena();
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const ::std::string& value) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#if LANG_CXX11
void NeuralNetworkClassifier::set_labelprobabilitylayername(::std::string&& value) {
  
  labelprobabilitylayername_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#endif
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value, size_t size) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
::std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  
  return labelprobabilitylayername_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != NULL) {
    
  } else {
    
  }
  labelprobabilitylayername_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkRegressor::kLayersFieldNumber;
const int NeuralNetworkRegressor::kPreprocessingFieldNumber;
const int NeuralNetworkRegressor::kArrayInputShapeMappingFieldNumber;
const int NeuralNetworkRegressor::kImageInputShapeMappingFieldNumber;
const int NeuralNetworkRegressor::kUpdateParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkRegressor::NeuralNetworkRegressor()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkRegressor)
}
NeuralNetworkRegressor::NeuralNetworkRegressor(const NeuralNetworkRegressor& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updateparams()) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters(*from.updateparams_);
  } else {
    updateparams_ = NULL;
  }
  ::memcpy(&arrayinputshapemapping_, &from.arrayinputshapemapping_,
    reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkRegressor)
}

void NeuralNetworkRegressor::SharedCtor() {
  ::memset(&updateparams_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&updateparams_) + sizeof(imageinputshapemapping_));
  _cached_size_ = 0;
}

NeuralNetworkRegressor::~NeuralNetworkRegressor() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkRegressor)
  SharedDtor();
}

void NeuralNetworkRegressor::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updateparams_;
  }
}

void NeuralNetworkRegressor::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkRegressor& NeuralNetworkRegressor::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkRegressor* NeuralNetworkRegressor::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkRegressor* n = new NeuralNetworkRegressor;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkRegressor::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkRegressor)
  layers_.Clear();
  preprocessing_.Clear();
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) {
    delete updateparams_;
  }
  updateparams_ = NULL;
  ::memset(&arrayinputshapemapping_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
}

bool NeuralNetworkRegressor::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkRegressor)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_arrayinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_imageinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updateparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkRegressor)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkRegressor)
  return false;
#undef DO_
}

void NeuralNetworkRegressor::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkRegressor)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->arrayinputshapemapping(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      6, this->imageinputshapemapping(), output);
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->updateparams_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkRegressor)
}

size_t NeuralNetworkRegressor::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkRegressor)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updateparams_);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->arrayinputshapemapping());
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->imageinputshapemapping());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkRegressor::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkRegressor*>(&from));
}

void NeuralNetworkRegressor::MergeFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkRegressor)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.has_updateparams()) {
    mutable_updateparams()->::CoreML::Specification::NetworkUpdateParameters::MergeFrom(from.updateparams());
  }
  if (from.arrayinputshapemapping() != 0) {
    set_arrayinputshapemapping(from.arrayinputshapemapping());
  }
  if (from.imageinputshapemapping() != 0) {
    set_imageinputshapemapping(from.imageinputshapemapping());
  }
}

void NeuralNetworkRegressor::CopyFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkRegressor)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkRegressor::IsInitialized() const {
  return true;
}

void NeuralNetworkRegressor::Swap(NeuralNetworkRegressor* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkRegressor::InternalSwap(NeuralNetworkRegressor* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  std::swap(updateparams_, other->updateparams_);
  std::swap(arrayinputshapemapping_, other->arrayinputshapemapping_);
  std::swap(imageinputshapemapping_, other->imageinputshapemapping_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkRegressor::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkRegressor";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkRegressor::layers_size() const {
  return layers_.size();
}
void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkRegressor::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
void NeuralNetworkRegressor::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkRegressor::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
void NeuralNetworkRegressor::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
void NeuralNetworkRegressor::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkRegressor::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
void NeuralNetworkRegressor::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
bool NeuralNetworkRegressor::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
void NeuralNetworkRegressor::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkRegressor::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return updateparams_;
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
void NeuralNetworkRegressor::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkRegressor.updateParams)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NetworkUpdateParameters::kLossLayersFieldNumber;
const int NetworkUpdateParameters::kOptimizerFieldNumber;
const int NetworkUpdateParameters::kEpochsFieldNumber;
const int NetworkUpdateParameters::kShuffleFieldNumber;
const int NetworkUpdateParameters::kSeedFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NetworkUpdateParameters::NetworkUpdateParameters()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NetworkUpdateParameters)
}
NetworkUpdateParameters::NetworkUpdateParameters(const NetworkUpdateParameters& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      losslayers_(from.losslayers_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_optimizer()) {
    optimizer_ = new ::CoreML::Specification::Optimizer(*from.optimizer_);
  } else {
    optimizer_ = NULL;
  }
  if (from.has_epochs()) {
    epochs_ = new ::CoreML::Specification::Int64Parameter(*from.epochs_);
  } else {
    epochs_ = NULL;
  }
  if (from.has_shuffle()) {
    shuffle_ = new ::CoreML::Specification::BoolParameter(*from.shuffle_);
  } else {
    shuffle_ = NULL;
  }
  if (from.has_seed()) {
    seed_ = new ::CoreML::Specification::Int64Parameter(*from.seed_);
  } else {
    seed_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NetworkUpdateParameters)
}

void NetworkUpdateParameters::SharedCtor() {
  ::memset(&optimizer_, 0, reinterpret_cast<char*>(&seed_) -
    reinterpret_cast<char*>(&optimizer_) + sizeof(seed_));
  _cached_size_ = 0;
}

NetworkUpdateParameters::~NetworkUpdateParameters() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NetworkUpdateParameters)
  SharedDtor();
}

void NetworkUpdateParameters::SharedDtor() {
  if (this != internal_default_instance()) {
    delete optimizer_;
  }
  if (this != internal_default_instance()) {
    delete epochs_;
  }
  if (this != internal_default_instance()) {
    delete shuffle_;
  }
  if (this != internal_default_instance()) {
    delete seed_;
  }
}

void NetworkUpdateParameters::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NetworkUpdateParameters& NetworkUpdateParameters::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NetworkUpdateParameters* NetworkUpdateParameters::New(::google::protobuf::Arena* arena) const {
  NetworkUpdateParameters* n = new NetworkUpdateParameters;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NetworkUpdateParameters::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NetworkUpdateParameters)
  losslayers_.Clear();
  if (GetArenaNoVirtual() == NULL && optimizer_ != NULL) {
    delete optimizer_;
  }
  optimizer_ = NULL;
  if (GetArenaNoVirtual() == NULL && epochs_ != NULL) {
    delete epochs_;
  }
  epochs_ = NULL;
  if (GetArenaNoVirtual() == NULL && shuffle_ != NULL) {
    delete shuffle_;
  }
  shuffle_ = NULL;
  if (GetArenaNoVirtual() == NULL && seed_ != NULL) {
    delete seed_;
  }
  seed_ = NULL;
}

bool NetworkUpdateParameters::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NetworkUpdateParameters)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.LossLayer lossLayers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_losslayers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Optimizer optimizer = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_optimizer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter epochs = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_epochs()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BoolParameter shuffle = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_shuffle()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter seed = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_seed()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NetworkUpdateParameters)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NetworkUpdateParameters)
  return false;
#undef DO_
}

void NetworkUpdateParameters::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NetworkUpdateParameters)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.LossLayer lossLayers = 1;
  for (unsigned int i = 0, n = this->losslayers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->losslayers(i), output);
  }

  // .CoreML.Specification.Optimizer optimizer = 2;
  if (this->has_optimizer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->optimizer_, output);
  }

  // .CoreML.Specification.Int64Parameter epochs = 3;
  if (this->has_epochs()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->epochs_, output);
  }

  // .CoreML.Specification.BoolParameter shuffle = 10;
  if (this->has_shuffle()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->shuffle_, output);
  }

  // .CoreML.Specification.Int64Parameter seed = 20;
  if (this->has_seed()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->seed_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NetworkUpdateParameters)
}

size_t NetworkUpdateParameters::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NetworkUpdateParameters)
  size_t total_size = 0;

  // repeated .CoreML.Specification.LossLayer lossLayers = 1;
  {
    unsigned int count = this->losslayers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->losslayers(i));
    }
  }

  // .CoreML.Specification.Optimizer optimizer = 2;
  if (this->has_optimizer()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->optimizer_);
  }

  // .CoreML.Specification.Int64Parameter epochs = 3;
  if (this->has_epochs()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->epochs_);
  }

  // .CoreML.Specification.BoolParameter shuffle = 10;
  if (this->has_shuffle()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->shuffle_);
  }

  // .CoreML.Specification.Int64Parameter seed = 20;
  if (this->has_seed()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->seed_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NetworkUpdateParameters::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NetworkUpdateParameters*>(&from));
}

void NetworkUpdateParameters::MergeFrom(const NetworkUpdateParameters& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NetworkUpdateParameters)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  losslayers_.MergeFrom(from.losslayers_);
  if (from.has_optimizer()) {
    mutable_optimizer()->::CoreML::Specification::Optimizer::MergeFrom(from.optimizer());
  }
  if (from.has_epochs()) {
    mutable_epochs()->::CoreML::Specification::Int64Parameter::MergeFrom(from.epochs());
  }
  if (from.has_shuffle()) {
    mutable_shuffle()->::CoreML::Specification::BoolParameter::MergeFrom(from.shuffle());
  }
  if (from.has_seed()) {
    mutable_seed()->::CoreML::Specification::Int64Parameter::MergeFrom(from.seed());
  }
}

void NetworkUpdateParameters::CopyFrom(const NetworkUpdateParameters& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NetworkUpdateParameters)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NetworkUpdateParameters::IsInitialized() const {
  return true;
}

void NetworkUpdateParameters::Swap(NetworkUpdateParameters* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NetworkUpdateParameters::InternalSwap(NetworkUpdateParameters* other) {
  losslayers_.InternalSwap(&other->losslayers_);
  std::swap(optimizer_, other->optimizer_);
  std::swap(epochs_, other->epochs_);
  std::swap(shuffle_, other->shuffle_);
  std::swap(seed_, other->seed_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NetworkUpdateParameters::GetTypeName() const {
  return "CoreML.Specification.NetworkUpdateParameters";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NetworkUpdateParameters

// repeated .CoreML.Specification.LossLayer lossLayers = 1;
int NetworkUpdateParameters::losslayers_size() const {
  return losslayers_.size();
}
void NetworkUpdateParameters::clear_losslayers() {
  losslayers_.Clear();
}
const ::CoreML::Specification::LossLayer& NetworkUpdateParameters::losslayers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Get(index);
}
::CoreML::Specification::LossLayer* NetworkUpdateParameters::mutable_losslayers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Mutable(index);
}
::CoreML::Specification::LossLayer* NetworkUpdateParameters::add_losslayers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >*
NetworkUpdateParameters::mutable_losslayers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return &losslayers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >&
NetworkUpdateParameters::losslayers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_;
}

// .CoreML.Specification.Optimizer optimizer = 2;
bool NetworkUpdateParameters::has_optimizer() const {
  return this != internal_default_instance() && optimizer_ != NULL;
}
void NetworkUpdateParameters::clear_optimizer() {
  if (GetArenaNoVirtual() == NULL && optimizer_ != NULL) delete optimizer_;
  optimizer_ = NULL;
}
const ::CoreML::Specification::Optimizer& NetworkUpdateParameters::optimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return optimizer_ != NULL ? *optimizer_
                         : *::CoreML::Specification::Optimizer::internal_default_instance();
}
::CoreML::Specification::Optimizer* NetworkUpdateParameters::mutable_optimizer() {
  
  if (optimizer_ == NULL) {
    optimizer_ = new ::CoreML::Specification::Optimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return optimizer_;
}
::CoreML::Specification::Optimizer* NetworkUpdateParameters::release_optimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.optimizer)
  
  ::CoreML::Specification::Optimizer* temp = optimizer_;
  optimizer_ = NULL;
  return temp;
}
void NetworkUpdateParameters::set_allocated_optimizer(::CoreML::Specification::Optimizer* optimizer) {
  delete optimizer_;
  optimizer_ = optimizer;
  if (optimizer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.optimizer)
}

// .CoreML.Specification.Int64Parameter epochs = 3;
bool NetworkUpdateParameters::has_epochs() const {
  return this != internal_default_instance() && epochs_ != NULL;
}
void NetworkUpdateParameters::clear_epochs() {
  if (GetArenaNoVirtual() == NULL && epochs_ != NULL) delete epochs_;
  epochs_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::epochs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.epochs)
  return epochs_ != NULL ? *epochs_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_epochs() {
  
  if (epochs_ == NULL) {
    epochs_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.epochs)
  return epochs_;
}
::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_epochs() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.epochs)
  
  ::CoreML::Specification::Int64Parameter* temp = epochs_;
  epochs_ = NULL;
  return temp;
}
void NetworkUpdateParameters::set_allocated_epochs(::CoreML::Specification::Int64Parameter* epochs) {
  delete epochs_;
  epochs_ = epochs;
  if (epochs) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.epochs)
}

// .CoreML.Specification.BoolParameter shuffle = 10;
bool NetworkUpdateParameters::has_shuffle() const {
  return this != internal_default_instance() && shuffle_ != NULL;
}
void NetworkUpdateParameters::clear_shuffle() {
  if (GetArenaNoVirtual() == NULL && shuffle_ != NULL) delete shuffle_;
  shuffle_ = NULL;
}
const ::CoreML::Specification::BoolParameter& NetworkUpdateParameters::shuffle() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.shuffle)
  return shuffle_ != NULL ? *shuffle_
                         : *::CoreML::Specification::BoolParameter::internal_default_instance();
}
::CoreML::Specification::BoolParameter* NetworkUpdateParameters::mutable_shuffle() {
  
  if (shuffle_ == NULL) {
    shuffle_ = new ::CoreML::Specification::BoolParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.shuffle)
  return shuffle_;
}
::CoreML::Specification::BoolParameter* NetworkUpdateParameters::release_shuffle() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.shuffle)
  
  ::CoreML::Specification::BoolParameter* temp = shuffle_;
  shuffle_ = NULL;
  return temp;
}
void NetworkUpdateParameters::set_allocated_shuffle(::CoreML::Specification::BoolParameter* shuffle) {
  delete shuffle_;
  shuffle_ = shuffle;
  if (shuffle) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.shuffle)
}

// .CoreML.Specification.Int64Parameter seed = 20;
bool NetworkUpdateParameters::has_seed() const {
  return this != internal_default_instance() && seed_ != NULL;
}
void NetworkUpdateParameters::clear_seed() {
  if (GetArenaNoVirtual() == NULL && seed_ != NULL) delete seed_;
  seed_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::seed() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.seed)
  return seed_ != NULL ? *seed_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_seed() {
  
  if (seed_ == NULL) {
    seed_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.seed)
  return seed_;
}
::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_seed() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.seed)
  
  ::CoreML::Specification::Int64Parameter* temp = seed_;
  seed_ = NULL;
  return temp;
}
void NetworkUpdateParameters::set_allocated_seed(::CoreML::Specification::Int64Parameter* seed) {
  delete seed_;
  seed_ = seed;
  if (seed) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.seed)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LossLayer::kNameFieldNumber;
const int LossLayer::kCategoricalCrossEntropyLossLayerFieldNumber;
const int LossLayer::kMeanSquaredErrorLossLayerFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LossLayer::LossLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LossLayer)
}
LossLayer::LossLayer(const LossLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.name().size() > 0) {
    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  clear_has_LossLayerType();
  switch (from.LossLayerType_case()) {
    case kCategoricalCrossEntropyLossLayer: {
      mutable_categoricalcrossentropylosslayer()->::CoreML::Specification::CategoricalCrossEntropyLossLayer::MergeFrom(from.categoricalcrossentropylosslayer());
      break;
    }
    case kMeanSquaredErrorLossLayer: {
      mutable_meansquarederrorlosslayer()->::CoreML::Specification::MeanSquaredErrorLossLayer::MergeFrom(from.meansquarederrorlosslayer());
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LossLayer)
}

void LossLayer::SharedCtor() {
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_LossLayerType();
  _cached_size_ = 0;
}

LossLayer::~LossLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LossLayer)
  SharedDtor();
}

void LossLayer::SharedDtor() {
  name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_LossLayerType()) {
    clear_LossLayerType();
  }
}

void LossLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LossLayer& LossLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LossLayer* LossLayer::New(::google::protobuf::Arena* arena) const {
  LossLayer* n = new LossLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LossLayer::clear_LossLayerType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.LossLayer)
  switch (LossLayerType_case()) {
    case kCategoricalCrossEntropyLossLayer: {
      delete LossLayerType_.categoricalcrossentropylosslayer_;
      break;
    }
    case kMeanSquaredErrorLossLayer: {
      delete LossLayerType_.meansquarederrorlosslayer_;
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LOSSLAYERTYPE_NOT_SET;
}


void LossLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LossLayer)
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_LossLayerType();
}

bool LossLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LossLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string name = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->name().data(), this->name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.LossLayer.name"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_categoricalcrossentropylosslayer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_meansquarederrorlosslayer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LossLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LossLayer)
  return false;
#undef DO_
}

void LossLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LossLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string name = 1;
  if (this->name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->name().data(), this->name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.LossLayer.name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->name(), output);
  }

  // .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
  if (has_categoricalcrossentropylosslayer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *LossLayerType_.categoricalcrossentropylosslayer_, output);
  }

  // .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
  if (has_meansquarederrorlosslayer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *LossLayerType_.meansquarederrorlosslayer_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LossLayer)
}

size_t LossLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LossLayer)
  size_t total_size = 0;

  // string name = 1;
  if (this->name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->name());
  }

  switch (LossLayerType_case()) {
    // .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
    case kCategoricalCrossEntropyLossLayer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *LossLayerType_.categoricalcrossentropylosslayer_);
      break;
    }
    // .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
    case kMeanSquaredErrorLossLayer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *LossLayerType_.meansquarederrorlosslayer_);
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LossLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LossLayer*>(&from));
}

void LossLayer::MergeFrom(const LossLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LossLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.name().size() > 0) {

    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  switch (from.LossLayerType_case()) {
    case kCategoricalCrossEntropyLossLayer: {
      mutable_categoricalcrossentropylosslayer()->::CoreML::Specification::CategoricalCrossEntropyLossLayer::MergeFrom(from.categoricalcrossentropylosslayer());
      break;
    }
    case kMeanSquaredErrorLossLayer: {
      mutable_meansquarederrorlosslayer()->::CoreML::Specification::MeanSquaredErrorLossLayer::MergeFrom(from.meansquarederrorlosslayer());
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
}

void LossLayer::CopyFrom(const LossLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LossLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LossLayer::IsInitialized() const {
  return true;
}

void LossLayer::Swap(LossLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LossLayer::InternalSwap(LossLayer* other) {
  name_.Swap(&other->name_);
  std::swap(LossLayerType_, other->LossLayerType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LossLayer::GetTypeName() const {
  return "CoreML.Specification.LossLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LossLayer

// string name = 1;
void LossLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& LossLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.name)
  return name_.GetNoArena();
}
void LossLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LossLayer.name)
}
#if LANG_CXX11
void LossLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.LossLayer.name)
}
#endif
void LossLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.LossLayer.name)
}
void LossLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.LossLayer.name)
}
::std::string* LossLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* LossLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void LossLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.name)
}

// .CoreML.Specification.CategoricalCrossEntropyLossLayer categoricalCrossEntropyLossLayer = 10;
bool LossLayer::has_categoricalcrossentropylosslayer() const {
  return LossLayerType_case() == kCategoricalCrossEntropyLossLayer;
}
void LossLayer::set_has_categoricalcrossentropylosslayer() {
  _oneof_case_[0] = kCategoricalCrossEntropyLossLayer;
}
void LossLayer::clear_categoricalcrossentropylosslayer() {
  if (has_categoricalcrossentropylosslayer()) {
    delete LossLayerType_.categoricalcrossentropylosslayer_;
    clear_has_LossLayerType();
  }
}
 const ::CoreML::Specification::CategoricalCrossEntropyLossLayer& LossLayer::categoricalcrossentropylosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  return has_categoricalcrossentropylosslayer()
      ? *LossLayerType_.categoricalcrossentropylosslayer_
      : ::CoreML::Specification::CategoricalCrossEntropyLossLayer::default_instance();
}
::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::mutable_categoricalcrossentropylosslayer() {
  if (!has_categoricalcrossentropylosslayer()) {
    clear_LossLayerType();
    set_has_categoricalcrossentropylosslayer();
    LossLayerType_.categoricalcrossentropylosslayer_ = new ::CoreML::Specification::CategoricalCrossEntropyLossLayer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  return LossLayerType_.categoricalcrossentropylosslayer_;
}
::CoreML::Specification::CategoricalCrossEntropyLossLayer* LossLayer::release_categoricalcrossentropylosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
  if (has_categoricalcrossentropylosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::CategoricalCrossEntropyLossLayer* temp = LossLayerType_.categoricalcrossentropylosslayer_;
    LossLayerType_.categoricalcrossentropylosslayer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void LossLayer::set_allocated_categoricalcrossentropylosslayer(::CoreML::Specification::CategoricalCrossEntropyLossLayer* categoricalcrossentropylosslayer) {
  clear_LossLayerType();
  if (categoricalcrossentropylosslayer) {
    set_has_categoricalcrossentropylosslayer();
    LossLayerType_.categoricalcrossentropylosslayer_ = categoricalcrossentropylosslayer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.categoricalCrossEntropyLossLayer)
}

// .CoreML.Specification.MeanSquaredErrorLossLayer meanSquaredErrorLossLayer = 11;
bool LossLayer::has_meansquarederrorlosslayer() const {
  return LossLayerType_case() == kMeanSquaredErrorLossLayer;
}
void LossLayer::set_has_meansquarederrorlosslayer() {
  _oneof_case_[0] = kMeanSquaredErrorLossLayer;
}
void LossLayer::clear_meansquarederrorlosslayer() {
  if (has_meansquarederrorlosslayer()) {
    delete LossLayerType_.meansquarederrorlosslayer_;
    clear_has_LossLayerType();
  }
}
 const ::CoreML::Specification::MeanSquaredErrorLossLayer& LossLayer::meansquarederrorlosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  return has_meansquarederrorlosslayer()
      ? *LossLayerType_.meansquarederrorlosslayer_
      : ::CoreML::Specification::MeanSquaredErrorLossLayer::default_instance();
}
::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::mutable_meansquarederrorlosslayer() {
  if (!has_meansquarederrorlosslayer()) {
    clear_LossLayerType();
    set_has_meansquarederrorlosslayer();
    LossLayerType_.meansquarederrorlosslayer_ = new ::CoreML::Specification::MeanSquaredErrorLossLayer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  return LossLayerType_.meansquarederrorlosslayer_;
}
::CoreML::Specification::MeanSquaredErrorLossLayer* LossLayer::release_meansquarederrorlosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
  if (has_meansquarederrorlosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::MeanSquaredErrorLossLayer* temp = LossLayerType_.meansquarederrorlosslayer_;
    LossLayerType_.meansquarederrorlosslayer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void LossLayer::set_allocated_meansquarederrorlosslayer(::CoreML::Specification::MeanSquaredErrorLossLayer* meansquarederrorlosslayer) {
  clear_LossLayerType();
  if (meansquarederrorlosslayer) {
    set_has_meansquarederrorlosslayer();
    LossLayerType_.meansquarederrorlosslayer_ = meansquarederrorlosslayer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.meanSquaredErrorLossLayer)
}

bool LossLayer::has_LossLayerType() const {
  return LossLayerType_case() != LOSSLAYERTYPE_NOT_SET;
}
void LossLayer::clear_has_LossLayerType() {
  _oneof_case_[0] = LOSSLAYERTYPE_NOT_SET;
}
LossLayer::LossLayerTypeCase LossLayer::LossLayerType_case() const {
  return LossLayer::LossLayerTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CategoricalCrossEntropyLossLayer::kInputFieldNumber;
const int CategoricalCrossEntropyLossLayer::kTargetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CategoricalCrossEntropyLossLayer::CategoricalCrossEntropyLossLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CategoricalCrossEntropyLossLayer)
}
CategoricalCrossEntropyLossLayer::CategoricalCrossEntropyLossLayer(const CategoricalCrossEntropyLossLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.input().size() > 0) {
    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.target().size() > 0) {
    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CategoricalCrossEntropyLossLayer)
}

void CategoricalCrossEntropyLossLayer::SharedCtor() {
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

CategoricalCrossEntropyLossLayer::~CategoricalCrossEntropyLossLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  SharedDtor();
}

void CategoricalCrossEntropyLossLayer::SharedDtor() {
  input_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void CategoricalCrossEntropyLossLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CategoricalCrossEntropyLossLayer& CategoricalCrossEntropyLossLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CategoricalCrossEntropyLossLayer* CategoricalCrossEntropyLossLayer::New(::google::protobuf::Arena* arena) const {
  CategoricalCrossEntropyLossLayer* n = new CategoricalCrossEntropyLossLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CategoricalCrossEntropyLossLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool CategoricalCrossEntropyLossLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string input = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input().data(), this->input().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CategoricalCrossEntropyLossLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string target = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_target()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->target().data(), this->target().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CategoricalCrossEntropyLossLayer.target"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  return false;
#undef DO_
}

void CategoricalCrossEntropyLossLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string input = 1;
  if (this->input().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input().data(), this->input().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CategoricalCrossEntropyLossLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->input(), output);
  }

  // string target = 2;
  if (this->target().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->target().data(), this->target().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CategoricalCrossEntropyLossLayer.target");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->target(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CategoricalCrossEntropyLossLayer)
}

size_t CategoricalCrossEntropyLossLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  size_t total_size = 0;

  // string input = 1;
  if (this->input().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->input());
  }

  // string target = 2;
  if (this->target().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->target());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CategoricalCrossEntropyLossLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CategoricalCrossEntropyLossLayer*>(&from));
}

void CategoricalCrossEntropyLossLayer::MergeFrom(const CategoricalCrossEntropyLossLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.input().size() > 0) {

    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  if (from.target().size() > 0) {

    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
}

void CategoricalCrossEntropyLossLayer::CopyFrom(const CategoricalCrossEntropyLossLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CategoricalCrossEntropyLossLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CategoricalCrossEntropyLossLayer::IsInitialized() const {
  return true;
}

void CategoricalCrossEntropyLossLayer::Swap(CategoricalCrossEntropyLossLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CategoricalCrossEntropyLossLayer::InternalSwap(CategoricalCrossEntropyLossLayer* other) {
  input_.Swap(&other->input_);
  target_.Swap(&other->target_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CategoricalCrossEntropyLossLayer::GetTypeName() const {
  return "CoreML.Specification.CategoricalCrossEntropyLossLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CategoricalCrossEntropyLossLayer

// string input = 1;
void CategoricalCrossEntropyLossLayer::clear_input() {
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CategoricalCrossEntropyLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return input_.GetNoArena();
}
void CategoricalCrossEntropyLossLayer::set_input(const ::std::string& value) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
#if LANG_CXX11
void CategoricalCrossEntropyLossLayer::set_input(::std::string&& value) {
  
  input_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
#endif
void CategoricalCrossEntropyLossLayer::set_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
void CategoricalCrossEntropyLossLayer::set_input(const char* value, size_t size) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}
::std::string* CategoricalCrossEntropyLossLayer::mutable_input() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  return input_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CategoricalCrossEntropyLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
  
  return input_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CategoricalCrossEntropyLossLayer::set_allocated_input(::std::string* input) {
  if (input != NULL) {
    
  } else {
    
  }
  input_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CategoricalCrossEntropyLossLayer.input)
}

// string target = 2;
void CategoricalCrossEntropyLossLayer::clear_target() {
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CategoricalCrossEntropyLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return target_.GetNoArena();
}
void CategoricalCrossEntropyLossLayer::set_target(const ::std::string& value) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
#if LANG_CXX11
void CategoricalCrossEntropyLossLayer::set_target(::std::string&& value) {
  
  target_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
#endif
void CategoricalCrossEntropyLossLayer::set_target(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
void CategoricalCrossEntropyLossLayer::set_target(const char* value, size_t size) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}
::std::string* CategoricalCrossEntropyLossLayer::mutable_target() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  return target_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CategoricalCrossEntropyLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
  
  return target_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CategoricalCrossEntropyLossLayer::set_allocated_target(::std::string* target) {
  if (target != NULL) {
    
  } else {
    
  }
  target_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), target);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CategoricalCrossEntropyLossLayer.target)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MeanSquaredErrorLossLayer::kInputFieldNumber;
const int MeanSquaredErrorLossLayer::kTargetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MeanSquaredErrorLossLayer::MeanSquaredErrorLossLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MeanSquaredErrorLossLayer)
}
MeanSquaredErrorLossLayer::MeanSquaredErrorLossLayer(const MeanSquaredErrorLossLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.input().size() > 0) {
    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.target().size() > 0) {
    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MeanSquaredErrorLossLayer)
}

void MeanSquaredErrorLossLayer::SharedCtor() {
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

MeanSquaredErrorLossLayer::~MeanSquaredErrorLossLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MeanSquaredErrorLossLayer)
  SharedDtor();
}

void MeanSquaredErrorLossLayer::SharedDtor() {
  input_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void MeanSquaredErrorLossLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MeanSquaredErrorLossLayer& MeanSquaredErrorLossLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MeanSquaredErrorLossLayer* MeanSquaredErrorLossLayer::New(::google::protobuf::Arena* arena) const {
  MeanSquaredErrorLossLayer* n = new MeanSquaredErrorLossLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MeanSquaredErrorLossLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MeanSquaredErrorLossLayer)
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool MeanSquaredErrorLossLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MeanSquaredErrorLossLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string input = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input().data(), this->input().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.MeanSquaredErrorLossLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string target = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_target()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->target().data(), this->target().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.MeanSquaredErrorLossLayer.target"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MeanSquaredErrorLossLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MeanSquaredErrorLossLayer)
  return false;
#undef DO_
}

void MeanSquaredErrorLossLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MeanSquaredErrorLossLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string input = 1;
  if (this->input().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input().data(), this->input().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.MeanSquaredErrorLossLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->input(), output);
  }

  // string target = 2;
  if (this->target().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->target().data(), this->target().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.MeanSquaredErrorLossLayer.target");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->target(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MeanSquaredErrorLossLayer)
}

size_t MeanSquaredErrorLossLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MeanSquaredErrorLossLayer)
  size_t total_size = 0;

  // string input = 1;
  if (this->input().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->input());
  }

  // string target = 2;
  if (this->target().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->target());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MeanSquaredErrorLossLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MeanSquaredErrorLossLayer*>(&from));
}

void MeanSquaredErrorLossLayer::MergeFrom(const MeanSquaredErrorLossLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MeanSquaredErrorLossLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.input().size() > 0) {

    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  if (from.target().size() > 0) {

    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
}

void MeanSquaredErrorLossLayer::CopyFrom(const MeanSquaredErrorLossLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MeanSquaredErrorLossLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MeanSquaredErrorLossLayer::IsInitialized() const {
  return true;
}

void MeanSquaredErrorLossLayer::Swap(MeanSquaredErrorLossLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MeanSquaredErrorLossLayer::InternalSwap(MeanSquaredErrorLossLayer* other) {
  input_.Swap(&other->input_);
  target_.Swap(&other->target_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MeanSquaredErrorLossLayer::GetTypeName() const {
  return "CoreML.Specification.MeanSquaredErrorLossLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MeanSquaredErrorLossLayer

// string input = 1;
void MeanSquaredErrorLossLayer::clear_input() {
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& MeanSquaredErrorLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return input_.GetNoArena();
}
void MeanSquaredErrorLossLayer::set_input(const ::std::string& value) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
#if LANG_CXX11
void MeanSquaredErrorLossLayer::set_input(::std::string&& value) {
  
  input_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
#endif
void MeanSquaredErrorLossLayer::set_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
void MeanSquaredErrorLossLayer::set_input(const char* value, size_t size) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}
::std::string* MeanSquaredErrorLossLayer::mutable_input() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  return input_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* MeanSquaredErrorLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MeanSquaredErrorLossLayer.input)
  
  return input_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void MeanSquaredErrorLossLayer::set_allocated_input(::std::string* input) {
  if (input != NULL) {
    
  } else {
    
  }
  input_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MeanSquaredErrorLossLayer.input)
}

// string target = 2;
void MeanSquaredErrorLossLayer::clear_target() {
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& MeanSquaredErrorLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return target_.GetNoArena();
}
void MeanSquaredErrorLossLayer::set_target(const ::std::string& value) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
#if LANG_CXX11
void MeanSquaredErrorLossLayer::set_target(::std::string&& value) {
  
  target_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
#endif
void MeanSquaredErrorLossLayer::set_target(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
void MeanSquaredErrorLossLayer::set_target(const char* value, size_t size) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}
::std::string* MeanSquaredErrorLossLayer::mutable_target() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  return target_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* MeanSquaredErrorLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MeanSquaredErrorLossLayer.target)
  
  return target_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void MeanSquaredErrorLossLayer::set_allocated_target(::std::string* target) {
  if (target != NULL) {
    
  } else {
    
  }
  target_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), target);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MeanSquaredErrorLossLayer.target)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int Optimizer::kSgdOptimizerFieldNumber;
const int Optimizer::kAdamOptimizerFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Optimizer::Optimizer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.Optimizer)
}
Optimizer::Optimizer(const Optimizer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_OptimizerType();
  switch (from.OptimizerType_case()) {
    case kSgdOptimizer: {
      mutable_sgdoptimizer()->::CoreML::Specification::SGDOptimizer::MergeFrom(from.sgdoptimizer());
      break;
    }
    case kAdamOptimizer: {
      mutable_adamoptimizer()->::CoreML::Specification::AdamOptimizer::MergeFrom(from.adamoptimizer());
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.Optimizer)
}

void Optimizer::SharedCtor() {
  clear_has_OptimizerType();
  _cached_size_ = 0;
}

Optimizer::~Optimizer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.Optimizer)
  SharedDtor();
}

void Optimizer::SharedDtor() {
  if (has_OptimizerType()) {
    clear_OptimizerType();
  }
}

void Optimizer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const Optimizer& Optimizer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

Optimizer* Optimizer::New(::google::protobuf::Arena* arena) const {
  Optimizer* n = new Optimizer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void Optimizer::clear_OptimizerType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.Optimizer)
  switch (OptimizerType_case()) {
    case kSgdOptimizer: {
      delete OptimizerType_.sgdoptimizer_;
      break;
    }
    case kAdamOptimizer: {
      delete OptimizerType_.adamoptimizer_;
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = OPTIMIZERTYPE_NOT_SET;
}


void Optimizer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.Optimizer)
  clear_OptimizerType();
}

bool Optimizer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.Optimizer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sgdoptimizer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_adamoptimizer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.Optimizer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.Optimizer)
  return false;
#undef DO_
}

void Optimizer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.Optimizer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
  if (has_sgdoptimizer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *OptimizerType_.sgdoptimizer_, output);
  }

  // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
  if (has_adamoptimizer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *OptimizerType_.adamoptimizer_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.Optimizer)
}

size_t Optimizer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.Optimizer)
  size_t total_size = 0;

  switch (OptimizerType_case()) {
    // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
    case kSgdOptimizer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *OptimizerType_.sgdoptimizer_);
      break;
    }
    // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
    case kAdamOptimizer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *OptimizerType_.adamoptimizer_);
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Optimizer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const Optimizer*>(&from));
}

void Optimizer::MergeFrom(const Optimizer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.Optimizer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.OptimizerType_case()) {
    case kSgdOptimizer: {
      mutable_sgdoptimizer()->::CoreML::Specification::SGDOptimizer::MergeFrom(from.sgdoptimizer());
      break;
    }
    case kAdamOptimizer: {
      mutable_adamoptimizer()->::CoreML::Specification::AdamOptimizer::MergeFrom(from.adamoptimizer());
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
}

void Optimizer::CopyFrom(const Optimizer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.Optimizer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Optimizer::IsInitialized() const {
  return true;
}

void Optimizer::Swap(Optimizer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void Optimizer::InternalSwap(Optimizer* other) {
  std::swap(OptimizerType_, other->OptimizerType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string Optimizer::GetTypeName() const {
  return "CoreML.Specification.Optimizer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Optimizer

// .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
bool Optimizer::has_sgdoptimizer() const {
  return OptimizerType_case() == kSgdOptimizer;
}
void Optimizer::set_has_sgdoptimizer() {
  _oneof_case_[0] = kSgdOptimizer;
}
void Optimizer::clear_sgdoptimizer() {
  if (has_sgdoptimizer()) {
    delete OptimizerType_.sgdoptimizer_;
    clear_has_OptimizerType();
  }
}
 const ::CoreML::Specification::SGDOptimizer& Optimizer::sgdoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.sgdOptimizer)
  return has_sgdoptimizer()
      ? *OptimizerType_.sgdoptimizer_
      : ::CoreML::Specification::SGDOptimizer::default_instance();
}
::CoreML::Specification::SGDOptimizer* Optimizer::mutable_sgdoptimizer() {
  if (!has_sgdoptimizer()) {
    clear_OptimizerType();
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = new ::CoreML::Specification::SGDOptimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.sgdOptimizer)
  return OptimizerType_.sgdoptimizer_;
}
::CoreML::Specification::SGDOptimizer* Optimizer::release_sgdoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.sgdOptimizer)
  if (has_sgdoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::SGDOptimizer* temp = OptimizerType_.sgdoptimizer_;
    OptimizerType_.sgdoptimizer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void Optimizer::set_allocated_sgdoptimizer(::CoreML::Specification::SGDOptimizer* sgdoptimizer) {
  clear_OptimizerType();
  if (sgdoptimizer) {
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = sgdoptimizer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Optimizer.sgdOptimizer)
}

// .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
bool Optimizer::has_adamoptimizer() const {
  return OptimizerType_case() == kAdamOptimizer;
}
void Optimizer::set_has_adamoptimizer() {
  _oneof_case_[0] = kAdamOptimizer;
}
void Optimizer::clear_adamoptimizer() {
  if (has_adamoptimizer()) {
    delete OptimizerType_.adamoptimizer_;
    clear_has_OptimizerType();
  }
}
 const ::CoreML::Specification::AdamOptimizer& Optimizer::adamoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.adamOptimizer)
  return has_adamoptimizer()
      ? *OptimizerType_.adamoptimizer_
      : ::CoreML::Specification::AdamOptimizer::default_instance();
}
::CoreML::Specification::AdamOptimizer* Optimizer::mutable_adamoptimizer() {
  if (!has_adamoptimizer()) {
    clear_OptimizerType();
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = new ::CoreML::Specification::AdamOptimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.adamOptimizer)
  return OptimizerType_.adamoptimizer_;
}
::CoreML::Specification::AdamOptimizer* Optimizer::release_adamoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.adamOptimizer)
  if (has_adamoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::AdamOptimizer* temp = OptimizerType_.adamoptimizer_;
    OptimizerType_.adamoptimizer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void Optimizer::set_allocated_adamoptimizer(::CoreML::Specification::AdamOptimizer* adamoptimizer) {
  clear_OptimizerType();
  if (adamoptimizer) {
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = adamoptimizer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Optimizer.adamOptimizer)
}

bool Optimizer::has_OptimizerType() const {
  return OptimizerType_case() != OPTIMIZERTYPE_NOT_SET;
}
void Optimizer::clear_has_OptimizerType() {
  _oneof_case_[0] = OPTIMIZERTYPE_NOT_SET;
}
Optimizer::OptimizerTypeCase Optimizer::OptimizerType_case() const {
  return Optimizer::OptimizerTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SGDOptimizer::kLearningRateFieldNumber;
const int SGDOptimizer::kMiniBatchSizeFieldNumber;
const int SGDOptimizer::kMomentumFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SGDOptimizer::SGDOptimizer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SGDOptimizer)
}
SGDOptimizer::SGDOptimizer(const SGDOptimizer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_learningrate()) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter(*from.learningrate_);
  } else {
    learningrate_ = NULL;
  }
  if (from.has_minibatchsize()) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter(*from.minibatchsize_);
  } else {
    minibatchsize_ = NULL;
  }
  if (from.has_momentum()) {
    momentum_ = new ::CoreML::Specification::DoubleParameter(*from.momentum_);
  } else {
    momentum_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SGDOptimizer)
}

void SGDOptimizer::SharedCtor() {
  ::memset(&learningrate_, 0, reinterpret_cast<char*>(&momentum_) -
    reinterpret_cast<char*>(&learningrate_) + sizeof(momentum_));
  _cached_size_ = 0;
}

SGDOptimizer::~SGDOptimizer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SGDOptimizer)
  SharedDtor();
}

void SGDOptimizer::SharedDtor() {
  if (this != internal_default_instance()) {
    delete learningrate_;
  }
  if (this != internal_default_instance()) {
    delete minibatchsize_;
  }
  if (this != internal_default_instance()) {
    delete momentum_;
  }
}

void SGDOptimizer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SGDOptimizer& SGDOptimizer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SGDOptimizer* SGDOptimizer::New(::google::protobuf::Arena* arena) const {
  SGDOptimizer* n = new SGDOptimizer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SGDOptimizer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SGDOptimizer)
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) {
    delete learningrate_;
  }
  learningrate_ = NULL;
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) {
    delete minibatchsize_;
  }
  minibatchsize_ = NULL;
  if (GetArenaNoVirtual() == NULL && momentum_ != NULL) {
    delete momentum_;
  }
  momentum_ = NULL;
}

bool SGDOptimizer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SGDOptimizer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.DoubleParameter learningRate = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_learningrate()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_minibatchsize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter momentum = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_momentum()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SGDOptimizer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SGDOptimizer)
  return false;
#undef DO_
}

void SGDOptimizer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SGDOptimizer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->learningrate_, output);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->minibatchsize_, output);
  }

  // .CoreML.Specification.DoubleParameter momentum = 3;
  if (this->has_momentum()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->momentum_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SGDOptimizer)
}

size_t SGDOptimizer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SGDOptimizer)
  size_t total_size = 0;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->learningrate_);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->minibatchsize_);
  }

  // .CoreML.Specification.DoubleParameter momentum = 3;
  if (this->has_momentum()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->momentum_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SGDOptimizer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SGDOptimizer*>(&from));
}

void SGDOptimizer::MergeFrom(const SGDOptimizer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SGDOptimizer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_learningrate()) {
    mutable_learningrate()->::CoreML::Specification::DoubleParameter::MergeFrom(from.learningrate());
  }
  if (from.has_minibatchsize()) {
    mutable_minibatchsize()->::CoreML::Specification::Int64Parameter::MergeFrom(from.minibatchsize());
  }
  if (from.has_momentum()) {
    mutable_momentum()->::CoreML::Specification::DoubleParameter::MergeFrom(from.momentum());
  }
}

void SGDOptimizer::CopyFrom(const SGDOptimizer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SGDOptimizer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SGDOptimizer::IsInitialized() const {
  return true;
}

void SGDOptimizer::Swap(SGDOptimizer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SGDOptimizer::InternalSwap(SGDOptimizer* other) {
  std::swap(learningrate_, other->learningrate_);
  std::swap(minibatchsize_, other->minibatchsize_);
  std::swap(momentum_, other->momentum_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SGDOptimizer::GetTypeName() const {
  return "CoreML.Specification.SGDOptimizer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SGDOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
bool SGDOptimizer::has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != NULL;
}
void SGDOptimizer::clear_learningrate() {
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) delete learningrate_;
  learningrate_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& SGDOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.learningRate)
  return learningrate_ != NULL ? *learningrate_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_learningrate() {
  
  if (learningrate_ == NULL) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.learningRate)
  return learningrate_;
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = NULL;
  return temp;
}
void SGDOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  delete learningrate_;
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
bool SGDOptimizer::has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != NULL;
}
void SGDOptimizer::clear_minibatchsize() {
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) delete minibatchsize_;
  minibatchsize_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& SGDOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return minibatchsize_ != NULL ? *minibatchsize_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* SGDOptimizer::mutable_minibatchsize() {
  
  if (minibatchsize_ == NULL) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return minibatchsize_;
}
::CoreML::Specification::Int64Parameter* SGDOptimizer::release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = NULL;
  return temp;
}
void SGDOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  delete minibatchsize_;
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter momentum = 3;
bool SGDOptimizer::has_momentum() const {
  return this != internal_default_instance() && momentum_ != NULL;
}
void SGDOptimizer::clear_momentum() {
  if (GetArenaNoVirtual() == NULL && momentum_ != NULL) delete momentum_;
  momentum_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& SGDOptimizer::momentum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.momentum)
  return momentum_ != NULL ? *momentum_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_momentum() {
  
  if (momentum_ == NULL) {
    momentum_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.momentum)
  return momentum_;
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::release_momentum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.momentum)
  
  ::CoreML::Specification::DoubleParameter* temp = momentum_;
  momentum_ = NULL;
  return temp;
}
void SGDOptimizer::set_allocated_momentum(::CoreML::Specification::DoubleParameter* momentum) {
  delete momentum_;
  momentum_ = momentum;
  if (momentum) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.momentum)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int AdamOptimizer::kLearningRateFieldNumber;
const int AdamOptimizer::kMiniBatchSizeFieldNumber;
const int AdamOptimizer::kBeta1FieldNumber;
const int AdamOptimizer::kBeta2FieldNumber;
const int AdamOptimizer::kEpsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AdamOptimizer::AdamOptimizer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AdamOptimizer)
}
AdamOptimizer::AdamOptimizer(const AdamOptimizer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_learningrate()) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter(*from.learningrate_);
  } else {
    learningrate_ = NULL;
  }
  if (from.has_minibatchsize()) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter(*from.minibatchsize_);
  } else {
    minibatchsize_ = NULL;
  }
  if (from.has_beta1()) {
    beta1_ = new ::CoreML::Specification::DoubleParameter(*from.beta1_);
  } else {
    beta1_ = NULL;
  }
  if (from.has_beta2()) {
    beta2_ = new ::CoreML::Specification::DoubleParameter(*from.beta2_);
  } else {
    beta2_ = NULL;
  }
  if (from.has_eps()) {
    eps_ = new ::CoreML::Specification::DoubleParameter(*from.eps_);
  } else {
    eps_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AdamOptimizer)
}

void AdamOptimizer::SharedCtor() {
  ::memset(&learningrate_, 0, reinterpret_cast<char*>(&eps_) -
    reinterpret_cast<char*>(&learningrate_) + sizeof(eps_));
  _cached_size_ = 0;
}

AdamOptimizer::~AdamOptimizer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AdamOptimizer)
  SharedDtor();
}

void AdamOptimizer::SharedDtor() {
  if (this != internal_default_instance()) {
    delete learningrate_;
  }
  if (this != internal_default_instance()) {
    delete minibatchsize_;
  }
  if (this != internal_default_instance()) {
    delete beta1_;
  }
  if (this != internal_default_instance()) {
    delete beta2_;
  }
  if (this != internal_default_instance()) {
    delete eps_;
  }
}

void AdamOptimizer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AdamOptimizer& AdamOptimizer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AdamOptimizer* AdamOptimizer::New(::google::protobuf::Arena* arena) const {
  AdamOptimizer* n = new AdamOptimizer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AdamOptimizer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AdamOptimizer)
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) {
    delete learningrate_;
  }
  learningrate_ = NULL;
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) {
    delete minibatchsize_;
  }
  minibatchsize_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta1_ != NULL) {
    delete beta1_;
  }
  beta1_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta2_ != NULL) {
    delete beta2_;
  }
  beta2_ = NULL;
  if (GetArenaNoVirtual() == NULL && eps_ != NULL) {
    delete eps_;
  }
  eps_ = NULL;
}

bool AdamOptimizer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AdamOptimizer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.DoubleParameter learningRate = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_learningrate()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_minibatchsize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter beta1 = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta1()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter beta2 = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta2()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter eps = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_eps()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AdamOptimizer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AdamOptimizer)
  return false;
#undef DO_
}

void AdamOptimizer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AdamOptimizer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->learningrate_, output);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->minibatchsize_, output);
  }

  // .CoreML.Specification.DoubleParameter beta1 = 3;
  if (this->has_beta1()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->beta1_, output);
  }

  // .CoreML.Specification.DoubleParameter beta2 = 4;
  if (this->has_beta2()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->beta2_, output);
  }

  // .CoreML.Specification.DoubleParameter eps = 5;
  if (this->has_eps()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *this->eps_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AdamOptimizer)
}

size_t AdamOptimizer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AdamOptimizer)
  size_t total_size = 0;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->learningrate_);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->minibatchsize_);
  }

  // .CoreML.Specification.DoubleParameter beta1 = 3;
  if (this->has_beta1()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta1_);
  }

  // .CoreML.Specification.DoubleParameter beta2 = 4;
  if (this->has_beta2()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta2_);
  }

  // .CoreML.Specification.DoubleParameter eps = 5;
  if (this->has_eps()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->eps_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AdamOptimizer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AdamOptimizer*>(&from));
}

void AdamOptimizer::MergeFrom(const AdamOptimizer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AdamOptimizer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_learningrate()) {
    mutable_learningrate()->::CoreML::Specification::DoubleParameter::MergeFrom(from.learningrate());
  }
  if (from.has_minibatchsize()) {
    mutable_minibatchsize()->::CoreML::Specification::Int64Parameter::MergeFrom(from.minibatchsize());
  }
  if (from.has_beta1()) {
    mutable_beta1()->::CoreML::Specification::DoubleParameter::MergeFrom(from.beta1());
  }
  if (from.has_beta2()) {
    mutable_beta2()->::CoreML::Specification::DoubleParameter::MergeFrom(from.beta2());
  }
  if (from.has_eps()) {
    mutable_eps()->::CoreML::Specification::DoubleParameter::MergeFrom(from.eps());
  }
}

void AdamOptimizer::CopyFrom(const AdamOptimizer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AdamOptimizer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AdamOptimizer::IsInitialized() const {
  return true;
}

void AdamOptimizer::Swap(AdamOptimizer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AdamOptimizer::InternalSwap(AdamOptimizer* other) {
  std::swap(learningrate_, other->learningrate_);
  std::swap(minibatchsize_, other->minibatchsize_);
  std::swap(beta1_, other->beta1_);
  std::swap(beta2_, other->beta2_);
  std::swap(eps_, other->eps_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AdamOptimizer::GetTypeName() const {
  return "CoreML.Specification.AdamOptimizer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AdamOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
bool AdamOptimizer::has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != NULL;
}
void AdamOptimizer::clear_learningrate() {
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) delete learningrate_;
  learningrate_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.learningRate)
  return learningrate_ != NULL ? *learningrate_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_learningrate() {
  
  if (learningrate_ == NULL) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.learningRate)
  return learningrate_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  delete learningrate_;
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
bool AdamOptimizer::has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != NULL;
}
void AdamOptimizer::clear_minibatchsize() {
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) delete minibatchsize_;
  minibatchsize_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& AdamOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return minibatchsize_ != NULL ? *minibatchsize_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* AdamOptimizer::mutable_minibatchsize() {
  
  if (minibatchsize_ == NULL) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return minibatchsize_;
}
::CoreML::Specification::Int64Parameter* AdamOptimizer::release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  delete minibatchsize_;
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter beta1 = 3;
bool AdamOptimizer::has_beta1() const {
  return this != internal_default_instance() && beta1_ != NULL;
}
void AdamOptimizer::clear_beta1() {
  if (GetArenaNoVirtual() == NULL && beta1_ != NULL) delete beta1_;
  beta1_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta1)
  return beta1_ != NULL ? *beta1_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta1() {
  
  if (beta1_ == NULL) {
    beta1_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta1)
  return beta1_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta1)
  
  ::CoreML::Specification::DoubleParameter* temp = beta1_;
  beta1_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_beta1(::CoreML::Specification::DoubleParameter* beta1) {
  delete beta1_;
  beta1_ = beta1;
  if (beta1) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta1)
}

// .CoreML.Specification.DoubleParameter beta2 = 4;
bool AdamOptimizer::has_beta2() const {
  return this != internal_default_instance() && beta2_ != NULL;
}
void AdamOptimizer::clear_beta2() {
  if (GetArenaNoVirtual() == NULL && beta2_ != NULL) delete beta2_;
  beta2_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta2)
  return beta2_ != NULL ? *beta2_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta2() {
  
  if (beta2_ == NULL) {
    beta2_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta2)
  return beta2_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta2)
  
  ::CoreML::Specification::DoubleParameter* temp = beta2_;
  beta2_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_beta2(::CoreML::Specification::DoubleParameter* beta2) {
  delete beta2_;
  beta2_ = beta2;
  if (beta2) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta2)
}

// .CoreML.Specification.DoubleParameter eps = 5;
bool AdamOptimizer::has_eps() const {
  return this != internal_default_instance() && eps_ != NULL;
}
void AdamOptimizer::clear_eps() {
  if (GetArenaNoVirtual() == NULL && eps_ != NULL) delete eps_;
  eps_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.eps)
  return eps_ != NULL ? *eps_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_eps() {
  
  if (eps_ == NULL) {
    eps_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.eps)
  return eps_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_eps() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.eps)
  
  ::CoreML::Specification::DoubleParameter* temp = eps_;
  eps_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_eps(::CoreML::Specification::DoubleParameter* eps) {
  delete eps_;
  eps_ = eps;
  if (eps) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.eps)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace Specification
}  // namespace CoreML

// @@protoc_insertion_point(global_scope)
